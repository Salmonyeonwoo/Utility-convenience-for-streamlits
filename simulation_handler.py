# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬ ëª¨ë“ˆ
ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´ì…˜, ì±„íŒ…/ì „í™” ëŒ€í™” ìƒì„±, íŒíŠ¸ ìƒì„± ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import json
import uuid
import tempfile
import random
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
import streamlit as st

from config import SIM_META_FILE, RAG_INDEX_DIR, DATA_DIR
from utils import _load_json, _save_json
from llm_client import get_api_key, run_llm
from lang_pack import LANG

# Langchain imports for RAG functionality
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

# ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
    IS_GEMINI_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_GEMINI_EMBEDDING_AVAILABLE = False

try:
    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings
    IS_NVIDIA_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_NVIDIA_EMBEDDING_AVAILABLE = False

# Word, PPTX, PDF ë‚´ë³´ë‚´ê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    IS_DOCX_AVAILABLE = True
except ImportError:
    IS_DOCX_AVAILABLE = False

try:
    from pptx import Presentation
    IS_PPTX_AVAILABLE = True
except ImportError:
    IS_PPTX_AVAILABLE = False

try:
    from reportlab.lib.pagesizes import letter
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.colors import black
    from reportlab.lib.units import inch
    IS_REPORTLAB_AVAILABLE = True
except ImportError:
    IS_REPORTLAB_AVAILABLE = False

def translate_text_with_llm(text_content: str, target_lang_code: str, source_lang_code: str) -> Tuple[str, bool]:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. (ì•ˆì •í™”ëœ í…ìŠ¤íŠ¸ ì¶œë ¥)
    **ìˆ˜ì • ì‚¬í•­:** LLM Fallback ìˆœì„œë¥¼ OpenAI ìš°ì„ ìœ¼ë¡œ ì¡°ì •í•˜ê³ , ì‘ë‹µì´ ë¹„ì–´ìˆì„ ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    
    Returns:
        tuple: (translated_text, is_success) - ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì™€ ì„±ê³µ ì—¬ë¶€
    """
    target_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(target_lang_code, "English")
    source_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(source_lang_code, "English")

    # ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì œ
    system_prompt = (
        f"You are a professional translation AI. Translate the entire following customer support chat history "
        f"from '{source_lang_name}' to '{target_lang_name}'. "
        f"You MUST translate the content to {target_lang_name} ONLY. "
        f"Do not include any mixed languages, the source text, or any introductory/concluding remarks. "
        f"Output ONLY the translated chat history text. "
    )
    prompt = f"Original Chat History:\n\n{text_content}"

    # LLM Fallback ìˆœì„œ: OpenAI -> Gemini -> Claude (OpenAIë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì¡°ì •)
    llm_attempts = [
        ("openai", get_api_key("openai"), "gpt-4o"),  # 1ìˆœìœ„: OpenAI (ê°€ì¥ ì•ˆì •ì )
        ("gemini", get_api_key("gemini"), "gemini-2.5-flash"),  # 2ìˆœìœ„
        ("claude", get_api_key("claude"), "claude-3-5-sonnet-latest"),  # 3ìˆœìœ„
    ]

    last_error = ""

    for provider, key, model_name in llm_attempts:
        if not key: continue

        try:
            translated_text = ""

            if provider == "openai":
                o_client = OpenAI(api_key=key)
                resp = o_client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}],
                    temperature=0.1
                )
                translated_text = resp.choices[0].message.content.strip()

            elif provider == "gemini":
                genai.configure(api_key=key)
                g_model = genai.GenerativeModel(model_name)
                resp = g_model.generate_content(
                    contents=prompt,
                    config=genai.types.GenerateContentConfig(system_instruction=system_prompt, temperature=0.1)
                )
                translated_text = resp.text.strip()

            elif provider == "claude":
                from anthropic import Anthropic
                c_client = Anthropic(api_key=key)
                resp = c_client.messages.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    system=system_prompt
                )
                translated_text = resp.content[0].text.strip()

            # ë²ˆì—­ ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸
            if translated_text and len(translated_text.strip()) > 0:
                return translated_text, True  # ë²ˆì—­ ì„±ê³µ
            else:
                last_error = f"Translation failed: {provider} returned empty response."
                continue  # ë‹¤ìŒ LLM ì‹œë„

        except Exception as e:
            last_error = f"Translation API call failed with {provider} ({model_name}): {e}"  # ëª¨ë¸ëª… ì¶”ê°€
            print(last_error)
            continue  # ë‹¤ìŒ LLM ì‹œë„

    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ, ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ê°€ ê³„ì† ì§„í–‰ë˜ë„ë¡ í•¨
    # (ì˜¤ë¥˜ ë©”ì‹œì§€ ëŒ€ì‹  ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì—¬ ë²ˆì—­ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥)
    print(f"Translation failed: {last_error or 'No active API key found.'}. Returning original text.")
    return text_content, False  # ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜, ë²ˆì—­ ì‹¤íŒ¨ í‘œì‹œ


# ----------------------------------------
# Realtime Hint Generation (ìš”ì²­ 2 ë°˜ì˜)
# ----------------------------------------

def generate_realtime_hint(current_lang_key: str, is_call: bool = False):
    """í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ì—ê²Œ ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸(í‚¤ì›Œë“œ/ì •ì±…/ì•¡ì…˜)ë¥¼ ì œê³µ"""
    # ì–¸ì–´ í‚¤ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not current_lang_key or current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])
    # ì±„íŒ…/ì „í™” êµ¬ë¶„í•˜ì—¬ ì´ë ¥ ì‚¬ìš©
    if is_call:
        # ì „í™” ì‹œë®¬ë ˆì´í„°ì—ì„œëŠ” í˜„ì¬ CC ì˜ì—­ì— í‘œì‹œëœ í…ìŠ¤íŠ¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ í•¨ê»˜ ì‚¬ìš©
        website_url = st.session_state.get("call_website_url", "").strip()
        website_context = f"\nWebsite URL: {website_url}" if website_url else ""
        history_text = (
            f"Initial Query: {st.session_state.call_initial_query}\n"
            f"Previous Customer Utterance: {st.session_state.current_customer_audio_text}\n"
            f"Previous Agent Utterance: {st.session_state.current_agent_audio_text}{website_context}"
        )
    else:
        history_text = get_chat_history_for_prompt(include_attachment=True)

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    hint_prompt = f"""
You are an AI Supervisor providing an **urgent, internal hint** to a human agent whose AHT is being monitored.
Analyze the conversation history, especially the customer's last message, which might be about complex issues like JR Pass, Universal Studio Japan (USJ), or a complex refund policy.

Provide ONE concise, actionable hint for the agent. The purpose is to save AHT time.

Output MUST be a single paragraph/sentence in {lang_name} containing actionable advice.
DO NOT use markdown headers or titles.
Do NOT direct the agent to check the general website.
Provide an actionable fact or the next specific step (e.g., check policy section, confirm coverage).

Examples of good hints (based on the content):
- Check the official JR Pass site for current exchange rates.
- The 'Universal Express Pass' is non-refundable; clearly cite policy section 3.2.
- Ask for the order confirmation number before proceeding with any action.
- The solution lies in the section of the Klook site titled '~'.

Conversation History:
{history_text}

HINT:
"""
    if not st.session_state.is_llm_ready:
        return "(Mock Hint: LLM Key is missing. Ask the customer for the booking number.)"

    with st.spinner(f"ğŸ’¡ {L['button_request_hint']}..."):
        try:
            return run_llm(hint_prompt).strip()
        except Exception as e:
            return f"âŒ Hint Generation Error. (Try again or check API Key: {e})"



def generate_agent_response_draft(current_lang_key: str) -> str:
    """ê³ ê° ì‘ë‹µì„ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ì—ì´ì „íŠ¸ ì‘ë‹µ ì´ˆì•ˆì„ ìƒì„± (ìš”ì²­ 1 ë°˜ì˜)"""
    # ì–¸ì–´ í‚¤ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not current_lang_key or current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])
    history_text = get_chat_history_for_prompt(include_attachment=True)
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ê³ ê°ì˜ ìµœì‹  ë¬¸ì˜ ë‚´ìš© ì¶”ì¶œ ë° ë¶„ì„ (ê°•í™”)
    latest_customer_message = ""
    initial_customer_query = st.session_state.get('customer_query_text_area', '')
    customer_query_analysis = ""
    
    # ëª¨ë“  ê³ ê° ë©”ì‹œì§€ ìˆ˜ì§‘
    all_customer_messages = []
    if st.session_state.simulator_messages:
        all_customer_messages = [msg["content"] for msg in st.session_state.simulator_messages 
                                if msg.get("role") in ["customer", "customer_rebuttal", "initial_query"]]
    
    # ì´ˆê¸° ë¬¸ì˜ë„ í¬í•¨
    if initial_customer_query and initial_customer_query not in all_customer_messages:
        all_customer_messages.insert(0, initial_customer_query)
    
    if all_customer_messages:
        latest_customer_message = all_customer_messages[-1]
        
        # ë¬¸ì˜ ë‚´ìš©ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        inquiry_keywords = []
        inquiry_text = " ".join(all_customer_messages).lower()
        
        # ì¼ë°˜ì ì¸ ë¬¸ì˜ í‚¤ì›Œë“œ íŒ¨í„´
        important_patterns = [
            r'\b\d{4,}\b',  # ì£¼ë¬¸ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ ë“± ìˆ«ì
            r'\b(ì£¼ë¬¸|order|æ³¨æ–‡)\b',
            r'\b(í™˜ë¶ˆ|refund|è¿”é‡‘)\b',
            r'\b(ì·¨ì†Œ|cancel|ã‚­ãƒ£ãƒ³ã‚»ãƒ«)\b',
            r'\b(ë°°ì†¡|delivery|é…é€)\b',
            r'\b(ë³€ê²½|change|å¤‰æ›´)\b',
            r'\b(ë¬¸ì œ|problem|issue|å•é¡Œ)\b',
            r'\b(ë„ì›€|help|åŠ©ã‘)\b',
        ]
        
        # í•µì‹¬ ë¬¸ì˜ ë‚´ìš© ìš”ì•½
        inquiry_summary = f"""
**CUSTOMER INQUIRY DETAILS:**

Initial Query: "{initial_customer_query if initial_customer_query else 'Not provided'}"

Latest Customer Message: "{latest_customer_message}"

All Customer Messages Context:
{chr(10).join([f"- {msg[:150]}..." if len(msg) > 150 else f"- {msg}" for msg in all_customer_messages[-3:]])}

**YOUR RESPONSE MUST DIRECTLY ADDRESS:**

1. **SPECIFIC ISSUE IDENTIFICATION**: 
   - What EXACT problem or question did the customer mention?
   - Extract and reference specific details: order numbers, dates, product names, locations, error messages, etc.
   - If multiple issues were mentioned, address EACH one specifically

2. **CONCRETE SOLUTION PROVIDED**:
   - Provide STEP-BY-STEP instructions tailored to their EXACT situation
   - Include specific actions they need to take (e.g., "Go to Settings > Account > Order History and click on order #12345")
   - Reference the exact products/services they mentioned
   - If they mentioned a location, reference it in your solution

3. **PERSONALIZATION**:
   - Use the customer's specific words/phrases when appropriate
   - Reference their exact situation (e.g., "Since you mentioned your eSIM isn't activating in Paris...")
   - Acknowledge their specific concern or frustration point

4. **COMPLETENESS**:
   - Answer ALL questions they asked
   - Address ALL problems they mentioned
   - If they asked "why", explain the specific reason for their situation
   - If they asked "how", provide detailed steps for their exact case

**CRITICAL REQUIREMENTS:**
- DO NOT use generic templates like "Thank you for contacting us" without addressing their specific issue
- DO NOT give vague answers like "Please check your settings" - be SPECIFIC about which settings and what to do
- DO NOT ignore specific details they mentioned (order numbers, dates, locations, etc.)
- Your response must read as if it was written SPECIFICALLY for this customer's exact inquiry
- If the customer mentioned "eSIM activation in Paris", your response MUST specifically address eSIM activation and Paris
- If the customer mentioned an order number, your response MUST reference that order number

**EXAMPLE OF GOOD RESPONSE:**
Bad: "Thank you for contacting us. We understand your concern and will help you resolve this issue."
Good: "I understand you're having trouble activating your eSIM in Paris. Let me help you resolve this step by step. First, please check if your phone's APN settings are configured correctly for the Paris network..."

**NOW GENERATE YOUR RESPONSE** following these requirements:
"""
        
        customer_query_analysis = inquiry_summary

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"\n[ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´: {attachment_context}]\n"
    else:
        attachment_context = ""

    # ê³ ê° ê²€ì¦ ìƒíƒœ í™•ì¸ (ë¡œê·¸ì¸/ê³„ì • ê´€ë ¨ ë¬¸ì˜ì¸ ê²½ìš°)
    is_login_inquiry = check_if_login_related_inquiry(initial_customer_query)
    is_customer_verified = st.session_state.get("is_customer_verified", False)
    verification_warning = ""
    
    if is_login_inquiry and not is_customer_verified:
        # ê²€ì¦ë˜ì§€ ì•Šì€ ê³ ê°ì—ê²ŒëŠ” ì •ë³´ íŒíŠ¸ ì œê³µ ê¸ˆì§€
        customer_email = st.session_state.get("customer_email", "")
        customer_phone = st.session_state.get("customer_phone", "")
        customer_name = st.session_state.get("customer_name", "")
        
        # ì´ë©”ì¼ì€ ë§ˆìŠ¤í‚¹í•˜ì—¬ íŒíŠ¸ ì œê³µ ê°€ëŠ¥ (ì•/ë’¤ 1-3ìë¦¬ë§Œ)
        masked_email = mask_email(customer_email, show_chars=2) if customer_email else ""
        
        verification_warning = f"""
**âš ï¸ CRITICAL SECURITY REQUIREMENT - CUSTOMER VERIFICATION NOT COMPLETED:**

This is a LOGIN/ACCOUNT related inquiry, but the customer has NOT been verified yet.

**STRICT RULES YOU MUST FOLLOW:**
1. **DO NOT provide ANY customer information hints** (email, phone, name, receipt number, card number) in your response
2. **EXCEPTION**: You MAY provide a masked email hint ONLY if absolutely necessary: "{masked_email}" (only first/last 2-3 characters visible, rest masked)
3. **DO NOT reveal**: Full email addresses, phone numbers, customer names, receipt numbers, card numbers, or any other personal information
4. **You MUST request verification information** from the customer before proceeding with account-related assistance
5. **Your response should ask the customer to provide verification details** (receipt number, card last 4 digits, name, email, phone) WITHOUT revealing what information you already have

**VERIFICATION REQUEST TEMPLATE:**
- "To ensure account security, please provide the following information for verification: [receipt number / card last 4 digits / name / email / phone]"
- DO NOT mention what information you already have
- DO NOT provide hints about the information

**ONLY AFTER VERIFICATION IS COMPLETED** can you provide full information hints and proceed with account assistance.

**CURRENT STATUS**: Customer verification: NOT COMPLETED âŒ
"""
    elif is_login_inquiry and is_customer_verified:
        verification_warning = """
**âœ… CUSTOMER VERIFICATION COMPLETED:**

The customer has been successfully verified. You may now provide information hints and proceed with account-related assistance.
"""

    # ê³ ê° ìœ í˜• ë° ë°˜ë³µ ë¶ˆë§Œ íŒ¨í„´ ë¶„ì„
    customer_type = st.session_state.get('customer_type_sim_select', 'ì¼ë°˜ì ì¸ ë¬¸ì˜')
    is_difficult_customer = customer_type in ["ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°", "Difficult Customer",
                                              "Highly Dissatisfied Customer", "é›£ã—ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"]

    # ê³ ê° ë©”ì‹œì§€ ìˆ˜ ë° ê°ì • ë¶„ì„
    customer_message_count = sum(
        1 for msg in st.session_state.simulator_messages if msg.get("role") in ["customer", "customer_rebuttal"])
    agent_message_count = sum(1 for msg in st.session_state.simulator_messages if msg.get("role") == "agent_response")

    # ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µë“¤ ì¶”ì¶œ (ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´)
    previous_agent_responses = [msg["content"] for msg in st.session_state.simulator_messages 
                                if msg.get("role") == "agent_response"]
    previous_responses_context = ""
    if previous_agent_responses:
        previous_responses_context = f"\n[ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µë“¤ (ì°¸ê³ ìš©, ë™ì¼í•˜ê²Œ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ):\n"
        for i, prev_resp in enumerate(previous_agent_responses[-3:], 1):  # ìµœê·¼ 3ê°œë§Œ
            prev_resp_preview = prev_resp[:200] + "..." if len(prev_resp) > 200 else prev_resp
            previous_responses_context += f"{i}. {prev_resp_preview}\n"
        previous_responses_context += "]\n"

    # ê³ ê°ì´ ê³„ì† ë”°ì§€ê±°ë‚˜ í™”ë‚´ëŠ” íŒ¨í„´ ê°ì§€ (ê³ ê° ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ë³´ë‹¤ ë§ê±°ë‚˜, ë°˜ë³µì ì¸ ë¶ˆë§Œ í‘œí˜„)
    is_repeating_complaints = False
    if customer_message_count > agent_message_count and customer_message_count >= 2:
        # ë§ˆì§€ë§‰ 2ê°œ ê³ ê° ë©”ì‹œì§€ ë¶„ì„
        recent_customer_messages = [msg["content"].lower() for msg in st.session_state.simulator_messages if
                                    msg.get("role") in ["customer", "customer_rebuttal"]][-2:]
        complaint_keywords = ["ì™œ", "ì´ìœ ", "ì„¤ëª…", "ë§ì´ ì•ˆ", "ì´í•´ê°€ ì•ˆ", "í™”ë‚˜", "ì§œì¦", "ë¶ˆë§Œ", "ì™œ", "why", "reason", "explain",
                              "angry", "frustrated", "complaint", "ãªãœ", "ç†ç”±", "èª¬æ˜", "æ€’ã‚Š", "ä¸æº€"]
        if any(any(keyword in msg for keyword in complaint_keywords) for msg in recent_customer_messages):
            is_repeating_complaints = True

    # ëŒ€ì²˜ë²• í¬ë©”ì´ì…˜ ì¶”ê°€ ì—¬ë¶€ ê²°ì •
    needs_coping_strategy = is_difficult_customer or (is_repeating_complaints and customer_message_count >= 2)

    # ëŒ€ì²˜ë²• ê°€ì´ë“œë¼ì¸ ìƒì„±
    coping_guidance = ""
    if needs_coping_strategy:
        coping_guidance = f"""

[CRITICAL: Handling Difficult Customer Situation]
The customer type is "{customer_type}" and the customer has sent {customer_message_count} messages while the agent has sent {agent_message_count} messages.
The customer may be showing signs of continued frustration or dissatisfaction.

**INCLUDE THE FOLLOWING COPING STRATEGY FORMAT IN YOUR RESPONSE:**

1. **Immediate Acknowledgment** (1-2 sentences):
   - Acknowledge their frustration/specific concern explicitly
   - Show deep empathy and understanding
   - Example formats:
     * "{'ì£„ì†¡í•©ë‹ˆë‹¤. ë¶ˆí¸ì„ ë“œë ¤ ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I sincerely apologize for the inconvenience. I fully understand your situation and frustration.' if current_lang_key == 'en' else 'å¤§å¤‰ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãŠå®¢æ§˜ã®çŠ¶æ³ã¨ã”ä¸ä¾¿ã‚’ååˆ†ã«ç†è§£ã—ã¦ãŠã‚Šã¾ã™ã€‚')}"
     * "{'ê³ ê°ë‹˜ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ì˜ ë“£ê³  ìˆìŠµë‹ˆë‹¤. ì •ë§ ë‹µë‹µí•˜ì…¨ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I hear your concerns clearly. This must have been very frustrating for you.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã”æ„è¦‹ã‚’ã—ã£ã‹ã‚Šã¨å—ã‘æ­¢ã‚ã¦ã„ã¾ã™ã€‚æœ¬å½“ã«ãŠå›°ã‚Šã ã£ãŸã¨æ€ã„ã¾ã™ã€‚')}"

2. **Specific Solution Recap** (2-3 sentences):
   - Clearly restate the solution/step provided previously (if any)
   - Offer a NEW concrete action or alternative solution
   - Be specific and actionable
   - Example formats:
     * "{'ì•ì„œ ì•ˆë‚´ë“œë¦° [êµ¬ì²´ì  í•´ê²°ì±…] ì™¸ì—ë„, [ìƒˆë¡œìš´ ëŒ€ì•ˆ/ì¶”ê°€ ì¡°ì¹˜]ë¥¼ ì§„í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('In addition to the [specific solution] I mentioned earlier, I can also [new alternative/additional action] for you.' if current_lang_key == 'en' else 'å…ˆã»ã©ã”æ¡ˆå†…ã—ãŸ[å…·ä½“çš„è§£æ±ºç­–]ã«åŠ ãˆã¦ã€[æ–°ã—ã„ä»£æ›¿æ¡ˆ/è¿½åŠ æªç½®]ã‚‚é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚')}"
     * "{'í˜¹ì‹œ [êµ¬ì²´ì  ë¬¸ì œì ] ë•Œë¬¸ì— ë¶ˆí¸í•˜ì…¨ë‹¤ë©´, [êµ¬ì²´ì  í•´ê²° ë°©ë²•]ì„ ë°”ë¡œ ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If you are experiencing [specific issue], I can immediately proceed with [specific solution].' if current_lang_key == 'en' else 'ã‚‚ã—[å…·ä½“çš„å•é¡Œ]ã§ã”ä¸ä¾¿ã§ã—ãŸã‚‰ã€[å…·ä½“çš„è§£æ±ºæ–¹æ³•]ã‚’ã™ãã«é€²ã‚ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"

3. **Escalation or Follow-up Offer** (1-2 sentences):
   - Offer to escalate to supervisor/higher level support
   - Promise immediate follow-up within specific time
   - Example formats:
     * "{'ë§Œì•½ ì—¬ì „íˆ ë¶ˆë§Œì´ í•´ì†Œë˜ì§€ ì•Šìœ¼ì‹ ë‹¤ë©´, ì¦‰ì‹œ ìƒê¸‰ ê´€ë¦¬ìì—ê²Œ ì´ê´€í•˜ì—¬ ë” ë‚˜ì€ í•´ê²°ì±…ì„ ì°¾ì•„ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If your concern is still not resolved, I can immediately escalate this to a supervisor to find a better solution.' if current_lang_key == 'en' else 'ã‚‚ã—ã”ä¸æº€ãŒè§£æ¶ˆã•ã‚Œãªã„å ´åˆã¯ã€ã™ãã«ä¸Šç´šç®¡ç†è€…ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ãƒˆã—ã¦ã€ã‚ˆã‚Šè‰¯ã„è§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"
     * "{'24ì‹œê°„ ì´ë‚´ì— [êµ¬ì²´ì  ì¡°ì¹˜/ê²°ê³¼]ë¥¼ í™•ì¸í•˜ì—¬ ê³ ê°ë‹˜ê»˜ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will follow up with you within 24 hours regarding [specific action/result].' if current_lang_key == 'en' else '24æ™‚é–“ä»¥å†…ã«[å…·ä½“çš„æªç½®/çµæœ]ã‚’ç¢ºèªã—ã€ãŠå®¢æ§˜ã«å†åº¦ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚')}"

4. **Closing with Assurance** (1 sentence):
   - Reassure that their concern is being taken seriously
   - Example formats:
     * "{'ê³ ê°ë‹˜ì˜ ëª¨ë“  ë¬¸ì˜ì‚¬í•­ì„ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will prioritize resolving all of your concerns.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã™ã¹ã¦ã®ã”è³ªå•ã‚’æœ€å„ªå…ˆã§å‡¦ç†ã„ãŸã—ã¾ã™ã€‚')}"

**IMPORTANT NOTES:**
- DO NOT repeat the exact same solution that was already provided
- DO NOT sound dismissive or automated
- DO sound genuinely concerned and willing to go the extra mile
- If policy restrictions exist, acknowledge them but still offer alternatives
- Use warm, respectful tone while being firm about what can/cannot be done

**RESPONSE STRUCTURE:**
[Immediate Acknowledgment]
[Specific Solution Recap + New Action]
[Escalation/Follow-up Offer]
[Closing with Assurance]

Now generate the agent's response draft following this structure:
"""

    # ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ì¶”ê°€ ì§€ì‹œì‚¬í•­ (ë” ê°•í™”)
    diversity_instruction = ""
    if previous_agent_responses:
        # ì´ì „ ì‘ë‹µë“¤ì˜ ì£¼ìš” í‚¤ì›Œë“œ/êµ¬ë¬¸ ì¶”ì¶œ (ë°˜ë³µ ë°©ì§€)
        previous_keywords = []
        for prev_resp in previous_agent_responses[-3:]:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (2-3ë‹¨ì–´ êµ¬ë¬¸)
            words = prev_resp.split()[:20]  # ì²˜ìŒ 20ë‹¨ì–´ë§Œ
            for i in range(len(words) - 1):
                if len(words[i]) > 3 and len(words[i+1]) > 3:
                    previous_keywords.append(f"{words[i]} {words[i+1]}")
        
        keywords_warning = ""
        if previous_keywords:
            unique_keywords = list(set(previous_keywords))[:10]  # ìµœëŒ€ 10ê°œë§Œ
            keywords_warning = f"\n- AVOID using these exact phrases from previous responses: {', '.join(unique_keywords[:5])}"
        
        diversity_instruction = f"""
**CRITICAL DIVERSITY REQUIREMENT - STRICTLY ENFORCED:**
- You MUST generate a COMPLETELY DIFFERENT response from ALL previous agent responses shown above
- Use COMPLETELY DIFFERENT wording, phrasing, sentence structures, and vocabulary
- If similar solutions are needed, present them in a COMPLETELY DIFFERENT way or from a COMPLETELY DIFFERENT angle
- Vary your opening sentences, transition phrases, and closing statements - NO REPETITION
- DO NOT copy, paraphrase, or reuse ANY phrases from previous responses - be CREATIVE and UNIQUE while maintaining professionalism
- Consider COMPLETELY different approaches: 
  * If previous responses were formal, try a warmer, more personal tone (or vice versa)
  * If previous responses focused on one aspect, emphasize a COMPLETELY different aspect this time
  * Use different examples, analogies, or explanations
  * Change the order of information presentation
  * Use different sentence lengths and structures
{keywords_warning}
- IMPORTANT: Read ALL previous responses carefully and ensure your response is DISTINCTLY different in style, tone, structure, and content
- If you find yourself writing something similar to a previous response, STOP and rewrite it completely differently
"""

    # ëœë¤ ìš”ì†Œ ì¶”ê°€ë¥¼ ìœ„í•œ ë³€í˜• ì§€ì‹œì‚¬í•­
    variation_approaches = [
        "Start with a different greeting or acknowledgment style",
        "Use a different problem-solving approach or framework",
        "Present information in a different order",
        "Use different examples or analogies",
        "Vary the level of formality or warmth",
        "Focus on different aspects of the solution",
        "Use different transition words and phrases",
        "Change the length and complexity of sentences"
    ]
    selected_approaches = random.sample(variation_approaches, min(3, len(variation_approaches)))
    variation_note = "\n".join([f"- {approach}" for approach in selected_approaches])

    draft_prompt = f"""
You are an AI assistant helping a customer support agent write a professional, tailored response.

**PRIMARY OBJECTIVE:**
Generate a response draft that is SPECIFICALLY tailored to the customer's EXACT inquiry, providing concrete, actionable solutions that directly address their specific situation. The response must read as if it was written personally for this customer's unique case.

**CRITICAL REQUIREMENTS (IN ORDER OF PRIORITY):**
1. **MOST CRITICAL**: Address the customer's SPECIFIC inquiry/question with DETAILED, ACTIONABLE solutions
   - Extract and reference specific details from their message (order numbers, dates, product names, locations, error messages, etc.)
   - Provide step-by-step instructions tailored to their EXACT situation
   - Answer ALL questions they asked completely
   - Address ALL problems they mentioned specifically

2. The response MUST be in {lang_name}

3. Be professional, empathetic, and solution-oriented

4. If the customer asked a question, provide a COMPLETE and SPECIFIC answer - do NOT give vague or generic responses
   - Bad: "Please check your settings"
   - Good: "Please go to Settings > Mobile Network > APN Settings and ensure the APN is set to 'internet'"

5. If the customer mentioned a problem, acknowledge it SPECIFICALLY and provide STEP-BY-STEP solutions
   - Reference their exact problem description
   - Provide solutions that directly address their specific issue

6. Reference specific details from their inquiry (order numbers, dates, products, locations, etc.) if mentioned
   - If they mentioned "order #12345", your response MUST include "order #12345"
   - If they mentioned "Paris", your response should reference Paris specifically
   - If they mentioned "eSIM", address eSIM specifically, not just "SIM card"

7. Keep the tone appropriate for the customer type: {customer_type}

8. Do NOT include any markdown formatting, just plain text

9. {f'**FOLLOW THE COPING STRATEGY FORMAT BELOW**' if needs_coping_strategy else 'Use natural, conversational flow'}

10. **CRITICAL**: Generate a COMPLETELY UNIQUE and VARIED response - avoid repeating ANY similar phrases, structures, or approaches from previous responses

11. **CRITICAL**: Your response must be HIGHLY RELEVANT to the customer's specific inquiry - generic template responses are NOT acceptable
    - DO NOT start with generic greetings without immediately addressing their specific issue
    - DO NOT use placeholder text like "[specific solution]" - provide ACTUAL specific solutions
    - Your response should make the customer feel like you read and understood THEIR specific message

**VARIATION TECHNIQUES TO APPLY:**
{variation_note}

{customer_query_analysis}

**FULL CONVERSATION HISTORY:**
{history_text}
{attachment_context}

{verification_warning}

**PREVIOUS RESPONSES TO AVOID REPEATING:**
{previous_responses_context if previous_responses_context else "No previous responses to compare against."}

**DIVERSITY REQUIREMENTS:**
{diversity_instruction if diversity_instruction else "This is the first response, so no previous responses to avoid."}

{coping_guidance if needs_coping_strategy else ''}

**YOUR TASK:**
Generate the agent's response draft NOW. The response must:

1. **FIRST**: Read the customer inquiry analysis above CAREFULLY and identify:
   - What is their EXACT problem or question?
   - What specific details did they mention (order numbers, dates, locations, products)?
   - What do they need help with specifically?

2. **SECOND**: Write a response that:
   - Addresses their EXACT problem/question (not a generic version)
   - References the specific details they mentioned
   - Provides concrete, actionable steps tailored to their situation
   - Answers ALL their questions completely
   - Makes them feel understood

3. **THIRD**: Ensure the response is:
   - COMPLETELY DIFFERENT and UNIQUE from any previous responses
   - Professional, empathetic, and solution-oriented
   - Written in {lang_name}
   - Free of markdown formatting

**BEFORE YOU WRITE**: Ask yourself:
- "Does this response address the customer's SPECIFIC inquiry?"
- "Would a generic template response work here?" (If yes, rewrite it to be more specific)
- "Does this response reference specific details from the customer's message?"
- "Would the customer feel like I read and understood THEIR specific message?"

**NOW GENERATE THE RESPONSE:**
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        draft = run_llm(draft_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if draft.startswith("```"):
            lines = draft.split("\n")
            draft = "\n".join(lines[1:-1]) if len(lines) > 2 else draft
        return draft
    except Exception as e:
        return f"âŒ ì‘ë‹µ ì´ˆì•ˆ ìƒì„± ì˜¤ë¥˜: {e}"


# â­ ìƒˆë¡œìš´ í•¨ìˆ˜: ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ ìš”ì•½ ìƒì„±

def generate_outbound_call_summary(customer_query: str, current_lang_key: str, target: str) -> str:
    """
    Simulates an outbound call to a local partner or customer and generates a summary of the outcome.
    """
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # Get the current chat history for context
    history_text = get_chat_history_for_prompt(include_attachment=True)
    if not history_text:
        history_text = f"Initial Customer Query: {customer_query}"

    # Policy context (from supervisor) should be included to guide the outcome
    policy_context = st.session_state.supervisor_policy_context or ""

    summary_prompt = f"""
You are an AI simulating a quick, high-stakes phone call placed by the customer support agent to a '{target}' (either a local partner/vendor or the customer).

The purpose of the call is to resolve a complex, policy-restricted issue (like an exceptional refund for a non-refundable item, or urgent confirmation of an airport transfer change).

Analyze the conversation history, the initial query, and any provided supervisor policy.
Generate a concise summary of the OUTCOME of this simulated phone call.
The summary MUST be professional and strictly in {lang_name}.

[CRITICAL RULE]: For non-refundable items (e.g., Universal Studio Express Pass, non-refundable hotel/transfer), the local partner should only grant an exception IF the customer has provided strong, unavoidable proof (like a flight cancellation notice, doctor's note, or natural disaster notice). If no such proof is evident in the chat history, the outcome should usually be a denial or a request for more proof, but keep the tone professional.
If the customer's query is about Airport Transfer change, the outcome should be: 'Confirmation complete. Change is approved/denied based on partner policy.'

Conversation History:
{history_text}

Supervisor Policy Context (If any):
{policy_context}

Target of Call: {target}

Generate the phone call summary (Outcome ONLY):
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key missing. (Simulated Outcome: The {target} requested the agent to send proof via email.)"

    try:
        summary = run_llm(summary_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if summary.startswith("```"):
            lines = summary.split("\n")
            summary = "\n".join(lines[1:-1]) if len(lines) > 2 else summary
        return summary
    except Exception as e:
        return f"âŒ Phone call simulation error: {e}"


# ========================================
# 3. Whisper / TTS Helper
# ========================================

def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = "audio/webm", lang_code: str = None, auto_detect: bool = True) -> str:
    """
    OpenAI Whisper API ë˜ëŠ” Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•©ë‹ˆë‹¤.
    OpenAIê°€ ì‹¤íŒ¨í•˜ë©´ Geminië¡œ ìë™ fallbackí•©ë‹ˆë‹¤.
    
    Args:
        audio_bytes: ì „ì‚¬í•  ì˜¤ë””ì˜¤ ë°”ì´íŠ¸
        mime_type: ì˜¤ë””ì˜¤ MIME íƒ€ì…
        lang_code: ì–¸ì–´ ì½”ë“œ (ko, en, ja ë“±). Noneì´ê±°ë‚˜ auto_detect=Trueì´ë©´ ìë™ ê°ì§€
        auto_detect: Trueì´ë©´ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ (lang_code ë¬´ì‹œ)
    """
    # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    
    # ì„ì‹œ íŒŒì¼ ì €ì¥ (API í˜¸í™˜ì„±)
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp.write(audio_bytes)
    tmp.flush()
    tmp.close()
    
    # 1ï¸âƒ£ OpenAI Whisper API ì‹œë„
    client = st.session_state.openai_client
    if client is not None:
        try:
            with open(tmp.name, "rb") as f:
                # ì–¸ì–´ ìë™ ê°ì§€ ë˜ëŠ” ì§€ì •ëœ ì–¸ì–´ ì‚¬ìš©
                if auto_detect or lang_code is None:
                    # language íŒŒë¼ë¯¸í„°ë¥¼ ìƒëµí•˜ë©´ Whisperê°€ ìë™ìœ¼ë¡œ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤
                    res = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                    )
                else:
                    whisper_lang = {"ko": "ko", "en": "en", "ja": "ja"}.get(lang_code, "en")
                    res = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                        language=whisper_lang,
                    )
            # res.text ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ res ìì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            result = res.text.strip() if hasattr(res, 'text') else str(res).strip()
            if result:
                try:
                    os.remove(tmp.name)
                except OSError:
                    pass
                return result
        except Exception as e:
            # OpenAI ì‹¤íŒ¨ ì‹œ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  Geminië¡œ fallback
            print(f"OpenAI Whisper failed: {e}")
    
    # 2ï¸âƒ£ Gemini API fallback
    gemini_key = get_api_key("gemini")
    if gemini_key:
        try:
            import base64
            genai.configure(api_key=gemini_key)
            
            # GeminiëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡
            with open(tmp.name, "rb") as f:
                audio_data = f.read()
                audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # Gemini 2.0 Flash ëª¨ë¸ ì‚¬ìš© (ì˜¤ë””ì˜¤ ì§€ì›)
            model = genai.GenerativeModel("gemini-2.0-flash-exp")
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            lang_prompt = ""
            if lang_code:
                lang_map = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}
                lang_prompt = f"ì´ ì˜¤ë””ì˜¤ëŠ” {lang_map.get(lang_code, 'English')}ë¡œ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. "
            
            prompt = f"{lang_prompt}ì´ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•´ì£¼ì„¸ìš”. ì˜¤ì§ ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”."
            
            # GeminiëŠ” íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹ ì‚¬ìš© (Gemini 2.0 FlashëŠ” ì˜¤ë””ì˜¤ ì§€ì›)
            try:
                audio_file = genai.upload_file(path=tmp.name, mime_type=mime_type)
                
                # íŒŒì¼ ì—…ë¡œë“œ í›„ ì ì‹œ ëŒ€ê¸° (ì—…ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°)
                import time
                time.sleep(1)
                
                response = model.generate_content([prompt, audio_file])
                result = response.text.strip() if response.text else ""
                
                # íŒŒì¼ ì‚­ì œ
                try:
                    genai.delete_file(audio_file.name)
                except Exception as del_err:
                    print(f"Failed to delete Gemini file: {del_err}")
            except Exception as upload_err:
                # íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                print(f"Gemini file upload failed: {upload_err}")
                # ëŒ€ì•ˆ: base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ë¥¼ ì§ì ‘ ì „ì†¡ (ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°)
                raise upload_err
            
            if result:
                try:
                    os.remove(tmp.name)
                except OSError:
                    pass
                return result
            else:
                raise Exception("Gemini returned empty result")
        except Exception as e:
            print(f"Gemini transcription failed: {e}")
            # Geminië„ ì‹¤íŒ¨í•œ ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
            try:
                os.remove(tmp.name)
            except OSError:
                pass
            return f"âŒ {L.get('whisper_client_error', 'ì „ì‚¬ ì‹¤íŒ¨')}: OpenAIì™€ Gemini ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({str(e)[:100]})"
    else:
        # ë‘ API ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€
        try:
            os.remove(tmp.name)
        except OSError:
            pass
        return f"âŒ {L.get('openai_missing', 'OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.')} ë˜ëŠ” Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."


def transcribe_audio(audio_bytes, filename="audio.wav"):
    client = st.session_state.openai_client

    # 1ï¸âƒ£ OpenAI Whisper ì‹œë„
    if client:
        try:
            import io
            bio = io.BytesIO(audio_bytes)
            bio.name = filename
            resp = client.audio.transcriptions.create(
                model="whisper-1",
                file=bio,
            )
            return resp.text
        except Exception as e:
            print("Whisper OpenAI failed:", e)

    # 2ï¸âƒ£ Gemini STT fallback
    try:
        genai.configure(api_key=get_api_key("gemini"))
        model = genai.GenerativeModel("gemini-2.5-flash")
        text = model.generate_content("Transcribe this audio:").text
        return text or ""
    except Exception as e:
        print("Gemini STT failed:", e)

    return "âŒ STT not available"


# ========================================
# ë¹„ë””ì˜¤ ë™ê¸°í™” ê´€ë ¨ í•¨ìˆ˜
# ========================================

def analyze_text_for_video_selection(text: str, current_lang_key: str, 
                                     agent_last_response: str = None,
                                     conversation_context: List[Dict] = None) -> Dict[str, Any]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ê°ì • ìƒíƒœì™€ ì œìŠ¤ì²˜ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
    OpenAI/Gemini APIë¥¼ í™œìš©í•œ ì˜ìƒ RAGì˜ í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
    
    â­ Gemini ì œì•ˆ ì ìš©: ê¸´ê¸‰ë„, ë§Œì¡±ë„ ë³€í™”, ì—ì´ì „íŠ¸ ë‹µë³€ ê¸°ë°˜ ì˜ˆì¸¡ ì¶”ê°€
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ê³ ê°ì˜ ì§ˆë¬¸/ì‘ë‹µ)
        current_lang_key: í˜„ì¬ ì–¸ì–´ í‚¤
        agent_last_response: ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ë‹µë³€ (ì„ íƒì , ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ)
        conversation_context: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì , ë§Œì¡±ë„ ë³€í™” ë¶„ì„ìš©)
    
    Returns:
        {
            "emotion": "NEUTRAL" | "HAPPY" | "ANGRY" | "ASKING" | "SAD",
            "gesture": "NONE" | "HAND_WAVE" | "NOD" | "SHAKE_HEAD" | "POINT",
            "urgency": "LOW" | "MEDIUM" | "HIGH",  # â­ ì¶”ê°€: ê¸´ê¸‰ë„
            "satisfaction_delta": -1.0 to 1.0,  # â­ ì¶”ê°€: ë§Œì¡±ë„ ë³€í™” (-1: ê°ì†Œ, 0: ìœ ì§€, 1: ì¦ê°€)
            "confidence": 0.0-1.0
        }
    """
    if not text or not text.strip():
        return {
            "emotion": "NEUTRAL", 
            "gesture": "NONE", 
            "urgency": "LOW",
            "satisfaction_delta": 0.0,
            "confidence": 0.5
        }
    
    L = LANG.get(current_lang_key, LANG["ko"])
    
    # â­ Gemini ì œì•ˆ: ì—ì´ì „íŠ¸ ë‹µë³€ ê¸°ë°˜ ì˜ˆì¸¡ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_info = ""
    if agent_last_response:
        context_info = f"""
ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ë‹µë³€: "{agent_last_response}"

ì—ì´ì „íŠ¸ì˜ ë‹µë³€ì„ ê³ ë ¤í–ˆì„ ë•Œ, ê³ ê°ì´ ì§€ê¸ˆ ë§í•˜ëŠ” ë‚´ìš©ì€ ì–´ë–¤ ê°ì •ì„ ìˆ˜ë°˜í•  ê²ƒì¸ì§€ ì˜ˆì¸¡í•˜ì„¸ìš”.
ì˜ˆë¥¼ ë“¤ì–´:
- ì—ì´ì „íŠ¸ê°€ ì†”ë£¨ì…˜ì„ ì œì‹œí–ˆë‹¤ë©´ â†’ ê³ ê°ì€ HAPPY ë˜ëŠ” ASKING (ì¶”ê°€ ì§ˆë¬¸)
- ì—ì´ì „íŠ¸ê°€ ê±°ì ˆí–ˆë‹¤ë©´ â†’ ê³ ê°ì€ ANGRY ë˜ëŠ” SAD
- ì—ì´ì „íŠ¸ê°€ ì§ˆë¬¸ì„ í–ˆë‹¤ë©´ â†’ ê³ ê°ì€ ASKING (ë‹µë³€) ë˜ëŠ” NEUTRAL
"""
    
    # â­ Gemini ì œì•ˆ: ë§Œì¡±ë„ ë³€í™” ë¶„ì„ ì»¨í…ìŠ¤íŠ¸
    satisfaction_context = ""
    if conversation_context and len(conversation_context) > 1:
        # ìµœê·¼ ëŒ€í™”ì˜ ê°ì • ë³€í™” ì¶”ì 
        recent_emotions = []
        for msg in conversation_context[-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€
            if msg.get("role") == "customer_rebuttal" or msg.get("role") == "customer":
                recent_emotions.append(msg.get("content", ""))
        
        if len(recent_emotions) >= 2:
            satisfaction_context = f"""
ìµœê·¼ ëŒ€í™” íë¦„:
- ì´ì „ ê³ ê° ë©”ì‹œì§€: "{recent_emotions[-2] if len(recent_emotions) >= 2 else ''}"
- í˜„ì¬ ê³ ê° ë©”ì‹œì§€: "{recent_emotions[-1]}"

ë§Œì¡±ë„ ë³€í™”ë¥¼ ë¶„ì„í•˜ì„¸ìš”:
- ì´ì „ë³´ë‹¤ ë” ê¸ì •ì ì´ë©´ satisfaction_delta > 0
- ì´ì „ë³´ë‹¤ ë” ë¶€ì •ì ì´ë©´ satisfaction_delta < 0
- ë¹„ìŠ·í•˜ë©´ satisfaction_delta â‰ˆ 0
"""
    
    # â­ Gemini ì œì•ˆ: ê°œì„ ëœ LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë‹¤ìŒ ê³ ê°ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê°ì • ìƒíƒœ, ì œìŠ¤ì²˜, ê¸´ê¸‰ë„, ë§Œì¡±ë„ ë³€í™”ë¥¼ íŒë‹¨í•˜ì„¸ìš”.

ê³ ê° í…ìŠ¤íŠ¸: "{text}"
{context_info}
{satisfaction_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):
{{
    "emotion": "NEUTRAL" | "HAPPY" | "ANGRY" | "ASKING" | "SAD",
    "gesture": "NONE" | "HAND_WAVE" | "NOD" | "SHAKE_HEAD" | "POINT",
    "urgency": "LOW" | "MEDIUM" | "HIGH",
    "satisfaction_delta": -1.0 to 1.0,
    "confidence": 0.0-1.0
}}

ê°ì • íŒë‹¨ ê¸°ì¤€ (ì„¸ë¶„í™”):
- HAPPY: ê¸ì •ì  í‘œí˜„, ê°ì‚¬, ë§Œì¡±, í•´ê²°ë¨ ("ê°ì‚¬í•©ë‹ˆë‹¤", "ì¢‹ì•„ìš”", "ì™„ë²½í•´ìš”", "ì´ì œ ì´í•´í–ˆì–´ìš”")
- ANGRY: ë¶ˆë§Œ, í™”ë‚¨, ê±°ë¶€, ê°•í•œ ë¶€ì • ("í™”ê°€ ë‚˜ìš”", "ë¶ˆê°€ëŠ¥í•´ìš”", "ê±°ì ˆí•©ë‹ˆë‹¤", "ë§ë„ ì•ˆ ë¼ìš”")
- ASKING: ì§ˆë¬¸, ê¶ê¸ˆí•¨, í™•ì¸ ìš”ì²­, ì •ë³´ ìš”êµ¬ ("ì–´ë–»ê²Œ", "ì™œ", "ì•Œë ¤ì£¼ì„¸ìš”", "ì£¼ë¬¸ë²ˆí˜¸ê°€ ë­ì˜ˆìš”?")
- SAD: ìŠ¬í””, ì‹¤ë§, ì¢Œì ˆ ("ìŠ¬í”„ë„¤ìš”", "ì‹¤ë§í–ˆì–´ìš”", "ì•„ì‰½ìŠµë‹ˆë‹¤", "ê·¸ë ‡ë‹¤ë©´ ì–´ì©” ìˆ˜ ì—†ë„¤ìš”")
- NEUTRAL: ì¤‘ë¦½ì  í‘œí˜„, ë‹¨ìˆœ ì •ë³´ ì „ë‹¬ (ê¸°ë³¸ê°’)

ì œìŠ¤ì²˜ íŒë‹¨ ê¸°ì¤€:
- HAND_WAVE: ì¸ì‚¬, í™˜ì˜ ("ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤")
- NOD: ë™ì˜, ê¸ì •, ì´í•´ ("ë„¤", "ë§ì•„ìš”", "ê·¸ë ‡ìŠµë‹ˆë‹¤", "ì•Œê² ìŠµë‹ˆë‹¤")
- SHAKE_HEAD: ë¶€ì •, ê±°ë¶€, ë¶ˆë§Œì¡± ("ì•„ë‹ˆìš”", "ì•ˆ ë©ë‹ˆë‹¤", "ê·¸ê±´ ì•„ë‹ˆì—ìš”")
- POINT: ì„¤ëª…, ì§€ì‹œ, íŠ¹ì • í•­ëª© ì–¸ê¸‰ ("ì—¬ê¸°", "ì´ê²ƒ", "ì €ê²ƒ", "ì£¼ë¬¸ë²ˆí˜¸ëŠ”")
- NONE: íŠ¹ë³„í•œ ì œìŠ¤ì²˜ ì—†ìŒ (ê¸°ë³¸ê°’)

ê¸´ê¸‰ë„ íŒë‹¨ ê¸°ì¤€:
- HIGH: ì¦‰ì‹œ í•´ê²° í•„ìš”, ê¸´ê¸‰í•œ ë¬¸ì œ ("ì§€ê¸ˆ ë‹¹ì¥", "ë°”ë¡œ", "ê¸´ê¸‰", "ì¤‘ìš”í•´ìš”")
- MEDIUM: ë¹ ë¥¸ í•´ê²° ì„ í˜¸, ì¤‘ìš”í•˜ì§€ë§Œ ê¸´ê¸‰í•˜ì§€ ì•ŠìŒ
- LOW: ì¼ë°˜ì ì¸ ë¬¸ì˜, ê¸´ê¸‰í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’)

ë§Œì¡±ë„ ë³€í™” (satisfaction_delta):
- 1.0: ë§¤ìš° ë§Œì¡±, ë¬¸ì œ í•´ê²°ë¨, ê°ì‚¬ í‘œí˜„
- 0.5: ë§Œì¡±, ê¸ì •ì  ë°˜ì‘
- 0.0: ì¤‘ë¦½, ë³€í™” ì—†ìŒ
- -0.5: ë¶ˆë§Œì¡±, ë¶€ì •ì  ë°˜ì‘
- -1.0: ë§¤ìš° ë¶ˆë§Œì¡±, í™”ë‚¨, ê±°ë¶€

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:"""

    try:
        # LLM í˜¸ì¶œ
        if st.session_state.is_llm_ready:
            response_text = run_llm(prompt)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
                import re
                json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    # ìœ íš¨ì„± ê²€ì‚¬
                    valid_emotions = ["NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD"]
                    valid_gestures = ["NONE", "HAND_WAVE", "NOD", "SHAKE_HEAD", "POINT"]
                    valid_urgencies = ["LOW", "MEDIUM", "HIGH"]
                    
                    emotion = result.get("emotion", "NEUTRAL")
                    gesture = result.get("gesture", "NONE")
                    urgency = result.get("urgency", "LOW")
                    satisfaction_delta = float(result.get("satisfaction_delta", 0.0))
                    confidence = float(result.get("confidence", 0.7))
                    
                    if emotion not in valid_emotions:
                        emotion = "NEUTRAL"
                    if gesture not in valid_gestures:
                        gesture = "NONE"
                    if urgency not in valid_urgencies:
                        urgency = "LOW"
                    
                    # â­ Gemini ì œì•ˆ: ìƒí™©ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
                    context_keywords = []
                    text_lower_for_context = text.lower()
                    
                    # ì£¼ìš” ìƒí™©ë³„ í‚¤ì›Œë“œ ë§¤í•‘
                    if any(word in text_lower_for_context for word in ["ì£¼ë¬¸ë²ˆí˜¸", "order number", "ì£¼ë¬¸ ë²ˆí˜¸"]):
                        context_keywords.append("order_number")
                    if any(word in text_lower_for_context for word in ["í•´ê²°", "ì™„ë£Œ", "ê°ì‚¬", "solution", "resolved"]):
                        if satisfaction_delta > 0.3:
                            context_keywords.append("solution_accepted")
                    if any(word in text_lower_for_context for word in ["ê±°ì ˆ", "ë¶ˆê°€", "ì•ˆ ë©ë‹ˆë‹¤", "denied", "cannot"]):
                        if emotion == "ANGRY":
                            context_keywords.append("policy_denial")
                    
                    return {
                        "emotion": emotion,
                        "gesture": gesture,
                        "urgency": urgency,
                        "satisfaction_delta": max(-1.0, min(1.0, satisfaction_delta)),
                        "context_keywords": context_keywords,  # â­ ì¶”ê°€
                        "confidence": max(0.0, min(1.0, confidence))
                    }
            except json.JSONDecodeError:
                pass
        
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ë¶„ì„
        text_lower = text.lower()
        emotion = "NEUTRAL"
        gesture = "NONE"
        urgency = "LOW"
        satisfaction_delta = 0.0
        
        # ê°ì • í‚¤ì›Œë“œ ë¶„ì„
        if any(word in text_lower for word in ["ê°ì‚¬", "ì¢‹ì•„", "ì™„ë²½", "ë§Œì¡±", "ê³ ë§ˆì›Œ", "í•´ê²°"]):
            emotion = "HAPPY"
            satisfaction_delta = 0.5
        elif any(word in text_lower for word in ["í™”", "ë¶ˆë§Œ", "ê±°ì ˆ", "ë¶ˆê°€ëŠ¥", "ì•ˆ ë©ë‹ˆë‹¤", "ë§ë„ ì•ˆ ë¼"]):
            emotion = "ANGRY"
            satisfaction_delta = -0.5
        elif any(word in text_lower for word in ["ì–´ë–»ê²Œ", "ì™œ", "ì•Œë ¤", "ì§ˆë¬¸", "ê¶ê¸ˆ", "ì£¼ë¬¸ë²ˆí˜¸"]):
            emotion = "ASKING"
        elif any(word in text_lower for word in ["ìŠ¬í”„", "ì‹¤ë§", "ì•„ì‰½", "ê·¸ë ‡ë‹¤ë©´"]):
            emotion = "SAD"
            satisfaction_delta = -0.3
        
        # ê¸´ê¸‰ë„ í‚¤ì›Œë“œ ë¶„ì„
        if any(word in text_lower for word in ["ì§€ê¸ˆ ë‹¹ì¥", "ë°”ë¡œ", "ê¸´ê¸‰", "ì¤‘ìš”í•´ìš”", "ì¦‰ì‹œ"]):
            urgency = "HIGH"
        elif any(word in text_lower for word in ["ë¹¨ë¦¬", "ê°€ëŠ¥í•œ í•œ", "ìµœëŒ€í•œ"]):
            urgency = "MEDIUM"
        
        # ì œìŠ¤ì²˜ í‚¤ì›Œë“œ ë¶„ì„
        if any(word in text_lower for word in ["ì•ˆë…•", "ë°˜ê°‘", "ì¸ì‚¬"]):
            gesture = "HAND_WAVE"
        elif any(word in text_lower for word in ["ë„¤", "ë§ì•„", "ê·¸ë˜", "ë™ì˜", "ì•Œê² ìŠµë‹ˆë‹¤"]):
            gesture = "NOD"
            if emotion == "HAPPY":
                satisfaction_delta = 0.3
        elif any(word in text_lower for word in ["ì•„ë‹ˆ", "ì•ˆ ë©ë‹ˆë‹¤", "ê±°ì ˆ"]):
            gesture = "SHAKE_HEAD"
            satisfaction_delta = -0.2
        elif any(word in text_lower for word in ["ì—¬ê¸°", "ì´ê²ƒ", "ì €ê²ƒ", "ì´ê±°", "ì£¼ë¬¸ë²ˆí˜¸"]):
            gesture = "POINT"
        
        # â­ Gemini ì œì•ˆ: ìƒí™©ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„)
        context_keywords = []
        if any(word in text_lower for word in ["ì£¼ë¬¸ë²ˆí˜¸", "order number", "ì£¼ë¬¸ ë²ˆí˜¸"]):
            context_keywords.append("order_number")
        if any(word in text_lower for word in ["í•´ê²°", "ì™„ë£Œ", "ê°ì‚¬", "solution"]):
            if satisfaction_delta > 0.3:
                context_keywords.append("solution_accepted")
        if any(word in text_lower for word in ["ê±°ì ˆ", "ë¶ˆê°€", "ì•ˆ ë©ë‹ˆë‹¤"]):
            if emotion == "ANGRY":
                context_keywords.append("policy_denial")
        
        return {
            "emotion": emotion,
            "gesture": gesture,
            "urgency": urgency,
            "satisfaction_delta": satisfaction_delta,
            "context_keywords": context_keywords,  # â­ ì¶”ê°€
            "confidence": 0.6  # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ì€ ë‚®ì€ ì‹ ë¢°ë„
        }
    
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "emotion": "NEUTRAL", 
            "gesture": "NONE", 
            "urgency": "LOW",
            "satisfaction_delta": 0.0,
            "context_keywords": [],  # â­ ì¶”ê°€
            "confidence": 0.5
        }


def get_video_path_by_avatar(gender: str, emotion: str, is_speaking: bool = False, 
                             gesture: str = "NONE", context_keywords: List[str] = None) -> str:
    """
    ê³ ê° ì•„ë°”íƒ€ ì •ë³´(ì„±ë³„, ê°ì • ìƒíƒœ, ì œìŠ¤ì²˜, ìƒí™©)ì— ë”°ë¼ ì ì ˆí•œ ë¹„ë””ì˜¤ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    OpenAI/Gemini ê¸°ë°˜ ì˜ìƒ RAG: LLMì´ ë¶„ì„í•œ ê°ì •/ì œìŠ¤ì²˜ì— ë”°ë¼ ë¹„ë””ì˜¤ í´ë¦½ì„ ì„ íƒí•©ë‹ˆë‹¤.
    
    â­ Gemini ì œì•ˆ: ìƒí™©ë³„ ë¹„ë””ì˜¤ í´ë¦½ íŒ¨í„´ í™•ì¥ (ì˜ˆ: male_asking_order_number.mp4)
    
    Args:
        gender: "male" ë˜ëŠ” "female"
        emotion: "NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD"
        is_speaking: ë§í•˜ëŠ” ì¤‘ì¸ì§€ ì—¬ë¶€
        gesture: "NONE", "HAND_WAVE", "NOD", "SHAKE_HEAD", "POINT"
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["order_number", "solution_accepted", "policy_denial"])
    
    Returns:
        ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
    """
    # ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì‚¬ìš©ìê°€ ì„¤ì •í•œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ì´ ì €ì¥ëœ ìœ„ì¹˜)
    video_base_dir = os.path.join(DATA_DIR, "videos")
    os.makedirs(video_base_dir, exist_ok=True)
    
    # â­ Gemini ì œì•ˆ: ìš°ì„ ìˆœìœ„ -1 - ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì¶”ì²œ ë¹„ë””ì˜¤ (ê°€ì¥ ìš°ì„ )
    if context_keywords:
        db_recommended = get_recommended_video_from_database(emotion, gesture, context_keywords)
        if db_recommended:
            return db_recommended
    else:
        db_recommended = get_recommended_video_from_database(emotion, gesture, [])
        if db_recommended:
            return db_recommended
    
    # â­ Gemini ì œì•ˆ: ìš°ì„ ìˆœìœ„ 0 - ìƒí™©ë³„ ë¹„ë””ì˜¤ í´ë¦½ (ê°€ì¥ êµ¬ì²´ì )
    if context_keywords:
        for keyword in context_keywords:
            # ìƒí™©ë³„ íŒŒì¼ëª… íŒ¨í„´ ì‹œë„ (ì˜ˆ: male_asking_order_number.mp4)
            context_filename = f"{gender}_{emotion.lower()}_{keyword}"
            if is_speaking:
                context_filename += "_speaking"
            context_filename += ".mp4"
            context_path = os.path.join(video_base_dir, context_filename)
            if os.path.exists(context_path):
                return context_path
            
            # ì„¸ì…˜ ìƒíƒœì—ì„œë„ í™•ì¸
            context_video_key = f"video_{gender}_{emotion.lower()}_{keyword}"
            if context_video_key in st.session_state and st.session_state[context_video_key]:
                video_path = st.session_state[context_video_key]
                if os.path.exists(video_path):
                    return video_path
    
    # ìš°ì„ ìˆœìœ„ 1: ì œìŠ¤ì²˜ê°€ ìˆëŠ” ê²½ìš° ì œìŠ¤ì²˜ë³„ ë¹„ë””ì˜¤ ì‹œë„
    if gesture != "NONE" and gesture:
        gesture_video_key = f"video_{gender}_{emotion.lower()}_{gesture.lower()}"
        if gesture_video_key in st.session_state and st.session_state[gesture_video_key]:
            video_path = st.session_state[gesture_video_key]
            if os.path.exists(video_path):
                return video_path
        
        # ì œìŠ¤ì²˜ë³„ íŒŒì¼ëª… íŒ¨í„´ ì‹œë„
        gesture_filename = f"{gender}_{emotion.lower()}_{gesture.lower()}"
        if is_speaking:
            gesture_filename += "_speaking"
        gesture_filename += ".mp4"
        gesture_path = os.path.join(video_base_dir, gesture_filename)
        if os.path.exists(gesture_path):
            return gesture_path
    
    # ìš°ì„ ìˆœìœ„ 2: ê°ì • ìƒíƒœë³„ ë¹„ë””ì˜¤ (ì œìŠ¤ì²˜ ì—†ì´)
    video_key = f"video_{gender}_{emotion.lower()}"
    if is_speaking:
        video_key += "_speaking"
    
    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ë¹„ë””ì˜¤ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if video_key in st.session_state and st.session_state[video_key]:
        video_path = st.session_state[video_key]
        if os.path.exists(video_path):
            return video_path
    
    # ê¸°ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ëª… íŒ¨í„´ ì‹œë„
    video_filename = f"{gender}_{emotion.lower()}"
    if is_speaking:
        video_filename += "_speaking"
    video_filename += ".mp4"
    
    video_path = os.path.join(video_base_dir, video_filename)
    if os.path.exists(video_path):
        return video_path
    
    # ìš°ì„ ìˆœìœ„ 3: ê¸°ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ ì‹œë„ (ì¤‘ë¦½ ìƒíƒœ)
    default_video = os.path.join(video_base_dir, f"{gender}_neutral.mp4")
    if os.path.exists(default_video):
        return default_video
    
    # ìš°ì„ ìˆœìœ„ 4: ì„¸ì…˜ ìƒíƒœì—ì„œ ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ í™•ì¸
    if "current_customer_video" in st.session_state and st.session_state.current_customer_video:
        return st.session_state.current_customer_video
    
    return None


# â­ Gemini ì œì•ˆ: ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í•¨ìˆ˜
def load_video_mapping_database() -> Dict[str, Any]:
    """ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(VIDEO_MAPPING_DB_FILE):
        try:
            with open(VIDEO_MAPPING_DB_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {"mappings": [], "feedback_history": []}
    return {"mappings": [], "feedback_history": []}


def save_video_mapping_database(db_data: Dict[str, Any]):
    """ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(VIDEO_MAPPING_DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(db_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")


def add_video_mapping_feedback(
    customer_text: str,
    selected_video_path: str,
    emotion: str,
    gesture: str,
    context_keywords: List[str],
    user_rating: int,  # 1-5 ì ìˆ˜
    user_comment: str = ""
) -> None:
    """
    â­ Gemini ì œì•ˆ: ì‚¬ìš©ì í”¼ë“œë°±ì„ ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        customer_text: ê³ ê°ì˜ í…ìŠ¤íŠ¸
        selected_video_path: ì„ íƒëœ ë¹„ë””ì˜¤ ê²½ë¡œ
        emotion: ë¶„ì„ëœ ê°ì •
        gesture: ë¶„ì„ëœ ì œìŠ¤ì²˜
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ
        user_rating: ì‚¬ìš©ì í‰ê°€ (1-5)
        user_comment: ì‚¬ìš©ì ì½”ë©˜íŠ¸ (ì„ íƒì )
    """
    db_data = load_video_mapping_database()
    
    feedback_entry = {
        "timestamp": datetime.now().isoformat(),
        "customer_text": customer_text[:200],  # ìµœëŒ€ 200ì
        "selected_video": os.path.basename(selected_video_path) if selected_video_path else None,
        "video_path": selected_video_path,
        "emotion": emotion,
        "gesture": gesture,
        "context_keywords": context_keywords,
        "user_rating": user_rating,
        "user_comment": user_comment[:500] if user_comment else "",  # ìµœëŒ€ 500ì
        "is_natural_match": user_rating >= 4  # 4ì  ì´ìƒì´ë©´ ìì—°ìŠ¤ëŸ¬ìš´ ë§¤ì¹­ìœ¼ë¡œ ê°„ì£¼
    }
    
    db_data["feedback_history"].append(feedback_entry)
    
    # ë§¤í•‘ ê·œì¹™ ì—…ë°ì´íŠ¸ (í‰ê°€ê°€ ë†’ì€ ê²½ìš°)
    if user_rating >= 4:
        mapping_key = f"{emotion}_{gesture}_{'_'.join(context_keywords) if context_keywords else 'none'}"
        
        # ê¸°ì¡´ ë§¤í•‘ ì°¾ê¸°
        existing_mapping = None
        for mapping in db_data["mappings"]:
            if mapping.get("key") == mapping_key:
                existing_mapping = mapping
                break
        
        if existing_mapping:
            # ê¸°ì¡´ ë§¤í•‘ ì—…ë°ì´íŠ¸ (í‰ê·  ì ìˆ˜ ê³„ì‚°)
            total_rating = existing_mapping.get("total_rating", 0) + user_rating
            count = existing_mapping.get("count", 0) + 1
            existing_mapping["total_rating"] = total_rating
            existing_mapping["count"] = count
            existing_mapping["avg_rating"] = total_rating / count
            existing_mapping["last_updated"] = datetime.now().isoformat()
        else:
            # ìƒˆ ë§¤í•‘ ì¶”ê°€
            db_data["mappings"].append({
                "key": mapping_key,
                "emotion": emotion,
                "gesture": gesture,
                "context_keywords": context_keywords,
                "recommended_video": os.path.basename(selected_video_path) if selected_video_path else None,
                "video_path": selected_video_path,
                "total_rating": user_rating,
                "count": 1,
                "avg_rating": float(user_rating),
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat()
            })
    
    save_video_mapping_database(db_data)


def get_recommended_video_from_database(
    emotion: str,
    gesture: str,
    context_keywords: List[str]
) -> str:
    """
    â­ Gemini ì œì•ˆ: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¶”ì²œ ë¹„ë””ì˜¤ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        emotion: ê°ì • ìƒíƒœ
        gesture: ì œìŠ¤ì²˜
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ
    
    Returns:
        ì¶”ì²œ ë¹„ë””ì˜¤ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
    """
    db_data = load_video_mapping_database()
    
    mapping_key = f"{emotion}_{gesture}_{'_'.join(context_keywords) if context_keywords else 'none'}"
    
    # ì •í™•í•œ ë§¤ì¹­ ì°¾ê¸°
    for mapping in db_data["mappings"]:
        if mapping.get("key") == mapping_key and mapping.get("avg_rating", 0) >= 4.0:
            video_path = mapping.get("video_path")
            if video_path and os.path.exists(video_path):
                return video_path
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ê°ì •ê³¼ ì œìŠ¤ì²˜ë§Œ)
    partial_key = f"{emotion}_{gesture}_none"
    for mapping in db_data["mappings"]:
        if mapping.get("key") == partial_key and mapping.get("avg_rating", 0) >= 4.0:
            video_path = mapping.get("video_path")
            if video_path and os.path.exists(video_path):
                return video_path
    
    return None


def render_synchronized_video(text: str, audio_bytes: bytes, gender: str, emotion: str, 
                               role: str = "customer", autoplay: bool = True,
                               gesture: str = "NONE", context_keywords: List[str] = None):
    """
    TTS ì˜¤ë””ì˜¤ì™€ ë™ê¸°í™”ëœ ë¹„ë””ì˜¤ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    
    â­ Gemini ì œì•ˆ: í”¼ë“œë°± í‰ê°€ ê¸°ëŠ¥ ì¶”ê°€
    
    Args:
        text: ë§í•˜ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©
        audio_bytes: TTSë¡œ ìƒì„±ëœ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸
        gender: ê³ ê° ì„±ë³„ ("male" ë˜ëŠ” "female")
        emotion: ê°ì • ìƒíƒœ ("NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD")
        role: ì—­í•  ("customer" ë˜ëŠ” "agent")
        autoplay: ìë™ ì¬ìƒ ì—¬ë¶€
        gesture: ì œìŠ¤ì²˜ (ì„ íƒì )
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ (ì„ íƒì )
    """
    if role == "customer":
        is_speaking = True
        if context_keywords is None:
            context_keywords = []
        
        # â­ Gemini ì œì•ˆ: ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì¶”ì²œ ë¹„ë””ì˜¤ ìš°ì„  ì‚¬ìš©
        video_path = get_video_path_by_avatar(gender, emotion, is_speaking, gesture, context_keywords)
        
        if video_path and os.path.exists(video_path):
            try:
                with open(video_path, "rb") as f:
                    video_bytes = f.read()
                
                # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ë¥¼ í•¨ê»˜ ì¬ìƒ
                # Streamlitì˜ st.videoëŠ” ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ìˆëŠ” ë¹„ë””ì˜¤ë¥¼ ì§€ì›í•©ë‹ˆë‹¤
                # ì—¬ê¸°ì„œëŠ” ë¹„ë””ì˜¤ë§Œ í‘œì‹œí•˜ê³ , ì˜¤ë””ì˜¤ëŠ” ë³„ë„ë¡œ ì¬ìƒí•©ë‹ˆë‹¤
                st.video(video_bytes, format="video/mp4", autoplay=autoplay, loop=False, muted=False)
                
                # ì˜¤ë””ì˜¤ë„ í•¨ê»˜ ì¬ìƒ (ë™ê¸°í™”)
                if audio_bytes:
                    st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
                
                # â­ Gemini ì œì•ˆ: ì‚¬ìš©ì í”¼ë“œë°± í‰ê°€ UI ì¶”ê°€ (ì±„íŒ…/ì´ë©”ì¼ íƒ­ìš©)
                if not autoplay:  # ìë™ ì¬ìƒì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ í”¼ë“œë°± UI í‘œì‹œ
                    st.markdown("---")
                    st.markdown("**ğŸ’¬ ë¹„ë””ì˜¤ ë§¤ì¹­ í‰ê°€**")
                    st.caption("ì´ ë¹„ë””ì˜¤ê°€ ê³ ê°ì˜ í…ìŠ¤íŠ¸ì™€ ê°ì •ì— ìì—°ìŠ¤ëŸ½ê²Œ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆê¹Œ?")
                    
                    feedback_key = f"video_feedback_chat_{st.session_state.get('sim_instance_id', 'default')}_{hash(text) % 10000}"
                    
                    col_rating, col_comment = st.columns([2, 3])
                    with col_rating:
                        rating = st.slider(
                            "í‰ê°€ ì ìˆ˜ (1-5ì )",
                            min_value=1,
                            max_value=5,
                            value=3,
                            key=f"{feedback_key}_rating",
                            help="1ì : ë§¤ìš° ë¶€ìì—°ìŠ¤ëŸ¬ì›€, 5ì : ë§¤ìš° ìì—°ìŠ¤ëŸ¬ì›€"
                        )
                    
                    with col_comment:
                        comment = st.text_input(
                            "ì˜ê²¬ (ì„ íƒì‚¬í•­)",
                            key=f"{feedback_key}_comment",
                            placeholder="ì˜ˆ: ë¹„ë””ì˜¤ê°€ í…ìŠ¤íŠ¸ì™€ ì˜ ë§ì•˜ìŠµë‹ˆë‹¤"
                        )
                    
                    if st.button("í”¼ë“œë°± ì œì¶œ", key=f"{feedback_key}_submit"):
                        # í”¼ë“œë°±ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        add_video_mapping_feedback(
                            customer_text=text[:200],
                            selected_video_path=video_path,
                            emotion=emotion,
                            gesture=gesture,
                            context_keywords=context_keywords,
                            user_rating=rating,
                            user_comment=comment
                        )
                        st.success(f"âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ì ìˆ˜: {rating}/5)")
                        st.info("ğŸ’¡ ì´ í”¼ë“œë°±ì€ í–¥í›„ ë¹„ë””ì˜¤ ì„ íƒ ì •í™•ë„ë¥¼ ê°œì„ í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.")
                
                return True
            except Exception as e:
                st.warning(f"ë¹„ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                # ë¹„ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨ ì‹œ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                if audio_bytes:
                    st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
                return False
        else:
            # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
            if audio_bytes:
                st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
            return False
    else:
        # ì—ì´ì „íŠ¸ëŠ” ë¹„ë””ì˜¤ ì—†ì´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
        if audio_bytes:
            st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
        return False


def generate_virtual_human_video(text: str, audio_bytes: bytes, gender: str, emotion: str, 
                                 provider: str = "hyperclova") -> bytes:
    """
    ê°€ìƒ íœ´ë¨¼ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ì— ë§ëŠ” ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    âš ï¸ ì£¼ì˜: OpenAI/Gemini APIë§Œìœ¼ë¡œëŠ” ì…ëª¨ì–‘ ë™ê¸°í™” ë¹„ë””ì˜¤ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
    ê°€ìƒ íœ´ë¨¼ ë¹„ë””ì˜¤ ìƒì„±ì€ ë³„ë„ì˜ ê°€ìƒ íœ´ë¨¼ API (ì˜ˆ: Hyperclova)ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    
    í˜„ì¬ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    
    Args:
        text: ë§í•˜ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©
        audio_bytes: TTSë¡œ ìƒì„±ëœ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸
        gender: ê³ ê° ì„±ë³„ ("male" ë˜ëŠ” "female")
        emotion: ê°ì • ìƒíƒœ ("NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD")
        provider: ê°€ìƒ íœ´ë¨¼ ì œê³µì ("hyperclova", "other")
    
    Returns:
        ìƒì„±ëœ ë¹„ë””ì˜¤ ë°”ì´íŠ¸ (ì—†ìœ¼ë©´ None)
    """
    # ê°€ìƒ íœ´ë¨¼ API í‚¤ í™•ì¸
    if provider == "hyperclova":
        api_key = get_api_key("hyperclova")
        if not api_key:
            return None
        
        # TODO: Hyperclova API ì—°ë™ êµ¬í˜„ (ë³„ë„ API í•„ìš”)
        # OpenAI/Gemini APIë§Œìœ¼ë¡œëŠ” ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, ì‹¤ì œ ê°€ìƒ íœ´ë¨¼ APIê°€ í•„ìš”í•©ë‹ˆë‹¤.
        # ì˜ˆì‹œ êµ¬ì¡°:
        # response = requests.post(
        #     "https://api.hyperclova.com/virtual-human/generate",
        #     headers={"Authorization": f"Bearer {api_key}"},
        #     json={
        #         "text": text,
        #         "audio": base64.b64encode(audio_bytes).decode(),
        #         "gender": gender,
        #         "emotion": emotion
        #     }
        # )
        # return response.content
    
    # ë‹¤ë¥¸ ì œê³µìë„ ì—¬ê¸°ì— ì¶”ê°€ ê°€ëŠ¥
    # elif provider == "other":
    #     ...
    
    return None


def get_virtual_human_config() -> Dict[str, Any]:
    """
    ê°€ìƒ íœ´ë¨¼ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ê°€ìƒ íœ´ë¨¼ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    return {
        "enabled": st.session_state.get("virtual_human_enabled", False),
        "provider": st.session_state.get("virtual_human_provider", "hyperclova"),
        "api_key": get_api_key("hyperclova") if st.session_state.get("virtual_human_provider", "hyperclova") == "hyperclova" else None
    }


# ì—­í• ë³„ TTS ìŒì„± ìŠ¤íƒ€ì¼ ì„¤ì •
TTS_VOICES = {
    "customer_male": {
        "gender": "male",
        "voice": "alloy"  # Male voice
    },
    "customer_female": {
        "gender": "female",
        "voice": "nova"  # Female voice
    },
    "customer": {
        "gender": "male",
        "voice": "alloy"  # Default male voice (fallback)
    },
    "agent": {
        "gender": "female",
        "voice": "shimmer"  # Distinct Female, Professional/Agent
    },
    "supervisor": {
        "gender": "female",
        "voice": "nova"  # Another Distinct Female, Informative/Supervisor
    }
}


def synthesize_tts(text: str, lang_key: str, role: str = "agent"):
    # lang_key ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not lang_key or lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"  # ìµœì¢… ê¸°ë³¸ê°’
    
    L = LANG.get(lang_key, LANG["ko"])  # ì•ˆì „í•œ ì ‘ê·¼
    client = st.session_state.openai_client
    if client is None:
        return None, L.get("openai_missing", "OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # â­ ìˆ˜ì •: ê³ ê° ì—­í• ì¸ ê²½ìš° ì„±ë³„ì— ë”°ë¼ ìŒì„± ì„ íƒ
    if role == "customer":
        customer_gender = st.session_state.customer_avatar.get("gender", "male")
        if customer_gender == "female":
            voice_key = "customer_female"
        else:
            voice_key = "customer_male"
        
        if voice_key in TTS_VOICES:
            voice_name = TTS_VOICES[voice_key]["voice"]
        else:
            voice_name = TTS_VOICES["customer"]["voice"]  # Fallback
    elif role in TTS_VOICES:
        voice_name = TTS_VOICES[role]["voice"]
    else:
        voice_name = TTS_VOICES["agent"]["voice"]  # Default fallback

    try:
        # â­ ìˆ˜ì •: í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œì„ ì œê±°í•˜ì—¬ ì „ì²´ ë¬¸ì˜ê°€ ì¬ìƒë˜ë„ë¡ í•¨
        # OpenAI TTSëŠ” ìµœëŒ€ 4096ìë¥¼ ì§€ì›í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë” ê¸´ í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬ ê°€ëŠ¥
        # ê³ ê°ì˜ ë¬¸ì˜ë¥¼ ëê¹Œì§€ ë‹¤ ë“¤ì–´ì•¼ ì›í™œí•œ ì‘ëŒ€ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬
        # ë§Œì•½ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ (ì˜ˆ: 10000ì ì´ìƒ) ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ,
        # ì¼ë°˜ì ì¸ ê³ ê° ë¬¸ì˜ëŠ” 4096ì ì´ë‚´ì´ë¯€ë¡œ ì „ì²´ë¥¼ ì²˜ë¦¬
        
        # tts-1 ëª¨ë¸ ì‚¬ìš© (ì•ˆì •ì„±)
        resp = client.audio.speech.create(
            model="tts-1",
            voice=voice_name,
            input=text
            # format="mp3"ì€ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.
        )
        return resp.read(), L["tts_status_success"]

    except Exception as e:
        return None, f"{L['tts_status_error']}: {e}"


# ----------------------------------------
# TTS Helper
# ----------------------------------------

def render_tts_button(text, lang_key, role="customer", prefix="", index: int = -1):
    # lang_key ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not lang_key or lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"  # ìµœì¢… ê¸°ë³¸ê°’
    
    L = LANG.get(lang_key, LANG["ko"])  # ì•ˆì „í•œ ì ‘ê·¼

    # â­ ìˆ˜ì •: index=-1ì¸ ê²½ìš°, UUIDë¥¼ ì‚¬ìš©í•˜ì—¬ safe_key ìƒì„±
    if index == -1:
        # ì´ê´€ ìš”ì•½ì²˜ëŸ¼ ì¸ë±ìŠ¤ê°€ ê³ ì •ë˜ì§€ ì•ŠëŠ” ê²½ìš°, í…ìŠ¤íŠ¸ í•´ì‹œì™€ ì„¸ì…˜ ì¸ìŠ¤í„´ìŠ¤ IDë¥¼ ì¡°í•©
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        session_id_part = st.session_state.get('sim_instance_id', 'default_session')
        # â­ ìˆ˜ì •: ì´ê´€ ìš”ì•½ì˜ ê²½ìš° ì•ˆì •ì ì¸ í‚¤ë¥¼ ìƒì„± (time.time_ns() ì œê±°í•˜ì—¬ ë§¤ë²ˆ ê°™ì€ í‚¤ ìƒì„±)
        # ì–¸ì–´ ì½”ë“œë„ ì¶”ê°€í•˜ì—¬ ì´ê´€ í›„ ì–¸ì–´ ë³€ê²½ ì‹œì—ë„ ê³ ìœ ì„± ë³´ì¥
        lang_code = st.session_state.get('language', lang_key)
        safe_key = f"{prefix}_SUMMARY_{session_id_part}_{lang_code}_{content_hash}"
    else:
        # ëŒ€í™” ë¡œê·¸ì²˜ëŸ¼ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        safe_key = f"{prefix}_{index}_{content_hash}"

    # ì¬ìƒ ë²„íŠ¼ì„ ëˆ„ë¥¼ ë•Œë§Œ TTS ìš”ì²­
    if st.button(L["button_listen_audio"], key=safe_key):
        if not st.session_state.openai_client:
            st.error(L["openai_missing"])
            return  # í‚¤ ì—†ìœ¼ë©´ ì¢…ë£Œ

        with st.spinner(L["tts_status_generating"]):
            try:
                audio_bytes, msg = synthesize_tts(text, lang_key, role=role)
                if audio_bytes:
                    # â­ st.audio í˜¸ì¶œ ì‹œ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì¬ìƒ ì‹œê°„ì„ í™•ë³´
                    # Streamlit ë¬¸ì„œ: autoplayëŠ” ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì—†ì´ëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                    try:
                        st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                        st.success(msg)
                        # â­ ìˆ˜ì •: ì¬ìƒì´ ì‹œì‘ë  ì¶©ë¶„í•œ ì‹œê°„ì„ í™•ë³´í•˜ê¸° ìœ„í•´ ëŒ€ê¸° ì‹œê°„ì„ 3ì´ˆë¡œ ëŠ˜ë¦¼
                        time.sleep(3)
                    except Exception as e:
                        st.warning(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜: {e}. ì˜¤ë””ì˜¤ íŒŒì¼ì€ ìƒì„±ë˜ì—ˆì§€ë§Œ ìë™ ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                        st.success(msg)
                else:
                    st.error(msg)
                    time.sleep(1)  # ì—ëŸ¬ ë°œìƒ ì‹œë„ ì ì‹œ ëŒ€ê¸°
            except Exception as e:
                # TTS API í˜¸ì¶œ ìì²´ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ (ë„¤íŠ¸ì›Œí¬ ë“±)
                st.error(f"âŒ TTS ìƒì„± ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                time.sleep(1)

            # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ í›„, ë¶ˆí•„ìš”í•œ ì¬ì‹¤í–‰ì„ ë§‰ê¸° ìœ„í•´ ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ
            return
        # [ì¤‘ëµ: TTS Helper ë]


# ========================================
# 4. ë¡œì»¬ ìŒì„± ê¸°ë¡ Helper
# ========================================

def load_voice_records() -> List[Dict[str, Any]]:
    return _load_json(VOICE_META_FILE, [])


def save_voice_records(records: List[Dict[str, Any]]):
    _save_json(VOICE_META_FILE, records)


def save_audio_record_local(
        audio_bytes: bytes,
        filename: str,
        transcript_text: str,
        mime_type: str = "audio/webm",
        meta: Dict[str, Any] = None,
) -> str:
    records = load_voice_records()
    rec_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    ext = filename.split(".")[-1] if "." in filename else "webm"
    audio_filename = f"{rec_id}.{ext}"
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "wb") as f:
        f.write(audio_bytes)

    rec = {
        "id": rec_id,
        "created_at": ts,
        "filename": filename,
        "audio_filename": audio_filename,
        "size": len(audio_bytes),
        "transcript": transcript_text,
        "mime_type": mime_type,
        "language": st.session_state.language,
        "meta": meta or {},
    }
    records.insert(0, rec)
    save_voice_records(records)
    return rec_id


def delete_audio_record_local(rec_id: str) -> bool:
    records = load_voice_records()
    idx = next((i for i, r in enumerate(records) if r.get("id") == rec_id), None)
    if idx is None:
        return False
    rec = records.pop(idx)
    audio_filename = rec.get("audio_filename")
    if audio_filename:
        audio_path = os.path.join(AUDIO_DIR, audio_filename)
        try:
            os.remove(audio_path)
        except FileNotFoundError:
            pass
    save_voice_records(records)
    return True


def get_audio_bytes_local(rec_id: str):
    records = load_voice_records()
    rec = next((r for r in records if r.get("id") == rec_id), None)
    if not rec:
        raise FileNotFoundError("record not found")
    audio_filename = rec["audio_filename"]
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "rb") as f:
        b = f.read()
    return b, rec


# ========================================
# 5. ë¡œì»¬ ì‹œë®¬ë ˆì´ì…˜ ì´ë ¥ Helper (ìš”ì²­ 4 ë°˜ì˜)
# ========================================


def load_simulation_histories_local(lang_key: str) -> List[Dict[str, Any]]:
    histories = _load_json(SIM_META_FILE, [])
    # í˜„ì¬ ì–¸ì–´ì™€ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ì´ë ¥ë§Œ í•„í„°ë§
    return [
        h for h in histories
        if h.get("language_key") == lang_key and (isinstance(h.get("messages"), list) or h.get("summary"))
    ]



def generate_chat_summary(messages: List[Dict[str, Any]], initial_query: str, customer_type: str,
                          current_lang_key: str) -> Dict[str, Any]:
    """ì±„íŒ… ë‚´ìš©ì„ AIë¡œ ìš”ì•½í•˜ì—¬ ì£¼ìš” ì •ë³´ì™€ ì ìˆ˜ë¥¼ ì¶”ì¶œ (ìš”ì²­ 4)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ëŒ€í™” ë‚´ìš© ì¶”ì¶œ
    conversation_text = f"Initial Query: {initial_query}\n\n"
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role in ["customer", "customer_rebuttal", "phone_exchange"]:
            conversation_text += f"Customer: {content}\n"
        elif role == "agent_response" or role == "agent":
            conversation_text += f"Agent: {content}\n"
        # supervisor ë©”ì‹œì§€ëŠ” LLMì— ì „ë‹¬í•˜ì§€ ì•Šì•„ ì—­í•  í˜¼ë™ ë°©ì§€

    # í° êµí™˜ ë¡œê·¸ëŠ” ì´ë¯¸ "Agent: ... | Customer: ..." í˜•íƒœë¡œ ê¸°ë¡ë˜ë¯€ë¡œ,
    # generate_summary_for_call í•¨ìˆ˜ì—ì„œ ë³„ë„ë¡œ ì²˜ë¦¬í•  í•„ìš” ì—†ì´,
    # ì—¬ê¸°ì„œëŠ” ë²”ìš© ì±„íŒ… ìš”ì•½ ë¡œì§ì„ ë”°ë¥´ë„ë¡ ë©”ì‹œì§€ë¥¼ ì •ì œí•©ë‹ˆë‹¤.

    summary_prompt = f"""
You are an AI analyst summarizing a customer support conversation. Your task is to extract comprehensive customer profile data and score various aspects numerically.

Analyze the conversation and provide a structured summary in JSON format (ONLY JSON, no markdown).

Extract and score:
1. Main inquiry topic (what the customer asked about)
2. Key responses provided by the agent (list of max 3 core actions/solutions)
3. Customer sentiment score (0-100, where 0=very negative, 50=neutral, 100=very positive)
4. Customer satisfaction score (0-100, based on final response)
5. Customer characteristics with detailed scoring:
   - Language preference (detected language code: ko/en/ja)
   - Cultural background hints (score 0-100, where higher = more cultural context detected)
   - Location/region (general region only, anonymize specific addresses)
   - Communication style (formal/casual, brief/detailed) with scores:
     * Formality score (0-100, 0=casual, 100=very formal)
     * Detail level score (0-100, 0=brief, 100=very detailed)
   - Customer personality traits (score each 0-100):
     * Patience level (0-100)
     * Assertiveness (0-100)
     * Politeness level (0-100)
     * Technical proficiency (0-100, if technical inquiry)
6. Privacy-sensitive information (anonymize: names, emails, phone numbers, specific addresses)
   - Extract patterns only (e.g., "email provided", "phone number provided", "resides in Asia region")
7. Customer behavior patterns:
   - Response time pattern (fast/moderate/slow based on message frequency)
   - Question complexity (simple/moderate/complex)
   - Escalation tendency (0-100, likelihood to escalate)

Output format (JSON only):
{{
  "main_inquiry": "brief description of main issue",
  "key_responses": ["response 1", "response 2"],
  "customer_sentiment_score": 75,
  "customer_satisfaction_score": 80,
  "customer_characteristics": {{
    "language": "ko/en/ja or unknown",
    "cultural_hints": "brief description or unknown",
    "cultural_score": 60,
    "region": "general region or unknown",
    "communication_style": "formal/casual/brief/detailed",
    "formality_score": 70,
    "detail_level_score": 65,
    "personality_traits": {{
      "patience_level": 60,
      "assertiveness": 70,
      "politeness_level": 80,
      "technical_proficiency": 50
    }}
  }},
  "privacy_info": {{
    "has_email": true/false,
    "has_phone": true/false,
    "has_address": true/false,
    "region_hint": "general region or unknown"
  }},
  "behavior_patterns": {{
    "response_time": "fast/moderate/slow",
    "question_complexity": "simple/moderate/complex",
    "escalation_tendency": 30
  }},
  "summary": "overall conversation summary in {lang_name}"
}}

Conversation:
{conversation_text}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        # Fallback summary with enhanced structure
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "cultural_score": 0,
                "region": "unknown",
                "communication_style": "unknown",
                "formality_score": 50,
                "detail_level_score": 50,
                "personality_traits": {
                    "patience_level": 50,
                    "assertiveness": 50,
                    "politeness_level": 50,
                    "technical_proficiency": 50
                }
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "behavior_patterns": {
                "response_time": "moderate",
                "question_complexity": "moderate",
                "escalation_tendency": 50
            },
            "summary": f"Customer inquiry about: {initial_query[:100]}"
        }

    try:
        summary_text = run_llm(summary_prompt).strip()
        # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if "```json" in summary_text:
            summary_text = summary_text.split("```json")[1].split("```")[0].strip()
        elif "```" in summary_text:
            summary_text = summary_text.split("```")[1].split("```")[0].strip()

        import json
        summary_data = json.loads(summary_text)
        return summary_data
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback with enhanced structure
        st.warning(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "cultural_score": 0,
                "region": "unknown",
                "communication_style": "unknown",
                "formality_score": 50,
                "detail_level_score": 50,
                "personality_traits": {
                    "patience_level": 50,
                    "assertiveness": 50,
                    "politeness_level": 50,
                    "technical_proficiency": 50
                }
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "behavior_patterns": {
                "response_time": "moderate",
                "question_complexity": "moderate",
                "escalation_tendency": 50
            },
            "summary": f"Error generating summary: {str(e)}"
        }



def recommend_guideline_for_customer(new_customer_summary: Dict[str, Any], histories: List[Dict[str, Any]], language: str = "ko") -> Optional[str]:
    """
    ì‹ ê·œ ê³ ê°ì˜ ë¬¸ì˜ì‚¬í•­ê³¼ ë§íˆ¬ ë“±ì„ ì¢…í•©í•˜ì—¬ ê³ ê° ì„±í–¥ ì ìˆ˜ë¥¼ ìˆ˜ì¹˜í™”í•˜ê³ ,
    ì €ì¥ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ê°€ì´ë“œë¼ì¸ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    
    Args:
        new_customer_summary: ì‹ ê·œ ê³ ê°ì˜ ìš”ì•½ ë°ì´í„° (ì„±í–¥ ì ìˆ˜ í¬í•¨)
        histories: ê³¼ê±° ê³ ê° ì‘ëŒ€ ì´ë ¥ ë¦¬ìŠ¤íŠ¸
        language: ì–¸ì–´ ì½”ë“œ
    
    Returns:
        ì¶”ì²œ ê°€ì´ë“œë¼ì¸ í…ìŠ¤íŠ¸ ë˜ëŠ” None
    """
    if not histories or not get_api_key("gemini") and not get_api_key("openai"):
        return None
    
    try:
        # ìœ ì‚¬í•œ ê³ ê° í”„ë¡œí•„ ì°¾ê¸°
        similar_customers = []
        new_scores = {
            "sentiment": new_customer_summary.get("customer_sentiment_score", 50),
            "satisfaction": new_customer_summary.get("customer_satisfaction_score", 50),
            "formality": new_customer_summary.get("customer_characteristics", {}).get("formality_score", 50),
            "patience": new_customer_summary.get("customer_characteristics", {}).get("personality_traits", {}).get("patience_level", 50),
            "assertiveness": new_customer_summary.get("customer_characteristics", {}).get("personality_traits", {}).get("assertiveness", 50),
        }
        
        for h in histories:
            if not h.get("summary") or not isinstance(h.get("summary"), dict):
                continue
            
            summary = h["summary"]
            old_scores = {
                "sentiment": summary.get("customer_sentiment_score", 50),
                "satisfaction": summary.get("customer_satisfaction_score", 50),
                "formality": summary.get("customer_characteristics", {}).get("formality_score", 50),
                "patience": summary.get("customer_characteristics", {}).get("personality_traits", {}).get("patience_level", 50),
                "assertiveness": summary.get("customer_characteristics", {}).get("personality_traits", {}).get("assertiveness", 50),
            }
            
            # ìœ ì‚¬ë„ ê³„ì‚° (ì ìˆ˜ ì°¨ì´ì˜ ì ˆëŒ€ê°’ í•©)
            similarity = sum(abs(new_scores[k] - old_scores[k]) for k in new_scores.keys())
            
            if similarity < 100:  # ì„ê³„ê°’ ì´í•˜ì¸ ê²½ìš° ìœ ì‚¬ ê³ ê°ìœ¼ë¡œ ê°„ì£¼
                similar_customers.append({
                    "history": h,
                    "similarity": similarity,
                    "scores": old_scores
                })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        similar_customers.sort(key=lambda x: x["similarity"])
        
        # ìƒìœ„ 5ê°œ ìœ ì‚¬ ê³ ê°ì˜ ì„±ê³µ ì‚¬ë¡€ ë¶„ì„
        if similar_customers:
            lang_name = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}.get(language, "í•œêµ­ì–´")
            
            similar_cases_text = json.dumps([
                {
                    "initial_query": c["history"].get("initial_query", ""),
                    "key_responses": c["history"].get("summary", {}).get("key_responses", []),
                    "scores": c["scores"],
                    "satisfaction": c["history"].get("summary", {}).get("customer_satisfaction_score", 50)
                }
                for c in similar_customers[:5]
            ], ensure_ascii=False, indent=2)
            
            recommendation_prompt = (
                f"ë‹¹ì‹ ì€ CS ì„¼í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹ ê·œ ê³ ê°ì˜ ì„±í–¥ ì ìˆ˜ë¥¼ ë¶„ì„í•˜ê³ , ìœ ì‚¬í•œ ê³¼ê±° ê³ ê°ë“¤ì˜ ì„±ê³µ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸ì„ ì¶”ì²œí•˜ì„¸ìš”.\n\n"
                f"ì‹ ê·œ ê³ ê° í”„ë¡œí•„:\n{json.dumps(new_customer_summary, ensure_ascii=False, indent=2)}\n\n"
                f"ìœ ì‚¬í•œ ê³¼ê±° ê³ ê° ì‚¬ë¡€ (ìƒìœ„ 5ê°œ):\n{similar_cases_text}\n\n"
                f"ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ {lang_name}ë¡œ ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•˜ì„¸ìš”:\n"
                f"1. ê³ ê° ì„±í–¥ ë¶„ì„ (ì ìˆ˜ ê¸°ë°˜)\n"
                f"2. ì˜ˆìƒë˜ëŠ” ê³ ê° ë°˜ì‘ íŒ¨í„´\n"
                f"3. íš¨ê³¼ì ì¸ ì‘ëŒ€ ì „ëµ (ìœ ì‚¬ ì‚¬ë¡€ ê¸°ë°˜)\n"
                f"4. ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­\n"
                f"5. ê¶Œì¥ ì‘ëŒ€ í†¤ ë° ìŠ¤íƒ€ì¼\n\n"
                f"ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•˜ì„¸ìš”."
            )
            
            recommendation = run_llm(recommendation_prompt)
            return recommendation if recommendation and not recommendation.startswith("âŒ") else None
        
        return None
        
    except Exception as e:
        print(f"ê°€ì´ë“œë¼ì¸ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def generate_daily_customer_guide(histories: List[Dict[str, Any]], language: str = "ko") -> Optional[str]:
    """
    ì¼ì¼ ê³ ê° ê°€ì´ë“œ ìƒì„± í•¨ìˆ˜
    ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ê° ê°€ì´ë“œë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        histories: ê³ ê° ì‘ëŒ€ ì´ë ¥ ë¦¬ìŠ¤íŠ¸
        language: ì–¸ì–´ ì½”ë“œ (ko, en, ja)
    
    Returns:
        ìƒì„±ëœ ê°€ì´ë“œ ë‚´ìš© (ë¬¸ìì—´) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    if not histories or not get_api_key("gemini") and not get_api_key("openai"):
        return None
    
    try:
        # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì´ë ¥ë§Œ í•„í„°ë§
        histories_with_summary = [h for h in histories if h.get("summary") and isinstance(h.get("summary"), dict)]
        
        if not histories_with_summary:
            return None
        
        # ìµœê·¼ 50ê°œ ì´ë ¥ ì‚¬ìš© (ë™ì¼ ê³ ê° ë°ì´í„° ëˆ„ì ì„ ìœ„í•´ ì „ì²´ ì´ë ¥ ì‚¬ìš©)
        recent_histories = histories_with_summary[:50]
        
        # ê³ ê°ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™” (ë™ì¼ ê³ ê°ì˜ ìƒí™©ì— ë”°ë¼ ë°ì´í„° ëˆ„ì )
        customer_data_map = {}
        for h in recent_histories:
            customer_id = h.get("id", "")
            customer_type = h.get("customer_type", "")
            summary = h.get("summary", {})
            
            if customer_id not in customer_data_map:
                customer_data_map[customer_id] = {
                    "customer_type": customer_type,
                    "histories": [],
                    "total_interactions": 0
                }
            
            customer_data_map[customer_id]["histories"].append({
                "initial_query": h.get("initial_query", ""),
                "summary": summary,
                "timestamp": h.get("timestamp", ""),
                "language": h.get("language_key", language)
            })
            customer_data_map[customer_id]["total_interactions"] += 1
        
        # LLMì„ ì‚¬ìš©í•˜ì—¬ ê³ ê° ê°€ì´ë“œ ìƒì„±
        lang_name = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}.get(language, "í•œêµ­ì–´")
        
        guide_prompt = (
            f"ë‹¹ì‹ ì€ CS ì„¼í„° êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê³ ê° ì‘ëŒ€ ì´ë ¥ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ê³ ê° ì‘ëŒ€ ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            f"ë¶„ì„í•  ì´ë ¥ ë°ì´í„° (ê³ ê°ë³„ ëˆ„ì  ë°ì´í„° í¬í•¨):\n{json.dumps(list(customer_data_map.values())[:20], ensure_ascii=False, indent=2)}\n\n"
            f"ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ê°€ì´ë“œë¼ì¸ì„ {lang_name}ë¡œ ì‘ì„±í•˜ì„¸ìš”:\n"
            f"1. ê³ ê° ìœ í˜•ë³„ ì‘ëŒ€ ì „ëµ (ì¼ë°˜/ê¹Œë‹¤ë¡œìš´/ë§¤ìš° ë¶ˆë§Œì¡±)\n"
            f"2. ë¬¸í™”ê¶Œë³„ ì‘ëŒ€ ê°€ì´ë“œ (ì–¸ì–´, ë¬¸í™”ì  ë°°ê²½ ê³ ë ¤)\n"
            f"3. ì£¼ìš” ë¬¸ì˜ ìœ í˜•ë³„ í•´ê²° ë°©ë²•\n"
            f"4. ê³ ê° ê°ì • ì ìˆ˜ì— ë”°ë¥¸ ì‘ëŒ€ ì „ëµ\n"
            f"5. ê°œì¸ì •ë³´ ì²˜ë¦¬ ê°€ì´ë“œ\n"
            f"6. íš¨ê³¼ì ì¸ ì†Œí†µ ìŠ¤íƒ€ì¼ ê¶Œì¥ì‚¬í•­\n"
            f"7. ë™ì¼ ê³ ê°ì˜ ë°˜ë³µ ë¬¸ì˜ì— ëŒ€í•œ ëŒ€ì‘ ì „ëµ\n"
            f"8. ê°•ì„± ê³ ê° ê°€ì´ë“œë¼ì¸ (ê¹Œë‹¤ë¡œìš´ ê³ ê°, ë§¤ìš° ë¶ˆë§Œì¡± ê³ ê°)\n\n"
            f"ê°€ì´ë“œë¼ì¸ì„ {lang_name}ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì‹¤ì œ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
        )
        
        guide_content = run_llm(guide_prompt)
        
        if not guide_content or guide_content.startswith("âŒ"):
            return None
        
        # ê°€ì´ë“œ ë‚´ìš© í¬ë§·íŒ…
        today_str = datetime.now().strftime("%y%m%d")
        formatted_guide = (
            f"ê³ ê° ì‘ëŒ€ ê°€ì´ë“œë¼ì¸\n"
            f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"ë¶„ì„ ì´ë ¥ ìˆ˜: {len(recent_histories)}\n"
            f"ê³ ê° ìˆ˜: {len(customer_data_map)}\n"
            f"=" * 80 + "\n\n"
            f"{guide_content}\n\n"
            f"=" * 80 + "\n"
            f"ì´ ê°€ì´ë“œëŠ” AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            f"ê³ ê° ë°ì´í„°ê°€ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤."
        )
        
        return formatted_guide
        
    except Exception as e:
        print(f"ê³ ê° ê°€ì´ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def save_daily_customer_guide(guide_content: str, language: str = "ko") -> Optional[str]:
    """
    ì¼ì¼ ê³ ê° ê°€ì´ë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        guide_content: ê°€ì´ë“œ ë‚´ìš©
        language: ì–¸ì–´ ì½”ë“œ
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    try:
        today_str = datetime.now().strftime("%y%m%d")
        guide_filename = f"{today_str}_ê³ ê°ê°€ì´ë“œ.TXT"
        guide_filepath = os.path.join(DATA_DIR, guide_filename)
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¶”ê°€ (ë™ì¼ ê³ ê° ë°ì´í„° ëˆ„ì )
        if os.path.exists(guide_filepath):
            with open(guide_filepath, "r", encoding="utf-8") as f:
                existing_content = f.read()
            
            # ìƒˆ ë‚´ìš©ì„ ê¸°ì¡´ ë‚´ìš©ì— ì¶”ê°€
            updated_content = (
                f"{existing_content}\n\n"
                f"{'=' * 80}\n"
                f"ì—…ë°ì´íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"{'=' * 80}\n\n"
                f"{guide_content}"
            )
            
            with open(guide_filepath, "w", encoding="utf-8") as f:
                f.write(updated_content)
        else:
            # ìƒˆ íŒŒì¼ ìƒì„±
            with open(guide_filepath, "w", encoding="utf-8") as f:
                f.write(guide_content)
        
        return guide_filepath
        
    except Exception as e:
        print(f"ê³ ê° ê°€ì´ë“œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def get_daily_data_statistics(language: str = "ko") -> Dict[str, Any]:
    """
    ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    í•˜ë£¨ì— ìµœì†Œ 1ê°œ ì´ìƒì”© ìµœì†Œ 5ì¸ì˜ ë°ì´í„° í™•ë³´ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.
    
    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    histories = _load_json(SIM_META_FILE, [])
    today = datetime.now().date()
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ ì´ë ¥ í•„í„°ë§
    today_histories = []
    for h in histories:
        try:
            ts = datetime.fromisoformat(h.get("timestamp", "")).date()
            if ts == today and h.get("summary") and isinstance(h.get("summary"), dict):
                today_histories.append(h)
        except:
            continue
    
    # ê³ ìœ  ê³ ê° ìˆ˜ ê³„ì‚° (ì´ë©”ì¼/ì „í™”ë²ˆí˜¸ ê¸°ë°˜ ë˜ëŠ” ID ê¸°ë°˜)
    unique_customers = set()
    for h in today_histories:
        # ê³ ê° ID ë˜ëŠ” ì´ˆê¸° ë¬¸ì˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  ê³ ê° ì‹ë³„
        customer_id = h.get("id", "")
        initial_query = h.get("initial_query", "")
        # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ê³ ìœ  ê³ ê° ì‹ë³„
        customer_hash = hashlib.md5(f"{customer_id}_{initial_query[:50]}".encode()).hexdigest()
        unique_customers.add(customer_hash)
    
    return {
        "date": today.isoformat(),
        "total_cases": len(today_histories),
        "unique_customers": len(unique_customers),
        "target_met": len(unique_customers) >= 5,  # ìµœì†Œ 5ì¸ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        "cases_with_summary": len([h for h in today_histories if h.get("summary")])
    }


def save_simulation_history_local(initial_query: str, customer_type: str, messages: List[Dict[str, Any]],
                                  is_chat_ended: bool, attachment_context: str, is_call: bool = False):
    """AI ìš”ì•½ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì´ë ¥ì„ ì €ì¥ (ìš”ì²­ 4 ë°˜ì˜)"""
    histories = _load_json(SIM_META_FILE, [])
    doc_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    # AI ìš”ì•½ ìƒì„± (ì±„íŒ… ì¢…ë£Œ ì‹œ ë˜ëŠ” ì¶©ë¶„í•œ ëŒ€í™”ê°€ ìˆì„ ë•Œ)
    summary_data = None
    if is_chat_ended or len(messages) > 4 or is_call:  # ì „í™” í†µí™”ëŠ” ë°”ë¡œ ìš”ì•½ ì‹œë„
        summary_data = generate_chat_summary(messages, initial_query, customer_type, st.session_state.language)

    # ìš”ì•½ ë°ì´í„°ê°€ ìƒì„±ëœ ê²½ìš°ì—ë§Œ ì €ì¥ (ìš”ì•½ ì¤‘ì‹¬ ì €ì¥)
    if summary_data:
        # ìš”ì•½ ë°ì´í„°ì— ì´ˆê¸° ë¬¸ì˜ì™€ í•µì‹¬ ì •ë³´ í¬í•¨
        data = {
            "id": doc_id,
            "initial_query": initial_query,  # ì´ˆê¸° ë¬¸ì˜ëŠ” ìœ ì§€
            "customer_type": customer_type,
            "messages": [],  # ì „ì²´ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ìš”ì•½ë§Œ ì €ì¥)
            "summary": summary_data,  # AI ìš”ì•½ ë°ì´í„° (ì£¼ìš” ì €ì¥ ë‚´ìš©)
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",  # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸
            "is_call": is_call,  # ì „í™” ì—¬ë¶€ í”Œë˜ê·¸
        }
    else:
        # ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° (ì§„í–‰ ì¤‘ì¸ ëŒ€í™”), ìµœì†Œí•œì˜ ì •ë³´ë§Œ ì €ì¥
        data = {
            "id": doc_id,
            "initial_query": initial_query,
            "customer_type": customer_type,
            "messages": messages[:10] if len(messages) > 10 else messages,  # ìµœê·¼ 10ê°œë§Œ ì €ì¥
            "summary": None,  # ìš”ì•½ ì—†ìŒ
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",
            "is_call": is_call,
        }

    # ê¸°ì¡´ ì´ë ¥ì— ì¶”ê°€ (ìµœì‹ ìˆœ)
    histories.insert(0, data)
    # ë„ˆë¬´ ë§ì€ ì´ë ¥ ë°©ì§€ (ì˜ˆ: 100ê°œë¡œ ì¦ê°€ - ìš”ì•½ë§Œ ì €ì¥í•˜ë¯€ë¡œ ìš©ëŸ‰ ë¶€ë‹´ ì ìŒ)
    _save_json(SIM_META_FILE, histories[:100])
    
    # â­ ì¶”ê°€: ê³ ê° ë°ì´í„°ê°€ ì €ì¥ë  ë•Œë§ˆë‹¤ ìë™ìœ¼ë¡œ ì¼ì¼ ê³ ê° ê°€ì´ë“œ ìƒì„±
    if summary_data and is_chat_ended:  # ì±„íŒ…ì´ ì¢…ë£Œë˜ê³  ìš”ì•½ì´ ìƒì„±ëœ ê²½ìš°ì—ë§Œ ê°€ì´ë“œ ìƒì„±
        try:
            # ì „ì²´ ì´ë ¥ ë¡œë“œ (ë™ì¼ ê³ ê° ë°ì´í„° ëˆ„ì ì„ ìœ„í•´)
            all_histories = _load_json(SIM_META_FILE, [])
            
            # ì˜¤ëŠ˜ ë‚ ì§œì˜ ê°€ì´ë“œê°€ ì´ë¯¸ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            today_str = datetime.now().strftime("%y%m%d")
            guide_filename = f"{today_str}_ê³ ê°ê°€ì´ë“œ.TXT"
            guide_filepath = os.path.join(DATA_DIR, guide_filename)
            
            # ê°€ì´ë“œ ìƒì„± (ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
            guide_content = generate_daily_customer_guide(all_histories, st.session_state.language)
            
            if guide_content:
                saved_path = save_daily_customer_guide(guide_content, st.session_state.language)
                if saved_path:
                    print(f"âœ… ê³ ê° ê°€ì´ë“œê°€ ìë™ ìƒì„±/ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤: {saved_path}")
        except Exception as e:
            print(f"ê³ ê° ê°€ì´ë“œ ìë™ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œë¨): {e}")
    
    return doc_id


def delete_all_history_local():
    _save_json(SIM_META_FILE, [])


# ========================================
# DB ì €ì¥ ê¸°ëŠ¥ (Word/PPTX/PDF)
# ========================================

def export_history_to_word(histories: List[Dict[str, Any]], filename: str = None, lang: str = "ko") -> str:
    """ì´ë ¥ì„ Word íŒŒì¼ë¡œ ì €ì¥"""
    if not IS_DOCX_AVAILABLE:
        raise ImportError("Word ì €ì¥ì„ ìœ„í•´ python-docxê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install python-docx")
    
    # ì–¸ì–´ ì„¤ì • í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    if lang not in ["ko", "en", "ja"]:
        lang = "ko"
    L = LANG.get(lang, LANG["ko"])
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
    filepath = os.path.join(DATA_DIR, filename)
    
    doc = Document()
    
    # ì œëª© ì¶”ê°€
    title = doc.add_heading(L.get("download_history_title", "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½"), 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ê° ì´ë ¥ ì¶”ê°€
    for i, hist in enumerate(histories, 1):
        # ì´ë ¥ ì œëª©
        doc.add_heading(f'{L.get("download_history_number", "ì´ë ¥ #")}{i}', level=1)
        
        # ê¸°ë³¸ ì •ë³´
        doc.add_paragraph(f'ID: {hist.get("id", "N/A")}')
        doc.add_paragraph(f'{L.get("date_label", "ë‚ ì§œ")}: {hist.get("timestamp", "N/A")}')
        doc.add_paragraph(f'{L.get("download_initial_inquiry", "ì´ˆê¸° ë¬¸ì˜")}: {hist.get("initial_query", "N/A")}')
        doc.add_paragraph(f'{L.get("customer_type_label", "ê³ ê° ìœ í˜•")}: {hist.get("customer_type", "N/A")}')
        doc.add_paragraph(f'{L.get("language_label", "ì–¸ì–´")}: {hist.get("language_key", "N/A")}')
        
        summary = hist.get('summary', {})
        if summary:
            # ìš”ì•½ ì„¹ì…˜
            doc.add_heading(L.get("download_summary", "ìš”ì•½"), level=2)
            doc.add_paragraph(f'{L.get("download_main_inquiry", "ì£¼ìš” ë¬¸ì˜")}: {summary.get("main_inquiry", "N/A")}')
            doc.add_paragraph(f'{L.get("download_key_response", "í•µì‹¬ ì‘ë‹µ")}: {", ".join(summary.get("key_responses", []))}')
            doc.add_paragraph(f'{L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜")}: {summary.get("customer_sentiment_score", "N/A")}/100')
            doc.add_paragraph(f'{L.get("customer_satisfaction_score_label", "ê³ ê° ë§Œì¡±ë„ ì ìˆ˜")}: {summary.get("customer_satisfaction_score", "N/A")}/100')
            
            # ê³ ê° íŠ¹ì„±
            characteristics = summary.get('customer_characteristics', {})
            doc.add_heading(L.get("download_customer_characteristics", "ê³ ê° íŠ¹ì„±"), level=2)
            doc.add_paragraph(f'{L.get("language_label", "ì–¸ì–´")}: {characteristics.get("language", "N/A")}')
            doc.add_paragraph(f'{L.get("download_cultural_background", "ë¬¸í™”ì  ë°°ê²½")}: {characteristics.get("cultural_hints", "N/A")}')
            doc.add_paragraph(f'{L.get("region_label", "ì§€ì—­")}: {characteristics.get("region", "N/A")}')
            doc.add_paragraph(f'{L.get("download_communication_style", "ì†Œí†µ ìŠ¤íƒ€ì¼")}: {characteristics.get("communication_style", "N/A")}')
            
            # ê°œì¸ì •ë³´ ìš”ì•½
            privacy = summary.get('privacy_info', {})
            doc.add_heading(L.get("download_privacy_summary", "ê°œì¸ì •ë³´ ìš”ì•½"), level=2)
            doc.add_paragraph(f'{L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_email") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            doc.add_paragraph(f'{L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_phone") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            doc.add_paragraph(f'{L.get("download_address_provided", "ì£¼ì†Œ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_address") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            doc.add_paragraph(f'{L.get("download_region_hint", "ì§€ì—­ íŒíŠ¸")}: {privacy.get("region_hint", "N/A")}')
            
            # ì „ì²´ ìš”ì•½
            doc.add_paragraph(f'{L.get("download_overall_summary", "ì „ì²´ ìš”ì•½")}: {summary.get("summary", "N/A")}')
        
        # êµ¬ë¶„ì„ 
        if i < len(histories):
            doc.add_paragraph('-' * 80)
    
    doc.save(filepath)
    return filepath



def export_history_to_pptx(histories: List[Dict[str, Any]], filename: str = None, lang: str = "ko") -> str:
    """ì´ë ¥ì„ PPTX íŒŒì¼ë¡œ ì €ì¥"""
    if not IS_PPTX_AVAILABLE:
        raise ImportError("PPTX ì €ì¥ì„ ìœ„í•´ python-pptxê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install python-pptx")
    
    # ì–¸ì–´ ì„¤ì • í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    if lang not in ["ko", "en", "ja"]:
        lang = "ko"
    L = LANG.get(lang, LANG["ko"])
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx"
    filepath = os.path.join(DATA_DIR, filename)
    
    prs = Presentation()
    prs.slide_width = Inches(10)
    prs.slide_height = Inches(7.5)
    
    # ì œëª© ìŠ¬ë¼ì´ë“œ
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    title = slide.shapes.title
    subtitle = slide.placeholders[1]
    title.text = L.get("download_history_title", "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½")
    subtitle.text = f"{L.get('download_created_date', 'ìƒì„±ì¼')}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    
    # ê° ì´ë ¥ì— ëŒ€í•´ ìŠ¬ë¼ì´ë“œ ìƒì„±
    for i, hist in enumerate(histories, 1):
        # ì œëª© ë° ë‚´ìš© ë ˆì´ì•„ì›ƒ
        bullet_slide_layout = prs.slide_layouts[1]
        slide = prs.slides.add_slide(bullet_slide_layout)
        shapes = slide.shapes
        
        title_shape = shapes.title
        body_shape = shapes.placeholders[1]
        
        title_shape.text = f"{L.get('download_history_number', 'ì´ë ¥ #')}{i}"
        
        tf = body_shape.text_frame
        tf.text = f"ID: {hist.get('id', 'N/A')}"
        
        p = tf.add_paragraph()
        p.text = f"{L.get('date_label', 'ë‚ ì§œ')}: {hist.get('timestamp', 'N/A')}"
        p.level = 0
        
        p = tf.add_paragraph()
        p.text = f"{L.get('download_initial_inquiry', 'ì´ˆê¸° ë¬¸ì˜')}: {hist.get('initial_query', 'N/A')}"
        p.level = 0
        
        p = tf.add_paragraph()
        p.text = f"{L.get('customer_type_label', 'ê³ ê° ìœ í˜•')}: {hist.get('customer_type', 'N/A')}"
        p.level = 0
        
        summary = hist.get('summary', {})
        if summary:
            p = tf.add_paragraph()
            p.text = f"{L.get('download_main_inquiry', 'ì£¼ìš” ë¬¸ì˜')}: {summary.get('main_inquiry', 'N/A')}"
            p.level = 0
            
            p = tf.add_paragraph()
            p.text = f"{L.get('sentiment_score_label', 'ê³ ê° ê°ì • ì ìˆ˜')}: {summary.get('customer_sentiment_score', 'N/A')}/100"
            p.level = 0
            
            p = tf.add_paragraph()
            p.text = f"{L.get('customer_satisfaction_score_label', 'ê³ ê° ë§Œì¡±ë„ ì ìˆ˜')}: {summary.get('customer_satisfaction_score', 'N/A')}/100"
            p.level = 0
    
    prs.save(filepath)
    return filepath



def export_history_to_pdf(histories: List[Dict[str, Any]], filename: str = None, lang: str = "ko") -> str:
    """ì´ë ¥ì„ PDF íŒŒì¼ë¡œ ì €ì¥ (í•œê¸€/ì¼ë³¸ì–´ ì¸ì½”ë”© ì§€ì› ê°•í™”)"""
    if not IS_REPORTLAB_AVAILABLE:
        raise ImportError("PDF ì €ì¥ì„ ìœ„í•´ reportlabì´ í•„ìš”í•©ë‹ˆë‹¤: pip install reportlab")
    
    # ì–¸ì–´ ì„¤ì • í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    if lang not in ["ko", "en", "ja"]:
        lang = "ko"
    L = LANG.get(lang, LANG["ko"])
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
    filepath = os.path.join(DATA_DIR, filename)
    
    # â­ ê°œì„ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ ì§€ì› ê°•í™” - ë‘˜ ë‹¤ ë“±ë¡í•˜ì—¬ í˜¼í•© ì‚¬ìš© ê°€ëŠ¥
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    
    # í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ ìƒíƒœ
    korean_font_registered = False
    japanese_font_registered = False
    korean_font_name = 'KoreanFont'
    japanese_font_name = 'JapaneseFont'
    
    def register_font(font_name: str, font_path: str) -> bool:
        """í°íŠ¸ë¥¼ ë“±ë¡í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        try:
            if font_path.endswith('.ttf'):
                # TTF íŒŒì¼ ë“±ë¡
                font = TTFont(font_name, font_path)
                pdfmetrics.registerFont(font)
                if font_name in pdfmetrics.getRegisteredFontNames():
                    return True
            elif font_path.endswith('.ttc'):
                # TTC íŒŒì¼ ì²˜ë¦¬ (ì—¬ëŸ¬ ì„œë¸Œí°íŠ¸ ì‹œë„)
                for subfont_idx in range(8):  # ì„œë¸Œí°íŠ¸ ì¸ë±ìŠ¤ í™•ëŒ€ (0-7)
                    try:
                        font = TTFont(font_name, font_path, subfontIndex=subfont_idx)
                        pdfmetrics.registerFont(font)
                        if font_name in pdfmetrics.getRegisteredFontNames():
                            return True
                    except Exception:
                        continue
            return False
        except Exception as e:
            print(f"âš ï¸ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ({font_name}, {font_path}): {e}")
            return False
    
    try:
        # ìš´ì˜ì²´ì œë³„ í°íŠ¸ ê²½ë¡œ ì„¤ì •
        import platform
        system = platform.system()
        
        if system == 'Windows':
            # Windows ê¸°ë³¸ í•œê¸€ í°íŠ¸ ê²½ë¡œ (ìš°ì„ ìˆœìœ„ ìˆœ)
            korean_font_paths = [
                "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”• (TTF)
                "C:/Windows/Fonts/malgunsl.ttf",  # ë§‘ì€ ê³ ë”• (TTF, ëŒ€ì²´)
                "C:/Windows/Fonts/NanumGothic.ttf",  # ë‚˜ëˆ”ê³ ë”•
                "C:/Windows/Fonts/NanumBarunGothic.ttf",  # ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•
                "C:/Windows/Fonts/NanumGothicBold.ttf",  # ë‚˜ëˆ”ê³ ë”• ë³¼ë“œ
                "C:/Windows/Fonts/gulim.ttc",  # êµ´ë¦¼ (TTC)
                "C:/Windows/Fonts/batang.ttc",  # ë°”íƒ• (TTC)
                "C:/Windows/Fonts/malgun.ttc",  # ë§‘ì€ ê³ ë”• (TTC)
                "C:/Windows/Fonts/NanumGothic.ttc",  # ë‚˜ëˆ”ê³ ë”• (TTC)
            ]
            
            # Windows ì¼ë³¸ì–´ í°íŠ¸ ê²½ë¡œ (í•œì ì§€ì› ê°•í™”)
            japanese_font_paths = [
                "C:/Windows/Fonts/msgothic.ttc",  # MS Gothic (ì¼ë³¸ì–´ í•œì ì§€ì›)
                "C:/Windows/Fonts/msmincho.ttc",  # MS Mincho (ì¼ë³¸ì–´ í•œì ì§€ì›)
                "C:/Windows/Fonts/meiryo.ttc",  # Meiryo (ì¼ë³¸ì–´)
                "C:/Windows/Fonts/yuanti.ttc",  # Microsoft YaHei (ì¤‘êµ­ì–´/ì¼ë³¸ì–´ í•œì ì§€ì›)
                "C:/Windows/Fonts/notosanscjksc-regular.otf",  # Noto Sans CJK (í•œì¤‘ì¼ í†µí•©)
            ]
        elif system == 'Darwin':  # macOS
            korean_font_paths = [
                "/System/Library/Fonts/Supplemental/AppleGothic.ttf",
                "/Library/Fonts/AppleGothic.ttf",
                "/System/Library/Fonts/AppleGothic.ttc",
            ]
            japanese_font_paths = [
                "/System/Library/Fonts/Supplemental/AppleGothic.ttf",  # í•œì¤‘ì¼ í†µí•©
                "/System/Library/Fonts/Hiragino Sans GB.ttc",
                "/System/Library/Fonts/STHeiti Light.ttc",
            ]
        else:  # Linux
            korean_font_paths = [
                "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
                "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            ]
            japanese_font_paths = [
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",  # í•œì¤‘ì¼ í†µí•©
                "/usr/share/fonts/truetype/takao/TakaoGothic.ttf",
            ]
        
        # í•œê¸€ í°íŠ¸ ë“±ë¡ (ëª¨ë“  ê²½ë¡œ ì‹œë„)
        for font_path in korean_font_paths:
            if os.path.exists(font_path):
                if register_font(korean_font_name, font_path):
                    korean_font_registered = True
                    print(f"âœ… í•œê¸€ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                    break
        
        # ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ (í•œê¸€ê³¼ ë…ë¦½ì ìœ¼ë¡œ ë“±ë¡ - ë‘˜ ë‹¤ ì‚¬ìš© ê°€ëŠ¥)
        for font_path in japanese_font_paths:
            if os.path.exists(font_path):
                if register_font(japanese_font_name, font_path):
                    japanese_font_registered = True
                    print(f"âœ… ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                    break
        
        # í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ê²½ê³ 
        if not korean_font_registered and not japanese_font_registered:
            print("âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDFì—ì„œ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print(f"   ì‹œìŠ¤í…œ: {system}")
            print("   ë“±ë¡ëœ í°íŠ¸ ëª©ë¡:", pdfmetrics.getRegisteredFontNames())
            if system == 'Windows':
                print("   í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”: C:/Windows/Fonts/")
            elif system == 'Darwin':
                print("   í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”: /System/Library/Fonts/")
            else:
                print("   í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”: /usr/share/fonts/")
            
    except Exception as e:
        error_msg = str(e)
        print(f"âš ï¸ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨: {error_msg}")
        korean_font_registered = False
        japanese_font_registered = False
    
    doc = SimpleDocTemplate(filepath, pagesize=letter)
    story = []
    styles = getSampleStyleSheet()
    
    # â­ ê°œì„ : í…ìŠ¤íŠ¸ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ í°íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” ìŠ¤íƒ€ì¼ ìƒì„± í•¨ìˆ˜
    def get_multilingual_style(base_style_name, default_font=None, **kwargs):
        """ë‹¤êµ­ì–´ ì§€ì› ìŠ¤íƒ€ì¼ ìƒì„± (í•œê¸€/ì¼ë³¸ì–´/ì˜ì–´)"""
        base_style = styles[base_style_name]
        style_kwargs = {
            'parent': base_style,
            **kwargs
        }
        
        # ê¸°ë³¸ í°íŠ¸ ì„¤ì • (í•œê¸€ ìš°ì„ , ì—†ìœ¼ë©´ ì¼ë³¸ì–´, ì—†ìœ¼ë©´ ê¸°ë³¸)
        registered_fonts = pdfmetrics.getRegisteredFontNames()
        if default_font and default_font in registered_fonts:
            style_kwargs['fontName'] = default_font
        elif korean_font_registered and korean_font_name in registered_fonts:
            style_kwargs['fontName'] = korean_font_name
        elif japanese_font_registered and japanese_font_name in registered_fonts:
            style_kwargs['fontName'] = japanese_font_name
        elif not korean_font_registered and not japanese_font_registered:
            print("âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return ParagraphStyle(f'Multilingual{base_style_name}', **style_kwargs)
    
    # ì œëª© ìŠ¤íƒ€ì¼ (í•œê¸€ í°íŠ¸ ìš°ì„  ì‚¬ìš©)
    title_style = get_multilingual_style(
        'Heading1',
        fontSize=24,
        textColor=black,
        spaceAfter=30,
        alignment=1,  # ì¤‘ì•™ ì •ë ¬
        default_font=korean_font_name if korean_font_registered else japanese_font_name
    )
    
    # ì¼ë°˜ í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
    normal_style = get_multilingual_style('Normal')
    heading1_style = get_multilingual_style('Heading1')
    heading2_style = get_multilingual_style('Heading2')
    
    # â­ ê°œì„ : í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ê³  ì ì ˆí•œ í°íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    def safe_text(text, detect_font=True):
        """í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ PDFì— í‘œì‹œ (í•œê¸€/ì¼ë³¸ì–´/í•œì ì§€ì› ê°•í™”)
        
        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            detect_font: í…ìŠ¤íŠ¸ ë‚´ìš©ì— ë”°ë¼ í°íŠ¸ë¥¼ ìë™ ì„ íƒí• ì§€ ì—¬ë¶€
        
        Returns:
            (ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸, ì¶”ì²œ í°íŠ¸ëª…) íŠœí”Œ
        """
        if text is None:
            return ("N/A", None)
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ëª…ì‹œì  ì²˜ë¦¬)
        text_str = None
        if isinstance(text, bytes):
            # ë°”ì´íŠ¸ ë¬¸ìì—´ì¸ ê²½ìš° UTF-8ë¡œ ë””ì½”ë”© ì‹œë„
            try:
                text_str = text.decode('utf-8', errors='replace')
            except:
                try:
                    # UTF-8 ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
                    text_str = text.decode('cp949', errors='replace')  # í•œêµ­ì–´ Windows ì¸ì½”ë”©
                except:
                    try:
                        text_str = text.decode('shift_jis', errors='replace')  # ì¼ë³¸ì–´ ì¸ì½”ë”©
                    except:
                        try:
                            text_str = text.decode('euc-kr', errors='replace')  # í•œêµ­ì–´ EUC-KR
                        except:
                            text_str = text.decode('latin-1', errors='replace')
        else:
            text_str = str(text)
        
        # None ì²´í¬
        if text_str is None:
            return ("N/A", None)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFC í˜•ì‹ìœ¼ë¡œ í†µì¼) - í•œê¸€/ì¼ë³¸ì–´ ë¬¸ì ì •í™•ë„ í–¥ìƒ
        try:
            import unicodedata
            text_str = unicodedata.normalize('NFC', text_str)
        except:
            pass
        
        # íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ (HTML ì—”í‹°í‹°ë¡œ ë³€í™˜) - ReportLab ParagraphëŠ” HTMLì„ ì§€ì›
        # í•˜ì§€ë§Œ &ëŠ” ë¨¼ì € ì²˜ë¦¬í•´ì•¼ í•¨
        text_str = text_str.replace('&', '&amp;')
        text_str = text_str.replace('<', '&lt;')
        text_str = text_str.replace('>', '&gt;')
        text_str = text_str.replace('"', '&quot;')
        text_str = text_str.replace("'", '&#39;')
        
        # í°íŠ¸ ì„ íƒ ë¡œì§ (í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„)
        recommended_font = None
        if detect_font:
            try:
                # ìœ ë‹ˆì½”ë“œ ë²”ìœ„ í™•ì¸
                # í•œê¸€: AC00-D7AF (ì™„ì„±í˜•), 1100-11FF (ìëª¨)
                # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜: 3040-309F, ê°€íƒ€ì¹´ë‚˜: 30A0-30FF, í•œì: 4E00-9FFF
                has_korean = any('\uAC00' <= char <= '\uD7AF' or '\u1100' <= char <= '\u11FF' for char in text_str)
                has_japanese = any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FFF' for char in text_str)
                
                registered_fonts = pdfmetrics.getRegisteredFontNames()
                
                if has_korean and korean_font_registered and korean_font_name in registered_fonts:
                    recommended_font = korean_font_name
                elif has_japanese and japanese_font_registered and japanese_font_name in registered_fonts:
                    recommended_font = japanese_font_name
                elif has_korean or has_japanese:
                    # í•œê¸€/ì¼ë³¸ì–´ ë¬¸ìê°€ ìˆì§€ë§Œ ì ì ˆí•œ í°íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
                    if korean_font_registered and korean_font_name in registered_fonts:
                        recommended_font = korean_font_name
                    elif japanese_font_registered and japanese_font_name in registered_fonts:
                        recommended_font = japanese_font_name
                    else:
                        print(f"âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ í°íŠ¸ê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        print(f"   í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {text_str[:50]}")
                        print(f"   ë“±ë¡ëœ í°íŠ¸: {registered_fonts}")
            except Exception as check_error:
                # í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                pass
        
        return (text_str, recommended_font)
    
    # Paragraph ìƒì„± í—¬í¼ í•¨ìˆ˜ (í°íŠ¸ ìë™ ì„ íƒ)
    def create_paragraph(text, style, auto_font=True):
        """í…ìŠ¤íŠ¸ì™€ ìŠ¤íƒ€ì¼ë¡œ Paragraph ìƒì„± (í°íŠ¸ ìë™ ì„ íƒ)"""
        text_str, recommended_font = safe_text(text, detect_font=auto_font)
        
        # ì¶”ì²œ í°íŠ¸ê°€ ìˆê³  ìŠ¤íƒ€ì¼ì— í°íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°
        if recommended_font and auto_font:
            # ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ ìƒì„± (í°íŠ¸ í¬í•¨)
            style_with_font = ParagraphStyle(
                name=f'{style.name}_with_font',
                parent=style,
                fontName=recommended_font
            )
            return Paragraph(text_str, style_with_font)
        
        return Paragraph(text_str, style)
    
    # ì œëª© ì¶”ê°€
    title_text, _ = safe_text(L.get("download_history_title", "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½"))
    story.append(Paragraph(title_text, title_style))
    story.append(Spacer(1, 0.2*inch))
    
    # ê° ì´ë ¥ ì¶”ê°€
    for i, hist in enumerate(histories, 1):
        # ì´ë ¥ ì œëª©
        heading_text, _ = safe_text(f'{L.get("download_history_number", "ì´ë ¥ #")}{i}')
        story.append(Paragraph(heading_text, heading1_style))
        story.append(Spacer(1, 0.1*inch))
        
        # ê¸°ë³¸ ì •ë³´ (í°íŠ¸ ìë™ ì„ íƒ)
        id_text, _ = safe_text(f'ID: {hist.get("id", "N/A")}')
        story.append(create_paragraph(id_text, normal_style))
        
        timestamp_text, _ = safe_text(f'{L.get("date_label", "ë‚ ì§œ")}: {hist.get("timestamp", "N/A")}')
        story.append(create_paragraph(timestamp_text, normal_style))
        
        query_text, _ = safe_text(f'{L.get("download_initial_inquiry", "ì´ˆê¸° ë¬¸ì˜")}: {hist.get("initial_query", "N/A")}')
        story.append(create_paragraph(query_text, normal_style))
        
        customer_type_text, _ = safe_text(f'{L.get("customer_type_label", "ê³ ê° ìœ í˜•")}: {hist.get("customer_type", "N/A")}')
        story.append(create_paragraph(customer_type_text, normal_style))
        
        language_text, _ = safe_text(f'{L.get("language_label", "ì–¸ì–´")}: {hist.get("language_key", "N/A")}')
        story.append(create_paragraph(language_text, normal_style))
        
        summary = hist.get('summary', {})
        if summary:
            story.append(Spacer(1, 0.1*inch))
            summary_title, _ = safe_text(L.get("download_summary", "ìš”ì•½"))
            story.append(Paragraph(summary_title, heading2_style))
            
            main_inquiry_text, _ = safe_text(f'{L.get("download_main_inquiry", "ì£¼ìš” ë¬¸ì˜")}: {summary.get("main_inquiry", "N/A")}')
            story.append(create_paragraph(main_inquiry_text, normal_style))
            
            key_responses = summary.get("key_responses", [])
            if isinstance(key_responses, list):
                responses_list = []
                for r in key_responses:
                    r_text, _ = safe_text(r)
                    responses_list.append(r_text)
                responses_text = ", ".join(responses_list)
            else:
                responses_text, _ = safe_text(key_responses)
            responses_para_text, _ = safe_text(f'{L.get("download_key_response", "í•µì‹¬ ì‘ë‹µ")}: {responses_text}')
            story.append(create_paragraph(responses_para_text, normal_style))
            
            sentiment_text, _ = safe_text(f'{L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜")}: {summary.get("customer_sentiment_score", "N/A")}/100')
            story.append(create_paragraph(sentiment_text, normal_style))
            
            satisfaction_text, _ = safe_text(f'{L.get("customer_satisfaction_score_label", "ê³ ê° ë§Œì¡±ë„ ì ìˆ˜")}: {summary.get("customer_satisfaction_score", "N/A")}/100')
            story.append(create_paragraph(satisfaction_text, normal_style))
            
            characteristics = summary.get('customer_characteristics', {})
            story.append(Spacer(1, 0.1*inch))
            char_title, _ = safe_text(L.get("download_customer_characteristics", "ê³ ê° íŠ¹ì„±"))
            story.append(Paragraph(char_title, heading2_style))
            
            lang_char_text, _ = safe_text(f'{L.get("language_label", "ì–¸ì–´")}: {characteristics.get("language", "N/A")}')
            story.append(create_paragraph(lang_char_text, normal_style))
            
            cultural_text, _ = safe_text(f'{L.get("download_cultural_background", "ë¬¸í™”ì  ë°°ê²½")}: {characteristics.get("cultural_hints", "N/A")}')
            story.append(create_paragraph(cultural_text, normal_style))
            
            region_text, _ = safe_text(f'{L.get("region_label", "ì§€ì—­")}: {characteristics.get("region", "N/A")}')
            story.append(create_paragraph(region_text, normal_style))
            
            comm_style_text, _ = safe_text(f'{L.get("download_communication_style", "ì†Œí†µ ìŠ¤íƒ€ì¼")}: {characteristics.get("communication_style", "N/A")}')
            story.append(create_paragraph(comm_style_text, normal_style))
            
            privacy = summary.get('privacy_info', {})
            story.append(Spacer(1, 0.1*inch))
            privacy_title, _ = safe_text(L.get("download_privacy_summary", "ê°œì¸ì •ë³´ ìš”ì•½"))
            story.append(Paragraph(privacy_title, heading2_style))
            
            email_text, _ = safe_text(f'{L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_email") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            story.append(create_paragraph(email_text, normal_style))
            
            phone_text, _ = safe_text(f'{L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_phone") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            story.append(create_paragraph(phone_text, normal_style))
            
            address_text, _ = safe_text(f'{L.get("download_address_provided", "ì£¼ì†Œ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_address") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            story.append(create_paragraph(address_text, normal_style))
            
            region_hint_text, _ = safe_text(f'{L.get("download_region_hint", "ì§€ì—­ íŒíŠ¸")}: {privacy.get("region_hint", "N/A")}')
            story.append(create_paragraph(region_hint_text, normal_style))
            
            full_summary_text, _ = safe_text(f'{L.get("download_overall_summary", "ì „ì²´ ìš”ì•½")}: {summary.get("summary", "N/A")}')
            story.append(create_paragraph(full_summary_text, normal_style))
        
        # êµ¬ë¶„ì„ 
        if i < len(histories):
            story.append(Spacer(1, 0.2*inch))
            divider_text, _ = safe_text('-' * 80)
            story.append(Paragraph(divider_text, normal_style))
            story.append(Spacer(1, 0.2*inch))
    
    # PDF ë¹Œë“œ (UTF-8 ì¸ì½”ë”© ëª…ì‹œ, í°íŠ¸ ì„œë¸Œì…‹íŒ… ê°•í™”)
    try:
        # í°íŠ¸ ë“±ë¡ ìƒíƒœ í™•ì¸ ë° ê²½ê³ 
        registered_fonts = pdfmetrics.getRegisteredFontNames()
        print(f"ğŸ“‹ ë“±ë¡ëœ í°íŠ¸ ëª©ë¡: {registered_fonts}")
        
        if not korean_font_registered and not japanese_font_registered:
            print("âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFì—ì„œ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("   ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•:")
            import platform
            system = platform.system()
            if system == 'Windows':
                print("   1. Windows í°íŠ¸ í´ë”(C:/Windows/Fonts/)ì— í•œê¸€ í°íŠ¸ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
                print("   2. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰")
                print("   3. ë§‘ì€ ê³ ë”•(malgun.ttf) ë˜ëŠ” ë‚˜ëˆ”ê³ ë”•(NanumGothic.ttf) ì„¤ì¹˜ í™•ì¸")
            elif system == 'Darwin':
                print("   1. macOS ì‹œìŠ¤í…œ í°íŠ¸(/System/Library/Fonts/) í™•ì¸")
                print("   2. AppleGothic í°íŠ¸ ì„¤ì¹˜ í™•ì¸")
            else:
                print("   1. Linux ì‹œìŠ¤í…œ í°íŠ¸(/usr/share/fonts/) í™•ì¸")
                print("   2. Noto Sans CJK ë˜ëŠ” Nanum í°íŠ¸ ì„¤ì¹˜ í™•ì¸")
        else:
            if korean_font_registered:
                print(f"âœ… í•œê¸€ í°íŠ¸ ë“±ë¡ í™•ì¸: {korean_font_name} in {registered_fonts}")
            if japanese_font_registered:
                print(f"âœ… ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ í™•ì¸: {japanese_font_name} in {registered_fonts}")
            print("âœ… í•œê¸€/ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë©ë‹ˆë‹¤.")
        
        # PDF ë¹Œë“œ ì‹¤í–‰ (í°íŠ¸ ì„œë¸Œì…‹íŒ… ìë™ ì ìš©)
        doc.build(story)
        print(f"âœ… PDF ìƒì„± ì™„ë£Œ: {filepath}")
        print(f"   íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath) / 1024:.2f} KB")
        
    except Exception as e:
        # ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì¬ì‹œë„
        error_msg = str(e)
        print(f"âš ï¸ PDF ë¹Œë“œ ì˜¤ë¥˜: {error_msg}")
        
        # í°íŠ¸ ê´€ë ¨ ì˜¤ë¥˜ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´ ì œê³µ
        if 'font' in error_msg.lower() or 'encoding' in error_msg.lower():
            print("   í°íŠ¸/ì¸ì½”ë”© ì˜¤ë¥˜ë¡œ ë³´ì…ë‹ˆë‹¤. í°íŠ¸ ë“±ë¡ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            registered_fonts = pdfmetrics.getRegisteredFontNames()
            print(f"   ë“±ë¡ëœ í°íŠ¸: {registered_fonts}")
            if korean_font_registered:
                print(f"   - í•œê¸€ í°íŠ¸: ë“±ë¡ë¨ ({korean_font_name})")
            else:
                print(f"   - í•œê¸€ í°íŠ¸: ë“±ë¡ë˜ì§€ ì•ŠìŒ")
            if japanese_font_registered:
                print(f"   - ì¼ë³¸ì–´ í°íŠ¸: ë“±ë¡ë¨ ({japanese_font_name})")
            else:
                print(f"   - ì¼ë³¸ì–´ í°íŠ¸: ë“±ë¡ë˜ì§€ ì•ŠìŒ")
        
        # ì¬ì‹œë„ (ë‹¨ìˆœ ì¬ì‹œë„ëŠ” ìœ„í—˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´)
        raise Exception(f"PDF ìƒì„± ì‹¤íŒ¨: {error_msg}")
    
    return filepath


# ========================================
# 6. RAG Helper (FAISS)
# ========================================
# RAG ê´€ë ¨ í•¨ìˆ˜ëŠ” ì‹œë®¬ë ˆì´í„°ì™€ ë¬´ê´€í•˜ë¯€ë¡œ ê¸°ì¡´ ì½”ë“œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

def load_documents(files) -> List[Document]:
    docs: List[Document] = []
    for f in files:
        name = f.name
        lower = name.lower()
        if lower.endswith(".pdf"):
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
            tmp.write(f.read())
            tmp.flush()
            tmp.close()
            loader = PyPDFLoader(tmp.name)
            file_docs = loader.load()
            for d in file_docs:
                d.metadata["source"] = name
            docs.extend(file_docs)
            try:
                os.remove(tmp.name)
            except OSError:
                pass
        elif lower.endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
        elif lower.endswith(".html") or lower.endswith(".htm"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
    return docs


def split_documents(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " ", ""],
    )
    return splitter.split_documents(docs)


def get_embedding_model():
    if get_api_key("openai"):
        try:
            return OpenAIEmbeddings(model="text-embedding-3-small")
        except:
            pass
    if get_api_key("gemini"):
        try:
            return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        except:
            pass
    return None


def get_embedding_function():
    """
    RAG ì„ë² ë”©ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ê²°ì •í•©ë‹ˆë‹¤.
    API í‚¤ ìœ íš¨ì„± ìˆœì„œ: OpenAI (ì‚¬ìš©ì ì„¤ì • ì‹œ) -> Gemini -> NVIDIA -> HuggingFace (fallback)
    API ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì´ë™í•˜ë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    # 1. OpenAI ì„ë² ë”© ì‹œë„ (ì‚¬ìš©ìê°€ ìœ íš¨í•œ í‚¤ë¥¼ ì„¤ì •í–ˆì„ ê²½ìš°)
    openai_key = get_api_key("openai")
    if openai_key:
        try:
            st.info("ğŸ”¹ RAG: OpenAI Embedding ì‚¬ìš© ì¤‘")
            return OpenAIEmbeddings(openai_api_key=openai_key)
        except Exception as e:
            st.warning(f"OpenAI ì„ë² ë”© ì‹¤íŒ¨ â†’ Geminië¡œ Fallback: {e}")

    # 2. Gemini ì„ë² ë”© ì‹œë„
    gemini_key = get_api_key("gemini")
    if IS_GEMINI_EMBEDDING_AVAILABLE and gemini_key:
        try:
            st.info("ğŸ”¹ RAG: Gemini Embedding ì‚¬ìš© ì¤‘")
            # â­ ìˆ˜ì •: ëª¨ë¸ ì´ë¦„ í˜•ì‹ì„ 'models/model-name'ìœ¼ë¡œ ìˆ˜ì •
            return GoogleGenerativeAIEmbeddings(google_api_key=gemini_key, model="models/text-embedding-004")
        except Exception as e:
            st.warning(f"Gemini ì„ë² ë”© ì‹¤íŒ¨ â†’ NVIDIAë¡œ Fallback: {e}")

    # 3. NVIDIA ì„ë² ë”© ì‹œë„
    nvidia_key = get_api_key("nvidia")
    if IS_NVIDIA_EMBEDDING_AVAILABLE and nvidia_key:
        try:
            st.info("ğŸ”¹ RAG: NVIDIA Embedding ì‚¬ìš© ì¤‘")
            # NIM ëª¨ë¸ ì‚¬ìš© (ì‹¤ì œ í‚¤ê°€ ìœ íš¨í•´ì•¼ í•¨)
            return NVIDIAEmbeddings(api_key=nvidia_key, model="ai-embed-qa-4")
        except Exception as e:
            st.warning(f"NVIDIA ì„ë² ë”© ì‹¤íŒ¨ â†’ HuggingFace Fallback: {e}")

    # 4. HuggingFace Embeddings (Local Fallback)
    try:
        st.info("ğŸ”¹ RAG: Local HuggingFace Embedding ì‚¬ìš© ì¤‘")
        # ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    except Exception as e:
        st.warning(f"ìµœì¢… Fallback ì„ë² ë”© ì‹¤íŒ¨: {e}")

    st.error("âŒ RAG ì„ë² ë”© ì‹¤íŒ¨: ì‚¬ìš© ê°€ëŠ¥í•œ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    return None


def build_rag_index(files):
    # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    if not files: return None, 0

    # ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‹œë„í•˜ëŠ” ê³¼ì •ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ try-exceptë¡œ ê°ìŒ‰ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception as e:
        st.error(f"RAG ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, 0

    if embeddings is None:
        # ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ë„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŒì„ ì•Œë¦¼
        error_msg = L["rag_embed_error_none"]

        # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ êµ¬ì„± (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)
        if not get_api_key("openai"):
            error_msg += f"\n- {L['rag_embed_error_openai']}"
        if not get_api_key("gemini"):
            error_msg += f"\n- {L['rag_embed_error_gemini']}"
        if not get_api_key("nvidia"):
            error_msg += f"\n- {L['rag_embed_error_nvidia']}"

        st.error(error_msg)
        return None, 0

    # ì„ë² ë”© ê°ì²´ ì´ˆê¸°í™” ì„±ê³µ í›„, ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    docs = load_documents(files)
    if not docs: return None, 0

    chunks = split_documents(docs)
    if not chunks: return None, 0

    try:
        vectorstore = FAISS.from_documents(chunks, embeddings)
        # ì €ì¥
        vectorstore.save_local(RAG_INDEX_DIR)
    except Exception as e:
        # API ì¸ì¦ ì‹¤íŒ¨ ë“± ì‹¤ì œ API í˜¸ì¶œ ì˜¤ë¥˜ ì²˜ë¦¬
        st.error(f"RAG ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, 0

    return vectorstore, len(chunks)


def load_rag_index():
    # RAG ì¸ë±ìŠ¤ ë¡œë“œ ì‹œì—ë„ ìœ íš¨í•œ ì„ë² ë”© í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception:
        # get_embedding_function ë‚´ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ìŠ¤í‚µí•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
        return None

    if embeddings is None:
        return None

    try:
        # allow_dangerous_deserialization=TrueëŠ” í•„ìˆ˜
        vs = FAISS.load_local(RAG_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception:
        return None


def rag_answer(question: str, vectorstore: FAISS, lang_key: str) -> str:
    # RAG AnswerëŠ” LLM í´ë¼ì´ì–¸íŠ¸ ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    llm_client, info = get_llm_client()
    if llm_client is None:
        # ì–¸ì–´ í‚¤ ê²€ì¦
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"
        return LANG.get(lang_key, LANG["ko"]).get("simulation_no_key_warning", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # Langchain ChatOpenAI ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ promptë¥¼ ì§ì ‘ êµ¬ì„±
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    docs = retriever.get_relevant_documents(question)
    context = "\n\n".join(d.page_content[:1500] for d in docs)

    # â­ RAG ë‹¤êµ­ì–´ ì¸ì‹ ì˜¤ë¥˜ í•´ê²°: ë‹µë³€ ìƒì„± ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ ì–¸ì–´ë¡œ ì¼ê´€ë˜ê²Œ ë‹µí•˜ë„ë¡ ê°•ë ¥íˆ ì§€ì‹œ
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(lang_key, "English")

    prompt = (
            f"You are a helpful AI tutor. Answer the question using ONLY the provided context.\n"
            f"The answer MUST be STRICTLY in {lang_name}, which is the language of the question.\n"
            f"If you cannot find the answer in the context, say you don't know in {lang_name}.\n"
            f"Note: The context may be in a different language, but you must still answer in {lang_name}.\n\n"
            "Question:\n" + question + "\n\n"
                                       "Context:\n" + context + "\n\n"
                                                                f"Answer (in {lang_name}):"
    )
    return run_llm(prompt)


# ========================================
# 7. LSTM Helper (ê°„ë‹¨ Mock + ì‹œê°í™”)
# ========================================

def load_or_train_lstm():
    # ì‹¤ì œ LSTM ëŒ€ì‹  ëœë¤ + sin íŒŒí˜• ê¸°ë°˜ Mock
    np.random.seed(42)
    n_points = 50
    ts = 60 + 20 * np.sin(np.linspace(0, 4 * np.pi, n_points)) + np.random.normal(0, 5, n_points)
    ts = np.clip(ts, 50, 100).astype(np.float32)
    return ts





# ========================================
# 8. LLM (ChatOpenAI) for Simulator / Content
# (RAGì™€ ë™ì¼í•˜ê²Œ run_llmìœ¼ë¡œ í†µí•©)
# ========================================

# ConversationChain ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„
# st.session_state.simulator_memoryëŠ” ìœ ì§€í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.


def get_chat_history_for_prompt(include_attachment=False):
    """ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ì¶”ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜ (ì±„íŒ…ìš©)"""
    history_str = ""
    for msg in st.session_state.simulator_messages:
        role = msg["role"]
        content = msg["content"]
        if role == "customer" or role == "customer_rebuttal":
            history_str += f"Customer: {content}\n"
        elif role == "agent_response":
            history_str += f"Agent: {content}\n"
        # supervisor ë©”ì‹œì§€ëŠ” LLMì— ì „ë‹¬í•˜ì§€ ì•Šì•„ ì—­í•  í˜¼ë™ ë°©ì§€
    return history_str



def generate_customer_reaction(current_lang_key: str, is_call: bool = False) -> str:
    """
    ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ì„ ìƒì„±í•˜ëŠ” LLM í˜¸ì¶œ (ì±„íŒ… ì „ìš©)
    **ìˆ˜ì • ì‚¬í•­:** ì—ì´ì „íŠ¸ ì •ë³´ ìš”ì²­ ì‹œ í•„ìˆ˜ ì •ë³´ (ì£¼ë¬¸ë²ˆí˜¸, eSIM, ìë…€ ë§Œ ë‚˜ì´, ì·¨ì†Œ ì‚¬ìœ ) ì œê³µ ì˜ë¬´ë¥¼ ê°•í™”í•¨.
    """
    history_text = get_chat_history_for_prompt()
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG.get(current_lang_key, LANG["ko"])

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        # LLMì—ê²Œ ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì—ì´ì „íŠ¸ì—ê²Œ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    next_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

Read the following conversation and respond naturally in {lang_name}.

Conversation so far:
{history_text}

RULES:
1. You are only the customer. Do not write as the agent.
2. **[CRITICAL: Mandatory Information Submission for Problem Resolution]** If the agent requested any of the following critical information, you MUST provide it:
    - Order/Booking Number (e.g., ABC123, 123456)
    - eSIM related details (e.g., Host device compatibility, local status/location, time of activation)
    - Child-related product details (e.g., Child's Date of Birth or Current Age)
    - Exception/Refund Reason (e.g., flight cancellation/delay, illness, local natural disaster)
    - **If you are a difficult customer and the agent requests this information, you MUST still provide it, but you may express frustration or impatience while doing so.**
3. **[Crucial Rule for Repetition/New Inquiry]** After the agent has provided an attempt at a solution or answer:
    - If you are still confused or the problem is not fully solved, you MUST state the remaining confusion/problem clearly and briefly. DO NOT REPEAT THE INITIAL QUERY. Focus only on the unresolved aspect or the new inquiry.
4. **[CRITICAL: Solution Acknowledgment]** If the agent provided a clear and accurate solution/confirmation:
    - You MUST respond with appreciation and satisfaction, like "{L_local['customer_positive_response']}" or similar positive acknowledgment. This applies even if you are a difficult customer.
5. If the agent's LAST message was the closing confirmation: "{L_local['customer_closing_confirm']}"
    - If you have NO additional questions: You MUST reply with "{L_local['customer_no_more_inquiries']}".
    - If you DO have additional questions: You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS IMMEDIATELY. DO NOT just repeat that you have an additional question.
6. Do NOT repeat your initial message or previous responses unless necessary.
7. Output ONLY the customer's next message.
"""
    try:
        reaction = run_llm(next_prompt)

        # â­ LLMì´ ì‘ë‹µí–ˆì§€ë§Œ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš°, ê¸ì • ì¢…ë£Œ ë¬¸êµ¬ë¥¼ ë°˜í™˜
        if not reaction or len(reaction.strip()) < 5:
            print("LLM returned insufficient response. Using positive closing fallback.")
            return L_local['customer_positive_response']

        return reaction.strip()
    except Exception as e:
        # â­ LLM í˜¸ì¶œ ìì²´ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ (API í‚¤, í• ë‹¹ëŸ‰) ê¸ì • ì¢…ë£Œ ë¬¸êµ¬ë¥¼ ê°•ì œ ë°˜í™˜
        print(f"LLM Customer Reaction generation failed: {e}. Falling back to positive closing.")
        return L_local['customer_positive_response']  # ê°•ì œ ì•ˆì „ì¥ì¹˜



def summarize_history_with_ai(current_lang_key: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ë¥¼ ì •ë¦¬í•˜ì—¬ LLMì— ì „ë‹¬í•˜ê³  ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ë°›ëŠ” í•¨ìˆ˜."""
    # ì „í™” ë¡œê·¸ëŠ” 'phone_exchange' ì—­í• ì„ ê°€ì§€ê±°ë‚˜, 'initial_query'ì— í¬í•¨ë˜ì–´ ìˆìŒ

    # 1. ë¡œê·¸ ì¶”ì¶œ
    conversation_text = ""
    initial_query = st.session_state.get("call_initial_query", "N/A")
    website_url = st.session_state.get("call_website_url", "").strip()
    if initial_query and initial_query != "N/A":
        conversation_text += f"Initial Query: {initial_query}\n"
    if website_url:
        conversation_text += f"Website URL: {website_url}\n"

    for msg in st.session_state.simulator_messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "phone_exchange":
            # phone_exchangeëŠ” "Agent: ... | Customer: ..." í˜•íƒœë¡œ ì´ë¯¸ ì •ë¦¬ë˜ì–´ ìˆìŒ
            conversation_text += f"{content}\n"

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    summary_prompt = f"""
You are an AI Analyst specialized in summarizing customer phone calls. 
Analyze the full conversation log below, identify the main issue, the steps taken by the agent, and the customer's sentiment.

Provide a concise, easy-to-read summary of the key exchange STRICTLY in {lang_name}.

--- Conversation Log ---
{conversation_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return "LLM Keyê°€ ì—†ì–´ ìš”ì•½ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤."

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}"



def generate_customer_reaction_for_call(current_lang_key: str, last_agent_response: str) -> str:
    """ì „í™” ì‹œë®¬ë ˆì´í„° ì „ìš© ê³ ê° ë°˜ì‘ ìƒì„± (ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µ ì¤‘ì‹¬)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]
    
    # â­ ì¶”ê°€: ê³ ê° ì„±ë³„ ë° ê°ì • ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
    customer_gender = st.session_state.customer_avatar.get("gender", "male")
    customer_emotion = st.session_state.customer_avatar.get("state", "NEUTRAL")
    
    # ê°ì • ìƒíƒœì— ë”°ë¥¸ í†¤ ì„¤ì •
    emotion_tone_map = {
        "HAPPY": "friendly, positive, and satisfied",
        "ASKING": "slightly frustrated, questioning, and seeking clarification",
        "ANGRY": "angry, frustrated, and demanding",
        "SAD": "sad, depressed, and disappointed",
        "NEUTRAL": "neutral, calm, and polite"
    }
    emotion_tone = emotion_tone_map.get(customer_emotion, "neutral, calm, and polite")
    
    gender_pronoun = "she" if customer_gender == "female" else "he"
    
    # â­ ì¶”ê°€: ì—ì´ì „íŠ¸ê°€ ì¢…ë£Œ í™•ì¸ ì§ˆë¬¸ì„ í–ˆëŠ”ì§€ í™•ì¸
    closing_msg = L_local['customer_closing_confirm']
    is_closing_question = closing_msg in last_agent_response or any(
        phrase in last_agent_response.lower() 
        for phrase in ["ë‹¤ë¥¸ ë¬¸ì˜", "ì¶”ê°€ ë¬¸ì˜", "ë‹¤ë¥¸ ë„ì›€", "anything else", "other questions"]
    )
    
    # â­ ìˆ˜ì •: ì´ˆê¸° ë¬¸ì˜ë¥¼ ì™„ì „íˆ ì œê±°í•˜ê³  ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µì—ë§Œ ì§‘ì¤‘
    # ìµœê·¼ ëŒ€í™” ì´ë ¥ë§Œ ì¶”ì¶œ (ìµœëŒ€ 3-4ê°œ êµí™˜ë§Œ)
    recent_exchanges = []
    for msg in reversed(st.session_state.simulator_messages):  # ì—­ìˆœìœ¼ë¡œ ìµœê·¼ ê²ƒë¶€í„°
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        if role == "phone_exchange":
            recent_exchanges.insert(0, content)  # ì•ì— ì‚½ì…í•˜ì—¬ ìˆœì„œ ìœ ì§€
            if len(recent_exchanges) >= 3:  # ìµœê·¼ 3ê°œë§Œ
                break
        elif role == "agent" or role == "agent_response":
            # agentì™€ agent_response ì—­í•  ëª¨ë‘ ì²˜ë¦¬
            recent_exchanges.insert(0, f"Agent: {content}")
            if len(recent_exchanges) >= 3:
                break
    
    # ìµœê·¼ ëŒ€í™” ì´ë ¥ (ìˆëŠ” ê²½ìš°ë§Œ)
    recent_history = "\n".join(recent_exchanges) if recent_exchanges else "(No previous exchanges)"
    
    website_url = st.session_state.get("call_website_url", "").strip()
    website_context = f"\nWebsite URL: {website_url}" if website_url else ""
    
    # â­ ìˆ˜ì •: ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µë§Œ ê°•ì¡° (ì´ˆê¸° ë¬¸ì˜ ì™„ì „ ì œê±°)
    last_agent_text = last_agent_response.strip() if last_agent_response else "None"
    
    history_text = f"""[Recent Conversation Context - For Reference Only]
{recent_history}{website_context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ YOUR TASK: Respond ONLY to the Agent's message below
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Agent just said: "{last_agent_text}"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IMPORTANT: 
- Respond DIRECTLY to what the agent JUST SAID above
- DO NOT repeat your initial query
- DO NOT refer to old conversation unless agent asks
- Keep your response short and conversational
- Your emotional state: {customer_emotion} - respond with {emotion_tone} tone
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""

    # â­ ì¶”ê°€: ì¢…ë£Œ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
    if is_closing_question:
        call_prompt = f"""
You are a CUSTOMER in a phone call. You are a {customer_gender} customer. Respond naturally in {lang_name}.

Your current emotional state: {customer_emotion}
Your response tone should be: {emotion_tone}

{history_text}

The agent just asked: "{last_agent_text}"

CRITICAL RULES FOR CLOSING CONFIRMATION:
1. If you have NO additional questions and the conversation is resolved:
   - You MUST reply with: "{L_local['customer_no_more_inquiries']}"
2. If you DO have additional questions or the issue is NOT fully resolved:
   - You MUST reply with: "{L_local['customer_has_additional_inquiries']}" AND immediately state your additional question
3. Your response MUST be ONLY one of the two options above, in {lang_name}.
4. Output ONLY the customer's response (must be one of the two rule options).

Your response (respond to the closing confirmation question):
"""
    else:
        call_prompt = f"""
You are a CUSTOMER in a phone call. You are a {customer_gender} customer. Respond naturally in {lang_name}.

Your current emotional state: {customer_emotion}
Your response tone should be: {emotion_tone}

{history_text}

RULES:
1. **CRITICAL**: Respond DIRECTLY and ACCURATELY to what the agent JUST SAID: "{last_agent_text}"
2. **If agent asked a question** â†’ Answer it SPECIFICALLY and DIRECTLY. Do not give vague or unrelated answers.
3. **If agent requested information** â†’ Provide the EXACT information requested. Be precise and relevant.
4. **If agent gave a solution or instruction** â†’ Acknowledge it clearly and indicate if you understand or need clarification, based on your emotional state ({customer_emotion})
5. **If agent asked for confirmation** â†’ Confirm or clarify based on what was asked
6. Keep your response short (1-2 sentences max) and focused ONLY on what the agent just said
7. DO NOT repeat your initial query unless the agent specifically asks about it
8. DO NOT mention old conversation unless the agent refers to it
9. IMPORTANT: Match your tone to your emotional state ({customer_emotion}) - be {emotion_tone}
10. **Your response must be a direct answer to the agent's last message above. Read it carefully and respond accordingly.**

Your response (respond ONLY to the agent's message above, with {emotion_tone} tone):
"""
    try:
        reaction = run_llm(call_prompt)
        reaction_text = reaction.strip()
        
        # â­ ì¶”ê°€: ì¢…ë£Œ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ê²€ì¦ ë° ê°•ì œ ì ìš©
        if is_closing_question:
            if L_local['customer_no_more_inquiries'] in reaction_text:
                return L_local['customer_no_more_inquiries']
            elif L_local['customer_has_additional_inquiries'] in reaction_text:
                return reaction_text  # ì¶”ê°€ ë¬¸ì˜ ë‚´ìš© í¬í•¨ ê°€ëŠ¥
            else:
                # LLMì´ ê·œì¹™ì„ ë”°ë¥´ì§€ ì•Šìœ¼ë©´, ëŒ€í™”ê°€ í•´ê²°ëœ ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê³  ì¢…ë£Œ ì‘ë‹µ ë°˜í™˜
                return L_local['customer_no_more_inquiries']
        
        return reaction_text
    except Exception as e:
        return f"âŒ ê³ ê° ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}"



def generate_customer_reaction_for_first_greeting(current_lang_key: str, agent_greeting: str, initial_query: str) -> str:
    """ì „í™” ì‹œë®¬ë ˆì´í„° ì „ìš©: ì²« ì¸ì‚¬ë§ì— ëŒ€í•œ ê³ ê°ì˜ ë§ì¶¤í˜• ë°˜ì‘ ìƒì„± (ì´ˆê¸° ë¬¸ì˜ ê³ ë ¤)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]
    
    # â­ ì¶”ê°€: ê³ ê° ì„±ë³„ ë° ê°ì • ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
    customer_gender = st.session_state.customer_avatar.get("gender", "male")
    customer_emotion = st.session_state.customer_avatar.get("state", "NEUTRAL")
    
    # ê°ì • ìƒíƒœì— ë”°ë¥¸ í†¤ ì„¤ì •
    emotion_tone_map = {
        "HAPPY": "friendly, positive, and satisfied",
        "ASKING": "slightly frustrated, questioning, and seeking clarification",
        "ANGRY": "angry, frustrated, and demanding",
        "SAD": "sad, depressed, and disappointed",
        "NEUTRAL": "neutral, calm, and polite"
    }
    emotion_tone = emotion_tone_map.get(customer_emotion, "neutral, calm, and polite")
    
    website_url = st.session_state.get("call_website_url", "").strip()
    website_context = f"\nWebsite URL: {website_url}" if website_url else ""
    
    agent_greeting_text = agent_greeting.strip() if agent_greeting else "None"
    initial_query_text = initial_query.strip() if initial_query else "None"
    
    call_prompt = f"""
You are a CUSTOMER in a phone call. You are a {customer_gender} customer. Respond naturally in {lang_name}.

Your current emotional state: {customer_emotion}
Your response tone should be: {emotion_tone}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ YOUR SITUATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You called because: "{initial_query_text}"

The agent just greeted you and said: "{agent_greeting_text}"
{website_context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOUR TASK: Respond to the agent's greeting in a way that:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Acknowledge the agent's greeting naturally
2. Briefly mention your inquiry/concern: "{initial_query_text}"
3. Show that you're ready to discuss your issue
4. Keep it conversational and natural (1-2 sentences max)
5. DO NOT be overly formal - this is a phone call, be natural
6. IMPORTANT: Match your tone to your emotional state ({customer_emotion}) - be {emotion_tone}

Example good responses (adjust tone based on your emotional state):
- If {customer_emotion}: [Respond with {emotion_tone} tone]
- "Hello, thank you. I'm calling because [brief mention of issue]..."
- "Hi, yes. I need help with [your issue]..."
- "Thank you. I have a question about [your issue]..."

Your response (respond naturally to the greeting and briefly mention your inquiry, with {emotion_tone} tone):
"""
    try:
        reaction = run_llm(call_prompt)
        return reaction.strip()
    except Exception as e:
        return f"âŒ ê³ ê° ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}"



def summarize_history_for_call(call_logs: List[Dict[str, str]], initial_query: str, current_lang_key: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ë³¸ì„ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ë¡œê·¸ ì¬êµ¬ì„± (phone_exchange ì—­í• ë§Œ ì‚¬ìš©)
    full_log_text = f"--- Initial Customer Query ---\nCustomer: {initial_query}\n"
    for log in call_logs:
        if log["role"] == "phone_exchange":
            full_log_text += f"{log['content']}\n"
        elif log["role"] == "agent" and "content" in log:
            # ìµœì´ˆ ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ì€ ì—¬ê¸°ì— í¬í•¨
            full_log_text += f"Agent (Greeting): {log['content']}\n"

    summary_prompt = f"""
You are an AI Supervisor. Analyze the following telephone support conversation log.
Provide a concise, neutral summary of the key issue, the steps taken by the agent, and the final outcome.
The summary MUST be STRICTLY in {lang_name}.

--- Conversation Log ---
{full_log_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key is missing. Cannot generate summary. Log length: {len(full_log_text.splitlines())}"

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ Summary Generation Error: {e}"



def generate_customer_closing_response(current_lang_key: str) -> str:
    """ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ê³ ê°ì˜ ìµœì¢… ë‹µë³€ ìƒì„± (ì±„íŒ…ìš©)"""
    history_text = get_chat_history_for_prompt()
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG.get(current_lang_key, LANG["ko"])  # â­ ìˆ˜ì •: í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ íŒ©

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ì˜ ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ì¸ì§€ í™•ì¸ (í”„ë¡¬í”„íŠ¸ì— í¬í•¨)
    closing_msg = L_local['customer_closing_confirm']

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    final_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

The agent's final message was the closing confirmation: "{closing_msg}".
You MUST respond to this confirmation based on the overall conversation.

Conversation history:
{history_text}

RULES:
1. If the conversation seems resolved and you have NO additional questions:
    - You MUST reply with "{L_local['customer_no_more_inquiries']}".
2. If the conversation is NOT fully resolved and you DO have additional questions (or the agent provided a cancellation denial that you want to appeal):
    - You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS. DO NOT just repeat that you have an additional question.
3. Your reply MUST be ONLY one of the two options above, in {lang_name}.
4. Output ONLY the customer's next message (must be one of the two rule options).
"""
    try:
        reaction = run_llm(final_prompt)
        # LLMì˜ ì¶œë ¥ì´ ê·œì¹™ì„ ë”°ë¥´ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê°•ì œ ì ìš©
        reaction_text = reaction.strip()
        # "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤"ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìƒì„¸ ë‚´ìš© í¬í•¨ ê°€ì •)
        if L_local['customer_no_more_inquiries'] in reaction_text:
            return L_local['customer_no_more_inquiries']
        elif L_local['customer_has_additional_inquiries'] in reaction_text:
            return reaction_text
        else:
            # LLMì´ ê·œì¹™ì„ ì–´ê²¼ì„ ê²½ìš°, "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆë‹¤"ê³  ê°€ì •í•˜ê³  ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ë„˜ê¹€
            return L_local['customer_has_additional_inquiries']
    except Exception as e:
        st.error(f"ê³ ê° ìµœì¢… ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}")
        return L_local['customer_has_additional_inquiries']  # ì˜¤ë¥˜ ì‹œ ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ë„


# ----------------------------------------
# Initial Advice/Draft Generation (ì´ê´€ í›„ ì¬ì‚¬ìš©) (ìš”ì²­ 4 ë°˜ì˜)
# ----------------------------------------

def generate_agent_first_greeting(lang_key: str, initial_query: str) -> str:
    """ì „í™” í†µí™” ì‹œì‘ ì‹œ ì—ì´ì „íŠ¸ì˜ ì²« ì¸ì‚¬ë§ì„ ìƒì„± (ì„ì‹œ í•¨ìˆ˜)"""
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"
    L_local = LANG.get(lang_key, LANG["ko"])
    # ë¬¸ì˜ ë‚´ìš©ì˜ ì²« 10ìë§Œ ì‚¬ìš© (too long)
    topic = initial_query.strip()[:15].replace('\n', ' ')
    if len(initial_query.strip()) > 15:
        topic += "..."

    if lang_key == 'ko':
        return f"ì•ˆë…•í•˜ì„¸ìš”, {topic} ê´€ë ¨ ë¬¸ì˜ ì£¼ì…¨ì£ ? ìƒë‹´ì› 000ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    elif lang_key == 'en':
        return f"Hello, thank you for calling. I see you're calling about {topic}. My name is 000. How may I help you today?"
    elif lang_key == 'ja':
        return f"ãŠé›»è©±ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚{topic}ã®ä»¶ã§ã™ã­ã€‚æ‹…å½“ã®000ã¨ç”³ã—ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹?"
    return "Hello, how may I help you?"



def detect_text_language(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•©ë‹ˆë‹¤.
    Returns: "ko", "en", "ja" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "ko")
    """
    if not text or not text.strip():
        return "ko"  # ê¸°ë³¸ê°’
    
    try:
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì¼ë³¸ì–´ ë¬¸ì(íˆë¼ê°€ë‚˜, ê°€íƒ€ì¹´ë‚˜, í•œì)ê°€ ë§ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¼ë³¸ì–´
        japanese_chars = sum(1 for char in text if '\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF')
        if japanese_chars > len(text) * 0.1:  # 10% ì´ìƒ ì¼ë³¸ì–´ ë¬¸ì
            return "ja"
        
        # ì˜ì–´ ë¬¸ì ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì˜ì–´
        english_chars = sum(1 for char in text if char.isascii() and char.isalpha())
        if english_chars > len(text) * 0.7:  # 70% ì´ìƒ ì˜ì–´ ë¬¸ì
            return "en"
        
        # LLMì„ ì‚¬ìš©í•œ ì •í™•í•œ ì–¸ì–´ ê°ì§€ ì‹œë„ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‹œí•˜ê³  íœ´ë¦¬ìŠ¤í‹± ê²°ê³¼ ì‚¬ìš©)
        if st.session_state.is_llm_ready:
            try:
                detection_prompt = f"""Detect the language of the following text. Respond with ONLY one word: "ko" (Korean), "en" (English), or "ja" (Japanese).

Text: {text[:200]}

Language:"""
                detected = run_llm(detection_prompt).strip().lower()
                # ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‚¬ìš©
                if detected and detected not in ["âŒ", "error", "failed"] and detected in ["ko", "en", "ja"]:
                    return detected
            except Exception as e:
                # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹± ê²°ê³¼ ì‚¬ìš©
                print(f"Language detection LLM call failed: {e}")
                pass
    except Exception as e:
        # ì „ì²´ í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        print(f"Language detection error: {e}")
        return "ko"
    
    # ê¸°ë³¸ê°’: í•œêµ­ì–´
    return "ko"



def analyze_customer_profile(customer_query: str, current_lang_key: str = None) -> Dict[str, Any]:
    """ì‹ ê·œ ê³ ê°ì˜ ë¬¸ì˜ì‚¬í•­ê³¼ ë§íˆ¬ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ê°ì„±í–¥ ì ìˆ˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì‚° (ìš”ì²­ 4)"""
    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    try:
        detected_lang = detect_text_language(customer_query)
    except Exception as e:
        print(f"Language detection failed in analyze_customer_profile: {e}")
        detected_lang = "ko"  # ê¸°ë³¸ê°’ ì‚¬ìš©
    
    # current_lang_keyê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©
    lang_key_to_use = current_lang_key if current_lang_key else detected_lang
    # lang_key_to_useê°€ ìœ íš¨í•œì§€ í™•ì¸
    if lang_key_to_use not in ["ko", "en", "ja"]:
        lang_key_to_use = "ko"  # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
    
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[lang_key_to_use]

    analysis_prompt = f"""
You are an AI analyst analyzing a customer's inquiry to determine their profile and sentiment.

Analyze the following customer inquiry and provide a structured analysis in JSON format (ONLY JSON, no markdown).

Analyze:
1. Customer gender (male/female/unknown - analyze based on name, language patterns, or cultural hints)
2. Customer sentiment score (0-100, where 0=very negative/angry, 50=neutral, 100=very positive/happy)
3. Communication style (formal/casual, brief/detailed, polite/direct)
4. Urgency level (low/medium/high)
5. Customer type prediction (normal/difficult/very_dissatisfied)
6. Language and cultural hints (if any)
7. Key concerns or pain points

Output format (JSON only):
{{
  "gender": "male",
  "sentiment_score": 45,
  "communication_style": "brief, direct, slightly frustrated",
  "urgency_level": "high",
  "predicted_customer_type": "difficult",
  "cultural_hints": "unknown",
  "key_concerns": ["issue 1", "issue 2"],
  "tone_analysis": "brief description of tone"
}}

Customer Inquiry:
{customer_query}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        return {
            "gender": "unknown",
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": "Unable to analyze"
        }

    try:
        analysis_text = run_llm(analysis_prompt).strip()
        # JSON ì¶”ì¶œ
        if "```json" in analysis_text:
            analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
        elif "```" in analysis_text:
            analysis_text = analysis_text.split("```")[1].split("```")[0].strip()

        import json
        analysis_data = json.loads(analysis_text)
        return analysis_data
    except Exception as e:
        return {
            "gender": "unknown",
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": f"Analysis error: {str(e)}"
        }



def find_similar_cases(customer_query: str, customer_profile: Dict[str, Any], current_lang_key: str,
                       limit: int = 5) -> List[Dict[str, Any]]:
    """ì €ì¥ëœ ìš”ì•½ ë°ì´í„°ì—ì„œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì•„ ë°˜í™˜ (ìš”ì²­ 4)"""
    histories = load_simulation_histories_local(current_lang_key)

    if not histories:
        return []

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
           and not h.get("is_call", False)  # ì „í™” ì´ë ¥ ì œì™¸
    ]

    if not cases_with_summary:
        return []

    # ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ + ì ìˆ˜ ìœ ì‚¬ë„)
    similar_cases = []
    query_lower = customer_query.lower()
    customer_sentiment = customer_profile.get("sentiment_score", 50)
    customer_style = customer_profile.get("communication_style", "")

    for case in cases_with_summary:
        summary = case.get("summary", {})
        main_inquiry = summary.get("main_inquiry", "").lower()
        case_sentiment = summary.get("customer_sentiment_score", 50)
        case_satisfaction = summary.get("customer_satisfaction_score", 50)

        # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        similarity_score = 0

        # 1. ë¬¸ì˜ ë‚´ìš© ìœ ì‚¬ë„ (í‚¤ì›Œë“œ ë§¤ì¹­)
        query_words = set(query_lower.split())
        inquiry_words = set(main_inquiry.split())
        if query_words and inquiry_words:
            word_overlap = len(query_words & inquiry_words) / len(query_words | inquiry_words)
            similarity_score += word_overlap * 40

        # 2. ê°ì • ì ìˆ˜ ìœ ì‚¬ë„
        sentiment_diff = abs(customer_sentiment - case_sentiment)
        sentiment_similarity = max(0, 1 - (sentiment_diff / 100)) * 30
        similarity_score += sentiment_similarity

        # 3. ë§Œì¡±ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì¼€ì´ìŠ¤)
        satisfaction_bonus = (case_satisfaction / 100) * 30
        similarity_score += satisfaction_bonus

        if similarity_score > 30:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            similar_cases.append({
                "case": case,
                "similarity_score": similarity_score,
                "summary": summary
            })

    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similar_cases.sort(key=lambda x: x["similarity_score"], reverse=True)
    return similar_cases[:limit]


def visualize_customer_profile_scores(customer_profile: Dict[str, Any], current_lang_key: str):
    """ê³ ê° í”„ë¡œí•„ ì ìˆ˜ë¥¼ ì‹œê°í™” (ê°ì • ì ìˆ˜, ê¸´ê¸‰ë„)"""
    if not IS_PLOTLY_AVAILABLE:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    sentiment_score = customer_profile.get("sentiment_score", 50)
    urgency_map = {"low": 25, "medium": 50, "high": 75}
    urgency_level = customer_profile.get("urgency_level", "medium")
    urgency_score = urgency_map.get(urgency_level.lower(), 50)

    # ê²Œì´ì§€ ì°¨íŠ¸ ìƒì„±
    fig = make_subplots(
        rows=1, cols=2,
        specs=[[{"type": "indicator"}, {"type": "indicator"}]],
        subplot_titles=(
            L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜"),
            L.get("urgency_score_label", "ê¸´ê¸‰ë„ ì ìˆ˜")
        )
    )

    # ê°ì • ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number+delta",
            value=sentiment_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("sentiment_score_label", "ê°ì • ì ìˆ˜")},
            delta={'reference': 50},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 33], 'color': "lightgray"},
                    {'range': [33, 66], 'color': "gray"},
                    {'range': [66, 100], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 90
                }
            }
        ),
        row=1, col=1
    )

    # ê¸´ê¸‰ë„ ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number",
            value=urgency_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("urgency_score_label", "ê¸´ê¸‰ë„")},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkred"},
                'steps': [
                    {'range': [0, 50], 'color': "lightgreen"},
                    {'range': [50, 75], 'color': "yellow"},
                    {'range': [75, 100], 'color': "lightcoral"}
                ],
            }
        ),
        row=1, col=2
    )

    fig.update_layout(height=300, margin=dict(l=20, r=20, t=50, b=20))
    return fig


def visualize_similarity_cases(similar_cases: List[Dict[str, Any]], current_lang_key: str):
    """ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ì²œì„ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not similar_cases:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    case_labels = []
    similarity_scores = []
    sentiment_scores = []
    satisfaction_scores = []

    for idx, similar_case in enumerate(similar_cases, 1):
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]
        case_labels.append(f"Case {idx}")
        similarity_scores.append(similarity)
        sentiment_scores.append(summary.get("customer_sentiment_score", 50))
        satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))

    # ìœ ì‚¬ë„ ì°¨íŠ¸
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=(
            L.get("similarity_chart_title", "ìœ ì‚¬ ì¼€ì´ìŠ¤ ìœ ì‚¬ë„"),
            L.get("scores_comparison_title",
                  "ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ")
        ),
        vertical_spacing=0.15
    )

    # ìœ ì‚¬ë„ ë°” ì°¨íŠ¸
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=similarity_scores,
            name=L.get("similarity_score_label", "ìœ ì‚¬ë„"),
            marker_color='lightblue',
            text=[f"{s:.1f}%" for s in similarity_scores],
            textposition='outside'
        ),
        row=1, col=1
    )

    # ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=sentiment_scores,
            name=L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
            marker_color='lightcoral'
        ),
        row=2, col=1
    )

    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=satisfaction_scores,
            name=L.get("satisfaction_score_label", "ë§Œì¡±ë„"),
            marker_color='lightgreen'
        ),
        row=2, col=1
    )

    fig.update_layout(
        height=600,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20),
        barmode='group'
    )
    fig.update_yaxes(title_text="ì ìˆ˜", row=2, col=1)
    fig.update_yaxes(title_text="ìœ ì‚¬ë„ (%)", row=1, col=1)

    return fig


def visualize_case_trends(histories: List[Dict[str, Any]], current_lang_key: str):
    """ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ íŠ¸ë Œë“œë¥¼ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not histories:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
    ]

    if not cases_with_summary:
        return None

    # ë‚ ì§œë³„ë¡œ ì •ë ¬
    cases_with_summary.sort(key=lambda x: x.get("timestamp", ""))

    dates = []
    sentiment_scores = []
    satisfaction_scores = []

    for case in cases_with_summary:
        summary = case.get("summary", {})
        timestamp = case.get("timestamp", "")
        try:
            dt = datetime.fromisoformat(timestamp)
            dates.append(dt)
            sentiment_scores.append(summary.get("customer_sentiment_score", 50))
            satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))
        except Exception:
            continue

    if not dates:
        return None

    # íŠ¸ë Œë“œ ë¼ì¸ ì°¨íŠ¸
    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=dates,
        y=sentiment_scores,
        mode='lines+markers',
        name=L.get("sentiment_trend_label", "ê°ì • ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightcoral', width=2),
        marker=dict(size=6)
    ))

    fig.add_trace(go.Scatter(
        x=dates,
        y=satisfaction_scores,
        mode='lines+markers',
        name=L.get("satisfaction_trend_label", "ë§Œì¡±ë„ ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightgreen', width=2),
        marker=dict(size=6)
    ))

    fig.update_layout(
        title=L.get("case_trends_title", "ê³¼ê±° ì¼€ì´ìŠ¤ ì ìˆ˜ ì¶”ì´"),
        xaxis_title=L.get("date_label", "ë‚ ì§œ"),
        yaxis_title=L.get("score_label", "ì ìˆ˜ (0-100)"),
        height=400,
        hovermode='x unified',
        legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
    )

    return fig


def visualize_customer_characteristics(summary: Dict[str, Any], current_lang_key: str):
    """ê³ ê° íŠ¹ì„±ì„ ì‹œê°í™” (ì–¸ì–´, ë¬¸í™”ê¶Œ, ì§€ì—­ ë“±)"""
    if not IS_PLOTLY_AVAILABLE or not summary:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    characteristics = summary.get("customer_characteristics", {})
    privacy_info = summary.get("privacy_info", {})

    # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
    labels = []
    values = []

    # ì–¸ì–´ ì •ë³´
    language = characteristics.get("language", "unknown")
    if language != "unknown":
        labels.append(L.get("language_label", "ì–¸ì–´"))
        lang_map = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}
        values.append(lang_map.get(language, language))

    # ê°œì¸ì •ë³´ ì œê³µ ì—¬ë¶€
    if privacy_info.get("has_email"):
        labels.append(L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ"))
        values.append("Yes")
    if privacy_info.get("has_phone"):
        labels.append(L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ"))
        values.append("Yes")

    # ì§€ì—­ ì •ë³´
    region = privacy_info.get("region_hint", characteristics.get("region", "unknown"))
    if region != "unknown":
        labels.append(L.get("region_label", "ì§€ì—­"))
        values.append(region)

    if not labels:
        return None

    # íŒŒì´ ì°¨íŠ¸
    fig = go.Figure(data=[go.Pie(
        labels=labels,
        values=[1] * len(labels),
        hole=0.4,
        marker_colors=px.colors.qualitative.Set3[:len(labels)]
    )])

    fig.update_layout(
        title=L.get("customer_characteristics_title",
                    "ê³ ê° íŠ¹ì„± ë¶„í¬"),
        height=300,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20)
    )

    return fig



def generate_guideline_from_past_cases(customer_query: str, customer_profile: Dict[str, Any],
                                       similar_cases: List[Dict[str, Any]], current_lang_key: str) -> str:
    """ê³¼ê±° ìœ ì‚¬ ì¼€ì´ìŠ¤ì˜ ì„±ê³µì ì¸ í•´ê²° ë°©ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¼ì¸ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    if not similar_cases:
        return ""

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìš”ì•½
    past_cases_text = ""
    for idx, similar_case in enumerate(similar_cases, 1):
        case = similar_case["case"]
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]

        past_cases_text += f"""
[Case {idx}] (Similarity: {similarity:.1f}%)
- Inquiry: {summary.get('main_inquiry', 'N/A')}
- Customer Sentiment: {summary.get('customer_sentiment_score', 50)}/100
- Customer Satisfaction: {summary.get('customer_satisfaction_score', 50)}/100
- Key Responses: {', '.join(summary.get('key_responses', [])[:3])}
- Summary: {summary.get('summary', 'N/A')[:200]}
"""

    guideline_prompt = f"""
You are an AI Customer Support Supervisor analyzing past successful cases to provide guidance.

Based on the following similar past cases and their successful resolution strategies, provide actionable guidelines for handling the current customer inquiry.

Current Customer Inquiry:
{customer_query}

Current Customer Profile:
- Gender: {customer_profile.get('gender', 'unknown')}
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}

Similar Past Cases (Successful Resolutions):
{past_cases_text}

Provide a concise guideline in {lang_name} that:
1. Identifies what worked well in similar past cases
2. Suggests specific approaches based on successful patterns
3. Warns about potential pitfalls based on past experiences
4. Recommends response strategies that led to high customer satisfaction

Guideline (in {lang_name}):
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        guideline = run_llm(guideline_prompt).strip()
        return guideline
    except Exception as e:
        return f"ê°€ì´ë“œë¼ì¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"



def _generate_initial_advice(customer_query, customer_type_display, customer_email, customer_phone, current_lang_key,
                             customer_attachment_file):
    """Supervisor ê°€ì´ë“œë¼ì¸ê³¼ ì´ˆì•ˆì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì €ì¥ëœ ë°ì´í„° í™œìš©)"""
    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    try:
        detected_lang = detect_text_language(customer_query)
    except Exception as e:
        print(f"Language detection failed in _generate_initial_advice: {e}")
        detected_lang = current_lang_key if current_lang_key else "ko"
    
    # ê°ì§€ëœ ì–¸ì–´ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë˜, current_lang_keyê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µë˜ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
    lang_key_to_use = detected_lang if detected_lang else current_lang_key
    # lang_key_to_useê°€ ìœ íš¨í•œì§€ í™•ì¸
    if lang_key_to_use not in ["ko", "en", "ja"]:
        lang_key_to_use = current_lang_key if current_lang_key else "ko"
    
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if lang_key_to_use not in ["ko", "en", "ja"]:
        lang_key_to_use = st.session_state.get("language", "ko")
        if lang_key_to_use not in ["ko", "en", "ja"]:
            lang_key_to_use = "ko"
    L = LANG.get(lang_key_to_use, LANG["ko"])
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[lang_key_to_use]

    contact_info_block = ""
    if customer_email or customer_phone:
        contact_info_block = (
            f"\n\n[Customer contact info for reference (DO NOT use these in your reply draft!)]"
            f"\n- Email: {customer_email or 'N/A'}"
            f"\n- Phone: {customer_phone or 'N/A'}"
        )

    attachment_block = ""
    if customer_attachment_file:
        file_name = customer_attachment_file.name
        attachment_block = f"\n\n[ATTACHMENT NOTE]: {L['attachment_info_llm'].format(filename=file_name)}"

    # ê³ ê° í”„ë¡œí•„ ë¶„ì„ (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
    customer_profile = analyze_customer_profile(customer_query, lang_key_to_use)

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì°¾ê¸° (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
    similar_cases = find_similar_cases(customer_query, customer_profile, lang_key_to_use, limit=5)

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ìƒì„± (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
    past_cases_guideline = ""
    if similar_cases:
        past_cases_guideline = generate_guideline_from_past_cases(
            customer_query, customer_profile, similar_cases, lang_key_to_use
        )

    # ê³ ê° í”„ë¡œí•„ ì •ë³´
    gender_display = customer_profile.get('gender', 'unknown')
    profile_block = f"""
[Customer Profile Analysis]
- Gender: {gender_display}
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency Level: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}
- Key Concerns: {', '.join(customer_profile.get('key_concerns', []))}
- Tone: {customer_profile.get('tone_analysis', 'unknown')}
"""

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ë¸”ë¡
    past_cases_block = ""
    if past_cases_guideline:
        past_cases_block = f"""
[Guidelines Based on {len(similar_cases)} Similar Past Cases]
{past_cases_guideline}
"""
    elif similar_cases:
        past_cases_block = f"""
[Note: Found {len(similar_cases)} similar past cases, but unable to generate detailed guidelines.
Consider reviewing past cases manually for patterns.]
"""

    # Output ALL text (guidelines and draft) STRICTLY in {lang_name}. <--- ê°•ë ¥í•œ ì–¸ì–´ ê°•ì œ ì§€ì‹œ
    initial_prompt = f"""
Output ALL text (guidelines and draft) STRICTLY in {lang_name}.

You are an AI Customer Support Supervisor. Your role is to analyze the following customer inquiry
from a **{st.session_state.customer_type_sim_select}** and provide:

1) A detailed **response guideline for the human agent** (step-by-step).
2) A **ready-to-send draft reply** in {lang_name}.

[FORMAT]
- Use the exact markdown headers:
  - "### {L['simulation_advice_header']}"
  - "### {L['simulation_draft_header']}"

[CRITICAL GUIDELINE RULES]
1. **Initial Information Collection (Req 3):** The first step in the guideline MUST be to request the necessary initial diagnostic information (e.g., device compatibility, local status/location, order number) BEFORE attempting to troubleshoot or solve the problem.
2. **Empathy for Difficult Customers (Req 5):** If the customer type is 'Difficult Customer' or 'Highly Dissatisfied Customer', the guideline MUST emphasize extreme politeness, empathy, and apologies, even if the policy (e.g., no refund) must be enforced.
3. **24-48 Hour Follow-up (Req 6):** If the issue cannot be solved immediately or requires confirmation from a local partner/supervisor, the guideline MUST state the procedure:
   - Acknowledge the issue.
   - Inform the customer they will receive a definite answer within 24 or 48 hours.
   - Request the customer's email or phone number for follow-up contact. (Use provided contact info if available)
4. **Past Cases Learning:** If past cases guidelines are provided, incorporate successful strategies from those cases into your recommendations.

Customer Inquiry:
{customer_query}
{contact_info_block}
{attachment_block}
{profile_block}
{past_cases_block}
"""
    if not st.session_state.is_llm_ready:
        mock_text = (
            f"### {L['simulation_advice_header']}\n\n"
            f"- (Mock) {st.session_state.customer_type_sim_select} ìœ í˜• ê³ ê° ì‘ëŒ€ ê°€ì´ë“œì…ë‹ˆë‹¤. (ìš”ì²­ 3, 5, 6 ë°˜ì˜)\n\n"
            f"### {L['simulation_draft_header']}\n\n"
            f"(Mock) ì—ì´ì „íŠ¸ ì‘ëŒ€ ì´ˆì•ˆì´ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤ã€‚\n\n"
        )
        return mock_text
    else:
        with st.spinner(L["response_generating"]):
            try:
                return run_llm(initial_prompt)
            except Exception as e:
                st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return f"âŒ AI Advice Generation Error: {e}"


# ========================================
# ê³ ê° ê²€ì¦ ê´€ë ¨ í•¨ìˆ˜
# ========================================

def mask_email(email: str, show_chars: int = 2) -> str:
    """
    ì´ë©”ì¼ ì£¼ì†Œë¥¼ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
    ì•/ë’¤ show_chars ìë¦¬ë§Œ í‘œì‹œí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” * ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        email: ë§ˆìŠ¤í‚¹í•  ì´ë©”ì¼ ì£¼ì†Œ
        show_chars: ì•/ë’¤ì— í‘œì‹œí•  ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 2)
    
    Returns:
        ë§ˆìŠ¤í‚¹ëœ ì´ë©”ì¼ ì£¼ì†Œ (ì˜ˆ: "ab***@ex***.com")
    """
    if not email or "@" not in email:
        return email
    
    local_part, domain = email.split("@", 1)
    domain_parts = domain.split(".", 1)
    
    # ë¡œì»¬ ë¶€ë¶„ ë§ˆìŠ¤í‚¹ (ì• show_chars ìë¦¬ë§Œ í‘œì‹œ)
    if len(local_part) <= show_chars:
        masked_local = local_part
    else:
        masked_local = local_part[:show_chars] + "*" * (len(local_part) - show_chars)
    
    # ë„ë©”ì¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹ (ì• show_chars ìë¦¬ë§Œ í‘œì‹œ)
    if len(domain_parts[0]) <= show_chars:
        masked_domain = domain_parts[0]
    else:
        masked_domain = domain_parts[0][:show_chars] + "*" * (len(domain_parts[0]) - show_chars)
    
    if len(domain_parts) > 1:
        return f"{masked_local}@{masked_domain}.{domain_parts[1]}"
    else:
        return f"{masked_local}@{masked_domain}"


def verify_customer_info(
    provided_info: Dict[str, str],
    stored_info: Dict[str, str]
) -> Tuple[bool, Dict[str, bool]]:
    """
    ê³ ê°ì´ ì œê³µí•œ ì •ë³´ì™€ ì €ì¥ëœ ì •ë³´ë¥¼ ë¹„êµí•˜ì—¬ ê²€ì¦í•©ë‹ˆë‹¤.
    ì‹œìŠ¤í…œ ë‚´ë¶€ì—ì„œë§Œ ì‹¤í–‰ë˜ë©°, confidential ì •ë³´ë¥¼ ë³´í˜¸í•©ë‹ˆë‹¤.
    
    Args:
        provided_info: ê³ ê°ì´ ì œê³µí•œ ê²€ì¦ ì •ë³´
            - receipt_number: ì˜ìˆ˜ì¦/ì˜ˆì•½ ë²ˆí˜¸
            - card_last4: ì¹´ë“œ ë’·ìë¦¬ 4ìë¦¬ (ì¹´ë“œ ê²°ì œì¸ ê²½ìš°)
            - account_number: ê³„ì¢Œë²ˆí˜¸ (ì˜¨ë¼ì¸ë±…í‚¹ì¸ ê²½ìš°)
            - payment_method: ê²°ì œ ìˆ˜ë‹¨
            - customer_name: ê³ ê° ì„±í•¨
            - customer_email: ê³ ê° ì´ë©”ì¼
            - customer_phone: ê³ ê° ì—°ë½ì²˜
            - file_uploaded: íŒŒì¼ ì—…ë¡œë“œ ì—¬ë¶€
        stored_info: ì‹œìŠ¤í…œì— ì €ì¥ëœ ê²€ì¦ ì •ë³´ (ë™ì¼í•œ êµ¬ì¡°)
    
    Returns:
        (ì „ì²´ ê²€ì¦ ì„±ê³µ ì—¬ë¶€, ê° í•„ë“œë³„ ê²€ì¦ ê²°ê³¼)
    """
    verification_results = {
        "receipt_number": False,
        "payment_info": False,  # ì¹´ë“œ ë’·ìë¦¬ ë˜ëŠ” ê³„ì¢Œë²ˆí˜¸
        "customer_name": False,
        "customer_email": False,
        "customer_phone": False,
        "file_uploaded": False
    }
    
    # íŒŒì¼ ì—…ë¡œë“œ í™•ì¸ (íŒŒì¼ì´ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜)
    if provided_info.get("file_uploaded"):
        verification_results["file_uploaded"] = True
    
    # ì˜ìˆ˜ì¦/ì˜ˆì•½ ë²ˆí˜¸ ê²€ì¦ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì œê±°)
    if provided_info.get("receipt_number") and stored_info.get("receipt_number"):
        provided_receipt = provided_info["receipt_number"].strip().upper().replace(" ", "")
        stored_receipt = stored_info["receipt_number"].strip().upper().replace(" ", "")
        verification_results["receipt_number"] = provided_receipt == stored_receipt
    
    # ê²°ì œ ì •ë³´ ê²€ì¦ (ê²°ì œ ìˆ˜ë‹¨ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
    payment_method = provided_info.get("payment_method", "")
    
    # ì¹´ë“œ ê²°ì œì¸ ê²½ìš°
    if "ì¹´ë“œ" in payment_method or "card" in payment_method.lower():
        if provided_info.get("card_last4") and stored_info.get("card_last4"):
            provided_card = "".join(filter(str.isdigit, provided_info["card_last4"]))[-4:]
            stored_card = "".join(filter(str.isdigit, stored_info["card_last4"]))[-4:]
            verification_results["payment_info"] = provided_card == stored_card and len(provided_card) == 4
    
    # ì˜¨ë¼ì¸ë±…í‚¹ì¸ ê²½ìš°
    elif "ì˜¨ë¼ì¸ë±…í‚¹" in payment_method or "online banking" in payment_method.lower():
        if provided_info.get("account_number") and stored_info.get("account_number"):
            provided_account = "".join(filter(str.isdigit, provided_info["account_number"]))
            stored_account = "".join(filter(str.isdigit, stored_info["account_number"]))
            # ê³„ì¢Œë²ˆí˜¸ëŠ” ë§ˆì§€ë§‰ 4-6ìë¦¬ ë¹„êµ
            verification_results["payment_info"] = provided_account[-6:] == stored_account[-6:] or provided_account[-4:] == stored_account[-4:]
    
    # ì¹´ì¹´ì˜¤í˜ì´, ë„¤ì´ë²„í˜ì´, GrabPay, Touch N Go ë“±ì€ ê²°ì œ ìˆ˜ë‹¨ ì •ë³´ë§Œìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥
    elif payment_method and stored_info.get("payment_method"):
        provided_payment = payment_method.strip().lower()
        stored_payment = stored_info["payment_method"].strip().lower()
        verification_results["payment_info"] = provided_payment == stored_payment
    
    # ê³ ê° ì„±í•¨ ê²€ì¦ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì •ê·œí™”)
    if provided_info.get("customer_name") and stored_info.get("customer_name"):
        provided_name = " ".join(provided_info["customer_name"].strip().split()).upper()
        stored_name = " ".join(stored_info["customer_name"].strip().split()).upper()
        verification_results["customer_name"] = provided_name == stored_name
    
    # ì´ë©”ì¼ ê²€ì¦ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    if provided_info.get("customer_email") and stored_info.get("customer_email"):
        provided_email = provided_info["customer_email"].strip().lower()
        stored_email = stored_info["customer_email"].strip().lower()
        verification_results["customer_email"] = provided_email == stored_email
    
    # ì—°ë½ì²˜ ê²€ì¦ (ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ)
    if provided_info.get("customer_phone") and stored_info.get("customer_phone"):
        provided_phone = "".join(filter(str.isdigit, provided_info["customer_phone"]))
        stored_phone = "".join(filter(str.isdigit, stored_info["customer_phone"]))
        # ë§ˆì§€ë§‰ 4-10ìë¦¬ ë¹„êµ (êµ­ê°€ì½”ë“œ ì œì™¸)
        verification_results["customer_phone"] = provided_phone[-10:] == stored_phone[-10:] or provided_phone[-4:] == stored_phone[-4:]
    
    # ì „ì²´ ê²€ì¦ ì„±ê³µ ì—¬ë¶€: ìµœì†Œ 3ê°œ ì´ìƒ ì¼ì¹˜í•´ì•¼ í•¨ (íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì¶”ê°€ ì ìˆ˜)
    matched_count = sum(verification_results.values())
    # íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš° 2ì  ì¶”ê°€
    if verification_results["file_uploaded"]:
        matched_count += 1
    is_verified = matched_count >= 3
    
    return is_verified, verification_results


def check_if_customer_provided_verification_info(messages: List[Dict[str, Any]]) -> bool:
    """
    ê³ ê°ì´ ê²€ì¦ ì •ë³´ë¥¼ ì œê³µí–ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    ê³ ê° ë©”ì‹œì§€ì—ì„œ ì˜ìˆ˜ì¦, ì˜ˆì•½ë²ˆí˜¸, ì¹´ë“œ, ê²°ì œìˆ˜ë‹¨, ì„±í•¨, ì´ë©”ì¼, ì—°ë½ì²˜ ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ê²€ì¦ ì •ë³´ ì œê³µ ì—¬ë¶€
    """
    if not messages:
        return False
    
    # ìµœê·¼ ê³ ê° ë©”ì‹œì§€ í™•ì¸ (ìµœê·¼ 10ê°œê¹Œì§€ í™•ì¸)
    # ëª¨ë“  ë©”ì‹œì§€ë¥¼ í™•ì¸í•˜ì—¬ ê³ ê° ê´€ë ¨ ë©”ì‹œì§€ ì¶”ì¶œ
    recent_customer_messages = []
    for msg in messages[-10:]:
        role = msg.get("role", "")
        content = msg.get("content", "")
        # ê³ ê° ì—­í•  í™•ì¸ (ë” í¬ê´„ì ìœ¼ë¡œ)
        if role in ["customer", "customer_rebuttal", "initial_query"] or "customer" in role.lower():
            recent_customer_messages.append(content)
    
    # ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ False
    if not recent_customer_messages:
        # ë””ë²„ê¹…: ë©”ì‹œì§€ê°€ ì—†ëŠ” ê²½ìš° ë¡œê·¸
        print(f"[DEBUG] No customer messages found. Total messages: {len(messages)}")
        if messages:
            print(f"[DEBUG] Available roles: {[msg.get('role') for msg in messages[-5:]]}")
        return False
    
    # ëª¨ë“  ê³ ê° ë©”ì‹œì§€ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•© (ì›ë³¸ ëŒ€ì†Œë¬¸ì ìœ ì§€)
    combined_text_original = " ".join(recent_customer_messages)
    combined_text = combined_text_original.lower()
    
    import re
    
    # ìˆ«ì íŒ¨í„´ í™•ì¸ (ì˜ˆì•½ ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ ë“±)
    has_numbers = bool(re.search(r'\d{4,}', combined_text))  # 4ìë¦¬ ì´ìƒ ìˆ«ì
    
    # ì´ë©”ì¼ íŒ¨í„´ í™•ì¸
    has_email = bool(re.search(r'[\w\.-]+@[\w\.-]+\.\w+', combined_text_original))
    
    # ì „í™”ë²ˆí˜¸ íŒ¨í„´ í™•ì¸ (010-1234-5678, 010 1234 5678, 01012345678 ë“±)
    has_phone = bool(re.search(r'010[-.\s]?\d{3,4}[-.\s]?\d{4}', combined_text_original) or 
                     re.search(r'010\d{8}', combined_text_original))
    
    # ìµœì†Œ 2ê°œ ì´ìƒì˜ ê²€ì¦ ì •ë³´ê°€ ìˆì–´ì•¼ í•¨
    info_count = 0
    
    # 1. ì˜ˆì•½/ì˜ìˆ˜ì¦ ë²ˆí˜¸ í™•ì¸ (ë” í¬ê´„ì ìœ¼ë¡œ)
    if (re.search(r'ì˜ˆì•½\s*ë²ˆí˜¸', combined_text) or 
        re.search(r'ì˜ìˆ˜ì¦\s*ë²ˆí˜¸', combined_text) or
        re.search(r'ì˜ˆì•½.*[:ï¼š]\s*\d{4,}', combined_text_original) or
        re.search(r'ì˜ìˆ˜ì¦.*[:ï¼š]\s*\d{4,}', combined_text_original) or
        re.search(r'ì˜ˆì•½ë²ˆí˜¸.*[:ï¼š]\s*\d{4,}', combined_text_original) or
        re.search(r'ì˜ˆì•½.*\d{4,}', combined_text) or
        re.search(r'ì˜ìˆ˜ì¦.*\d{4,}', combined_text) or
        re.search(r'booking.*number', combined_text) or
        re.search(r'receipt.*number', combined_text) or
        re.search(r'\d{5,}', combined_text_original)):  # 5ìë¦¬ ì´ìƒ ìˆ«ìë„ ì˜ˆì•½ë²ˆí˜¸ë¡œ ê°„ì£¼
        info_count += 1
    
    # 2. ê²°ì œ ìˆ˜ë‹¨ í™•ì¸ (ì¹´ë“œ, ì¹´ì¹´ì˜¤í˜ì´, ë„¤ì´ë²„í˜ì´, VISA, Master ë“±)
    payment_keywords = [
        "ì¹´ë“œ", "card", "visa", "master", "amex", "american express",
        "ì‹ ìš©ì¹´ë“œ", "ì²´í¬ì¹´ë“œ", "credit card", "debit card",
        "ì¹´ì¹´ì˜¤í˜ì´", "kakao", "kakaopay",
        "ë„¤ì´ë²„í˜ì´", "naver", "naverpay",
        "ì˜¨ë¼ì¸ë±…í‚¹", "online banking", "online",
        "grabpay", "grab pay", "grab",
        "touch n go", "touch n' go", "tng",
        "ê²°ì œ ìˆ˜ë‹¨", "payment method", "payment", "ê²°ì œí•˜", "ê²°ì œ ë‚´ì—­"
    ]
    # ê²°ì œ ìˆ˜ë‹¨ í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, "ê²°ì œ ìˆ˜ë‹¨ :" ê°™ì€ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš°
    if (any(kw in combined_text for kw in payment_keywords) or
        re.search(r'ê²°ì œ\s*ìˆ˜ë‹¨\s*[:ï¼š]', combined_text_original)):
        info_count += 1
    
    # 3. ì„±í•¨ í™•ì¸ (ì´ë¦„, ì„±í•¨ í‚¤ì›Œë“œ + í•œê¸€/ì˜ë¬¸ ì´ë¦„ íŒ¨í„´)
    name_keywords = ["ì„±í•¨", "ì´ë¦„", "name", "ì œ ì´ë¦„", "ê³ ê°ë‹˜ì˜ ì„±í•¨", "my name", "ê³ ê°ë‹˜ì˜ ì´ë¦„"]
    # í•œê¸€ ì´ë¦„ íŒ¨í„´ (2-4ì í•œê¸€, ì„±í•¨/ì´ë¦„ í‚¤ì›Œë“œ ë’¤ì— ì˜¤ëŠ” ê²½ìš°)
    korean_name_pattern = (
        re.search(r'ì„±í•¨\s*[:ï¼š]\s*[ê°€-í£]{2,4}', combined_text_original) or
        re.search(r'ì´ë¦„\s*[:ï¼š]\s*[ê°€-í£]{2,4}', combined_text_original) or
        re.search(r'ê³ ê°ë‹˜ì˜\s*ì„±í•¨\s*[:ï¼š]\s*[ê°€-í£]{2,4}', combined_text_original) or
        re.search(r'[ê°€-í£]{2,4}', combined_text_original)  # ë‹¨ìˆœ í•œê¸€ ì´ë¦„ íŒ¨í„´ë„ í™•ì¸
    )
    # ì˜ë¬¸ ì´ë¦„ íŒ¨í„´
    english_name_pattern = re.search(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b', combined_text_original)
    
    if (any(kw in combined_text for kw in name_keywords) or 
        korean_name_pattern or 
        english_name_pattern):
        info_count += 1
    
    # 4. ì´ë©”ì¼ í™•ì¸
    if has_email:
        info_count += 1
    
    # 5. ì—°ë½ì²˜ í™•ì¸
    if (has_phone or 
        re.search(r'ì—°ë½ì²˜', combined_text) or 
        re.search(r'ì „í™”ë²ˆí˜¸', combined_text) or
        re.search(r'phone', combined_text)):
        info_count += 1
    
    # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì •ë³´ê°€ ì œê³µë˜ì—ˆê±°ë‚˜, ì´ë©”ì¼/ì „í™”ë²ˆí˜¸ ì¤‘ í•˜ë‚˜ì™€ ë‹¤ë¥¸ ì •ë³´ê°€ í•¨ê»˜ ìˆëŠ” ê²½ìš°
    # ë˜ëŠ” ìˆ«ì(ì˜ˆì•½ë²ˆí˜¸)ì™€ ê²°ì œìˆ˜ë‹¨/ì„±í•¨ì´ í•¨ê»˜ ìˆëŠ” ê²½ìš°
    result = info_count >= 2 or (has_email and info_count >= 1) or (has_phone and info_count >= 1)
    
    # ë””ë²„ê¹…: ê°ì§€ ê²°ê³¼ ë¡œê·¸
    print(f"[DEBUG] Verification info detection:")
    print(f"  - Combined text (first 200 chars): {combined_text_original[:200]}")
    print(f"  - Info count: {info_count}")
    print(f"  - Has email: {has_email}")
    print(f"  - Has phone: {has_phone}")
    print(f"  - Has numbers: {has_numbers}")
    print(f"  - Result: {result}")
    
    return result


def check_if_login_related_inquiry(customer_query: str) -> bool:
    """
    ê³ ê° ë¬¸ì˜ê°€ ë¡œê·¸ì¸/ê³„ì • ê´€ë ¨ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        customer_query: ê³ ê° ë¬¸ì˜ ë‚´ìš©
    
    Returns:
        ë¡œê·¸ì¸/ê³„ì • ê´€ë ¨ ë¬¸ì˜ì¸ì§€ ì—¬ë¶€
    """
    if not customer_query or not customer_query.strip():
        return False
    
    login_keywords = [
        "ë¡œê·¸ì¸", "login", "ãƒ­ã‚°ã‚¤ãƒ³",
        "ê³„ì •", "account", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ",
        "ë¹„ë°€ë²ˆí˜¸", "password", "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰",
        "ì•„ì´ë””", "id", "ID", "ãƒ¦ãƒ¼ã‚¶ãƒ¼ID",
        "ì ‘ì†", "access", "ã‚¢ã‚¯ã‚»ã‚¹",
        "ì¸ì¦", "authentication", "èªè¨¼",
        "ë¡œê·¸ì¸ ì•ˆë¨", "cannot login", "ãƒ­ã‚°ã‚¤ãƒ³ã§ããªã„",
        "ë¡œê·¸ì¸ ì˜¤ë¥˜", "login error", "ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼",
        "ë¡œê·¸ì¸ ì•ˆ", "ë¡œê·¸ì¸ ì‹¤íŒ¨", "ë¡œê·¸ì¸ ë¬¸ì œ",
        "ê³„ì • ë¬¸ì œ", "ê³„ì • ì ê¸ˆ", "ê³„ì • ì˜¤ë¥˜",
        "account problem", "account error", "account locked",
        "password reset", "ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì •", "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒªã‚»ãƒƒãƒˆ",
        "forgot password", "ë¹„ë°€ë²ˆí˜¸ ë¶„ì‹¤", "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å¿˜ã‚Œ"
    ]
    
    query_lower = customer_query.lower()
    # ê° í‚¤ì›Œë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ í™•ì¸ (ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­)
    for keyword in login_keywords:
        if keyword.lower() in query_lower:
            return True
    
    return False

