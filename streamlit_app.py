# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ========================================
# streamlit_app.py (ì „ì²´ ìˆ˜ì •ëœ ì½”ë“œ)
#
# ì£¼ìš” ê°œì„  ì‚¬í•­:
# 1. ì±„íŒ…/ì´ë©”ì¼ íƒ­ì— 'ì „í™” ë°œì‹  (í˜„ì§€ ì—…ì²´/ê³ ê°)' ë²„íŠ¼ ë° ê¸°ëŠ¥ ì¶”ê°€ (ì˜ˆì™¸ ì²˜ë¦¬ ëŒ€ì‘)
# 2. ì „í™” íƒ­ì— 'ì „í™” ë°œì‹ ' ë²„íŠ¼ ì¶”ê°€ ë° ë°œì‹  í†µí™” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì§€ì›
# 3. ê´€ë ¨ ì–¸ì–´ íŒ© ì¶”ê°€ ë° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
# 4. í€´ì¦ˆ ê¸°ëŠ¥ì˜ ì •ë‹µ í™•ì¸, í•´ì„¤, ì ìˆ˜ í‘œì‹œ ë¡œì§ ì™„ì„±
# 5. [BUG FIX] ì–¸ì–´ ì´ê´€ ì‹œ 'ë²ˆì—­ ë‹¤ì‹œ ì‹œë„' ë²„íŠ¼ì˜ DuplicateWidgetID ì˜¤ë¥˜ í•´ê²°
# 6. [BUG FIX] ì½˜í…ì¸  ìƒì„± íƒ­ì˜ LLM ì‘ë‹µ ë° ë¼ë””ì˜¤ ë²„íŠ¼ ì´ˆê¸°í™” ì˜¤ë¥˜ í•´ê²°
# â­ [ì „í™” ì•„ë°”íƒ€ ë²„ê·¸ ìˆ˜ì •]
# 7. ì „í™” ì‘ë‹µ í›„ ì¸ì‚¬ë§ ë¯¸ì¶œë ¥ ì˜¤ë¥˜ ìˆ˜ì • (just_entered_call í”Œë˜ê·¸ ìœ„ì¹˜ ìˆ˜ì •)
# 8. ì•„ë°”íƒ€ Lottie íŒŒì¼ ë¡œë”© ê²½ë¡œ ìˆ˜ì • (ì—…ë¡œë“œëœ íŒŒì¼ëª… ì°¸ì¡°)
# ========================================

# â­ OpenMP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶©ëŒ í•´ê²°
# ì—¬ëŸ¬ OpenMP ëŸ°íƒ€ì„ì´ ë™ì‹œì— ë¡œë“œë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import io
import json
import time
import uuid
import base64
import tempfile
import hashlib
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Union, Tuple
import google.generativeai as genai
import numpy as np
import streamlit as st
from matplotlib import pyplot as plt
import requests  # â­ ì¶”ê°€: requests ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots

    IS_PLOTLY_AVAILABLE = True
except ImportError:
    IS_PLOTLY_AVAILABLE = False

from openai import OpenAI
from anthropic import Anthropic

# mic_recorder (0.0.8) - returns dict with key "bytes"
from streamlit_mic_recorder import mic_recorder

# LangChain / RAG ê´€ë ¨
from langchain_core.documents import Document
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    raise ImportError(
        "âŒ 'langchain-text-splitters' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install langchain-text-splitters\n"
        "ë˜ëŠ” requirements.txtì˜ ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜: pip install -r requirements.txt"
    )
from langchain_core.prompts import PromptTemplate
try:
    # ìµœì‹  langchainì—ì„œëŠ” ì—¬ëŸ¬ ê²½ë¡œë¥¼ ì‹œë„
    try:
        from langchain.memory import ConversationBufferMemory
    except ImportError:
        try:
            from langchain_classic.memory import ConversationBufferMemory
        except ImportError:
            from langchain_core.memory import ConversationBufferMemory
except ImportError:
    raise ImportError(
        "âŒ 'langchain' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ 'langchain.memory' ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install langchain langchain-classic\n"
        "ë˜ëŠ” requirements.txtì˜ ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜: pip install -r requirements.txt"
    )
try:
    # ìµœì‹  langchainì—ì„œëŠ” ì—¬ëŸ¬ ê²½ë¡œë¥¼ ì‹œë„
    try:
        from langchain.chains import ConversationChain
    except ImportError:
        try:
            from langchain_classic.chains import ConversationChain
        except ImportError:
            # ConversationChainì´ ì—†ì„ ê²½ìš° Noneìœ¼ë¡œ ì„¤ì • (ì½”ë“œì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
            ConversationChain = None
except ImportError:
    # ConversationChainì€ ì„ íƒì ì´ë¯€ë¡œ ImportErrorë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ
    ConversationChain = None

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

# Word, PPTX, PDF ìƒì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from docx import Document as DocxDocument
    from docx.shared import Pt, RGBColor, Inches
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    IS_DOCX_AVAILABLE = True
except ImportError:
    IS_DOCX_AVAILABLE = False

try:
    from pptx import Presentation
    from pptx.util import Inches, Pt
    IS_PPTX_AVAILABLE = True
except ImportError:
    IS_PPTX_AVAILABLE = False

try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.lib import colors
    from reportlab.lib.colors import black
    IS_REPORTLAB_AVAILABLE = True
except ImportError:
    IS_REPORTLAB_AVAILABLE = False



try:
    from langchain_google_genai import GoogleGenerativeAIEmbeddings

    IS_GEMINI_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_GEMINI_EMBEDDING_AVAILABLE = False

try:
    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

    IS_NVIDIA_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_NVIDIA_EMBEDDING_AVAILABLE = False

# ========================================
# Streamlit í˜ì´ì§€ ì„¤ì • (ë°˜ë“œì‹œ ìµœìƒë‹¨ì— ìœ„ì¹˜)
# ========================================
st.set_page_config(
    page_title="AI Study Coach & Customer Service Simulator",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ========================================
# 0. ê¸°ë³¸ ê²½ë¡œ/ë¡œì»¬ DB ì„¤ì •
# ========================================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "local_db")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
RAG_INDEX_DIR = os.path.join(DATA_DIR, "rag_index")

VOICE_META_FILE = os.path.join(DATA_DIR, "voice_records.json")
SIM_META_FILE = os.path.join(DATA_DIR, "simulation_histories.json")
VIDEO_MAPPING_DB_FILE = os.path.join(DATA_DIR, "video_mapping_database.json")  # â­ Gemini ì œì•ˆ: ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤
FAQ_DB_FILE = os.path.join(DATA_DIR, "faq_database.json")  # FAQ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼
PRODUCT_IMAGE_CACHE_FILE = os.path.join(DATA_DIR, "product_image_cache.json")  # ì œí’ˆ ì´ë¯¸ì§€ ìºì‹œ íŒŒì¼
PRODUCT_IMAGE_DIR = os.path.join(DATA_DIR, "product_images")  # ìƒì„±ëœ ì œí’ˆ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(PRODUCT_IMAGE_DIR, exist_ok=True)
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(RAG_INDEX_DIR, exist_ok=True)

# ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ë„ ì´ˆê¸°í™” ì‹œ ìƒì„±
VIDEO_DIR = os.path.join(DATA_DIR, "videos")
os.makedirs(VIDEO_DIR, exist_ok=True)




# ----------------------------------------
# JSON Helper
# ----------------------------------------
def _load_json(path: str, default: Any):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    except (FileNotFoundError, json.JSONDecodeError):
        return default


def _save_json(path: str, data: Any):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# FAQ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í•¨ìˆ˜
def load_faq_database() -> Dict[str, Any]:
    """FAQ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
    return _load_json(FAQ_DB_FILE, {"companies": {}})


def save_faq_database(faq_data: Dict[str, Any]):
    """FAQ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
    _save_json(FAQ_DB_FILE, faq_data)


def get_company_info_faq(company: str, lang: str = "ko") -> Dict[str, Any]:
    """íšŒì‚¬ ì†Œê°œ ë° FAQ ê°€ì ¸ì˜¤ê¸°"""
    faq_data = load_faq_database()
    if company in faq_data.get("companies", {}):
        company_data = faq_data["companies"][company]
        return {
            "info": company_data.get(f"info_{lang}", company_data.get("info_ko", "")),
            "popular_products": company_data.get("popular_products", []),
            "trending_topics": company_data.get("trending_topics", []),
            "faqs": company_data.get("faqs", [])
        }
    return {"info": "", "popular_products": [], "trending_topics": [], "faqs": []}


def visualize_company_data(company_data: Dict[str, Any], lang: str = "ko") -> Dict[str, Any]:
    """íšŒì‚¬ ë°ì´í„° ì‹œê°í™” (Plotly ì‚¬ìš©)"""
    charts = {}
    
    if not IS_PLOTLY_AVAILABLE:
        return charts
    
    try:
        import plotly.graph_objects as go
        import plotly.express as px
        
        # ì–¸ì–´ë³„ ë ˆì´ë¸”
        lang_labels = {
            "ko": {
                "popular_products": "ì¸ê¸° ìƒí’ˆ",
                "product_name": "ìƒí’ˆëª…",
                "popularity": "ì¸ê¸°ë„",
                "trending_topics": "í™”ì œì˜ ì†Œì‹",
                "topic": "ì†Œì‹",
                "trend_score": "í™”ì œë„"
            },
            "en": {
                "popular_products": "Popular Products",
                "product_name": "Product Name",
                "popularity": "Popularity",
                "trending_topics": "Trending News",
                "topic": "News",
                "trend_score": "Trend Score"
            },
            "ja": {
                "popular_products": "äººæ°—å•†å“",
                "product_name": "å•†å“å",
                "popularity": "äººæ°—åº¦",
                "trending_topics": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "topic": "ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "trend_score": "è©±é¡Œåº¦"
            }
        }
        labels = lang_labels.get(lang, lang_labels["ko"])
        
        # ì¸ê¸° ìƒí’ˆ ì‹œê°í™”
        popular_products = company_data.get("popular_products", [])
        if popular_products:
            product_names = []
            product_scores = []
            for product in popular_products:
                name = product.get(f"text_{lang}", product.get("text_ko", ""))
                score = product.get("score", 0)
                if name:
                    product_names.append(name[:20])  # ì´ë¦„ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¦„
                    product_scores.append(score if score > 0 else 50)  # ê¸°ë³¸ê°’ 50
            
            if product_names:
                # ë§‰ëŒ€ ê·¸ë˜í”„
                fig_products_bar = go.Figure(data=[
                    go.Bar(
                        x=product_names,
                        y=product_scores,
                        marker_color='lightblue',
                        text=product_scores,
                        textposition='auto',
                    )
                ])
                fig_products_bar.update_layout(
                    title=f"{labels['popular_products']} (ë§‰ëŒ€ ê·¸ë˜í”„)",
                    xaxis_title=labels["product_name"],
                    yaxis_title=labels["popularity"],
                    height=300,
                    showlegend=False
                )
                charts["products_bar"] = fig_products_bar
                
                # ì„ í˜• ê·¸ë˜í”„ (LSTM ìŠ¤íƒ€ì¼)
                fig_products_line = go.Figure(data=[
                    go.Scatter(
                        x=product_names,
                        y=product_scores,
                        mode='lines+markers',
                        marker=dict(size=10, color='lightblue'),
                        line=dict(width=3, color='lightblue'),
                        text=product_scores,
                        textposition='top center',
                    )
                ])
                fig_products_line.update_layout(
                    title=f"{labels['popular_products']} (ì„ í˜• ê·¸ë˜í”„)",
                    xaxis_title=labels["product_name"],
                    yaxis_title=labels["popularity"],
                    height=300,
                    showlegend=False
                )
                charts["products_line"] = fig_products_line
        
        # í™”ì œì˜ ì†Œì‹ ì‹œê°í™”
        trending_topics = company_data.get("trending_topics", [])
        if trending_topics:
            topic_names = []
            topic_scores = []
            for topic in trending_topics:
                name = topic.get(f"text_{lang}", topic.get("text_ko", ""))
                score = topic.get("score", 0)
                if name:
                    topic_names.append(name[:20])
                    topic_scores.append(score if score > 0 else 50)
            
            if topic_names:
                # ë§‰ëŒ€ ê·¸ë˜í”„
                fig_topics_bar = go.Figure(data=[
                    go.Bar(
                        x=topic_names,
                        y=topic_scores,
                        marker_color='lightcoral',
                        text=topic_scores,
                        textposition='auto',
                    )
                ])
                fig_topics_bar.update_layout(
                    title=f"{labels['trending_topics']} (ë§‰ëŒ€ ê·¸ë˜í”„)",
                    xaxis_title=labels["topic"],
                    yaxis_title=labels["trend_score"],
                    height=300,
                    showlegend=False
                )
                charts["topics_bar"] = fig_topics_bar
                
                # ì„ í˜• ê·¸ë˜í”„
                fig_topics_line = go.Figure(data=[
                    go.Scatter(
                        x=topic_names,
                        y=topic_scores,
                        mode='lines+markers',
                        marker=dict(size=10, color='lightcoral'),
                        line=dict(width=3, color='lightcoral'),
                        text=topic_scores,
                        textposition='top center',
                    )
                ])
                fig_topics_line.update_layout(
                    title=f"{labels['trending_topics']} (ì„ í˜• ê·¸ë˜í”„)",
                    xaxis_title=labels["topic"],
                    yaxis_title=labels["trend_score"],
                    height=300,
                    showlegend=False
                )
                charts["topics_line"] = fig_topics_line
        
    except Exception as e:
        pass  # ì‹œê°í™” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    return charts


def load_product_image_cache() -> Dict[str, str]:
    """ì œí’ˆ ì´ë¯¸ì§€ ìºì‹œ ë¡œë“œ"""
    return _load_json(PRODUCT_IMAGE_CACHE_FILE, {})


def save_product_image_cache(cache_data: Dict[str, str]):
    """ì œí’ˆ ì´ë¯¸ì§€ ìºì‹œ ì €ì¥"""
    _save_json(PRODUCT_IMAGE_CACHE_FILE, cache_data)


def generate_product_image_prompt(product_name: str) -> str:
    """ì œí’ˆëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    product_lower = product_name.lower()
    
    # ì–¸ì–´ë³„ ì œí’ˆëª… ì¶”ì¶œ (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´)
    lang_versions = []
    if any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in product_name):  # í•œê¸€ í¬í•¨
        lang_versions.append(("ko", product_name))
    if any(ord(c) >= 0x3040 and ord(c) <= 0x309F or ord(c) >= 0x30A0 and ord(c) <= 0x30FF for c in product_name):  # ì¼ë³¸ì–´ í¬í•¨
        lang_versions.append(("ja", product_name))
    if any(c.isalpha() and ord(c) < 128 for c in product_name):  # ì˜ì–´ í¬í•¨
        lang_versions.append(("en", product_name))
    
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    base_prompt = f"Professional product photo of {product_name}, "
    
    # ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    if "ë””ì¦ˆë‹ˆ" in product_name or "disney" in product_lower or "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼" in product_name:
        return f"Beautiful, vibrant photo of Disneyland theme park entrance ticket for {product_name}, magical atmosphere, colorful, professional product photography, high quality, commercial style"
    elif "ìœ ë‹ˆë²„ì…œ" in product_name or "universal" in product_lower or "ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«" in product_name:
        return f"Professional photo of Universal Studios theme park ticket for {product_name}, exciting theme park atmosphere, high quality product photography, commercial style"
    elif "ìŠ¤ì¹´ì´íŠ¸ë¦¬" in product_name or "skytree" in product_lower or "ë„ì¿„ íƒ€ì›Œ" in product_name or "tokyo tower" in product_lower or "ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼" in product_name or "æ±äº¬ã‚¿ãƒ¯ãƒ¼" in product_name:
        return f"Beautiful photo of Tokyo Skytree or Tokyo Tower admission ticket for {product_name}, modern Tokyo cityscape background, professional product photography, high quality"
    elif "ê°¤ëŸ­ì‹œ" in product_name or "galaxy" in product_lower:
        return f"Professional product photo of Samsung Galaxy smartphone {product_name}, sleek modern design, premium quality, white background, commercial product photography, high resolution"
    elif "qled" in product_lower or "tv" in product_lower or "í‹°ë¹„" in product_name or "í…”ë ˆë¹„ì „" in product_name:
        return f"Professional product photo of Samsung QLED TV {product_name}, modern sleek design, premium quality, minimalist background, commercial product photography, high resolution"
    elif "í‹°ì¼“" in product_name or "ticket" in product_lower or "ãƒã‚±ãƒƒãƒˆ" in product_name:
        return f"Professional photo of admission ticket for {product_name}, clean design, high quality product photography, commercial style"
    elif "í˜¸í…”" in product_name or "hotel" in product_lower or "ãƒ›ãƒ†ãƒ«" in product_name:
        return f"Beautiful photo of hotel booking voucher or hotel room for {product_name}, luxurious atmosphere, professional photography, high quality"
    elif "í•­ê³µ" in product_name or "flight" in product_lower or "èˆªç©º" in product_name:
        return f"Professional photo of airline ticket or boarding pass for {product_name}, clean design, high quality product photography"
    elif "ì—¬í–‰" in product_name or "travel" in product_lower or "íˆ¬ì–´" in product_name or "tour" in product_lower or "æ—…è¡Œ" in product_name or "ãƒ„ã‚¢ãƒ¼" in product_name:
        return f"Beautiful travel-related photo for {product_name}, scenic destination, professional photography, high quality, travel brochure style"
    elif "ìŒì‹" in product_name or "food" in product_lower or "ë ˆìŠ¤í† ë‘" in product_name or "restaurant" in product_lower or "é£Ÿäº‹" in product_name or "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³" in product_name:
        return f"Appetizing food photo for {product_name}, restaurant dish, professional food photography, high quality, commercial style"
    else:
        return f"Professional product photo of {product_name}, clean background, high quality product photography, commercial style, well-lit"


def generate_product_image_with_ai(product_name: str) -> str:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì œí’ˆ ì´ë¯¸ì§€ ìƒì„± (DALL-E ì‚¬ìš©)"""
    try:
        # ìºì‹œ í™•ì¸
        cache = load_product_image_cache()
        cache_key = product_name.lower().strip()
        
        if cache_key in cache:
            cached_path = cache[cache_key]
            if os.path.exists(cached_path):
                return cached_path
        
        # OpenAI API í‚¤ í™•ì¸
        openai_key = get_api_key("openai")
        if not openai_key:
            # OpenAI í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¯¸ì§€ URL ë°˜í™˜
            return ""
        
        # ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ìƒì„±
        image_prompt = generate_product_image_prompt(product_name)
        
        # DALL-E API í˜¸ì¶œ
        client = OpenAI(api_key=openai_key)
        response = client.images.generate(
            model="dall-e-3",
            prompt=image_prompt,
            size="1024x1024",
            quality="standard",
            n=1,
        )
        
        # ìƒì„±ëœ ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°
        image_url = response.data[0].url
        
        # ì´ë¯¸ì§€ë¥¼ ë¡œì»¬ì— ì €ì¥
        import hashlib
        image_hash = hashlib.md5(product_name.encode('utf-8')).hexdigest()
        image_filename = f"{image_hash}.png"
        image_path = os.path.join(PRODUCT_IMAGE_DIR, image_filename)
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
        img_response = requests.get(image_url, timeout=10)
        if img_response.status_code == 200:
            with open(image_path, 'wb') as f:
                f.write(img_response.content)
            
            # ìºì‹œì— ì €ì¥
            cache[cache_key] = image_path
            save_product_image_cache(cache)
            
            return image_path
        else:
            return ""
            
    except Exception as e:
        print(f"âš ï¸ AI ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ({product_name}): {e}")
        return ""


def get_product_image_url(product_name: str) -> str:
    """ìƒí’ˆëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ URL ìƒì„± - AI ì´ë¯¸ì§€ ìƒì„± ìš°ì„  ì‚¬ìš©"""
    try:
        # â­ 1ìˆœìœ„: AI ì´ë¯¸ì§€ ìƒì„± ì‹œë„ (DALL-E)
        ai_image_path = generate_product_image_with_ai(product_name)
        if ai_image_path and os.path.exists(ai_image_path):
            return ai_image_path
        
        # â­ 2ìˆœìœ„: ê¸°ì¡´ í‚¤ì›Œë“œ ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ (í´ë°±)
        product_lower = product_name.lower()
        
        # ë””ì¦ˆë‹ˆëœë“œ ê´€ë ¨ ìƒí’ˆ - ë¯¸í‚¤ë§ˆìš°ìŠ¤ ì´ë¯¸ì§€ (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ëª¨ë‘ ì²´í¬)
        if ("ë””ì¦ˆë‹ˆ" in product_name or "disney" in product_lower or "disneyland" in product_lower or 
            "tokyo disneyland" in product_lower or "hong kong disneyland" in product_lower or
            "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼" in product_name or "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ãƒ©ãƒ³ãƒ‰" in product_name):
            return "https://images.unsplash.com/photo-1606813907291-d86efa9b94db?w=400&h=300&fit=crop&q=80"
        
        # ìœ ë‹ˆë²„ì…œ ìŠ¤íŠœë””ì˜¤ ê´€ë ¨ ìƒí’ˆ - ìœ ë‹ˆë²„ì…œ ë¡œê³ /ì§€êµ¬ë³¸ ì´ë¯¸ì§€ (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ëª¨ë‘ ì²´í¬)
        if ("ìœ ë‹ˆë²„ì…œ" in product_name or "universal" in product_lower or "universal studio" in product_lower or
            "universal studios" in product_lower or "ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«" in product_name or "ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ã‚¹ã‚¿ã‚¸ã‚ª" in product_name):
            return "https://images.unsplash.com/photo-1526304640581-d334cdbbf45e?w=400&h=300&fit=crop&q=80"
        
        # ë„ì¿„ ìŠ¤ì¹´ì´íŠ¸ë¦¬ ê´€ë ¨ ìƒí’ˆ - ìŠ¤ì¹´ì´íŠ¸ë¦¬ ê±´ë¬¼ ì´ë¯¸ì§€ (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ëª¨ë‘ ì²´í¬)
        if ("ìŠ¤ì¹´ì´íŠ¸ë¦¬" in product_name or "skytree" in product_lower or "ë„ì¿„ íƒ€ì›Œ" in product_name or 
            "tokyo tower" in product_lower or "tokyo skytree" in product_lower or
            "ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼" in product_name or "æ±äº¬ã‚¿ãƒ¯ãƒ¼" in product_name or "æ±äº¬ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼" in product_name):
            return "https://images.unsplash.com/photo-1540959733332-eab4deabeeaf?w=400&h=300&fit=crop&q=80"
        
        # í™ì½© ê´€ë ¨ ìƒí’ˆ (ë””ì¦ˆë‹ˆëœë“œ ì™¸)
        if ("í™ì½©" in product_name or "hong kong" in product_lower or "é¦™æ¸¯" in product_name):
            if "disney" not in product_lower and "ë””ì¦ˆë‹ˆ" not in product_name:
                # í™ì½© ê³µí•­ ìµìŠ¤í”„ë ˆìŠ¤ ë“±
                return "https://images.unsplash.com/photo-1552465011-b4e21bf6e79a?w=400&h=300&fit=crop&q=80"
        
        # ë°©ì½• ê´€ë ¨ ìƒí’ˆ (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ëª¨ë‘ ì²´í¬)
        if ("ë°©ì½•" in product_name or "bangkok" in product_lower or "ãƒãƒ³ã‚³ã‚¯" in product_name):
            return "https://images.unsplash.com/photo-1552465011-b4e21bf6e79a?w=400&h=300&fit=crop&q=80"
        
        # ì‚¼ì„± ê°¤ëŸ­ì‹œ S ì‹œë¦¬ì¦ˆ ê´€ë ¨ ìƒí’ˆ
        if ("ê°¤ëŸ­ì‹œ s" in product_lower or "galaxy s" in product_lower or "galaxy s24" in product_lower or
            "galaxy s23" in product_lower or "galaxy s22" in product_lower or "galaxy s21" in product_lower or
            "galaxy s20" in product_lower or "samsung galaxy s" in product_lower):
            return "https://images.unsplash.com/photo-1610945265064-0e34e5519bbf?w=400&h=300&fit=crop&q=80"
        
        # ì‚¼ì„± ê°¤ëŸ­ì‹œ ë…¸íŠ¸ ì‹œë¦¬ì¦ˆ ê´€ë ¨ ìƒí’ˆ
        if ("ê°¤ëŸ­ì‹œ ë…¸íŠ¸" in product_lower or "galaxy note" in product_lower or "galaxy note24" in product_lower or
            "galaxy note23" in product_lower or "galaxy note22" in product_lower or "galaxy note21" in product_lower or
            "galaxy note20" in product_lower or "samsung galaxy note" in product_lower):
            return "https://images.unsplash.com/photo-1610945265064-0e34e5519bbf?w=400&h=300&fit=crop&q=80"
        
        # ì‚¼ì„± QLED TV ê´€ë ¨ ìƒí’ˆ
        if ("qled" in product_lower or "ì‚¼ì„± qled" in product_lower or "samsung qled" in product_lower or
            "ì‚¼ì„± tv" in product_lower or "samsung tv" in product_lower):
            return "https://images.unsplash.com/photo-1593359677879-a4b92c0a3b8b?w=400&h=300&fit=crop&q=80"
        
        # ì‚¼ì„± ì œí’ˆ ì¼ë°˜ (ìœ„ì—ì„œ ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš°)
        if ("ì‚¼ì„±" in product_name or "samsung" in product_lower):
            # ìŠ¤ë§ˆíŠ¸í° ê´€ë ¨
            if ("ìŠ¤ë§ˆíŠ¸í°" in product_name or "smartphone" in product_lower or "phone" in product_lower or
                "ê°¤ëŸ­ì‹œ" in product_name or "galaxy" in product_lower):
                return "https://images.unsplash.com/photo-1610945265064-0e34e5519bbf?w=400&h=300&fit=crop&q=80"
            # TV ê´€ë ¨
            elif ("tv" in product_lower or "í‹°ë¹„" in product_name or "í…”ë ˆë¹„ì „" in product_name):
                return "https://images.unsplash.com/photo-1593359677879-a4b92c0a3b8b?w=400&h=300&fit=crop&q=80"
            # ê¸°ë³¸ ì‚¼ì„± ì œí’ˆ
            else:
                return "https://images.unsplash.com/photo-1610945265064-0e34e5519bbf?w=400&h=300&fit=crop&q=80"
        
        # í‹°ì¼“ ê´€ë ¨ ìƒí’ˆ
        if ("í‹°ì¼“" in product_name or "ticket" in product_lower or "ãƒã‚±ãƒƒãƒˆ" in product_name):
            return "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=300&fit=crop&q=80"
        
        # í˜¸í…” ê´€ë ¨ ìƒí’ˆ
        if ("í˜¸í…”" in product_name or "hotel" in product_lower or "ãƒ›ãƒ†ãƒ«" in product_name):
            return "https://images.unsplash.com/photo-1566073771259-6a8506099945?w=400&h=300&fit=crop&q=80"
        
        # í•­ê³µ ê´€ë ¨ ìƒí’ˆ
        if ("í•­ê³µ" in product_name or "flight" in product_lower or "èˆªç©º" in product_name or "airline" in product_lower):
            return "https://images.unsplash.com/photo-1436491865332-7a61a109cc05?w=400&h=300&fit=crop&q=80"
        
        # ì—¬í–‰/íˆ¬ì–´ ê´€ë ¨ ìƒí’ˆ
        if ("ì—¬í–‰" in product_name or "travel" in product_lower or "íˆ¬ì–´" in product_name or "tour" in product_lower or
            "æ—…è¡Œ" in product_name or "ãƒ„ã‚¢ãƒ¼" in product_name):
            return "https://images.unsplash.com/photo-1488646953014-85cb44e25828?w=400&h=300&fit=crop&q=80"
        
        # ìŒì‹/ë ˆìŠ¤í† ë‘ ê´€ë ¨ ìƒí’ˆ
        if ("ìŒì‹" in product_name or "food" in product_lower or "ë ˆìŠ¤í† ë‘" in product_name or "restaurant" in product_lower or
            "é£Ÿäº‹" in product_name or "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³" in product_name):
            return "https://images.unsplash.com/photo-1504674900247-0877df9cc836?w=400&h=300&fit=crop&q=80"
        
        # ê¸°ë³¸ê°’: ìƒí’ˆëª… ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ ì´ë¯¸ì§€ ìƒì„±
        # ì œí’ˆ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ë¡ í•˜ì—¬ ì ì ˆí•œ ì´ë¯¸ì§€ ì„ íƒ
        import hashlib
        
        # ì œí’ˆëª…ì„ í•´ì‹œí•˜ì—¬ ì¼ê´€ëœ ì´ë¯¸ì§€ ID ìƒì„±
        hash_obj = hashlib.md5(product_name.encode('utf-8'))
        hash_int = int(hash_obj.hexdigest(), 16)
        image_seed = hash_int % 1000
        
        # ì¹´í…Œê³ ë¦¬ë³„ Unsplash ì´ë¯¸ì§€ (ë” ì•ˆì •ì ì¸ ì´ë¯¸ì§€ ID ì‚¬ìš©)
        category_images = [
            "https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400&h=300&fit=crop&q=80",  # í‹°ì¼“/ì—¬í–‰
            "https://images.unsplash.com/photo-1488646953014-85cb44e25828?w=400&h=300&fit=crop&q=80",  # ì—¬í–‰ì§€
            "https://images.unsplash.com/photo-1566073771259-6a8506099945?w=400&h=300&fit=crop&q=80",  # í˜¸í…”
            "https://images.unsplash.com/photo-1436491865332-7a61a109cc05?w=400&h=300&fit=crop&q=80",  # í•­ê³µ
            "https://images.unsplash.com/photo-1504674900247-0877df9cc836?w=400&h=300&fit=crop&q=80",  # ìŒì‹
        ]
        
        # í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ ì´ë¯¸ì§€ ì„ íƒ
        selected_image = category_images[image_seed % len(category_images)]
        return selected_image
    except Exception:
        return ""


def search_faq(faq_data: Dict[str, Any], company: str, query: str, lang: str = "ko") -> List[Dict[str, str]]:
    """FAQ ê²€ìƒ‰"""
    if not query or not query.strip():
        return []
    
    results = []
    query_lower = query.lower().strip()
    
    # íšŒì‚¬ë³„ FAQ ê²€ìƒ‰
    if company and company in faq_data.get("companies", {}):
        company_faqs = faq_data["companies"][company].get("faqs", [])
        for faq in company_faqs:
            question = faq.get(f"question_{lang}", faq.get("question_ko", ""))
            answer = faq.get(f"answer_{lang}", faq.get("answer_ko", ""))
            
            if query_lower in question.lower() or query_lower in answer.lower():
                results.append({
                    "question": question,
                    "answer": answer,
                    "company": company
                })
    
    # ê¸°ë³¸ FAQ ê²€ìƒ‰ (íšŒì‚¬ê°€ ì—†ê±°ë‚˜ ê¸°ë³¸ ì„¤ì •ì¸ ê²½ìš°)
    default_settings_texts = ["ê¸°ë³¸ ì„¤ì •", "Default Settings", "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"]
    if not company or company in default_settings_texts:
        default_faqs = faq_data.get("default", {}).get("faqs", [])
        for faq in default_faqs:
            question = faq.get(f"question_{lang}", faq.get("question_ko", ""))
            answer = faq.get(f"answer_{lang}", faq.get("answer_ko", ""))
            
            if query_lower in question.lower() or query_lower in answer.lower():
                results.append({
                    "question": question,
                    "answer": answer,
                    "company": "ê¸°ë³¸"
                })
    
    return results


def get_common_product_faqs(company_name: str, lang: str = "ko") -> List[Dict[str, str]]:
    """ê³µë™ ëŒ€í‘œ ì œí’ˆ FAQ ë°˜í™˜"""
    company_lower = company_name.lower()
    common_faqs = []
    
    # Klook ê³µë™ ì œí’ˆ FAQ
    if "klook" in company_lower or "í´ë£©" in company_name:
        if lang == "ko":
            common_faqs = [
                {
                    "question_ko": "eSIMì€ ì–´ë–¤ êµ­ê°€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                    "answer_ko": "eSIMì€ ì „ ì„¸ê³„ ëŒ€ë¶€ë¶„ì˜ êµ­ê°€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. ì£¼ìš” ì—¬í–‰ì§€ì¸ ìœ ëŸ½, ì•„ì‹œì•„, ì•„ë©”ë¦¬ì¹´, ì˜¤ì„¸ì•„ë‹ˆì•„ ë“± ì „ ì„¸ê³„ 190ê°œ ì´ìƒì˜ êµ­ê°€ì™€ ì§€ì—­ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° êµ­ê°€ë³„ ë°ì´í„° ìš”ê¸ˆì œì™€ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ëŠ” ìƒí’ˆ í˜ì´ì§€ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "question_en": "Which countries can I use eSIM in?",
                    "answer_en": "eSIM can be used in most countries around the world. It is available in over 190 countries and regions including major travel destinations in Europe, Asia, Americas, and Oceania. Data plans and availability for each country can be checked on the product page.",
                    "question_ja": "eSIMã¯ã©ã®å›½ã§ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã¯ä¸–ç•Œä¸­ã®ã»ã¨ã‚“ã©ã®å›½ã§ä½¿ç”¨ã§ãã¾ã™ã€‚ä¸»è¦ãªæ—…è¡Œå…ˆã§ã‚ã‚‹ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã€ã‚¢ã‚¸ã‚¢ã€ã‚¢ãƒ¡ãƒªã‚«ã€ã‚ªã‚»ã‚¢ãƒ‹ã‚¢ãªã©ã€ä¸–ç•Œ190ä»¥ä¸Šã®å›½ã¨åœ°åŸŸã§ä½¿ç”¨å¯èƒ½ã§ã™ã€‚å„å›½ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒ³ã¨åˆ©ç”¨å¯å¦ã¯å•†å“ãƒšãƒ¼ã‚¸ã§ã”ç¢ºèªã„ãŸã ã‘ã¾ã™ã€‚"
                },
                {
                    "question_ko": "eSIM í™œì„±í™”ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
                    "answer_ko": "eSIM í™œì„±í™”ëŠ” ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤. 1) êµ¬ë§¤ í›„ ì´ë©”ì¼ë¡œ ë°›ì€ QR ì½”ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”. 2) ì—¬í–‰ì§€ì— ë„ì°©í•œ í›„ ìŠ¤ë§ˆíŠ¸í° ì„¤ì •ì—ì„œ eSIMì„ ì¶”ê°€í•˜ì„¸ìš”. 3) QR ì½”ë“œë¥¼ ìŠ¤ìº”í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”. 4) ë°ì´í„° ìš”ê¸ˆì œë¥¼ í™œì„±í™”í•˜ì„¸ìš”. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ìë™ìœ¼ë¡œ í™œì„±í™”ë˜ë©°, ìˆ˜ë™ í™œì„±í™”ê°€ í•„ìš”í•œ ê²½ìš° ìƒí’ˆ í˜ì´ì§€ì˜ ì•ˆë‚´ë¥¼ ë”°ë¥´ì‹œë©´ ë©ë‹ˆë‹¤.",
                    "question_en": "How do I activate eSIM?",
                    "answer_en": "Activating eSIM is very simple. 1) Check the QR code received via email after purchase. 2) After arriving at your destination, add eSIM in your smartphone settings. 3) Scan the QR code or enter manually. 4) Activate your data plan. In most cases, it activates automatically, and if manual activation is required, please follow the instructions on the product page.",
                    "question_ja": "eSIMã®æœ‰åŠ¹åŒ–ã¯ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã®æœ‰åŠ¹åŒ–ã¯éå¸¸ã«ç°¡å˜ã§ã™ã€‚1) è³¼å…¥å¾Œãƒ¡ãƒ¼ãƒ«ã§å—ã‘å–ã£ãŸQRã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚2) æ—…è¡Œå…ˆã«åˆ°ç€ã—ãŸã‚‰ã€ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®è¨­å®šã§eSIMã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚3) QRã‚³ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹ã‹ã€æ‰‹å‹•ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚4) ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒ³ã‚’æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„ã€‚ã»ã¨ã‚“ã©ã®å ´åˆã€è‡ªå‹•çš„ã«æœ‰åŠ¹åŒ–ã•ã‚Œã¾ã™ãŒã€æ‰‹å‹•æœ‰åŠ¹åŒ–ãŒå¿…è¦ãªå ´åˆã¯ã€å•†å“ãƒšãƒ¼ã‚¸ã®æ¡ˆå†…ã«å¾“ã£ã¦ãã ã•ã„ã€‚"
                },
                {
                    "question_ko": "eSIMì„ ì—¬ëŸ¬ êµ­ê°€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                    "answer_ko": "ë„¤, ì¼ë¶€ eSIM ìš”ê¸ˆì œëŠ” ì—¬ëŸ¬ êµ­ê°€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸€ë¡œë²Œ í”Œëœì„ ì œê³µí•©ë‹ˆë‹¤. ì§€ì—­ë³„ í”Œëœ(ì˜ˆ: ìœ ëŸ½ ì—¬ëŸ¬ êµ­ê°€, ì•„ì‹œì•„ ì—¬ëŸ¬ êµ­ê°€)ë„ ìˆìŠµë‹ˆë‹¤. êµ¬ë§¤ ì „ ìƒí’ˆ ì„¤ëª…ì—ì„œ ì§€ì› êµ­ê°€ ëª©ë¡ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ë‹¨ì¼ êµ­ê°€ ì „ìš© í”Œëœë„ ìˆìœ¼ë¯€ë¡œ ì—¬í–‰ ê³„íšì— ë§ëŠ” í”Œëœì„ ì„ íƒí•˜ì‹œë©´ ë©ë‹ˆë‹¤.",
                    "question_en": "Can I use eSIM in multiple countries?",
                    "answer_en": "Yes, some eSIM plans offer global plans that can be used in multiple countries. There are also regional plans (e.g., multiple European countries, multiple Asian countries). Please check the list of supported countries in the product description before purchase. There are also single-country exclusive plans, so please choose a plan that suits your travel plans.",
                    "question_ja": "eSIMã‚’è¤‡æ•°ã®å›½ã§ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "ã¯ã„ã€ä¸€éƒ¨ã®eSIMãƒ—ãƒ©ãƒ³ã¯è¤‡æ•°ã®å›½ã§ä½¿ç”¨ã§ãã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ©ãƒ³ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚åœ°åŸŸåˆ¥ãƒ—ãƒ©ãƒ³ï¼ˆä¾‹ï¼šãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘è¤‡æ•°å›½ã€ã‚¢ã‚¸ã‚¢è¤‡æ•°å›½ï¼‰ã‚‚ã‚ã‚Šã¾ã™ã€‚è³¼å…¥å‰ã«å•†å“èª¬æ˜ã§ã‚µãƒãƒ¼ãƒˆå›½ãƒªã‚¹ãƒˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚å˜ä¸€å›½å°‚ç”¨ãƒ—ãƒ©ãƒ³ã‚‚ã‚ã‚‹ãŸã‚ã€æ—…è¡Œè¨ˆç”»ã«åˆã£ãŸãƒ—ãƒ©ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
                },
                {
                    "question_ko": "eSIMì€ ì–´ë–¤ ê¸°ê¸°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                    "answer_ko": "eSIMì€ eSIM ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ìŠ¤ë§ˆíŠ¸í°, íƒœë¸”ë¦¿, ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ ë“±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” í˜¸í™˜ ê¸°ì¢…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n**iPhone:**\n- iPhone XS, XS Max, XR ì´í›„ ëª¨ë¸ (iPhone 14 ì‹œë¦¬ì¦ˆ ì´ìƒ ê¶Œì¥)\n- iPhone SE (2020ë…„ ì´í›„ ëª¨ë¸)\n\n**Android:**\n- Google Pixel 3 ì´í›„ ëª¨ë¸\n- Samsung Galaxy S20 ì‹œë¦¬ì¦ˆ ì´í›„ (S21, S22, S23, S24, S25 ë“±)\n- Samsung Galaxy Note 20 ì‹œë¦¬ì¦ˆ ì´í›„\n- Samsung Galaxy Z Fold, Z Flip ì‹œë¦¬ì¦ˆ\n- Samsung Galaxy Tab ì‹œë¦¬ì¦ˆ (ì¼ë¶€ ëª¨ë¸)\n- OnePlus 6 ì´í›„ ëª¨ë¸\n- Xiaomi, Huawei, Oppo ë“± ì£¼ìš” ë¸Œëœë“œì˜ ìµœì‹  ëª¨ë¸\n\n**ê¸°íƒ€:**\n- iPad Pro (2018ë…„ ì´í›„), iPad Air (2020ë…„ ì´í›„), iPad mini (2019ë…„ ì´í›„)\n- Apple Watch Series 3 ì´í›„ (ì…€ë£°ëŸ¬ ëª¨ë¸)\n\nê¸°ê¸° í˜¸í™˜ì„±ì€ ì œì¡°ì‚¬ì™€ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, êµ¬ë§¤ ì „ ìƒí’ˆ í˜ì´ì§€ì—ì„œ ì‚¬ìš©í•˜ì‹œëŠ” ê¸°ê¸° ëª¨ë¸ì˜ í˜¸í™˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ë˜í•œ ì¼ë¶€ ê¸°ê¸°ëŠ” íŠ¹ì • êµ­ê°€ë‚˜ í†µì‹ ì‚¬ì—ì„œë§Œ eSIMì„ ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "question_en": "Which devices support eSIM?",
                    "answer_en": "eSIM can be used on smartphones, tablets, smartwatches, and other devices that support eSIM functionality. Main compatible devices include:\n\n**iPhone:**\n- iPhone XS, XS Max, XR and later models (iPhone 14 series and above recommended)\n- iPhone SE (2020 and later models)\n\n**Android:**\n- Google Pixel 3 and later models\n- Samsung Galaxy S20 series and later (S21, S22, S23, S24, S25, etc.)\n- Samsung Galaxy Note 20 series and later\n- Samsung Galaxy Z Fold, Z Flip series\n- Samsung Galaxy Tab series (some models)\n- OnePlus 6 and later models\n- Latest models from Xiaomi, Huawei, Oppo, and other major brands\n\n**Others:**\n- iPad Pro (2018 and later), iPad Air (2020 and later), iPad mini (2019 and later)\n- Apple Watch Series 3 and later (cellular models)\n\nDevice compatibility may vary by manufacturer and model, so please check the product page before purchase to confirm compatibility with your device model. Some devices may only support eSIM in specific countries or with specific carriers.",
                    "question_ja": "eSIMã¯ã©ã®ãƒ‡ãƒã‚¤ã‚¹ã§ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã¯ã€eSIMæ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã€ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆã€ã‚¹ãƒãƒ¼ãƒˆã‚¦ã‚©ãƒƒãƒãªã©ã§ä½¿ç”¨ã§ãã¾ã™ã€‚ä¸»ãªäº’æ›ãƒ‡ãƒã‚¤ã‚¹ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n\n**iPhone:**\n- iPhone XSã€XS Maxã€XRä»¥é™ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆiPhone 14ã‚·ãƒªãƒ¼ã‚ºä»¥é™æ¨å¥¨ï¼‰\n- iPhone SEï¼ˆ2020å¹´ä»¥é™ã®ãƒ¢ãƒ‡ãƒ«ï¼‰\n\n**Android:**\n- Google Pixel 3ä»¥é™ã®ãƒ¢ãƒ‡ãƒ«\n- Samsung Galaxy S20ã‚·ãƒªãƒ¼ã‚ºä»¥é™ï¼ˆS21ã€S22ã€S23ã€S24ã€S25ãªã©ï¼‰\n- Samsung Galaxy Note 20ã‚·ãƒªãƒ¼ã‚ºä»¥é™\n- Samsung Galaxy Z Foldã€Z Flipã‚·ãƒªãƒ¼ã‚º\n- Samsung Galaxy Tabã‚·ãƒªãƒ¼ã‚ºï¼ˆä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ï¼‰\n- OnePlus 6ä»¥é™ã®ãƒ¢ãƒ‡ãƒ«\n- Xiaomiã€Huaweiã€Oppoãªã©ã®ä¸»è¦ãƒ–ãƒ©ãƒ³ãƒ‰ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«\n\n**ãã®ä»–:**\n- iPad Proï¼ˆ2018å¹´ä»¥é™ï¼‰ã€iPad Airï¼ˆ2020å¹´ä»¥é™ï¼‰ã€iPad miniï¼ˆ2019å¹´ä»¥é™ï¼‰\n- Apple Watch Series 3ä»¥é™ï¼ˆã‚»ãƒ«ãƒ©ãƒ¼ãƒ¢ãƒ‡ãƒ«ï¼‰\n\nãƒ‡ãƒã‚¤ã‚¹ã®äº’æ›æ€§ã¯ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç•°ãªã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€è³¼å…¥å‰ã«å•†å“ãƒšãƒ¼ã‚¸ã§ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äº’æ›æ€§ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ã¾ãŸã€ä¸€éƒ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã¯ç‰¹å®šã®å›½ã‚„é€šä¿¡äº‹æ¥­è€…ã§ã®ã¿eSIMã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚"
                }
            ]
        elif lang == "en":
            common_faqs = [
                {
                    "question_en": "Which countries can I use eSIM in?",
                    "answer_en": "eSIM can be used in most countries around the world. It is available in over 190 countries and regions including major travel destinations in Europe, Asia, Americas, and Oceania. Data plans and availability for each country can be checked on the product page.",
                    "question_ko": "eSIMì€ ì–´ë–¤ êµ­ê°€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                    "answer_ko": "eSIMì€ ì „ ì„¸ê³„ ëŒ€ë¶€ë¶„ì˜ êµ­ê°€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                    "question_ja": "eSIMã¯ã©ã®å›½ã§ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã¯ä¸–ç•Œä¸­ã®ã»ã¨ã‚“ã©ã®å›½ã§ä½¿ç”¨ã§ãã¾ã™ã€‚"
                },
                {
                    "question_en": "How do I activate eSIM?",
                    "answer_en": "Activating eSIM is very simple. 1) Check the QR code received via email after purchase. 2) After arriving at your destination, add eSIM in your smartphone settings. 3) Scan the QR code or enter manually. 4) Activate your data plan.",
                    "question_ko": "eSIM í™œì„±í™”ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
                    "answer_ko": "eSIM í™œì„±í™”ëŠ” ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤.",
                    "question_ja": "eSIMã®æœ‰åŠ¹åŒ–ã¯ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã®æœ‰åŠ¹åŒ–ã¯éå¸¸ã«ç°¡å˜ã§ã™ã€‚"
                },
                {
                    "question_en": "Which devices support eSIM?",
                    "answer_en": "eSIM can be used on smartphones, tablets, smartwatches, and other devices that support eSIM functionality. Main compatible devices include:\n\n**iPhone:**\n- iPhone XS, XS Max, XR and later models (iPhone 14 series and above recommended)\n- iPhone SE (2020 and later models)\n\n**Android:**\n- Google Pixel 3 and later models\n- Samsung Galaxy S20 series and later (S21, S22, S23, S24, S25, etc.)\n- Samsung Galaxy Note 20 series and later\n- Samsung Galaxy Z Fold, Z Flip series\n- Samsung Galaxy Tab series (some models)\n- OnePlus 6 and later models\n- Latest models from Xiaomi, Huawei, Oppo, and other major brands\n\n**Others:**\n- iPad Pro (2018 and later), iPad Air (2020 and later), iPad mini (2019 and later)\n- Apple Watch Series 3 and later (cellular models)\n\nDevice compatibility may vary by manufacturer and model, so please check the product page before purchase to confirm compatibility with your device model. Some devices may only support eSIM in specific countries or with specific carriers.",
                    "question_ko": "eSIMì€ ì–´ë–¤ ê¸°ê¸°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                    "answer_ko": "eSIMì€ eSIM ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ìŠ¤ë§ˆíŠ¸í°, íƒœë¸”ë¦¿, ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ ë“±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "question_ja": "eSIMã¯ã©ã®ãƒ‡ãƒã‚¤ã‚¹ã§ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã¯ã€eSIMæ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã€ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆã€ã‚¹ãƒãƒ¼ãƒˆã‚¦ã‚©ãƒƒãƒãªã©ã§ä½¿ç”¨ã§ãã¾ã™ã€‚"
                }
            ]
        else:  # ja
            common_faqs = [
                {
                    "question_ja": "eSIMã¯ã©ã®å›½ã§ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã¯ä¸–ç•Œä¸­ã®ã»ã¨ã‚“ã©ã®å›½ã§ä½¿ç”¨ã§ãã¾ã™ã€‚ä¸»è¦ãªæ—…è¡Œå…ˆã§ã‚ã‚‹ãƒ¨ãƒ¼ãƒ­ãƒƒãƒ‘ã€ã‚¢ã‚¸ã‚¢ã€ã‚¢ãƒ¡ãƒªã‚«ã€ã‚ªã‚»ã‚¢ãƒ‹ã‚¢ãªã©ã€ä¸–ç•Œ190ä»¥ä¸Šã®å›½ã¨åœ°åŸŸã§ä½¿ç”¨å¯èƒ½ã§ã™ã€‚",
                    "question_ko": "eSIMì€ ì–´ë–¤ êµ­ê°€ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                    "answer_ko": "eSIMì€ ì „ ì„¸ê³„ ëŒ€ë¶€ë¶„ì˜ êµ­ê°€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                    "question_en": "Which countries can I use eSIM in?",
                    "answer_en": "eSIM can be used in most countries around the world."
                },
                {
                    "question_ja": "eSIMã¯ã©ã®ãƒ‡ãƒã‚¤ã‚¹ã§ä½¿ç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "eSIMã¯ã€eSIMæ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã€ã‚¿ãƒ–ãƒ¬ãƒƒãƒˆã€ã‚¹ãƒãƒ¼ãƒˆã‚¦ã‚©ãƒƒãƒãªã©ã§ä½¿ç”¨ã§ãã¾ã™ã€‚ä¸»ãªäº’æ›ãƒ‡ãƒã‚¤ã‚¹ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n\n**iPhone:**\n- iPhone XSã€XS Maxã€XRä»¥é™ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆiPhone 14ã‚·ãƒªãƒ¼ã‚ºä»¥é™æ¨å¥¨ï¼‰\n- iPhone SEï¼ˆ2020å¹´ä»¥é™ã®ãƒ¢ãƒ‡ãƒ«ï¼‰\n\n**Android:**\n- Google Pixel 3ä»¥é™ã®ãƒ¢ãƒ‡ãƒ«\n- Samsung Galaxy S20ã‚·ãƒªãƒ¼ã‚ºä»¥é™ï¼ˆS21ã€S22ã€S23ã€S24ã€S25ãªã©ï¼‰\n- Samsung Galaxy Note 20ã‚·ãƒªãƒ¼ã‚ºä»¥é™\n- Samsung Galaxy Z Foldã€Z Flipã‚·ãƒªãƒ¼ã‚º\n- Samsung Galaxy Tabã‚·ãƒªãƒ¼ã‚ºï¼ˆä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ï¼‰\n- OnePlus 6ä»¥é™ã®ãƒ¢ãƒ‡ãƒ«\n- Xiaomiã€Huaweiã€Oppoãªã©ã®ä¸»è¦ãƒ–ãƒ©ãƒ³ãƒ‰ã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«\n\n**ãã®ä»–:**\n- iPad Proï¼ˆ2018å¹´ä»¥é™ï¼‰ã€iPad Airï¼ˆ2020å¹´ä»¥é™ï¼‰ã€iPad miniï¼ˆ2019å¹´ä»¥é™ï¼‰\n- Apple Watch Series 3ä»¥é™ï¼ˆã‚»ãƒ«ãƒ©ãƒ¼ãƒ¢ãƒ‡ãƒ«ï¼‰\n\nãƒ‡ãƒã‚¤ã‚¹ã®äº’æ›æ€§ã¯ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç•°ãªã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€è³¼å…¥å‰ã«å•†å“ãƒšãƒ¼ã‚¸ã§ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ãƒ¢ãƒ‡ãƒ«ã®äº’æ›æ€§ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ã¾ãŸã€ä¸€éƒ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã¯ç‰¹å®šã®å›½ã‚„é€šä¿¡äº‹æ¥­è€…ã§ã®ã¿eSIMã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚",
                    "question_ko": "eSIMì€ ì–´ë–¤ ê¸°ê¸°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?",
                    "answer_ko": "eSIMì€ eSIM ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ìŠ¤ë§ˆíŠ¸í°, íƒœë¸”ë¦¿, ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ ë“±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "question_en": "Which devices support eSIM?",
                    "answer_en": "eSIM can be used on smartphones, tablets, smartwatches, and other devices that support eSIM functionality."
                }
            ]
    
    # ì‚¼ì„± ê³µë™ ì œí’ˆ FAQ
    elif "samsung" in company_lower or "ì‚¼ì„±" in company_name:
        if lang == "ko":
            common_faqs = [
                {
                    "question_ko": "Galaxy S25 Ultraì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "answer_ko": "Galaxy S25 UltraëŠ” ì‚¼ì„±ì˜ ìµœì‹  í”Œë˜ê·¸ì‹­ ìŠ¤ë§ˆíŠ¸í°ìœ¼ë¡œ, ê³ ì„±ëŠ¥ í”„ë¡œì„¸ì„œ, ê³ í•´ìƒë„ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ, ê¸´ ë°°í„°ë¦¬ ìˆ˜ëª…, ë¹ ë¥¸ ì¶©ì „ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. íŠ¹íˆ AI ê¸°ëŠ¥ì´ ê°•í™”ë˜ì–´ ì‚¬ì§„ ì´¬ì˜, ìƒì‚°ì„± í–¥ìƒ, ì¼ìƒ ì‘ì—… ìë™í™”ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.",
                    "question_en": "What are the main features of Galaxy S25 Ultra?",
                    "answer_en": "Galaxy S25 Ultra is Samsung's latest flagship smartphone, offering high-performance processor, high-resolution camera system, long battery life, and fast charging. AI features are particularly enhanced to help with photography, productivity, and daily task automation.",
                    "question_ja": "Galaxy S25 Ultraã®ä¸»ãªç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                    "answer_ja": "Galaxy S25 Ultraã¯ã‚µãƒ ã‚¹ãƒ³ã®æœ€æ–°ãƒ•ãƒ©ã‚°ã‚·ãƒƒãƒ—ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§ã€é«˜æ€§èƒ½ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã€é«˜è§£åƒåº¦ã‚«ãƒ¡ãƒ©ã‚·ã‚¹ãƒ†ãƒ ã€é•·ã„ãƒãƒƒãƒ†ãƒªãƒ¼å¯¿å‘½ã€é«˜é€Ÿå……é›»æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚ç‰¹ã«AIæ©Ÿèƒ½ãŒå¼·åŒ–ã•ã‚Œã€å†™çœŸæ’®å½±ã€ç”Ÿç”£æ€§å‘ä¸Šã€æ—¥å¸¸ä½œæ¥­ã®è‡ªå‹•åŒ–ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
                },
                {
                    "question_ko": "ì‹ ê·œ ì¶œì‹œ ì˜ˆì • ì œí’ˆì€ ì–¸ì œ ì¶œì‹œë˜ë‚˜ìš”?",
                    "answer_ko": "ì‚¼ì„±ì€ ì •ê¸°ì ìœ¼ë¡œ ì‹ ì œí’ˆì„ ì¶œì‹œí•©ë‹ˆë‹¤. ì •í™•í•œ ì¶œì‹œ ì¼ì •ì€ ê³µì‹ ë°œí‘œë¥¼ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìœ¼ë©°, ì¼ë°˜ì ìœ¼ë¡œ ê°¤ëŸ­ì‹œ ì‹œë¦¬ì¦ˆëŠ” ì—° 1-2íšŒ ì£¼ìš” ì—…ë°ì´íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ì‹ ì œí’ˆ ì¶œì‹œ ì†Œì‹ì€ ì‚¼ì„± ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë‚˜ ê³µì‹ ì±„ë„ì„ í†µí•´ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
                    "question_en": "When will the new products be released?",
                    "answer_en": "Samsung regularly releases new products. Exact release schedules can be confirmed through official announcements, and generally, the Galaxy series has 1-2 major updates per year. Please check Samsung's official website or official channels for new product release news.",
                    "question_ja": "æ–°è£½å“ã¯ã„ã¤ç™ºå£²ã•ã‚Œã¾ã™ã‹ï¼Ÿ",
                    "answer_ja": "ã‚µãƒ ã‚¹ãƒ³ã¯å®šæœŸçš„ã«æ–°è£½å“ã‚’ç™ºå£²ã—ã¦ã„ã¾ã™ã€‚æ­£ç¢ºãªç™ºå£²ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯å…¬å¼ç™ºè¡¨ã§ç¢ºèªã§ãã€ä¸€èˆ¬çš„ã«ã‚®ãƒ£ãƒ©ã‚¯ã‚·ãƒ¼ã‚·ãƒªãƒ¼ã‚ºã¯å¹´é–“1-2å›ã®ä¸»è¦ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒã‚ã‚Šã¾ã™ã€‚æ–°è£½å“ç™ºå£²ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚µãƒ ã‚¹ãƒ³å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã¾ãŸã¯å…¬å¼ãƒãƒ£ãƒ³ãƒãƒ«ã§ã”ç¢ºèªãã ã•ã„ã€‚"
                }
            ]
        elif lang == "en":
            common_faqs = [
                {
                    "question_en": "What are the main features of Galaxy S25 Ultra?",
                    "answer_en": "Galaxy S25 Ultra is Samsung's latest flagship smartphone, offering high-performance processor, high-resolution camera system, long battery life, and fast charging.",
                    "question_ko": "Galaxy S25 Ultraì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "answer_ko": "Galaxy S25 UltraëŠ” ì‚¼ì„±ì˜ ìµœì‹  í”Œë˜ê·¸ì‹­ ìŠ¤ë§ˆíŠ¸í°ì…ë‹ˆë‹¤.",
                    "question_ja": "Galaxy S25 Ultraã®ä¸»ãªç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                    "answer_ja": "Galaxy S25 Ultraã¯ã‚µãƒ ã‚¹ãƒ³ã®æœ€æ–°ãƒ•ãƒ©ã‚°ã‚·ãƒƒãƒ—ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§ã™ã€‚"
                }
            ]
        else:  # ja
            common_faqs = [
                {
                    "question_ja": "Galaxy S25 Ultraã®ä¸»ãªç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                    "answer_ja": "Galaxy S25 Ultraã¯ã‚µãƒ ã‚¹ãƒ³ã®æœ€æ–°ãƒ•ãƒ©ã‚°ã‚·ãƒƒãƒ—ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§ã€é«˜æ€§èƒ½ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã€é«˜è§£åƒåº¦ã‚«ãƒ¡ãƒ©ã‚·ã‚¹ãƒ†ãƒ ã‚’æä¾›ã—ã¾ã™ã€‚",
                    "question_ko": "Galaxy S25 Ultraì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "answer_ko": "Galaxy S25 UltraëŠ” ì‚¼ì„±ì˜ ìµœì‹  í”Œë˜ê·¸ì‹­ ìŠ¤ë§ˆíŠ¸í°ì…ë‹ˆë‹¤.",
                    "question_en": "What are the main features of Galaxy S25 Ultra?",
                    "answer_en": "Galaxy S25 Ultra is Samsung's latest flagship smartphone."
                }
            ]
    
    return common_faqs


def generate_company_info_with_llm(company_name: str, lang: str = "ko") -> Dict[str, Any]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ íšŒì‚¬ ì •ë³´ ìƒì„±"""
    lang_prompts = {
        "ko": f"""ë‹¤ìŒ íšŒì‚¬ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”: {company_name}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "company_info": "íšŒì‚¬ ì†Œê°œ (500ì ì´ìƒ)",
    "popular_products": [
        {{"text_ko": "ìƒí’ˆëª…1", "score": 85, "image_url": ""}},
        {{"text_ko": "ìƒí’ˆëª…2", "score": 80, "image_url": ""}},
        {{"text_ko": "ìƒí’ˆëª…3", "score": 75, "image_url": ""}}
    ],
    "trending_topics": [
        {{"text_ko": "í™”ì œ ì†Œì‹1", "score": 90, "detail_ko": "í™”ì œ ì†Œì‹1ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©ì…ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ë°°ê²½ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."}},
        {{"text_ko": "í™”ì œ ì†Œì‹2", "score": 85, "detail_ko": "í™”ì œ ì†Œì‹2ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©ì…ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ë°°ê²½ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."}},
        {{"text_ko": "í™”ì œ ì†Œì‹3", "score": 80, "detail_ko": "í™”ì œ ì†Œì‹3ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©ì…ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì„¤ëª…ê³¼ ë°°ê²½ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."}}
    ],
    "faqs": [
        {{"question_ko": "ì§ˆë¬¸1", "answer_ko": "ë‹µë³€1"}},
        {{"question_ko": "ì§ˆë¬¸2", "answer_ko": "ë‹µë³€2"}},
        {{"question_ko": "ì§ˆë¬¸3", "answer_ko": "ë‹µë³€3"}},
        {{"question_ko": "ì§ˆë¬¸4", "answer_ko": "ë‹µë³€4"}},
        {{"question_ko": "ì§ˆë¬¸5", "answer_ko": "ë‹µë³€5"}},
        {{"question_ko": "ì§ˆë¬¸6", "answer_ko": "ë‹µë³€6"}},
        {{"question_ko": "ì§ˆë¬¸7", "answer_ko": "ë‹µë³€7"}},
        {{"question_ko": "ì§ˆë¬¸8", "answer_ko": "ë‹µë³€8"}},
        {{"question_ko": "ì§ˆë¬¸9", "answer_ko": "ë‹µë³€9"}},
        {{"question_ko": "ì§ˆë¬¸10", "answer_ko": "ë‹µë³€10"}}
    ]
}}

FAQëŠ” 10ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì‹¤ì œë¡œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í¬í•¨í•´ì£¼ì„¸ìš”.""",
        "en": f"""Please provide detailed information about the following company: {company_name}

Respond in JSON format as follows:
{{
    "company_info": "Company introduction (500+ characters)",
    "popular_products": [
        {{"text_en": "Product1", "score": 85, "image_url": ""}},
        {{"text_en": "Product2", "score": 80, "image_url": ""}},
        {{"text_en": "Product3", "score": 75, "image_url": ""}}
    ],
    "trending_topics": [
        {{"text_en": "Trending news1", "score": 90, "detail_en": "Detailed content about trending news1, including specific explanations and background information."}},
        {{"text_en": "Trending news2", "score": 85, "detail_en": "Detailed content about trending news2, including specific explanations and background information."}},
        {{"text_en": "Trending news3", "score": 80, "detail_en": "Detailed content about trending news3, including specific explanations and background information."}}
    ],
    "faqs": [
        {{"question_en": "Question1", "answer_en": "Answer1"}},
        {{"question_en": "Question2", "answer_en": "Answer2"}},
        {{"question_en": "Question3", "answer_en": "Answer3"}},
        {{"question_en": "Question4", "answer_en": "Answer4"}},
        {{"question_en": "Question5", "answer_en": "Answer5"}},
        {{"question_en": "Question6", "answer_en": "Answer6"}},
        {{"question_en": "Question7", "answer_en": "Answer7"}},
        {{"question_en": "Question8", "answer_en": "Answer8"}},
        {{"question_en": "Question9", "answer_en": "Answer9"}},
        {{"question_en": "Question10", "answer_en": "Answer10"}}
    ]
}}

Generate 10 FAQs with real frequently asked questions and answers.""",
        "ja": f"""æ¬¡ã®ä¼šç¤¾ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„: {company_name}

æ¬¡ã®å½¢å¼ã§JSONã§å¿œç­”ã—ã¦ãã ã•ã„:
{{
    "company_info": "ä¼šç¤¾ç´¹ä»‹ (500æ–‡å­—ä»¥ä¸Š)",
    "popular_products": [
        {{"text_ja": "å•†å“å1", "score": 85, "image_url": ""}},
        {{"text_ja": "å•†å“å2", "score": 80, "image_url": ""}},
        {{"text_ja": "å•†å“å3", "score": 75, "image_url": ""}}
    ],
    "trending_topics": [
        {{"text_ja": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹1", "score": 90, "detail_ja": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹1ã«é–¢ã™ã‚‹è©³ç´°å†…å®¹ã§ã™ã€‚å…·ä½“çš„ãªèª¬æ˜ã¨èƒŒæ™¯æƒ…å ±ã‚’å«ã¿ã¾ã™ã€‚"}},
        {{"text_ja": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹2", "score": 85, "detail_ja": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹2ã«é–¢ã™ã‚‹è©³ç´°å†…å®¹ã§ã™ã€‚å…·ä½“çš„ãªèª¬æ˜ã¨èƒŒæ™¯æƒ…å ±ã‚’å«ã¿ã¾ã™ã€‚"}},
        {{"text_ja": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹3", "score": 80, "detail_ja": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹3ã«é–¢ã™ã‚‹è©³ç´°å†…å®¹ã§ã™ã€‚å…·ä½“çš„ãªèª¬æ˜ã¨èƒŒæ™¯æƒ…å ±ã‚’å«ã¿ã¾ã™ã€‚"}}
    ],
    "faqs": [
        {{"question_ja": "è³ªå•1", "answer_ja": "å›ç­”1"}},
        {{"question_ja": "è³ªå•2", "answer_ja": "å›ç­”2"}},
        {{"question_ja": "è³ªå•3", "answer_ja": "å›ç­”3"}},
        {{"question_ja": "è³ªå•4", "answer_ja": "å›ç­”4"}},
        {{"question_ja": "è³ªå•5", "answer_ja": "å›ç­”5"}},
        {{"question_ja": "è³ªå•6", "answer_ja": "å›ç­”6"}},
        {{"question_ja": "è³ªå•7", "answer_ja": "å›ç­”7"}},
        {{"question_ja": "è³ªå•8", "answer_ja": "å›ç­”8"}},
        {{"question_ja": "è³ªå•9", "answer_ja": "å›ç­”9"}},
        {{"question_ja": "è³ªå•10", "answer_ja": "å›ç­”10"}}
    ]
}}

FAQã¯10å€‹ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚å®Ÿéš›ã«ã‚ˆãã‚ã‚‹è³ªå•ã¨å›ç­”ã‚’å«ã‚ã¦ãã ã•ã„ã€‚"""
    }
    
    prompt = lang_prompts.get(lang, lang_prompts["ko"])
    
    try:
        response = run_llm(prompt)
        
        # JSON íŒŒì‹± ì‹œë„
        import re
        json_match = re.search(r'\{[\s\S]*\}', response)
        if json_match:
            json_str = json_match.group()
            try:
                company_data = json.loads(json_str)
                
                # ê³µë™ ëŒ€í‘œ ì œí’ˆ FAQ ì¶”ê°€
                common_faqs = get_common_product_faqs(company_name, lang)
                if common_faqs:
                    existing_faqs = company_data.get("faqs", [])
                    # ê³µë™ FAQë¥¼ ê¸°ì¡´ FAQ ì•ì— ì¶”ê°€
                    company_data["faqs"] = common_faqs + existing_faqs
                
                # FAQê°€ 10ê°œ ë¯¸ë§Œì´ë©´ ê¸°ë³¸ FAQ ì¶”ê°€
                if len(company_data.get("faqs", [])) < 10:
                    # ì–¸ì–´ë³„ ê¸°ë³¸ FAQ
                    default_faqs_by_lang = {
                        "ko": [
                        {"question_ko": "íšŒì‚¬ ì„¤ë¦½ì¼ì€ ì–¸ì œì¸ê°€ìš”?", "answer_ko": "íšŒì‚¬ ì„¤ë¦½ì¼ì— ëŒ€í•œ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤."},
                        {"question_ko": "ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "answer_ko": "ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ì— ëŒ€í•œ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤."},
                        {"question_ko": "ë³¸ì‚¬ ìœ„ì¹˜ëŠ” ì–´ë””ì¸ê°€ìš”?", "answer_ko": "ë³¸ì‚¬ ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤."},
                        {"question_ko": "ì§ì› ìˆ˜ëŠ” ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?", "answer_ko": "ì§ì› ìˆ˜ì— ëŒ€í•œ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤."},
                        {"question_ko": "ì£¼ìš” ì œí’ˆ/ì„œë¹„ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "answer_ko": "ì£¼ìš” ì œí’ˆ/ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤."},
                        ],
                        "en": [
                            {"question_en": "When was the company founded?", "answer_en": "We are checking information about the company's founding date."},
                            {"question_en": "What are the main business areas?", "answer_en": "We are checking information about the main business areas."},
                            {"question_en": "Where is the headquarters located?", "answer_en": "We are checking information about the headquarters location."},
                            {"question_en": "How many employees does the company have?", "answer_en": "We are checking information about the number of employees."},
                            {"question_en": "What are the main products/services?", "answer_en": "We are checking information about the main products/services."},
                        ],
                        "ja": [
                            {"question_ja": "ä¼šç¤¾ã®è¨­ç«‹æ—¥ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ", "answer_ja": "ä¼šç¤¾ã®è¨­ç«‹æ—¥ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ç¢ºèªä¸­ã§ã™ã€‚"},
                            {"question_ja": "ä¸»è¦ãªäº‹æ¥­åˆ†é‡ã¯ä½•ã§ã™ã‹ï¼Ÿ", "answer_ja": "ä¸»è¦ãªäº‹æ¥­åˆ†é‡ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ç¢ºèªä¸­ã§ã™ã€‚"},
                            {"question_ja": "æœ¬ç¤¾ã®æ‰€åœ¨åœ°ã¯ã©ã“ã§ã™ã‹ï¼Ÿ", "answer_ja": "æœ¬ç¤¾ã®æ‰€åœ¨åœ°ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ç¢ºèªä¸­ã§ã™ã€‚"},
                            {"question_ja": "å¾“æ¥­å“¡æ•°ã¯ä½•äººã§ã™ã‹ï¼Ÿ", "answer_ja": "å¾“æ¥­å“¡æ•°ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ç¢ºèªä¸­ã§ã™ã€‚"},
                            {"question_ja": "ä¸»è¦ãªè£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ", "answer_ja": "ä¸»è¦ãªè£½å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ç¢ºèªä¸­ã§ã™ã€‚"},
                        ]
                    }
                    default_faqs = default_faqs_by_lang.get(lang, default_faqs_by_lang["ko"])
                    existing_faqs = company_data.get("faqs", [])
                    # ë¶€ì¡±í•œ ë§Œí¼ ê¸°ë³¸ FAQ ì¶”ê°€
                    while len(existing_faqs) < 10:
                        idx = len(existing_faqs) % len(default_faqs)
                        existing_faqs.append(default_faqs[idx])
                    company_data["faqs"] = existing_faqs[:10]
                return company_data
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
                return {
                    "company_info": response[:1000] if len(response) > 1000 else response,
                    "popular_products": [],
                    "trending_topics": [],
                    "faqs": []
                }
        else:
            # JSONì´ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "company_info": response[:1000] if len(response) > 1000 else response,
                "popular_products": [],
                "trending_topics": [],
                "faqs": []
            }
    except Exception as e:
        # ì–¸ì–´ë³„ ì—ëŸ¬ ë©”ì‹œì§€
        error_messages = {
            "ko": f"íšŒì‚¬ ì •ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "en": f"An error occurred while generating company information: {str(e)}",
            "ja": f"ä¼šç¤¾æƒ…å ±ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        }
        return {
            "company_info": error_messages.get(lang, error_messages["ko"]),
            "popular_products": [],
            "trending_topics": [],
            "faqs": []
        }





# ========================================
# 1. ë‹¤êµ­ì–´ ì„¤ì • (ì „í™” ë°œì‹  ê´€ë ¨ í…ìŠ¤íŠ¸ ì¶”ê°€)
# ========================================
DEFAULT_LANG = "ko"

LANG: Dict[str, Dict[str, str]] = {
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜ (ìŒì„± ë° DB í†µí•©)",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "sim_tab_chat_email": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì±„íŒ…/ì´ë©”ì¼)",
        "sim_tab_phone": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì „í™”)",
        "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "company_info_tab": "íšŒì‚¬ ì •ë³´ ë° FAQ",
        "company_info_tab_desc": "íšŒì‚¬ë³„ ìƒì„¸ ì •ë³´, ì¸ê¸° ìƒí’ˆ, í™”ì œì˜ ì†Œì‹, FAQë¥¼ ê²€ìƒ‰í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.",
        "sim_tab_chat_email_desc": "ê³ ê° ì‘ëŒ€ ì—…ë¬´ì—ì„œ ì±„íŒ… ë° ì´ë©”ì¼ë¡œ ì‹¤ì œë¡œ ë¬¸ì˜ ì‘ëŒ€ê°€ ë  ìˆ˜ ìˆëŠ” ì‹¤ì „ ëŒ€ë¹„ ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ì…ë‹ˆë‹¤. AIê°€ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸ê³¼ ì´ˆì•ˆì„ ìƒì„±í•˜ë©°, ê³ ê° ë°˜ì‘ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ì‹¤ì „ ëŒ€ë¹„ í›ˆë ¨ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "sim_tab_phone_desc": "ê³ ê° ì‘ëŒ€ ì—…ë¬´ì—ì„œ ì „í™”ë¡œ ì‹¤ì œë¡œ ë¬¸ì˜ ì‘ëŒ€ê°€ ë  ìˆ˜ ìˆëŠ” ì‹¤ì „ ëŒ€ë¹„ ê°€ìƒ ì‹œë‚˜ë¦¬ì˜¤ì…ë‹ˆë‹¤. ìŒì„± ë…¹ìŒ ë° ì‹¤ì‹œê°„ CC ìë§‰ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, ì „í™” í†µí™” ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì‹¤ì „ ì‘ëŒ€ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "rag_tab_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì§€ì‹ ì±—ë´‡ì…ë‹ˆë‹¤. PDF, TXT, HTML íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ RAG(Retrieval-Augmented Generation) ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³ , ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "content_tab_desc": "AIë¥¼ í™œìš©í•˜ì—¬ ê°œì¸ ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° í•µì‹¬ ìš”ì•½ ë…¸íŠ¸, ê°ê´€ì‹ í€´ì¦ˆ, ì‹¤ìŠµ ì˜ˆì œ ë“±ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "lstm_tab_desc": "LSTM ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í•™ìŠµìì˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ê³  ëŒ€ì‹œë³´ë“œë¡œ ì‹œê°í™”í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ê³ , í•™ìŠµ ì„±ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "company_info_tab_desc": "íšŒì‚¬ë³„ ìƒì„¸ ì •ë³´, ì¸ê¸° ìƒí’ˆ, í™”ì œì˜ ì†Œì‹, FAQë¥¼ ê²€ìƒ‰í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. íšŒì‚¬ ì†Œê°œ, ì¸ê¸° ìƒí’ˆ, í™”ì œì˜ ì†Œì‹ì„ ì‹œê°í™”í•˜ì—¬ í•œëˆˆì— í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "voice_rec_header_desc": "ìŒì„± ë…¹ìŒ ë° ì „ì‚¬ ê²°ê³¼ë¥¼ ê´€ë¦¬í•˜ê³  ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ë§ˆì´í¬ë¡œ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ Whisper APIë¥¼ í†µí•´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³ , ì „ì‚¬ ê²°ê³¼ë¥¼ ì €ì¥ ë° ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "more_features_label": "ë”ë³´ê¸° ê¸°ëŠ¥",
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
        "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
        "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "company_info_faq_settings": "íšŒì‚¬ë³„ ìƒì„¸ ì •ë³´ ë° FAQ",
        "search_company": "íšŒì‚¬ëª… ê²€ìƒ‰",
        "company_info": "íšŒì‚¬ ì†Œê°œ",
        "company_faq": "ìì£¼ ë‚˜ì˜¤ëŠ” ì§ˆë¬¸",
        "faq_question": "ì§ˆë¬¸",
        "faq_answer": "ë‹µë³€",
        "popular_products": "ì¸ê¸° ìƒí’ˆ",
        "trending_topics": "í™”ì œì˜ ì†Œì‹",
        "company_details": "íšŒì‚¬ ìƒì„¸ ì •ë³´",
        "no_company_found": "ì— í•´ë‹¹í•˜ëŠ” íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "no_company_selected": "íšŒì‚¬ëª…ì„ ê²€ìƒ‰í•˜ê±°ë‚˜ ì„ íƒí•´ì£¼ì„¸ìš”.",
        "product_popularity": "ìƒí’ˆ ì¸ê¸°ë„",
        "topic_trends": "í™”ì œ íŠ¸ë Œë“œ",
        "select_company": "íšŒì‚¬ ì„ íƒ",
        "faq_search": "FAQ ê²€ìƒ‰",
        "faq_search_placeholder": "FAQ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        "faq_search_placeholder_extended": "FAQ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ìƒí’ˆëª…, ì„œë¹„ìŠ¤ëª… ë“±ë„ ê²€ìƒ‰ ê°€ëŠ¥)",
        "button_search_faq": "ê²€ìƒ‰",
        "company_search_placeholder": "ì˜ˆ: ì‚¼ì„±, ë„¤ì´ë²„, êµ¬ê¸€, ì• í”Œ ë“±",
        "company_search_button": "ê²€ìƒ‰",
        "generating_company_info": "íšŒì‚¬ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...",
        "button_copy_answer": "ë‹µì•ˆ ë³µì‚¬",
        "button_copy_hint": "íŒíŠ¸ ë³µì‚¬",
        "button_download_answer": "ë‹µì•ˆ ë‹¤ìš´ë¡œë“œ",
        "button_download_hint": "íŒíŠ¸ ë‹¤ìš´ë¡œë“œ",
        "copy_instruction": "ğŸ’¡ ìœ„ í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ê³  Ctrl+C (Mac: Cmd+C)ë¡œ ë³µì‚¬í•˜ì„¸ìš”.",
        "copy_help_text": "í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ê³  Ctrl+C (ë˜ëŠ” Cmd+C)ë¡œ ë³µì‚¬í•˜ì„¸ìš”.",
        "button_reset": "ìƒˆë¡œ ì‹œì‘",
        "answer_displayed": "ë‹µì•ˆì´ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•˜ì„¸ìš”.",
        "hint_displayed": "íŒíŠ¸ê°€ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ í…ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•˜ì„¸ìš”.",
        "ai_answer_generated": "AI ë‹µì•ˆì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "hint_generated": "ì‘ëŒ€ íŒíŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "warning_enter_inquiry": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
        "customer_inquiry_review_desc": "ì—ì´ì „íŠ¸ê°€ ìƒì‚¬ë“¤ì—ê²Œ ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì¬í™•ì¸í•˜ê³ , AI ë‹µì•ˆ ë° íŒíŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.",
        "all_companies": "ì „ì²´",
        "optional": "ì„ íƒì‚¬í•­",
        "no_faq_for_company": "{company}ì˜ FAQê°€ ì—†ìŠµë‹ˆë‹¤.",
        "related_products": "ê´€ë ¨ ìƒí’ˆ",
        "related_trending_news": "ê´€ë ¨ í™”ì œ ì†Œì‹",
        "related_company_info": "ê´€ë ¨ íšŒì‚¬ ì†Œê°œ ë‚´ìš©",
        "related_faq": "ê´€ë ¨ FAQ",
        "items": "ê°œ",
        "popularity": "ì¸ê¸°ë„",
        "no_faq_for_product": "í•´ë‹¹ ìƒí’ˆê³¼ ê´€ë ¨ëœ FAQë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒí’ˆ ì •ë³´ë§Œ í‘œì‹œë©ë‹ˆë‹¤.",
        "generating_detail": "ìƒì„¸ ë‚´ìš©ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...",
        "checking_additional_info": "ìƒì„¸ ë‚´ìš©: {topic}ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.",
        "button_generate_faq": "FAQ ìƒì„±",
        "button_add_company": "ê³ ê° ë¬¸ì˜ ì¬í™•ì¸",
        "customer_inquiry_review": "ê³ ê° ë¬¸ì˜ ì¬í™•ì¸",
        "inquiry_question_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš©",
        "inquiry_question_placeholder": "ê³ ê°ì´ ë¬¸ì˜í•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”",
        "inquiry_attachment_label": "ğŸ“ ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ (ì‚¬ì§„/ìŠ¤í¬ë¦°ìƒ·)",
        "inquiry_attachment_help": "íŠ¹íˆ ì·¨ì†Œ ë¶ˆê°€ ì—¬í–‰ìƒí’ˆì˜ ë¹„í–‰ê¸° ì§€ì—°, ì—¬ê¶Œ ì´ìŠˆ ë“± ë¶ˆê°€í”¼í•œ ì‚¬ìœ ì˜ ê²½ìš°, ë°˜ë“œì‹œ ì‚¬ì§„ì´ë‚˜ ìŠ¤í¬ë¦°ìƒ·ì„ ì²¨ë¶€í•´ì£¼ì„¸ìš”.",
        "inquiry_attachment_uploaded": "âœ… ì²¨ë¶€ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {filename}",
        "extracting_file_content": "íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘...",
        "detecting_language": "ì–¸ì–´ ê°ì§€ ì¤‘...",
        "translating_content": "íŒŒì¼ ë‚´ìš© ë²ˆì—­ ì¤‘...",
        "file_translated": "âœ… íŒŒì¼ ë‚´ìš©ì´ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "file_extraction_error": "íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}",
        "ocr_requires_manual": "ì´ë¯¸ì§€ OCRì„ ìœ„í•´ì„œëŠ” Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.",
        "ocr_error": "ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {error}",
        "button_generate_ai_answer": "AI ë‹µì•ˆ ìƒì„±",
        "button_generate_hint": "ì‘ëŒ€ íŒíŠ¸ ìƒì„±",
        "ai_answer_header": "AI ì¶”ì²œ ë‹µì•ˆ",
        "hint_header": "ì‘ëŒ€ íŒíŠ¸",
        "generating_ai_answer": "AI ë‹µì•ˆì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "generating_hint": "ì‘ëŒ€ íŒíŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...",
        "button_edit_company": "íšŒì‚¬ ì •ë³´ ìˆ˜ì •",
        "button_show_company_info": "íšŒì‚¬ ì†Œê°œ ë³´ê¸°",
        "no_faq_results": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "faq_search_results": "FAQ ê²€ìƒ‰ ê²°ê³¼",
        "add_company_name": "íšŒì‚¬ëª…",
        "add_company_info": "íšŒì‚¬ ì†Œê°œ",
        "generate_faq_question": "ì§ˆë¬¸",
        "generate_faq_answer": "ë‹µë³€",
        "button_save_faq": "FAQ ì €ì¥",
        "button_cancel": "ì·¨ì†Œ",
        "faq_saved_success": "FAQê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "company_added_success": "íšŒì‚¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "company_updated_success": "íšŒì‚¬ ì •ë³´ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
        "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
        "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
        "check_answer": "ì •ë‹µ í™•ì¸",
        "next_question": "ë‹¤ìŒ ë¬¸í•­",
        "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
        "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤ã€‚ğŸ˜",
        "correct_is": "ì •ë‹µ",
        "explanation": "í•´ì„¤",
        "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
        "score": "ì ìˆ˜",
        "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
        "question_label": "ë¬¸í•­",
        "correct_questions": "ë§ì€ ë¬¸ì œ",
        "incorrect_questions": "í‹€ë¦° ë¬¸ì œ",
        "question_result": "ë¬¸ì œ ê²°ê³¼",
        "your_answer": "ë‚´ ë‹µì•ˆ",
        "correct_answer_label": "ì •ë‹µ",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
        "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
        "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚",
        "db_save_complete": "(DB ì €ì¥ ì™„ë£Œ)",
        "data_analysis_progress": "ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘...",
        "response_generating": "ë‹µë³€ ìƒì„± ì¤‘...",
        "lstm_result_header": "í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼",
        "lstm_score_metric": "í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„",
        "lstm_score_info": "ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!",
        "lstm_rerun_button": "ìƒˆë¡œìš´ ê°€ìƒ ë°ì´í„°ë¡œ ì˜ˆì¸¡",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ ---
        "toast_like": "ğŸ”¥ ì»¨í…ì¸ ê°€ ë§˜ì— ë“œì…¨êµ°ìš”! (ì¢‹ì•„ìš” ì¹´ìš´íŠ¸ +1)",
        "toast_dislike": "ğŸ˜” ë” ë‚˜ì€ ì½˜í…ì¸ ë¥¼ ìœ„í•´ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ê² ìŠµë‹ˆë‹¤ã€‚",
        "toast_share": "ğŸŒ ì½˜í…ì¸  ë§í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "toast_copy": "âœ… ì½˜í…ì¸ ê°€ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!",
        "toast_more": "â„¹ï¸ ì¶”ê°€ ì˜µì…˜ (PDF, ì¸ì‡„ë³¸ ì €ì¥ ë“±)",
        "mock_pdf_save": "ğŸ“¥ PDF ì €ì¥",
        "mock_word_open": "ğŸ“‘ Wordë¡œ ì—´ê¸°",
        "mock_print": "ğŸ–¨ ì¸ì‡„",

        # --- ì‹œë®¬ë ˆì´í„° ---
        "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤ã€‚",
        "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
        "customer_type_label": "ê³ ê° ì„±í–¥",
        "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
        "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
        "customer_generate_response_button": "ê³ ê° ë°˜ì‘ ìƒì„±",
        "send_closing_confirm_button": "ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸°",
        "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ê¸° ë•Œë¬¸ì— ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
        "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
        "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
        "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
        "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
        "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!",
        "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
        "history_expander_title": "ğŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê±´)",
        "initial_query_sample": "í”„ë‘ìŠ¤ íŒŒë¦¬ì— ë„ì°©í–ˆëŠ”ë°, í´ë£©ì—ì„œ êµ¬ë§¤í•œ eSIMì´ í™œì„±í™”ê°€ ì•ˆ ë©ë‹ˆë‹¤...",
        "button_mic_input": "ğŸ™ ìŒì„± ì…ë ¥",
        "button_mic_stop": "â¹ï¸ ë…¹ìŒ ì¢…ë£Œ",
        "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤ã€‚",
        "prompt_survey": "ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› 000ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì‹œê¸° ë°”ëë‹ˆë‹¤. [ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬]",
        "customer_closing_confirm": "ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹­ë‹ˆê¹Œ?",
        "customer_positive_response": "ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤ã€‚",
        "button_email_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ìš”ì²­)",
        "error_mandatory_contact": "ì´ë©”ì¼ê³¼ ì „í™”ë²ˆí˜¸ ì…ë ¥ì€ í•„ìˆ˜ì…ë‹ˆë‹¤ã€‚",
        "customer_attachment_label": "ğŸ“ ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ",
        "attachment_info_llm": "[ê³ ê° ì²¨ë¶€ íŒŒì¼: {filename}ì´(ê°€) í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”.]",
        "button_retry_translation": "ë²ˆì—­ ë‹¤ì‹œ ì‹œë„",
        "button_request_hint": "ğŸ’¡ ì‘ëŒ€ íŒíŠ¸ ìš”ì²­ (AHT ëª¨ë‹ˆí„°ë§ ì¤‘)",
        "button_generate_draft": "ğŸ¤– AI ì‘ë‹µ ì´ˆì•ˆ ìƒì„±",
        "draft_generating": "AIê°€ ì‘ë‹µ ì´ˆì•ˆì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...",
        "draft_success": "âœ… AI ì‘ë‹µ ì´ˆì•ˆì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.",
        "hint_placeholder": "ë¬¸ì˜ ì‘ëŒ€ì— ëŒ€í•œ íŒíŠ¸:",
        "survey_sent_confirm": "ğŸ“¨ ì„¤ë¬¸ì¡°ì‚¬ ë§í¬ê°€ ì „ì†¡ë˜ì—ˆìœ¼ë©°, ì´ ìƒë‹´ì€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "new_simulation_ready": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ã€‚",
        "agent_response_header": "âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ",
        "agent_response_placeholder": "ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš”...",
        "send_response_button": "ì‘ë‹µ ì „ì†¡",
        "customer_turn_info": "ì—ì´ì „íŠ¸ ì‘ë‹µ ì „ì†¡ ì™„ë£Œ. ê³ ê° ë°˜ì‘ì„ ìë™ìœ¼ë¡œ ìƒì„± ì¤‘ì…ë‹ˆë‹¤ã€‚",
        "generating_customer_response": "ê³ ê° ë°˜ì‘ ìƒì„± ì¤‘...",
        "call_started_message": "í†µí™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì¸ì‚¬ë§ì„ ë…¹ìŒí•˜ì„¸ìš”.",
        "call_on_hold_message": "í†µí™”ê°€ Hold ì¤‘ì…ë‹ˆë‹¤. í†µí™” ì¬ê°œ í›„ ë…¹ìŒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "recording_complete_transcribing": "ğŸ™ï¸ ë…¹ìŒ ì™„ë£Œ. ì „ì‚¬ ì²˜ë¦¬ ì¤‘...",
        "recording_complete_press_transcribe": "âœ… ë…¹ìŒ ì™„ë£Œ! ì•„ë˜ ì „ì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì„¸ìš”.",
        "customer_positive_solution_reaction": "ê³ ê°ì´ ì†”ë£¨ì…˜ì— ê¸ì •ì ìœ¼ë¡œ ë°˜ì‘í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.",
        "transcription_empty_warning": "âš ï¸ ì „ì‚¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë…¹ìŒí•´ì£¼ì„¸ìš”. (ë§ˆì´í¬ ì…ë ¥ì´ ì—†ê±°ë‚˜ ìŒì†Œê±°ëœ ê²½ìš°)",
        "transcription_error": "[ERROR: ì „ì‚¬ ì‹¤íŒ¨]",
        "transcription_no_result": "âŒ ì „ì‚¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "customer_escalation_start": "ìƒê¸‰ìì™€ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
        "request_rebuttal_button": "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­",
        "new_simulation_button": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘",
        "history_selectbox_label": "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
        "history_load_button": "ì„ íƒëœ ì´ë ¥ ë¡œë“œ",
        "delete_history_button": "âŒ ëª¨ë“  ì´ë ¥ ì‚­ì œ",
        "delete_confirm_message": "ì •ë§ë¡œ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        "delete_confirm_yes": "ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤",
        "download_history_word": "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (Word)",
        "download_history_pptx": "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PPTX)",
        "download_history_pdf": "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PDF)",
        "download_current_session": "ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‹¤ìš´ë¡œë“œ",
        "delete_confirm_no": "ì•„ë‹ˆì˜¤, ìœ ì§€í•©ë‹ˆë‹¤",
        "delete_success": "âœ… ì‚­ì œ ì™„ë£Œ!",
        "deleting_history_progress": "ì´ë ¥ ì‚­ì œ ì¤‘...",
        "search_history_label": "ì´ë ¥ ê²€ìƒ‰",
        "date_range_label": "ë‚ ì§œ ë²”ìœ„ í•„í„°",
        "history_search_button": "ğŸ” ê²€ìƒ‰",
        "no_history_found": "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "customer_email_label": "ê³ ê° ì´ë©”ì¼ (í•„ìˆ˜)",
        "customer_phone_label": "ê³ ê° ì—°ë½ì²˜ / ì „í™”ë²ˆí˜¸ (í•„ìˆ˜)",
        "transfer_header": "ì–¸ì–´ ì´ê´€ ìš”ì²­ (ë‹¤ë¥¸ íŒ€)",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ ì˜ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ ì¼ë³¸ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_system_msg": "ğŸ“Œ ì‹œìŠ¤í…œ ë©”ì‹œì§€: ê³ ê° ìš”ì²­ì— ë”°ë¼ ìƒë‹´ ì–¸ì–´ê°€ {target_lang} íŒ€ìœ¼ë¡œ ì´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìƒë‹´ì›(AI)ì´ ì‘ëŒ€í•©ë‹ˆë‹¤ã€‚",
        "transfer_loading": "ì´ê´€ ì²˜ë¦¬ ì¤‘: ì´ì „ ëŒ€í™” ì´ë ¥ ë²ˆì—­ ë° ê²€í†  (ê³ ê°ë‹˜ê»˜ 3~10ë¶„ ì–‘í•´ ìš”ì²­)",
        "transfer_summary_header": "ğŸ” ì´ê´€ëœ ìƒë‹´ì›ì„ ìœ„í•œ ìš”ì•½ (ë²ˆì—­ë¨)",
        "transfer_summary_intro": "ê³ ê°ë‹˜ê³¼ì˜ ì´ì „ ëŒ€í™” ì´ë ¥ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ëŒ€ë¥¼ ì´ì–´ë‚˜ê°€ì„¸ìš”ã€‚",
        "llm_translation_error": "âŒ ë²ˆì—­ ì‹¤íŒ¨: LLM ì‘ë‹µ ì˜¤ë¥˜",
        "timer_metric": "ìƒë‹´ ê²½ê³¼ ì‹œê°„ (AHT)",
        "timer_info_ok": "AHT (15ë¶„ ê¸°ì¤€)",
        "timer_info_warn": "AHT (10ë¶„ ì´ˆê³¼)",
        "timer_info_risk": "ğŸš¨ 15ë¶„ ì´ˆê³¼: ë†’ì€ ë¦¬ìŠ¤í¬",
        "solution_check_label": "âœ… ì´ ì‘ë‹µì— ì†”ë£¨ì…˜/í•´ê²°ì±…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ã€‚",
        "sentiment_score_label": "ê³ ê° ê°ì • ì ìˆ˜",
        "urgency_score_label": "ê¸´ê¸‰ë„ ì ìˆ˜",
        "customer_gender_label": "ê³ ê° ì„±ë³„",
        "customer_emotion_label": "ê³ ê° ê°ì • ìƒíƒœ",
        "gender_male": "ë‚¨ì„±",
        "gender_female": "ì—¬ì„±",
        "emotion_happy": "ê¸°ë¶„ ì¢‹ì€ ê³ ê°",
        "emotion_dissatisfied": "ë¶ˆë§Œì¸ ê³ ê°",
        "emotion_angry": "í™”ë‚œ ê³ ê°",
        "emotion_sad": "ìŠ¬í”ˆ/ìš°ìš¸í•œ ê³ ê°",
        "emotion_neutral": "ì¤‘ë¦½",
        "similarity_chart_title": "ìœ ì‚¬ ì¼€ì´ìŠ¤ ìœ ì‚¬ë„",
        "scores_comparison_title": "ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ",
        "similarity_score_label": "ìœ ì‚¬ë„",
        "satisfaction_score_label": "ë§Œì¡±ë„",
        "customer_satisfaction_score_label": "ê³ ê° ë§Œì¡±ë„ ì ìˆ˜",
        "sentiment_trend_label": "ê°ì • ì ìˆ˜ ì¶”ì´",
        "satisfaction_trend_label": "ë§Œì¡±ë„ ì ìˆ˜ ì¶”ì´",
        "case_trends_title": "ê³¼ê±° ì¼€ì´ìŠ¤ ì ìˆ˜ ì¶”ì´",
        "date_label": "ë‚ ì§œ",
        "score_label": "ì ìˆ˜ (0-100)",
        "customer_characteristics_title": "ê³ ê° íŠ¹ì„± ë¶„í¬",
        "language_label": "ì–¸ì–´",
        "email_provided_label": "ì´ë©”ì¼ ì œê³µ",
        "phone_provided_label": "ì „í™”ë²ˆí˜¸ ì œê³µ",
        "region_label": "ì§€ì—­",
        "btn_request_phone_summary": "ì´ë ¥ ìš”ì•½ ìš”ì²­",
        
        # --- ë‹¤ìš´ë¡œë“œ ë¬¸ì„œ ê´€ë ¨ í…ìŠ¤íŠ¸ ---
        "download_history_title": "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½",
        "download_history_number": "ì´ë ¥ #",
        "download_initial_inquiry": "ì´ˆê¸° ë¬¸ì˜",
        "download_summary": "ìš”ì•½",
        "download_main_inquiry": "ì£¼ìš” ë¬¸ì˜",
        "download_key_response": "í•µì‹¬ ì‘ë‹µ",
        "download_customer_characteristics": "ê³ ê° íŠ¹ì„±",
        "download_privacy_summary": "ê°œì¸ì •ë³´ ìš”ì•½",
        "download_address_provided": "ì£¼ì†Œ ì œê³µ",
        "download_overall_summary": "ì „ì²´ ìš”ì•½",
        "download_yes": "ì˜ˆ",
        "download_no": "ì•„ë‹ˆì˜¤",
        "download_created_date": "ìƒì„±ì¼",
        "download_cultural_background": "ë¬¸í™”ì  ë°°ê²½",
        "download_communication_style": "ì†Œí†µ ìŠ¤íƒ€ì¼",
        "download_region_hint": "ì§€ì—­ íŒíŠ¸",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "ì „í™” ë°œì‹ ",
        "button_call_outbound_to_customer": "ê³ ê°ì—ê²Œ ì „í™” ë°œì‹ ",
        "button_call_outbound_to_provider": "í˜„ì§€ ì—…ì²´ì—ê²Œ ì „í™” ë°œì‹ ",
        "call_outbound_system_msg": "ğŸ“Œ ì‹œìŠ¤í…œ ë©”ì‹œì§€: ì—ì´ì „íŠ¸ê°€ {target}ì—ê²Œ ì „í™” ë°œì‹ ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤ã€‚",
        "call_outbound_simulation_header": "ğŸ“ ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼",
        "call_outbound_summary_header": "ğŸ“ í˜„ì§€ ì—…ì²´/ê³ ê°ê³¼ì˜ í†µí™” ìš”ì•½",
        "call_outbound_loading": "ì „í™” ì—°ê²° ë° í†µí™” ê²°ê³¼ ì •ë¦¬ ì¤‘... (LLM í˜¸ì¶œ)",
        "call_target_select_label": "ë°œì‹  ëŒ€ìƒ ì„ íƒ",
        "call_target_customer": "ê³ ê°ì—ê²Œ ë°œì‹ ",
        "call_target_partner": "í˜„ì§€ ì—…ì²´ ë°œì‹ ",

        # --- ìŒì„± ê¸°ë¡ ---
        "voice_rec_header": "ìŒì„± ê¸°ë¡ & ê´€ë¦¬",
        "record_help": "ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "uploaded_file": "ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ",
        "rec_list_title": "ì €ì¥ëœ ìŒì„± ê¸°ë¡",
        "transcribe_btn": "ì „ì‚¬(Whisper)",
        "save_btn": "ìŒì„± ê¸°ë¡ ì €ì¥",
        "transcribing": "ìŒì„± ì „ì‚¬ ì¤‘...",
        "transcript_result": "ì „ì‚¬ ê²°ê³¼:",
        "transcript_text": "ì „ì‚¬ í…ìŠ¤íŠ¸",
        "openai_missing": "OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "whisper_client_error": "âŒ Whisper API Client ì´ˆê¸°í™” ì‹¤íŒ¨",
        "whisper_auth_error": "âŒ Whisper API ì¸ì¦ ì‹¤íŒ¨",
        "whisper_format_error": "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤ã€‚",
        "whisper_success": "âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ!",
        "playback": "ë…¹ìŒ ì¬ìƒ",
        "retranscribe": "ì¬ì „ì‚¬",
        "delete": "ì‚­ì œ",
        "no_records": "ì €ì¥ëœ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "saved_success": "ì €ì¥ ì™„ë£Œ!",
        "delete_confirm_rec": "ì •ë§ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        "gcs_not_conf": "GCS ë¯¸ì„¤ì •",
        "gcs_playback_fail": "ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨",
        "gcs_no_audio": "ì˜¤ë””ì˜¤ ì—†ìŒ",
        "error": "ì˜¤ë¥˜:",
        "firestore_no_db_connect": "DB ì—°ê²° ì‹¤íŒ¨",
        "save_history_success": "ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "save_history_fail": "ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨",
        "delete_fail": "ì‚­ì œ ì‹¤íŒ¨",
        "rec_header": "ìŒì„± ì…ë ¥ ë° ì „ì‚¬",
        "whisper_processing": "ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘..",
        "empty_response_warning": "ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”.",
        "customer_no_more_inquiries": "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.",
        "customer_has_additional_inquiries": "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤.",
        "sim_end_chat_button": "ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë° ì‘ëŒ€ ì¢…ë£Œ",
        "delete_mic_record": "âŒ ë…¹ìŒ ì‚­ì œ",
        "agent_confirmed_additional_inquiry": "ì—ì´ì „íŠ¸ê°€ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê³ ê°ì˜ ìµœì¢… ë‹µë³€ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.",
        "llm_key_missing_customer_response": "LLM Keyê°€ ì—†ì–´ ê³ ê° ë°˜ì‘ ìë™ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ 'ê³ ê° ë°˜ì‘ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜ AGENT_TURNìœ¼ë¡œ ëŒì•„ê°€ì„¸ìš”ã€‚",
        "customer_response_generation_failed": "ê³ ê° ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        "no_more_inquiries_confirmed": "âœ… ê³ ê°ì´ ë” ì´ìƒ ë¬¸ì˜í•  ì‚¬í•­ì´ ì—†ë‹¤ê³  í™•ì¸í–ˆìŠµë‹ˆë‹¤ã€‚",
        "consultation_end_header": "ğŸ“‹ ìƒë‹´ ì¢…ë£Œ",
        "click_survey_button_to_end": "ì•„ë˜ **ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë° ì‘ëŒ€ ì¢…ë£Œ** ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìƒë‹´ì„ ì¢…ë£Œí•˜ì„¸ìš”.",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ (ìŠ¤í¬ë¦°ìƒ· ë“±)",
        "attachment_placeholder": "íŒŒì¼ì„ ì²¨ë¶€í•˜ì—¬ ìƒí™©ì„ ì„¤ëª…í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­)",
        "attachment_info_llm": "[ê³ ê° ì²¨ë¶€ íŒŒì¼: {filename}ì´(ê°€) í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”.]",
        "agent_attachment_label": "ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ (ìŠ¤í¬ë¦°ìƒ· ë“±)",
        "agent_attachment_placeholder": "ì‘ë‹µì— ì²¨ë¶€í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­)",
        "agent_attachment_status": "ğŸ“ ì—ì´ì „íŠ¸ê°€ **{filename}** íŒŒì¼ì„ ì‘ë‹µì— ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. (íŒŒì¼ íƒ€ì…: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG ì„ë² ë”© ì‹¤íŒ¨: OpenAI API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_gemini": "RAG ì„ë² ë”© ì‹¤íŒ¨: Gemini API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_nvidia": "RAG ì„ë² ë”© ì‹¤íŒ¨: NVIDIA API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_none": "RAG ì„ë² ë”©ì— í•„ìš”í•œ ëª¨ë“  í‚¤(OpenAI, Gemini, NVIDIA)ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”ã€‚",

        # --- ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ì¶”ê°€ ---
        "phone_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì „í™”)",
        "call_status_waiting": "ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...",
        "call_status_ringing": "ì „í™” ìˆ˜ì‹  ì¤‘: {number}",
        "button_answer": "ğŸ“ ì „í™” ì‘ë‹µ",
        "button_hangup": "ğŸ”´ ì „í™” ëŠê¸°",
        "button_hold": "â¸ï¸ Hold (ì†ŒìŒ ì°¨ë‹¨)",
        "button_resume": "â–¶ï¸ í†µí™” ì¬ê°œ",
        "hold_status": "í†µí™” Hold ì¤‘ (ëˆ„ì  Hold ì‹œê°„: {duration})",
        "cc_live_transcript": "ğŸ¤ ì‹¤ì‹œê°„ CC ìë§‰ / ì „ì‚¬",
        "mic_input_status": "ğŸ™ï¸ ì—ì´ì „íŠ¸ ìŒì„± ì…ë ¥",
        "customer_audio_playback": "ğŸ—£ï¸ ê³ ê° ìŒì„± ì¬ìƒ",
        "agent_response_prompt": "ê³ ê°ì—ê²Œ ë§í•  ì‘ë‹µì„ ë…¹ìŒí•˜ì„¸ìš”ã€‚",
        "agent_response_stop_and_send": "â¹ï¸ ë…¹ìŒ ì¢…ë£Œ ë° ì‘ë‹µ ì „ì†¡",
        "call_end_message": "í†µí™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. AHT ë° ì´ë ¥ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "call_query_placeholder": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”ã€‚",
        "call_number_placeholder": "+82 10-xxxx-xxxx (ê°€ìƒ ë²ˆí˜¸)",
        "website_url_label": "í™ˆí˜ì´ì§€ ì›¹ ì£¼ì†Œ (ì„ íƒì‚¬í•­)",
        "website_url_placeholder": "https://example.com (í™ˆí˜ì´ì§€ ì£¼ì†Œê°€ ìˆìœ¼ë©´ ì…ë ¥í•˜ì„¸ìš”)",
        "call_summary_header": "AI í†µí™” ìš”ì•½",
        "customer_audio_header": "ê³ ê° ìµœì´ˆ ë¬¸ì˜ (ìŒì„±)",
        "aht_not_recorded": "âš ï¸ í†µí™” ì‹œì‘ ì‹œê°„ì´ ê¸°ë¡ë˜ì§€ ì•Šì•„ AHTë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "no_audio_record": "ê³ ê°ì˜ ìµœì´ˆ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "customer_query_playing": "ğŸ”Š ê³ ê° ë¬¸ì˜ ì¬ìƒ ì¤‘ì…ë‹ˆë‹¤.",
        "query_content_label": "ğŸ“ ë¬¸ì˜ ë‚´ìš©:",
        "auto_play_failed": "ìë™ ì¬ìƒ ì‹¤íŒ¨: {error}. ìˆ˜ë™ìœ¼ë¡œ ì¬ìƒí•´ì£¼ì„¸ìš”.",
        "generating_customized_response": "ê³ ê° ë§ì¶¤í˜• ë°˜ì‘ ìƒì„± ì¤‘...",
        "customer_responded": "ğŸ—£ï¸ ê³ ê°ì´ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤: {reaction}",
        "customer_voice_generation_error": "âŒ ê³ ê° ìŒì„± ìƒì„± ì˜¤ë¥˜: {error}",
        "button_retry_translation": "ë²ˆì—­ ë‹¤ì‹œ ì‹œë„",
        "customer_waiting_hold": "[ê³ ê°: ì ì‹œ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤...]",
        "agent_hold_message": "[ì—ì´ì „íŠ¸: Hold ì¤‘ì…ë‹ˆë‹¤. í†µí™” ì¬ê°œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.]",
        
        # --- ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ê´€ë ¨ ---
        "video_upload_expander": "ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ/ë¡œë“œ",
        "video_sync_enable": "ë¹„ë””ì˜¤ ë™ê¸°í™” í™œì„±í™” (TTSì™€ í•¨ê»˜ ì¬ìƒ)",
        "video_rag_title": "ğŸ¥ OpenAI/Gemini ê¸°ë°˜ ì˜ìƒ RAG ê¸°ëŠ¥",
        "video_rag_desc": "âœ… **í˜„ì¬ êµ¬í˜„ ë°©ì‹ (ì˜ìƒ RAG):**\n\n1. **LLM í…ìŠ¤íŠ¸ ë¶„ì„**: OpenAI/Gemini APIê°€ ê³ ê°ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ê°ì • ìƒíƒœì™€ ì œìŠ¤ì²˜ë¥¼ ìë™ íŒë‹¨í•©ë‹ˆë‹¤.\n\n2. **ì§€ëŠ¥í˜• ë¹„ë””ì˜¤ ì„ íƒ**: ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë¹„ë””ì˜¤ í´ë¦½ì„ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.\n   - ê°ì • ìƒíƒœ: HAPPY, ANGRY, ASKING, SAD, NEUTRAL\n   - ì œìŠ¤ì²˜: HAND_WAVE, NOD, SHAKE_HEAD, POINT, NONE\n\n3. **TTS ë™ê¸°í™” ì¬ìƒ**: ì„ íƒëœ ë¹„ë””ì˜¤ì™€ TTSë¡œ ìƒì„±ëœ ìŒì„±ì„ ë™ì‹œì— ì¬ìƒí•©ë‹ˆë‹¤.\n\n**ì‚¬ìš© ë°©ë²•:**\n- ì„±ë³„(ë‚¨ì/ì—¬ì)ê³¼ ê°ì • ìƒíƒœë³„ë¡œ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\n- ì œìŠ¤ì²˜ë³„ ë¹„ë””ì˜¤ë„ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤ (ì˜ˆ: `male_happy_hand_wave.mp4`).\n- ê³ ê°ì´ ë§í•˜ëŠ” ë‚´ìš©ì— ë”°ë¼ LLMì´ ìë™ìœ¼ë¡œ ì ì ˆí•œ ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.",
        "video_gender_emotion_setting": "ì„±ë³„ ë° ê°ì • ìƒíƒœë³„ ë¹„ë””ì˜¤ ì„¤ì •",
        "video_gender_label": "ì„±ë³„",
        "video_gender_male": "ë‚¨ì",
        "video_gender_female": "ì—¬ì",
        "video_emotion_label": "ê°ì • ìƒíƒœ",
        "video_upload_label": "ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ({gender} - {emotion})",
        "video_current_selection": "ğŸ“¹ í˜„ì¬ ì„ íƒ: {gender} - {emotion}",
        "video_upload_prompt": "ğŸ’¡ '{filename}' ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
        "video_save_path": "ğŸ“‚ ë¹„ë””ì˜¤ ì €ì¥ ê²½ë¡œ:",
        "video_directory_empty": "âš ï¸ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
        "video_directory_not_exist": "âš ï¸ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}",
        "video_local_path_input": "ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì…ë ¥",
        "video_local_path_placeholder": "ì˜ˆ: C:\\Users\\Admin\\Downloads\\video.mp4 ë˜ëŠ” video.mp4",
        "video_current_avatar": "ğŸ“º í˜„ì¬ ê³ ê° ì•„ë°”íƒ€ ì˜ìƒ",
        "video_avatar_upload_prompt": "ğŸ’¡ '{filename}' ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì˜ìƒì´ í‘œì‹œë©ë‹ˆë‹¤.",
        "video_uploaded_files": "ğŸ“ ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ íŒŒì¼:",
        "video_bytes_saved": "âœ… ë¹„ë””ì˜¤ ë°”ì´íŠ¸ ì €ì¥ ì™„ë£Œ: {name} ({size} MB)",
        "video_empty_error": "âŒ ë¹„ë””ì˜¤ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
        "video_upload_error": "âŒ ë¹„ë””ì˜¤ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}",
        "video_playback_error": "âŒ ë¹„ë””ì˜¤ ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
        "video_auto_play_info": "ğŸ’¡ ì´ ë¹„ë””ì˜¤ëŠ” '{gender} - {emotion}' ìƒíƒœì—ì„œ ìë™ìœ¼ë¡œ ì¬ìƒë©ë‹ˆë‹¤.",
        "video_preview_error": "ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜",
        "video_similar_gender": "ê°™ì€ ì„±ë³„ì˜ ë‹¤ë¥¸ ë¹„ë””ì˜¤",
        "video_rename_hint": "ğŸ’¡ ìœ„ ë¹„ë””ì˜¤ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ íŒŒì¼ëª…ì„ ë³€ê²½í•˜ê±°ë‚˜ ìƒˆë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”.",
        "video_more_files": "... ì™¸ {count}ê°œ",
        "avatar_status_info": "ìƒíƒœ: {state} | ì„±ë³„: {gender}",
        "customer_video_simulation": "ê³ ê° ì˜ìƒ ì‹œë®¬ë ˆì´ì…˜",
        "customer_avatar": "ê³ ê° ì•„ë°”íƒ€",
        "faq_question_prefix": "Q{num}.",
        "visualization_chart": "ì‹œê°í™” ì°¨íŠ¸",
        "company_search_or_select": "íšŒì‚¬ëª…ì„ ê²€ìƒ‰í•˜ê±°ë‚˜ ì„ íƒí•´ì£¼ì„¸ìš”.",
    },

    # --- â­ ì˜ì–´ ë²„ì „ (í•œêµ­ì–´ 100% ë§¤ì¹­) ---
    "en": {
        "title": "Personalized AI Study Coach (Voice & Local DB)",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction Dashboard",
        "sim_tab_chat_email": "AI Customer Support Simulator (Chat / Email)",
        "sim_tab_phone": "AI Customer Support Simulator (Phone)",
        "simulator_tab": "AI Customer Support Simulator",
        "company_info_tab": "Company Information & FAQ",
        "sim_tab_chat_email_desc": "A virtual scenario for practical training in handling customer inquiries via chat and email in customer service work. AI generates response guidelines and drafts, and simulates customer reactions for real-world training.",
        "sim_tab_phone_desc": "A virtual scenario for practical training in handling customer inquiries via phone in customer service work. Provides voice recording and real-time CC subtitle features, allowing you to improve your practical response skills through phone call simulations.",
        "rag_tab_desc": "A knowledge chatbot that answers questions based on uploaded documents. Upload PDF, TXT, or HTML files to build a RAG (Retrieval-Augmented Generation) index and provide accurate answers based on document content.",
        "content_tab_desc": "A feature that generates personalized learning content using AI. You can generate key summary notes, multiple-choice quizzes, and practical examples tailored to learning topics and difficulty levels.",
        "lstm_tab_desc": "A feature that predicts learner achievement using LSTM models and visualizes it in a dashboard. Analyzes past quiz score data to predict future achievement and visually check learning performance.",
        "company_info_tab_desc": "Search and manage company-specific detailed information, popular products, trending news, and FAQs. Visualize company introductions, popular products, and trending news at a glance.",
        "voice_rec_header_desc": "A feature for managing and storing voice recordings and transcription results. Record with a microphone or upload files to convert speech to text via Whisper API, and save and manage transcription results.",
        "more_features_label": "More Features",
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answer questions based on uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "llm_error_key": "âš ï¸ Warning: GEMINI_API_KEY is not set.",
        "llm_error_init": "LLM initialization error. Please check your API key.",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content based on the topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "10 MCQ Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Achievement Prediction Dashboard",
        "lstm_desc": "Train an LSTM model on hypothetical quiz scores and predict performance.",
        "lang_select": "Select Language",
        "company_info_faq_settings": "Company Details & FAQ",
        "search_company": "Search Company",
        "company_info": "Company Information",
        "company_faq": "Frequently Asked Questions",
        "faq_question": "Question",
        "faq_answer": "Answer",
        "popular_products": "Popular Products",
        "trending_topics": "Trending News",
        "company_details": "Company Details",
        "no_company_found": "No matching company found.",
        "no_company_selected": "Please search or select a company name.",
        "product_popularity": "Product Popularity",
        "topic_trends": "Topic Trends",
        "select_company": "Select Company",
        "faq_search": "FAQ Search",
        "faq_search_placeholder": "Enter FAQ search term",
        "faq_search_placeholder_extended": "Enter FAQ search term (product names, service names, etc. can also be searched)",
        "button_search_faq": "Search",
        "company_search_placeholder": "e.g., Samsung, Naver, Google, Apple",
        "company_search_button": "Search",
        "generating_company_info": "Generating company information...",
        "button_copy_answer": "Copy Answer",
        "button_copy_hint": "Copy Hint",
        "button_download_answer": "Download Answer",
        "button_download_hint": "Download Hint",
        "copy_instruction": "ğŸ’¡ Select the text above and press Ctrl+C (Mac: Cmd+C) to copy.",
        "copy_help_text": "Select the text and press Ctrl+C (or Cmd+C) to copy.",
        "button_reset": "Reset",
        "answer_displayed": "Answer displayed. Please copy the text above.",
        "hint_displayed": "Hint displayed. Please copy the text above.",
        "ai_answer_generated": "AI answer has been generated.",
        "hint_generated": "Response hint has been generated.",
        "warning_enter_inquiry": "Please enter the customer inquiry.",
        "customer_inquiry_review_desc": "A feature that allows agents to reconfirm customer inquiries with supervisors and generate AI answers and hints.",
        "all_companies": "All",
        "optional": "Optional",
        "no_faq_for_company": "No FAQs available for {company}.",
        "related_products": "Related Products",
        "related_trending_news": "Related Trending News",
        "related_company_info": "Related Company Information",
        "related_faq": "Related FAQ",
        "items": "",
        "popularity": "Popularity",
        "no_faq_for_product": "No FAQs related to this product were found. Only product information is displayed.",
        "generating_detail": "Generating detailed content...",
        "checking_additional_info": "Checking additional information for: {topic}",
        "button_generate_faq": "Generate FAQ",
        "button_add_company": "Customer Inquiry Review",
        "customer_inquiry_review": "Customer Inquiry Review",
        "inquiry_question_label": "Customer Inquiry",
        "inquiry_question_placeholder": "Enter the customer's inquiry",
        "inquiry_attachment_label": "ğŸ“ Customer Attachment Upload (Photo/Screenshot)",
        "inquiry_attachment_help": "For non-refundable travel products with unavoidable reasons (flight delays, passport issues, etc.), please attach photos or screenshots.",
        "inquiry_attachment_uploaded": "âœ… Attachment uploaded: {filename}",
        "extracting_file_content": "Extracting file content...",
        "detecting_language": "Detecting language...",
        "translating_content": "Translating file content...",
        "file_translated": "âœ… File content has been translated.",
        "file_extraction_error": "Error occurred while extracting file content: {error}",
        "ocr_requires_manual": "Gemini API key is required for image OCR. Please manually enter the text from the image.",
        "ocr_error": "Error extracting text from image: {error}",
        "button_generate_ai_answer": "Generate AI Answer",
        "button_generate_hint": "Generate Response Hint",
        "ai_answer_header": "AI Recommended Answer",
        "hint_header": "Response Hint",
        "generating_ai_answer": "Generating AI answer...",
        "generating_hint": "Generating response hint...",
        "button_edit_company": "Edit Company Info",
        "button_show_company_info": "Show Company Info",
        "no_faq_results": "No search results found.",
        "faq_search_results": "FAQ Search Results",
        "add_company_name": "Company Name",
        "add_company_info": "Company Information",
        "generate_faq_question": "Question",
        "generate_faq_answer": "Answer",
        "button_save_faq": "Save FAQ",
        "button_cancel": "Cancel",
        "faq_saved_success": "FAQ saved successfully.",
        "company_added_success": "Company added successfully.",
        "company_updated_success": "Company information updated successfully.",
        "embed_success": "Learning DB built with {count} chunks!",
        "embed_fail": "Embedding failed: quota exceeded or network issue.",
        "warning_no_files": "Please upload study materials first.",
        "warning_rag_not_ready": "RAG is not ready. Upload materials and analyze.",
        "quiz_fail_structure": "Quiz data structure is invalid.",
        "select_answer": "Select answer",
        "check_answer": "Check answer",
        "next_question": "Next question",
        "correct_answer": "Correct! ğŸ‰",
        "incorrect_answer": "Incorrect ğŸ˜",
        "correct_is": "Correct answer",
        "explanation": "Explanation",
        "quiz_complete": "Quiz Complete!",
        "score": "Score",
        "retake_quiz": "Retake Quiz",
        "question_label": "Question",
        "correct_questions": "Correct",
        "incorrect_questions": "Incorrect",
        "question_result": "Question Results",
        "your_answer": "Your Answer",
        "correct_answer_label": "Correct Answer",
        "quiz_error_llm": "Quiz generation failed: invalid JSON.",
        "quiz_original_response": "Original LLM Response",
        "firestore_loading": "Loading RAG index...",
        "firestore_no_index": "No existing RAG index found.",
        "db_save_complete": "(DB Save Complete)",
        "data_analysis_progress": "Analyzing materials and building DB...",
        "response_generating": "Generating response...",
        "lstm_result_header": "Achievement Prediction",
        "lstm_score_metric": "Predicted Achievement",
        "lstm_score_info": "Estimated next quiz score: **{predicted_score:.1f}**.",
        "lstm_rerun_button": "Predict with New Data",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ ---
        "toast_like": "ğŸ”¥ Content liked! (+1 Count Reflected)",
        "toast_dislike": "ğŸ˜” Feedback recorded for better content.",
        "toast_share": "ğŸŒ Content link generated.",
        "toast_copy": "âœ… Content copied to clipboard!",
        "toast_more": "â„¹ï¸ Additional options (Print, PDF Save, etc.)",
        "mock_pdf_save": "ğŸ“¥ Save as PDF",
        "mock_word_open": "ğŸ“‘ Open via Word",
        "mock_print": "ğŸ–¨ Print",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ë ---

        # Simulator
        "simulator_header": "AI Customer Response Simulator",
        "simulator_desc": "AI generates draft responses and guidelines for customer inquiries.",
        "customer_query_label": "Customer Message (links allowed)",
        "customer_type_label": "Customer Type",
        "customer_type_options": ["General Inquiry", "Difficult Customer", "Highly Dissatisfied Customer"],
        "button_simulate": "Generate Response",
        "customer_generate_response_button": "Generate Customer Response",
        "send_closing_confirm_button": "Send Closing Confirmation",
        "simulation_warning_query": "Please enter the customerâ€™s message.",
        "simulation_no_key_warning": "âš ï¸ API Key missing. Simulation cannot proceed.",
        "simulation_advice_header": "AI Response Guidelines",
        "simulation_draft_header": "Recommended Response Draft",
        "button_listen_audio": "Play as Audio",
        "tts_status_ready": "Ready to generate audio",
        "tts_status_generating": "Generating audio...",
        "tts_status_success": "Audio ready!",
        "tts_status_error": "TTS error occurred",
        "history_expander_title": "ğŸ“ Load Previous Sessions (Last 10)",
        "initial_query_sample": "I arrived in Paris but my Klook eSIM won't activateâ€¦",
        "button_mic_input": "ğŸ™ Voice Input",
        "button_mic_stop": "â¹ï¸ Stop recording",
        "prompt_customer_end": "No further inquiries. Ending chat.",
        "prompt_survey": "This was Agent 000. Have a nice day. [Survey Link]",
        "customer_closing_confirm": "Is there anything else we can assist you with?",
        "customer_positive_response": "I understand. Thank you.",
        "button_email_end_chat": "End supports (Survey Request)",
        "error_mandatory_contact": "Email and Phone number input are mandatory.",
        "customer_attachment_label": "ğŸ“ Customer Attachment Upload",
        "attachment_info_llm": "[Customer Attachment: {filename} is confirmed. Reference this file in your response.]",
        "button_retry_translation": "Retry Translation",
        "button_request_hint": "ğŸ’¡ Request Response Hint (AHT Monitored)",
        "button_generate_draft": "ğŸ¤– Generate AI Response Draft",
        "draft_generating": "AI is generating a response draft...",
        "draft_success": "âœ… AI response draft has been generated. Please review and modify below.",
        "hint_placeholder": "Hints for responses",
        "survey_sent_confirm": "ğŸ“¨ The survey link has been sent. This chat session is now closedã€‚",
        "new_simulation_ready": "You can now start a new simulation.",
        "agent_response_header": "âœï¸ Agent Response",
        "agent_response_placeholder": "Write a response...",
        "send_response_button": "Send Response",
        "customer_turn_info": "Agent response sent. Generating customer reaction automaticallyã€‚",
        "generating_customer_response": "Generating customer response...",
        "call_started_message": "Call started. Please click the microphone button below to record your greeting.",
        "call_on_hold_message": "Call is on hold. Recording is available after resuming the call.",
        "recording_complete_transcribing": "ğŸ™ï¸ Recording complete. Transcribing...",
        "recording_complete_press_transcribe": "âœ… Recording complete! Press the transcribe button below to convert to text.",
        "customer_positive_solution_reaction": "The customer responded positively to the solution. Please check if there are any additional inquiries.",
        "transcription_empty_warning": "âš ï¸ Transcription result is empty. Please record again. (No microphone input or muted)",
        "transcription_error": "[ERROR: Transcription failed]",
        "transcription_no_result": "âŒ No transcription result.",
        "customer_escalation_start": "I want to speak to a supervisor",
        "request_rebuttal_button": "Request Customer Reaction",
        "new_simulation_button": "Start New Simulation",
        "history_selectbox_label": "Choose a record to load:",
        "history_load_button": "Load Selected Record",
        "delete_history_button": "âŒ Delete All History",
        "delete_confirm_message": "Are you sure you want to delete all records?",
        "delete_confirm_yes": "Yes, Delete",
        "delete_confirm_no": "Cancel",
        "download_history_word": "ğŸ“¥ Download History (Word)",
        "download_history_pptx": "ğŸ“¥ Download History (PPTX)",
        "download_history_pdf": "ğŸ“¥ Download History (PDF)",
        "download_current_session": "ğŸ“¥ Download Current Session",
        "delete_success": "Deleted successfully!",
        "deleting_history_progress": "Deleting history...",
        "search_history_label": "Search History",
        "date_range_label": "Date Filter",
        "history_search_button": "ğŸ” Search",
        "no_history_found": "No matching history found.",
        "customer_email_label": "Customer Email (Mandatory)",
        "customer_phone_label": "Customer Phone / WhatsApp (Mandatory)",
        "transfer_header": "Language Transfer Request (To Other Teams)",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ English Team Transfer",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ Japanese Team Transfer",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· Korean Team Transfer",
        "transfer_system_msg": "ğŸ“Œ System Message: The session language has been transferred to the {target_lang} team per customer request. A new agent (AI) will now respondã€‚",
        "transfer_loading": "Transferring: Translating and reviewing chat history (3-10 minute wait requested from customer)",
        "transfer_summary_header": "ğŸ” Summary for Transferred Agent (Translated)",
        "transfer_summary_intro": "This is the previous chat history. Please continue the support based on this summaryã€‚",
        "llm_translation_error": "âŒ Translation failed: LLM response error",
        "timer_metric": "Elapsed Time",
        "timer_info_ok": "AHT (15 min standard)",
        "timer_info_warn": "AHT (Over 10 min)",
        "timer_info_risk": "ğŸš¨ Over 15 min: High Risk",
        "solution_check_label": "âœ… This response includes a solution/fixã€‚",
        "sentiment_score_label": "Customer Sentiment Score",
        "urgency_score_label": "Urgency Score",
        "customer_gender_label": "Customer Gender",
        "customer_emotion_label": "Customer Emotional State",
        "gender_male": "Male",
        "gender_female": "Female",
        "emotion_happy": "Happy Customer",
        "emotion_dissatisfied": "Dissatisfied Customer",
        "emotion_angry": "Angry Customer",
        "emotion_sad": "Sad/Depressed Customer",
        "emotion_neutral": "Neutral",
        "similarity_chart_title": "Case Similarity",
        "scores_comparison_title": "Sentiment & Satisfaction Scores",
        "similarity_score_label": "Similarity",
        "satisfaction_score_label": "Satisfaction",
        "customer_satisfaction_score_label": "Customer Satisfaction Score",
        "sentiment_trend_label": "Sentiment Trend",
        "satisfaction_trend_label": "Satisfaction Trend",
        "case_trends_title": "Case Score Trends",
        "date_label": "Date",
        "score_label": "Score (0-100)",
        "customer_characteristics_title": "Customer Characteristics",
        "language_label": "Language",
        "email_provided_label": "Email Provided",
        "phone_provided_label": "Phone Provided",
        "region_label": "Region",
        "btn_request_phone_summary": "Request to summarize histories",
        
        # --- ë‹¤ìš´ë¡œë“œ ë¬¸ì„œ ê´€ë ¨ í…ìŠ¤íŠ¸ ---
        "download_history_title": "Customer Interaction History Summary",
        "download_history_number": "History #",
        "download_initial_inquiry": "Initial Inquiry",
        "download_summary": "Summary",
        "download_main_inquiry": "Main Inquiry",
        "download_key_response": "Key Response",
        "download_customer_characteristics": "Customer Characteristics",
        "download_privacy_summary": "Privacy Summary",
        "download_address_provided": "Address Provided",
        "download_overall_summary": "Overall Summary",
        "download_yes": "Yes",
        "download_no": "No",
        "download_created_date": "Created Date",
        "download_cultural_background": "Cultural Background",
        "download_communication_style": "Communication Style",
        "download_region_hint": "Region Hint",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "Call Outbound",
        "button_call_outbound_to_customer": "Call Outbound to Customer",
        "button_call_outbound_to_provider": "Call Outbound to Provider",
        "call_outbound_system_msg": "ğŸ“Œ System Message: Agent attempted an outbound call to {target}ã€‚",
        "call_outbound_simulation_header": "ğŸ“ Outbound Call Simulation Result",
        "call_outbound_summary_header": "ğŸ“ Summary of Call with Local Partner/Customer",
        "call_outbound_loading": "Connecting call and summarizing outcome... (LLM Call)",
        "call_target_select_label": "Select Call Target",
        "call_target_customer": "Call Customer",
        "call_target_partner": "Call Local Partner",

        # --- ìŒì„± ê¸°ë¡ ---
        "voice_rec_header": "Voice Record & Management",
        "record_help": "Record using the microphone or upload a fileã€‚",
        "uploaded_file": "Upload Audio File",
        "rec_list_title": "Saved Voice Records",
        "transcribe_btn": "Transcribe (Whisper)",
        "save_btn": "Save Record",
        "transcribing": "Transcribing...",
        "transcript_result": "Transcription:",
        "transcript_text": "Transcribed Text",
        "openai_missing": "OpenAI API Key is missingã€‚",
        "whisper_client_error": "âŒ Error: Whisper API client not initializedã€‚",
        "whisper_auth_error": "âŒ Whisper API authentication failedã€‚",
        "whisper_format_error": "âŒ Error: Unsupported audio formatã€‚",
        "whisper_success": "âœ… Voice Transcription Complete!",
        "playback": "Playback Recording",
        "retranscribe": "Re-transcribe",
        "delete": "Delete",
        "no_records": "No saved voice recordsã€‚",
        "saved_success": "Saved successfully!",
        "delete_confirm_rec": "Are you sure you want to delete this voice record?",
        "gcs_not_conf": "GCS not configured or no audio available",
        "gcs_playback_fail": "Failed to play audio",
        "gcs_no_audio": "No audio file found",
        "error": "Error:",
        "firestore_no_db_connect": "DB connection failed",
        "save_history_success": "Saved successfullyã€‚",
        "save_history_fail": "Save failedã€‚",
        "delete_fail": "Delete failed",
        "rec_header": "Voice Input & Transcription",
        "whisper_processing": "Processing...",
        "empty_response_warning": "Please enter a responseã€‚",
        "customer_no_more_inquiries": "No, that will be all, thank youã€‚",
        "customer_has_additional_inquiries": "Yes, I have an additional questionã€‚",
        "sim_end_chat_button": "Send Survey Link and End Consultations",
        "delete_mic_record": "âŒ Delete recordings",
        "agent_confirmed_additional_inquiry": "The agent has confirmed if there are any additional inquiries. Automatically generating the customer's final response.",
        "llm_key_missing_customer_response": "LLM Key is missing. Customer response auto-generation is unavailable. Please manually click the 'Generate Customer Response' button or return to AGENT_TURNã€‚",
        "customer_response_generation_failed": "Failed to generate customer response. Please try again.",
        "no_more_inquiries_confirmed": "âœ… Confirmed that the customer has no further inquiriesã€‚",
        "consultation_end_header": "ğŸ“‹ End of Consultation",
        "click_survey_button_to_end": "Please end the consultation by clicking the **Send Survey Link and End Consultations** button below.",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "Customer Attachment Upload (Screenshot, etcã€‚)",
        "attachment_placeholder": "Attach a file to explain the situation (optional)",
        "attachment_info_llm": "[Customer Attachment: {filename} is confirmed. Reference this file in your responseã€‚]",
        "agent_attachment_label": "Agent Attachment (Screenshot, etcã€‚)",
        "agent_attachment_placeholder": "Select a file to attach to the response (optional)",
        "agent_attachment_status": "ğŸ“ Agent attached **{filename}** file to the responseã€‚ (File type: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG embedding failed: OpenAI API Key is invalid or not setã€‚",
        "rag_embed_error_gemini": "RAG embedding failed: Gemini API Key is invalid or not setã€‚",
        "rag_embed_error_nvidia": "RAG embedding failed: NVIDIA API Key is invalid or not setã€‚",
        "rag_embed_error_none": "RAG embedding failed: All required keys (OpenAI, Gemini, NVIDIA) are invalid or not setã€‚ Please configure a keyã€‚",

        # --- ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ì¶”ê°€ ---
        "phone_header": "AI Customer Support Simulator (Phone)",
        "call_status_waiting": "Waiting for incoming call...",
        "call_status_ringing": "Incoming Call from: {number}",
        "button_answer": "ğŸ“ Answer Call",
        "button_hangup": "ğŸ”´ Hang Up",
        "button_hold": "â¸ï¸ Hold (Mute)",
        "button_resume": "â–¶ï¸ Resume Call",
        "hold_status": "On Hold (Total Hold Time: {duration})",
        "cc_live_transcript": "ğŸ¤ Live CC Transcript",
        "mic_input_status": "ğŸ™ï¸ Agent Voice Input",
        "customer_audio_playback": "ğŸ—£ï¸ Customer Audio Playback",
        "agent_response_prompt": "Record your response to the customerã€‚",
        "agent_response_stop_and_send": "â¹ï¸ Stop and share recording to customers",
        "call_end_message": "Call ended. Check AHT and historyã€‚",
        "call_query_placeholder": "Enter customer's initial queryã€‚",
        "call_number_placeholder": "+1 (555) 123-4567 (Mock Number)",
        "website_url_label": "Website URL (Optional)",
        "website_url_placeholder": "https://example.com (Enter website URL if available)",
        "call_summary_header": "AI Call Summary",
        "customer_audio_header": "Customer Initial Query (Voice)",
        "aht_not_recorded": "âš ï¸ Call start time not recordedã€‚ Cannot calculate AHTã€‚",
        "no_audio_record": "No initial customer voice recordã€‚",
        "customer_query_playing": "ğŸ”Š Playing customer inquiry...",
        "query_content_label": "ğŸ“ Inquiry content:",
        "auto_play_failed": "Auto-play failed: {error}. Please play manually.",
        "generating_customized_response": "Generating customized customer response...",
        "customer_responded": "ğŸ—£ï¸ Customer responded: {reaction}",
        "customer_voice_generation_error": "âŒ Customer voice generation error: {error}",
        "button_retry_translation": "Retry Translation",
        "customer_waiting_hold": "[Customer: Please wait...]",
        "agent_hold_message": "[Agent: Call is on hold. Please click the resume button.]",
        
        # --- Video File Upload Related ---
        "video_upload_expander": "Video File Upload/Load",
        "video_sync_enable": "Enable Video Synchronization (Play with TTS)",
        "video_rag_title": "ğŸ¥ OpenAI/Gemini Based Video RAG Feature",
        "video_rag_desc": "âœ… **Current Implementation (Video RAG):**\n\n1. **LLM Text Analysis**: OpenAI/Gemini API analyzes customer's text to automatically determine emotional state and gestures.\n\n2. **Intelligent Video Selection**: Automatically selects appropriate video clips based on analysis results.\n   - Emotional State: HAPPY, ANGRY, ASKING, SAD, NEUTRAL\n   - Gestures: HAND_WAVE, NOD, SHAKE_HEAD, POINT, NONE\n\n3. **TTS Synchronized Playback**: Plays selected video and TTS-generated audio simultaneously.\n\n**Usage:**\n- Upload video files by gender (male/female) and emotional state.\n- Gesture-specific videos can also be uploaded (e.g., `male_happy_hand_wave.mp4`).\n- LLM automatically selects appropriate videos based on customer's speech content.",
        "video_gender_emotion_setting": "Video Settings by Gender and Emotional State",
        "video_gender_label": "Gender",
        "video_gender_male": "Male",
        "video_gender_female": "Female",
        "video_emotion_label": "Emotional State",
        "video_upload_label": "Video File Upload ({gender} - {emotion})",
        "video_current_selection": "ğŸ“¹ Current Selection: {gender} - {emotion}",
        "video_upload_prompt": "ğŸ’¡ Please upload the '{filename}' video file.",
        "video_save_path": "ğŸ“‚ Video Save Path:",
        "video_directory_empty": "âš ï¸ There is no file in the video directory. Please upload the file.",
        "video_directory_not_exist": "âš ï¸ Video directory does not exist: {path}",
        "video_local_path_input": "Or Enter Local File Path",
        "video_local_path_placeholder": "e.g., C:\\Users\\Admin\\Downloads\\video.mp4 or video.mp4",
        "video_current_avatar": "ğŸ“º Current Customer Avatar Video",
        "video_avatar_upload_prompt": "ğŸ’¡ Upload the '{filename}' video file to display the video.",
        "video_uploaded_files": "ğŸ“ Uploaded Video Files:",
        "video_bytes_saved": "âœ… Video bytes saved: {name} ({size} MB)",
        "video_empty_error": "âŒ Video file is empty. Please upload again.",
        "video_upload_error": "âŒ Error occurred during video upload: {error}",
        "video_playback_error": "âŒ Failed to play video.",
        "video_auto_play_info": "ğŸ’¡ This video will automatically play in '{gender} - {emotion}' state.",
        "video_preview_error": "Video preview error",
        "video_similar_gender": "Other videos of the same gender",
        "video_rename_hint": "ğŸ’¡ To use one of the videos above, rename the file or upload a new one.",
        "video_more_files": "... and {count} more",
        "avatar_status_info": "Status: {state} | Gender: {gender}",
        "customer_video_simulation": "Customer Video Simulation",
        "customer_avatar": "Customer Avatar",
        "faq_question_prefix": "Q{num}.",
        "visualization_chart": "Visualization Chart",
        "company_search_or_select": "Please search or select a company name.",

    },

    # --- â­ ì¼ë³¸ì–´ ë²„ì „ (í•œêµ­ì–´ 100% ë§¤ì¹­) ---
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«DB)",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
        "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
        "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "content_tab": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "sim_tab_chat_email": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(ãƒãƒ£ãƒƒãƒˆãƒ»ãƒ¡ãƒ¼ãƒ«)",
        "sim_tab_phone": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(é›»è©±)",
        "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "company_info_tab": "ä¼šç¤¾æƒ…å ±ãŠã‚ˆã³FAQ",
        "sim_tab_chat_email_desc": "é¡§å®¢å¯¾å¿œæ¥­å‹™ã«ãŠã„ã¦ã€ãƒãƒ£ãƒƒãƒˆã‚„ãƒ¡ãƒ¼ãƒ«ã§å®Ÿéš›ã«å•ã„åˆã‚ã›å¯¾å¿œãŒã§ãã‚‹å®Ÿæˆ¦å‘ã‘ã®ä»®æƒ³ã‚·ãƒŠãƒªã‚ªã§ã™ã€‚AIãŒå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨è‰æ¡ˆã‚’ç”Ÿæˆã—ã€é¡§å®¢ã®åå¿œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¦å®Ÿæˆ¦å‘ã‘ã®è¨“ç·´ãŒå¯èƒ½ã§ã™ã€‚",
        "sim_tab_phone_desc": "é¡§å®¢å¯¾å¿œæ¥­å‹™ã«ãŠã„ã¦ã€é›»è©±ã§å®Ÿéš›ã«å•ã„åˆã‚ã›å¯¾å¿œãŒã§ãã‚‹å®Ÿæˆ¦å‘ã‘ã®ä»®æƒ³ã‚·ãƒŠãƒªã‚ªã§ã™ã€‚éŸ³å£°éŒ²éŸ³ãŠã‚ˆã³ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ CCå­—å¹•æ©Ÿèƒ½ã‚’æä¾›ã—ã€é›»è©±é€šè©±ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦å®Ÿæˆ¦å¯¾å¿œèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚",
        "rag_tab_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡æ›¸ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹çŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚PDFã€TXTã€HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦RAGï¼ˆRetrieval-Augmented Generationï¼‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã€æ–‡æ›¸å†…å®¹ã«åŸºã¥ã„ã¦æ­£ç¢ºãªå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚",
        "content_tab_desc": "AIã‚’æ´»ç”¨ã—ã¦å€‹äººå‘ã‘ã®å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«åˆã‚ã›ã¦è¦ç‚¹ã‚µãƒãƒªãƒ¼ã€é¸æŠå¼ã‚¯ã‚¤ã‚ºã€å®Ÿè·µä¾‹ãªã©ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚",
        "lstm_tab_desc": "LSTMãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã¦å­¦ç¿’è€…ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—ã€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§å¯è¦–åŒ–ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚éå»ã®ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦å°†æ¥ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—ã€å­¦ç¿’æˆæœã‚’è¦–è¦šçš„ã«ç¢ºèªã§ãã¾ã™ã€‚",
        "company_info_tab_desc": "ä¼šç¤¾åˆ¥ã®è©³ç´°æƒ…å ±ã€äººæ°—å•†å“ã€è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€FAQã‚’æ¤œç´¢ãƒ»ç®¡ç†ã§ãã‚‹æ©Ÿèƒ½ã§ã™ã€‚ä¼šç¤¾ç´¹ä»‹ã€äººæ°—å•†å“ã€è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦–è¦šåŒ–ã—ã¦ä¸€ç›®ã§ç¢ºèªã§ãã¾ã™ã€‚",
        "voice_rec_header_desc": "éŸ³å£°éŒ²éŸ³ãŠã‚ˆã³è»¢å†™çµæœã‚’ç®¡ç†ãƒ»ä¿å­˜ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚ãƒã‚¤ã‚¯ã§éŒ²éŸ³ã™ã‚‹ã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦Whisper APIã‚’é€šã˜ã¦éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã€è»¢å†™çµæœã‚’ä¿å­˜ãƒ»ç®¡ç†ã§ãã¾ã™ã€‚",
        "more_features_label": "ãã®ä»–ã®æ©Ÿèƒ½",
        "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
        "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
        "rag_input_placeholder": "è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
        "llm_error_key": "âš ï¸ æ³¨æ„: GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "llm_error_init": "LLM åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«å¿œã˜ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
        "level_label": "é›£æ˜“åº¦",
        "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¨®é¡",
        "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
        "content_options": ["è¦ç‚¹ã‚µãƒãƒªãƒ¼", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã‚¢ã‚¤ãƒ‡ã‚¢"],
        "button_generate": "ç”Ÿæˆã™ã‚‹",
        "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "lstm_header": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "lstm_desc": "ä»®æƒ³ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨ã—ã¦é”æˆåº¦ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚",
        "lang_select": "è¨€èªé¸æŠ",
        "company_info_faq_settings": "ä¼šç¤¾åˆ¥è©³ç´°æƒ…å ±ã¨FAQ",
        "search_company": "ä¼šç¤¾åæ¤œç´¢",
        "company_info": "ä¼šç¤¾æƒ…å ±",
        "company_faq": "ã‚ˆãã‚ã‚‹è³ªå•",
        "faq_question": "è³ªå•",
        "faq_answer": "å›ç­”",
        "popular_products": "äººæ°—å•†å“",
        "trending_topics": "è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "company_details": "ä¼šç¤¾è©³ç´°æƒ…å ±",
        "no_company_found": "ã«è©²å½“ã™ã‚‹ä¼šç¤¾ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
        "no_company_selected": "ä¼šç¤¾åã‚’æ¤œç´¢ã¾ãŸã¯é¸æŠã—ã¦ãã ã•ã„ã€‚",
        "product_popularity": "å•†å“äººæ°—åº¦",
        "topic_trends": "è©±é¡Œãƒˆãƒ¬ãƒ³ãƒ‰",
        "select_company": "ä¼šç¤¾é¸æŠ",
        "faq_search": "FAQæ¤œç´¢",
        "faq_search_placeholder": "FAQæ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        "faq_search_placeholder_extended": "FAQæ¤œç´¢èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆå•†å“åã€ã‚µãƒ¼ãƒ“ã‚¹åãªã©ã‚‚æ¤œç´¢å¯èƒ½ï¼‰",
        "button_search_faq": "æ¤œç´¢",
        "company_search_placeholder": "ä¾‹: ã‚µãƒ ã‚¹ãƒ³ã€ãƒã‚¤ãƒãƒ¼ã€ã‚°ãƒ¼ã‚°ãƒ«ã€ã‚¢ãƒƒãƒ—ãƒ«ãªã©",
        "company_search_button": "æ¤œç´¢",
        "generating_company_info": "ä¼šç¤¾æƒ…å ±ã‚’ç”Ÿæˆä¸­...",
        "button_copy_answer": "å›ç­”ã‚³ãƒ”ãƒ¼",
        "button_copy_hint": "ãƒ’ãƒ³ãƒˆã‚³ãƒ”ãƒ¼",
        "button_download_answer": "å›ç­”ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        "button_download_hint": "ãƒ’ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        "copy_instruction": "ğŸ’¡ ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠã—ã¦Ctrl+Cï¼ˆMac: Cmd+Cï¼‰ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚",
        "copy_help_text": "ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠã—ã¦Ctrl+Cï¼ˆã¾ãŸã¯Cmd+Cï¼‰ã§ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚",
        "button_reset": "ãƒªã‚»ãƒƒãƒˆ",
        "answer_displayed": "å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã—ãŸã€‚ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚",
        "hint_displayed": "ãƒ’ãƒ³ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã—ãŸã€‚ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚",
        "ai_answer_generated": "AIå›ç­”ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚",
        "hint_generated": "å¯¾å¿œãƒ’ãƒ³ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚",
        "warning_enter_inquiry": "é¡§å®¢å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "customer_inquiry_review_desc": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä¸Šå¸ã«é¡§å®¢å•ã„åˆã‚ã›å†…å®¹ã‚’å†ç¢ºèªã—ã€AIå›ç­”ã¨ãƒ’ãƒ³ãƒˆã‚’ç”Ÿæˆã§ãã‚‹æ©Ÿèƒ½ã§ã™ã€‚",
        "all_companies": "ã™ã¹ã¦",
        "optional": "ä»»æ„",
        "no_faq_for_company": "{company}ã®FAQãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "related_products": "é–¢é€£å•†å“",
        "related_trending_news": "é–¢é€£è©±é¡Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "related_company_info": "é–¢é€£ä¼šç¤¾ç´¹ä»‹å†…å®¹",
        "related_faq": "é–¢é€£FAQ",
        "items": "ä»¶",
        "popularity": "äººæ°—åº¦",
        "no_faq_for_product": "è©²å½“å•†å“ã«é–¢é€£ã™ã‚‹FAQãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å•†å“æƒ…å ±ã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚",
        "generating_detail": "è©³ç´°å†…å®¹ã‚’ç”Ÿæˆä¸­...",
        "checking_additional_info": "è©³ç´°å†…å®¹: {topic}ã«é–¢ã™ã‚‹è¿½åŠ æƒ…å ±ã‚’ç¢ºèªä¸­ã§ã™ã€‚",
        "button_generate_faq": "FAQç”Ÿæˆ",
        "button_add_company": "é¡§å®¢å•ã„åˆã‚ã›å†ç¢ºèª",
        "customer_inquiry_review": "é¡§å®¢å•ã„åˆã‚ã›å†ç¢ºèª",
        "inquiry_question_label": "é¡§å®¢å•ã„åˆã‚ã›å†…å®¹",
        "inquiry_question_placeholder": "é¡§å®¢ãŒå•ã„åˆã‚ã›ãŸå†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        "inquiry_attachment_label": "ğŸ“ é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (å†™çœŸ/ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ)",
        "inquiry_attachment_help": "ç‰¹ã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«ä¸å¯ã®æ—…è¡Œå•†å“ã§ã€é£›è¡Œæ©Ÿã®é…å»¶ã€ãƒ‘ã‚¹ãƒãƒ¼ãƒˆã®å•é¡Œãªã©ã‚„ã‚€ã‚’å¾—ãªã„ç†ç”±ãŒã‚ã‚‹å ´åˆã¯ã€å¿…ãšå†™çœŸã‚„ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ·»ä»˜ã—ã¦ãã ã•ã„ã€‚",
        "inquiry_attachment_uploaded": "âœ… æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ: {filename}",
        "extracting_file_content": "ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æŠ½å‡ºä¸­...",
        "detecting_language": "è¨€èªã‚’æ¤œå‡ºä¸­...",
        "translating_content": "ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¿»è¨³ä¸­...",
        "file_translated": "âœ… ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ãŒç¿»è¨³ã•ã‚Œã¾ã—ãŸã€‚",
        "file_extraction_error": "ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}",
        "ocr_requires_manual": "ç”»åƒOCRã«ã¯Gemini APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚ç”»åƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰‹å‹•ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "ocr_error": "ç”»åƒã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}",
        "button_generate_ai_answer": "AIå›ç­”ç”Ÿæˆ",
        "button_generate_hint": "å¯¾å¿œãƒ’ãƒ³ãƒˆç”Ÿæˆ",
        "ai_answer_header": "AIæ¨å¥¨å›ç­”",
        "hint_header": "å¯¾å¿œãƒ’ãƒ³ãƒˆ",
        "generating_ai_answer": "AIå›ç­”ã‚’ç”Ÿæˆä¸­...",
        "generating_hint": "å¯¾å¿œãƒ’ãƒ³ãƒˆã‚’ç”Ÿæˆä¸­...",
        "button_edit_company": "ä¼šç¤¾æƒ…å ±ç·¨é›†",
        "button_show_company_info": "ä¼šç¤¾ç´¹ä»‹ã‚’è¦‹ã‚‹",
        "no_faq_results": "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "faq_search_results": "FAQæ¤œç´¢çµæœ",
        "add_company_name": "ä¼šç¤¾å",
        "add_company_info": "ä¼šç¤¾æƒ…å ±",
        "generate_faq_question": "è³ªå•",
        "generate_faq_answer": "å›ç­”",
        "button_save_faq": "FAQä¿å­˜",
        "button_cancel": "ã‚­ãƒ£ãƒ³ã‚»ãƒ«",
        "faq_saved_success": "FAQãŒä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚",
        "company_added_success": "ä¼šç¤¾ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸã€‚",
        "company_updated_success": "ä¼šç¤¾æƒ…å ±ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚",
        "embed_success": "{count}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã§DBæ§‹ç¯‰å®Œäº†!",
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—ï¼šã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œã€‚",
        "warning_no_files": "è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "warning_rag_not_ready": "RAGãŒæº–å‚™ã§ãã¦ã„ã¾ã›ã‚“ã€‚",
        "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "select_answer": "å›ç­”ã‚’é¸æŠã—ã¦ãã ã•ã„",
        "check_answer": "å›ç­”ã‚’ç¢ºèª",
        "next_question": "æ¬¡ã®è³ªå•",
        "correct_answer": "æ­£è§£ï¼ ğŸ‰",
        "incorrect_answer": "ä¸æ­£è§£ ğŸ˜",
        "correct_is": "æ­£è§£",
        "explanation": "è§£èª¬",
        "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
        "score": "ã‚¹ã‚³ã‚¢",
        "retake_quiz": "å†æŒ‘æˆ¦",
        "question_label": "è³ªå•",
        "correct_questions": "æ­£è§£",
        "incorrect_questions": "ä¸æ­£è§£",
        "question_result": "å•é¡Œçµæœ",
        "your_answer": "ã‚ãªãŸã®ç­”ãˆ",
        "correct_answer_label": "æ­£è§£",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨ï¼šJSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "quiz_original_response": "LLM åŸæœ¬å›ç­”",
        "firestore_loading": "RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ä¸­...",
        "firestore_no_index": "ä¿å­˜ã•ã‚ŒãŸRAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
        "db_save_complete": "(DBä¿å­˜å®Œäº†)",
        "data_analysis_progress": "è³‡æ–™åˆ†æä¸­...",
        "response_generating": "å¿œç­”ç”Ÿæˆä¸­...",
        "lstm_result_header": "é”æˆåº¦äºˆæ¸¬çµæœ",
        "lstm_score_metric": "äºˆæ¸¬é”æˆåº¦",
        "lstm_score_info": "æ¬¡ã®ã‚¹ã‚³ã‚¢äºˆæ¸¬: **{predicted_score:.1f}ì **",
        "lstm_rerun_button": "æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§å†äºˆæ¸¬",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ ---
        "toast_like": "ğŸ”¥ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ°—ã«å…¥ã£ã¦ã„ãŸã ã‘ã¾ã—ãŸï¼ (+1 ã‚«ã‚¦ãƒ³ãƒˆåæ˜ )",
        "toast_dislike": "ğŸ˜” ã‚ˆã‚Šè‰¯ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚",
        "toast_share": "ğŸŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªãƒ³ã‚¯ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚",
        "toast_copy": "âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼ã•ã‚Œã¾ã—ãŸï¼",
        "toast_more": "â„¹ï¸ ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå°åˆ·ã€PDFä¿å­˜ãªã©ï¼‰",
        "mock_pdf_save": "ğŸ“¥ PDFã§ä¿å­˜",
        "mock_word_open": "ğŸ“‘ Wordã§é–‹ã",
        "mock_print": "ğŸ–¨ å°åˆ·",
        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ë ---

        # --- Simulator ---
        "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "simulator_desc": "é›£ã—ã„é¡§å®¢å•ã„åˆã‚ã›ã«å¯¾ã™ã‚‹AIã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨è‰æ¡ˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "customer_query_label": "é¡§å®¢ã‹ã‚‰ã®å•ã„åˆã‚ã›å†…å®¹ (ãƒªãƒ³ã‚¯å¯)",
        "customer_type_label": "é¡§å®¢ã‚¿ã‚¤ãƒ—",
        "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "é›£ã—ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"],
        "button_simulate": "å¿œå¯¾ã‚¬ã‚¤ãƒ‰ç”Ÿæˆ",
        "customer_generate_response_button": "é¡§å®¢ã®è¿”ä¿¡ã‚’ç”Ÿæˆ",
        "send_closing_confirm_button": "è¿½åŠ ã®ã”è³ªå•æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡",
        "simulation_warning_query": "ãŠå•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "simulation_no_key_warning": "âš ï¸ APIã‚­ãƒ¼ä¸è¶³ã®ãŸã‚å¿œå¯¾ç”Ÿæˆä¸å¯ã€‚",
        "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
        "simulation_draft_header": "æ¨å¥¨å¿œå¯¾è‰æ¡ˆ",
        "button_listen_audio": "éŸ³å£°ã§èã",
        "tts_status_ready": "éŸ³å£°ç”Ÿæˆæº–å‚™å®Œäº†",
        "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
        "tts_status_success": "éŸ³å£°æº–å‚™å®Œäº†ï¼",
        "tts_status_error": "TTS ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "history_expander_title": "ğŸ“ éå»ã®å¯¾å¿œå±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ (æœ€æ–°10ä»¶)",
        "initial_query_sample": "ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã®eSIMãŒä½¿ãˆã¾ã›ã‚“â€¦",
        "button_mic_input": "ğŸ™ éŸ³å£°å…¥åŠ›",
        "button_mic_stop": "â¹ï¸ éŒ²éŸ³çµ‚äº†",
        "prompt_customer_end": "è¿½åŠ ã®è³ªå•ãŒãªã„ãŸã‚ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚",
        "prompt_survey": "æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ000ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚ [ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯]",
        "customer_closing_confirm": "ä»–ã®ãŠå•åˆã›ã¯ã”ã–ã„ã¾ã›ã‚“ã§ã—ã‚‡ã†ã‹ã€‚",
        "customer_positive_response": "æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
        "button_email_end_chat": "å¿œå¯¾çµ‚äº†ï¼ˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆï¼‰",
        "error_mandatory_contact": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨é›»è©±ç•ªå·ã®å…¥åŠ›ã¯å¿…é ˆã§ã™ã€‚",
        "customer_attachment_label": "ğŸ“ é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "attachment_info_llm": "[é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {filename}ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦å¯¾å¿œã—ã¦ãã ã•ã„ã€‚]",
        "button_retry_translation": "ç¿»è¨³ã‚’å†è©¦è¡Œ",
        "button_request_hint": "ğŸ’¡ å¿œå¯¾ãƒ’ãƒ³ãƒˆã‚’è¦è«‹ (AHT ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ä¸­)",
        "button_generate_draft": "ğŸ¤– AIå¿œç­”è‰æ¡ˆç”Ÿæˆ",
        "draft_generating": "AIãŒå¿œç­”è‰æ¡ˆã‚’ç”Ÿæˆä¸­ã§ã™...",
        "draft_success": "âœ… AIå¿œç­”è‰æ¡ˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚ä»¥ä¸‹ã§ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚",
        "hint_placeholder": "ãŠå•åˆã›ã®å¿œå¯¾ã«å¯¾ã™ã‚‹ãƒ’ãƒ³ãƒˆï¼š",
        "new_simulation_ready": "æ–°ã—ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã§ãã¾ã™ã€‚",
        "survey_sent_confirm": "ğŸ“¨ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚ã“ã®ãƒãƒ£ãƒƒãƒˆã¯çµ‚äº†ã—ã¾ã—ãŸã€‚",
        "agent_response_header": "âœï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”",
        "agent_response_placeholder": "é¡§å®¢ã¸è¿”ä¿¡å†…å®¹ã‚’å…¥åŠ›â€¦",
        "generating_customer_response": "é¡§å®¢ã®è¿”ä¿¡ã‚’ç”Ÿæˆä¸­...",
        "call_started_message": "é€šè©±ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸã€‚ä¸‹ã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æŒ¨æ‹¶ã‚’éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚",
        "call_on_hold_message": "é€šè©±ãŒä¿ç•™ä¸­ã§ã™ã€‚é€šè©±ã‚’å†é–‹ã—ãŸå¾Œã€éŒ²éŸ³ãŒå¯èƒ½ã§ã™ã€‚",
        "recording_complete_transcribing": "ğŸ™ï¸ éŒ²éŸ³å®Œäº†ã€‚è»¢å†™å‡¦ç†ä¸­...",
        "recording_complete_press_transcribe": "âœ… éŒ²éŸ³å®Œäº†ï¼ä»¥ä¸‹ã®è»¢å†™ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚",
        "customer_positive_solution_reaction": "ãŠå®¢æ§˜ãŒã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«è‚¯å®šçš„ã«åå¿œã—ã¾ã—ãŸã€‚è¿½åŠ ã®å•ã„åˆã‚ã›ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "transcription_empty_warning": "âš ï¸ è»¢å†™çµæœãŒç©ºã§ã™ã€‚ã‚‚ã†ä¸€åº¦éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚ï¼ˆãƒã‚¤ã‚¯å…¥åŠ›ãŒãªã„ã‹ã€ãƒŸãƒ¥ãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼‰",
        "transcription_error": "[ERROR: è»¢å†™å¤±æ•—]",
        "transcription_no_result": "âŒ è»¢å†™çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "send_response_button": "è¿”ä¿¡é€ä¿¡",
        "customer_turn_info": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”é€ä¿¡å®Œäº†ã€‚é¡§å®¢ã®åå¿œã‚’è‡ªå‹•ç”Ÿæˆä¸­ã§ã™ã€‚",
        "generating_customer_response": "é¡§å®¢ã®åå¿œã‚’ç”Ÿæˆä¸­...",
        "customer_escalation_start": "ä¸Šç´šã®æ‹…å½“è€…ã¨è©±ã—ãŸã„",
        "request_rebuttal_button": "é¡§å®¢ã®åå¿œã‚’ç”Ÿæˆ",
        "new_simulation_button": "æ–°è¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
        "history_selectbox_label": "å±¥æ­´ã‚’é¸æŠ:",
        "history_load_button": "å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€",
        "delete_history_button": "âŒ å…¨å±¥æ­´å‰Šé™¤",
        "delete_confirm_message": "ã™ã¹ã¦ã®å±¥æ­´ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ",
        "delete_confirm_yes": "ã¯ã„ã€å‰Šé™¤ã—ã¾ã™ã€‚",
        "download_history_word": "ğŸ“¥ å±¥æ­´ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Word)",
        "download_history_pptx": "ğŸ“¥ å±¥æ­´ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PPTX)",
        "download_history_pdf": "ğŸ“¥ å±¥æ­´ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PDF)",
        "download_current_session": "ğŸ“¥ ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        "delete_confirm_no": "ã„ã„ãˆã€ç¶­æŒã—ã¾ã™ã€‚",
        "delete_success": "å‰Šé™¤å®Œäº†ï¼",
        "deleting_history_progress": "å‰Šé™¤ä¸­...",
        "search_history_label": "å±¥æ­´æ¤œç´¢",
        "date_range_label": "æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼",
        "history_search_button": "ğŸ” æ¤œç´¢",
        "no_history_found": "è©²å½“ã™ã‚‹å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "customer_email_label": "é¡§å®¢ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆå¿…ä¿®ï¼‰",
        "customer_phone_label": "é¡§å®¢é€£çµ¡å…ˆ / é›»è©±ç•ªå·ï¼ˆå¿…ä¿®ï¼‰",
        "transfer_header": "è¨€èªåˆ‡ã‚Šæ›¿ãˆè¦è«‹ï¼ˆä»–ãƒãƒ¼ãƒ ã¸ï¼‰",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ è‹±èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· éŸ“å›½èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_system_msg": "ğŸ“Œ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: é¡§å®¢ã®è¦è«‹ã«ã‚ˆã‚Šã€å¯¾å¿œè¨€èªãŒ {target_lang} ãƒãƒ¼ãƒ ã¸åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã—ãŸã€‚æ–°ã—ã„æ‹…å½“è€…(AI)ãŒå¯¾å¿œã—ã¾ã™ã€‚",
        "transfer_loading": "è»¢é€ä¸­: éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç¿»è¨³ãŠã‚ˆã³ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ã„ã¾ã™ (ãŠå®¢æ§˜ã«ã¯3ã€œ10åˆ†ã®ãŠæ™‚é–“ã‚’ã„ãŸã ã„ã¦ã„ã¾ã™)",
        "transfer_summary_header": "ğŸ” è»¢é€ã•ã‚ŒãŸæ‹…å½“è€…å‘ã‘ã®è¦ç´„ (ç¿»è¨³æ¸ˆã¿)",
        "transfer_summary_intro": "ã“ã‚ŒãŒé¡§å®¢ã¨ã®éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã§ã™ã€‚ã“ã®è¦ç´„ã«åŸºã¥ã„ã¦ã‚µãƒãƒ¼ãƒˆã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚",
        "llm_translation_error": "âŒ ç¿»è¨³å¤±æ•—: LLMå¿œç­”ã‚¨ãƒ©ãƒ¼",
        "timer_metric": "çµŒéæ™‚é–“",
        "timer_info_ok": "AHT (15åˆ†åŸºæº–)",
        "timer_info_warn": "AHT (10åˆ†çµŒé)",
        "timer_info_risk": "ğŸš¨ 15åˆ†çµŒé: é«˜ã„ãƒªã‚¹ã‚¯",
        "solution_check_label": "âœ… ã“ã®å¿œç­”ã«è§£æ±ºç­–/å¯¾å¿œç­–ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚",
        "sentiment_score_label": "é¡§å®¢ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢",  # <--- ì¶”ê°€/ìˆ˜ì •
        "urgency_score_label": "ç·Šæ€¥åº¦ã‚¹ã‚³ã‚¢",
        "customer_gender_label": "é¡§å®¢æ€§åˆ¥",
        "customer_emotion_label": "é¡§å®¢æ„Ÿæƒ…çŠ¶æ…‹",
        "gender_male": "ç”·æ€§",
        "gender_female": "å¥³æ€§",
        "emotion_happy": "æ°—åˆ†è‰¯ã„é¡§å®¢",
        "emotion_dissatisfied": "ä¸æº€ãªé¡§å®¢",
        "emotion_angry": "æ€’ã£ãŸé¡§å®¢",
        "emotion_sad": "æ‚²ã—ã„/æ†‚é¬±ãªé¡§å®¢",
        "emotion_neutral": "ä¸­ç«‹",
        "similarity_chart_title": "é¡ä¼¼æ€§ã‚±ãƒ¼ã‚¹ã®æ¯”ç‡",
        "scores_comparison_title": "æ„Ÿæƒ…åŠã³æº€è¶³åº¦ã®ã‚¹ã‚³ã‚¢",
        "similarity_score_label": "é¡ä¼¼æ€§",
        "satisfaction_score_label": "æº€è¶³åº¦",
        "customer_satisfaction_score_label": "é¡§å®¢æº€è¶³åº¦ã‚¹ã‚³ã‚¢",
        "sentiment_trend_label": "æ„Ÿæƒ…ã®ã‚¹ã‚³ã‚¢ã®æ¨æ¸¬",
        "satisfaction_trend_label": "æº€è¶³åº¦ã®ã‚¹ã‚³ã‚¢ã®æ¨æ¸¬",
        "case_trends_title": "éå»ã«æ¨å®šã•ã‚ŒãŸã‚¹ã‚³ã‚¢",
        "date_label": "æ—¥ä»˜",
        "score_label": "ã‚¹ã‚³ã‚¢ (0-100)",
        "customer_characteristics_title": "é¡§å®¢ã®æ€§æ ¼",
        "language_label": "è¨€èª",
        "email_provided_label": "æä¾›ã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹",
        "phone_provided_label": "æä¾›ã•ã‚ŒãŸé›»è©±ç•ªå·",
        "region_label": "åœ°åŸŸ",
        "btn_request_phone_summary": "å±¥æ­´ã‚’è¦ç´„ã™ã‚‹",
        
        # --- ë‹¤ìš´ë¡œë“œ ë¬¸ì„œ ê´€ë ¨ í…ìŠ¤íŠ¸ ---
        "download_history_title": "é¡§å®¢å¯¾å¿œå±¥æ­´è¦ç´„",
        "download_history_number": "å±¥æ­´ #",
        "download_initial_inquiry": "åˆæœŸå•ã„åˆã‚ã›",
        "download_summary": "è¦ç´„",
        "download_main_inquiry": "ä¸»ãªå•ã„åˆã‚ã›",
        "download_key_response": "æ ¸å¿ƒå¿œç­”",
        "download_customer_characteristics": "é¡§å®¢ç‰¹æ€§",
        "download_privacy_summary": "å€‹äººæƒ…å ±è¦ç´„",
        "download_address_provided": "ä½æ‰€æä¾›",
        "download_overall_summary": "å…¨ä½“è¦ç´„",
        "download_yes": "ã¯ã„",
        "download_no": "ã„ã„ãˆ",
        "download_created_date": "ä½œæˆæ—¥",
        "download_cultural_background": "æ–‡åŒ–çš„èƒŒæ™¯",
        "download_communication_style": "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«",
        "download_region_hint": "åœ°åŸŸãƒ’ãƒ³ãƒˆ",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "é›»è©±ç™ºä¿¡",
        "button_call_outbound_to_customer": "é¡§å®¢ã¸é›»è©±ç™ºä¿¡",
        "button_call_outbound_to_provider": "ç¾åœ°æ¥­è€…ã¸é›»è©±ç™ºä¿¡",
        "call_outbound_system_msg": "ğŸ“Œ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ{target}ã¸é›»è©±ç™ºä¿¡ã‚’è©¦ã¿ã¾ã—ãŸã€‚",
        "call_outbound_simulation_header": "ğŸ“ é›»è©±ç™ºä¿¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ",
        "call_outbound_summary_header": "ğŸ“ ç¾åœ°æ¥­è€…/é¡§å®¢ã¨ã®é€šè©±è¦ç´„",
        "call_outbound_loading": "é›»è©±æ¥ç¶šã¨é€šè©±çµæœã®æ•´ç†ä¸­... (LLMã‚³ãƒ¼ãƒ«)",
        "call_target_select_label": "ç™ºä¿¡å…ˆé¸æŠ",
        "call_target_customer": "é¡§å®¢ã¸é›»è©±ç™ºä¿¡",
        "call_target_partner": "ç¾åœ°æ¥­è€…ã¸é›»è©±ç™ºä¿¡",

        # --- Voice ---
        "voice_rec_header": "éŸ³å£°è¨˜éŒ²ï¼†ç®¡ç†",
        "record_help": "éŒ²éŸ³ã™ã‚‹ã‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚",
        "uploaded_file": "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "rec_list_title": "ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ²",
        "transcribe_btn": "è»¢å†™ (Whisper)",
        "save_btn": "éŸ³å£°è¨˜éŒ²ã‚’ä¿å­˜",
        "transcribing": "éŸ³å£°ã‚’è»¢å†™ä¸­...",
        "transcript_result": "è»¢å†™çµæœ:",
        "transcript_text": "è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆ",
        "openai_missing": "OpenAI APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "whisper_client_error": "âŒ ã‚¨ãƒ©ãƒ¼: Whisper APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "whisper_auth_error": "âŒ Whisper APIèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "whisper_format_error": "âŒ ã‚¨ãƒ©ãƒ¼: ã“ã®éŸ³å£°å½¢å¼ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "whisper_success": "âœ… éŸ³å£°è»¢å†™å®Œäº†ï¼",
        "playback": "éŒ²éŸ³å†ç”Ÿ",
        "retranscribe": "å†è»¢å†™",
        "delete": "å‰Šé™¤",
        "no_records": "ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "saved_success": "ä¿å­˜ã—ã¾ã—ãŸï¼",
        "delete_confirm_rec": "ã“ã®éŸ³å£°è¨˜éŒ²ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ",
        "gcs_not_conf": "GCSãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€éŸ³å£°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "gcs_playback_fail": "éŸ³å£°ã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "gcs_no_audio": "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "error": "ã‚¨ãƒ©ãƒ¼:",
        "firestore_no_db_connect": "DBæ¥ç¶šå¤±æ•—",
        "save_history_success": "ä¿å­˜å®Œäº†ã€‚",
        "save_history_fail": "ä¿å­˜å¤±æ•—ã€‚",
        "delete_fail": "å‰Šé™¤å¤±æ•—",
        "rec_header": "éŸ³å£°å…¥åŠ›ï¼†è»¢å†™",
        "whisper_processing": "å‡¦ç†ä¸­...",
        "empty_response_warning": "å¿œç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "customer_no_more_inquiries": "ã„ã„ãˆã€çµæ§‹ã§ã™ã€‚å¤§ä¸ˆå¤«ã§ã™ã€‚æœ‰é›£ã†å¾¡åº§ã„ã¾ã—ãŸã€‚",
        "customer_has_additional_inquiries": "ã¯ã„ã€è¿½åŠ ã®å•ã„åˆã‚ã›ãŒã‚ã‚Šã¾ã™ã€‚",
        "sim_end_chat_button": "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¦å¿œå¯¾çµ‚äº†",
        "delete_mic_record": "éŒ²éŸ³ã‚’å‰Šé™¤ã™ã‚‹",
        "agent_confirmed_additional_inquiry": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¿½åŠ ã®å•ã„åˆã‚ã›ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚ãŠå®¢æ§˜ã®æœ€çµ‚å›ç­”ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚",
        "llm_key_missing_customer_response": "LLMã‚­ãƒ¼ãŒãªã„ãŸã‚ã€é¡§å®¢ã®åå¿œã‚’è‡ªå‹•ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚æ‰‹å‹•ã§ã€Œé¡§å®¢ã®è¿”ä¿¡ã‚’ç”Ÿæˆã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã‹ã€AGENT_TURNã«æˆ»ã£ã¦ãã ã•ã„ã€‚",
        "customer_response_generation_failed": "é¡§å®¢ã®å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
        "no_more_inquiries_confirmed": "âœ… ãŠå®¢æ§˜ã«è¿½åŠ ã®å•ã„åˆã‚ã›ãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚",
        "consultation_end_header": "ğŸ“‹ ç›¸è«‡çµ‚äº†",
        "click_survey_button_to_end": "ä»¥ä¸‹ã®**ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¦å¿œå¯¾çµ‚äº†**ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ç›¸è«‡ã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€‚",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "é¡§å®¢ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãªã©)",
        "attachment_placeholder": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜ã—ã¦çŠ¶æ³ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
        "attachment_status_llm": "é¡§å®¢ãŒ **{filename}** íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ìŠ¤í¬ë¦°ìƒ·ì´ë¼ê³  ê°€ì •í•˜ê³  ì‘ëŒ€ ì´ˆì•ˆê³¼ ê°€ì´ë“œë¼ì¸ì— ë°˜ì˜í•´ì£¼ì„¸ìš”. (ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {filetype})",
        "agent_attachment_label": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãªã©)",
        "agent_attachment_placeholder": "å¿œç­”ã«æ·»ä»˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
        "agent_attachment_status": "ğŸ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ **{filename}** ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¿œç­”ã«æ·»ä»˜ã—ã¾ã—ãŸã€‚(ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG embedding failed: OpenAI API Key is invalid or not setã€‚",
        "rag_embed_error_gemini": "RAG embedding failed: Gemini API Key is invalid or not setã€‚",
        "rag_embed_error_nvidia": "RAG embedding failed: NVIDIA API Key is invalid or not setã€‚",
        "rag_embed_error_none": "RAG embedding failed: All required keys (OpenAI, Gemini, NVIDIA) are invalid or not setã€‚ Please configure a keyã€‚",

        # --- é›»è©±æ©Ÿèƒ½é–¢é€£è¿½åŠ  ---
        "phone_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(é›»è©±)",
        "call_status_waiting": "ç€ä¿¡å¾…ã¡...",
        "call_status_ringing": "ç€ä¿¡ä¸­: {number}",
        "button_answer": "ğŸ“ é›»è©±ã«å‡ºã‚‹",
        "button_hangup": "ğŸ”´ é›»è©±ã‚’åˆ‡ã‚‹",
        "button_hold": "â¸ï¸ ä¿ç•™ (ãƒã‚¤ã‚ºé®æ–­)",
        "button_resume": "â–¶ï¸ é€šè©±å†é–‹",
        "hold_status": "ä¿ç•™ä¸­ (ç´¯è¨ˆä¿ç•™æ™‚é–“: {duration})",
        "cc_live_transcript": "ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ CCå­—å¹• / è»¢å†™",
        "mic_input_status": "ğŸ™ï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®éŸ³å£°å…¥åŠ›",
        "customer_audio_playback": "ğŸ—£ï¸ é¡§å®¢ã®éŸ³å£°å†ç”Ÿ",
        "agent_response_prompt": "é¡§å®¢ã¸ã®å¿œç­”ã‚’éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚",
        "agent_response_stop_and_send": "â¹ï¸éŒ²éŸ³ã‚’çµ‚äº†ã—ã¦ã€é¡§å®¢ã¸è»¢é€ã™ã‚‹",
        "call_end_message": "é€šè©±ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚AHTã¨å±¥æ­´ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "call_query_placeholder": "é¡§å®¢ã‹ã‚‰ã®æœ€åˆã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "call_number_placeholder": "+81 90-xxxx-xxxx (ä»®æƒ³ç•ªå·)",
        "website_url_label": "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®ã‚¦ã‚§ãƒ–ã‚¢ãƒ‰ãƒ¬ã‚¹ (ä»»æ„)",
        "website_url_placeholder": "https://example.com (ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒã‚ã‚‹å ´åˆã¯å…¥åŠ›ã—ã¦ãã ã•ã„)",
        "call_summary_header": "AI é€šè©±è¦ç´„",
        "customer_audio_header": "é¡§å®¢ã®æœ€åˆã®å•ã„åˆã‚ã› (éŸ³å£°)",
        "aht_not_recorded": "âš ï¸ é€šè©±é–‹å§‹æ™‚é–“ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€AHTã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã€‚",
        "no_audio_record": "é¡§å®¢ã®æœ€åˆã®éŸ³å£°è¨˜éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "customer_query_playing": "ğŸ”Š é¡§å®¢ã®å•ã„åˆã‚ã›ã‚’å†ç”Ÿä¸­ã§ã™ã€‚",
        "query_content_label": "ğŸ“ å•ã„åˆã‚ã›å†…å®¹:",
        "auto_play_failed": "è‡ªå‹•å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ: {error}ã€‚æ‰‹å‹•ã§å†ç”Ÿã—ã¦ãã ã•ã„ã€‚",
        "generating_customized_response": "é¡§å®¢å‘ã‘ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸåå¿œã‚’ç”Ÿæˆä¸­...",
        "customer_responded": "ğŸ—£ï¸ é¡§å®¢ãŒå¿œç­”ã—ã¾ã—ãŸ: {reaction}",
        "customer_voice_generation_error": "âŒ é¡§å®¢ã®éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error}",
        "button_retry_translation": "ç¿»è¨³ã‚’å†è©¦è¡Œ",
        "customer_waiting_hold": "[é¡§å®¢: ãŠå¾…ã¡ãã ã•ã„...]",
        "agent_hold_message": "[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: é€šè©±ãŒä¿ç•™ä¸­ã§ã™ã€‚é€šè©±å†é–‹ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚]",
        
        # --- ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢é€£ ---
        "video_upload_expander": "ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰/ãƒ­ãƒ¼ãƒ‰",
        "video_sync_enable": "ãƒ“ãƒ‡ã‚ªåŒæœŸã‚’æœ‰åŠ¹åŒ– (TTSã¨ä¸€ç·’ã«å†ç”Ÿ)",
        "video_rag_title": "ğŸ¥ OpenAI/Geminiãƒ™ãƒ¼ã‚¹ã®ãƒ“ãƒ‡ã‚ªRAGæ©Ÿèƒ½",
        "video_rag_desc": "âœ… **ç¾åœ¨ã®å®Ÿè£…æ–¹å¼ (ãƒ“ãƒ‡ã‚ªRAG):**\n\n1. **LLMãƒ†ã‚­ã‚¹ãƒˆåˆ†æ**: OpenAI/Gemini APIãŒé¡§å®¢ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€æ„Ÿæƒ…çŠ¶æ…‹ã¨ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’è‡ªå‹•åˆ¤å®šã—ã¾ã™ã€‚\n\n2. **ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ“ãƒ‡ã‚ªé¸æŠ**: åˆ†æçµæœã«åŸºã¥ã„ã¦é©åˆ‡ãªãƒ“ãƒ‡ã‚ªã‚¯ãƒªãƒƒãƒ—ã‚’è‡ªå‹•é¸æŠã—ã¾ã™ã€‚\n   - æ„Ÿæƒ…çŠ¶æ…‹: HAPPY, ANGRY, ASKING, SAD, NEUTRAL\n   - ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼: HAND_WAVE, NOD, SHAKE_HEAD, POINT, NONE\n\n3. **TTSåŒæœŸå†ç”Ÿ**: é¸æŠã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªã¨TTSã§ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚’åŒæ™‚ã«å†ç”Ÿã—ã¾ã™ã€‚\n\n**ä½¿ç”¨æ–¹æ³•:**\n- æ€§åˆ¥(ç”·æ€§/å¥³æ€§)ã¨æ„Ÿæƒ…çŠ¶æ…‹åˆ¥ã«ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n- ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥ã®ãƒ“ãƒ‡ã‚ªã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã§ã™ (ä¾‹: `male_happy_hand_wave.mp4`)ã€‚\n- é¡§å®¢ãŒè©±ã™å†…å®¹ã«å¿œã˜ã¦LLMãŒè‡ªå‹•çš„ã«é©åˆ‡ãªãƒ“ãƒ‡ã‚ªã‚’é¸æŠã—ã¾ã™ã€‚",
        "video_gender_emotion_setting": "æ€§åˆ¥ãŠã‚ˆã³æ„Ÿæƒ…çŠ¶æ…‹åˆ¥ãƒ“ãƒ‡ã‚ªè¨­å®š",
        "video_gender_label": "æ€§åˆ¥",
        "video_gender_male": "ç”·æ€§",
        "video_gender_female": "å¥³æ€§",
        "video_emotion_label": "æ„Ÿæƒ…çŠ¶æ…‹",
        "video_upload_label": "ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ({gender} - {emotion})",
        "video_current_selection": "ğŸ“¹ ç¾åœ¨ã®é¸æŠ: {gender} - {emotion}",
        "video_upload_prompt": "ğŸ’¡ '{filename}' ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "video_save_path": "ğŸ“‚ ãƒ“ãƒ‡ã‚ªä¿å­˜ãƒ‘ã‚¹:",
        "video_directory_empty": "âš ï¸ ãƒ“ãƒ‡ã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "video_directory_not_exist": "âš ï¸ ãƒ“ãƒ‡ã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {path}",
        "video_local_path_input": "ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å…¥åŠ›",
        "video_local_path_placeholder": "ä¾‹: C:\\Users\\Admin\\Downloads\\video.mp4 ã¾ãŸã¯ video.mp4",
        "video_current_avatar": "ğŸ“º ç¾åœ¨ã®é¡§å®¢ã‚¢ãƒã‚¿ãƒ¼æ˜ åƒ",
        "video_avatar_upload_prompt": "ğŸ’¡ '{filename}' ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨æ˜ åƒãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚",
        "video_uploaded_files": "ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«:",
        "video_bytes_saved": "âœ… ãƒ“ãƒ‡ã‚ªãƒã‚¤ãƒˆä¿å­˜å®Œäº†: {name} ({size} MB)",
        "video_empty_error": "âŒ ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚å†åº¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "video_upload_error": "âŒ ãƒ“ãƒ‡ã‚ªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}",
        "video_playback_error": "âŒ ãƒ“ãƒ‡ã‚ªå†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "video_auto_play_info": "ğŸ’¡ ã“ã®ãƒ“ãƒ‡ã‚ªã¯ '{gender} - {emotion}' çŠ¶æ…‹ã§è‡ªå‹•çš„ã«å†ç”Ÿã•ã‚Œã¾ã™ã€‚",
        "video_preview_error": "ãƒ“ãƒ‡ã‚ªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼",
        "video_similar_gender": "åŒã˜æ€§åˆ¥ã®ä»–ã®ãƒ“ãƒ‡ã‚ª",
        "video_rename_hint": "ğŸ’¡ ä¸Šè¨˜ã®ãƒ“ãƒ‡ã‚ªã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´ã™ã‚‹ã‹ã€æ–°ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "video_more_files": "... ä»– {count}ä»¶",
        "avatar_status_info": "çŠ¶æ…‹: {state} | æ€§åˆ¥: {gender}",
        "customer_video_simulation": "é¡§å®¢æ˜ åƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
        "customer_avatar": "é¡§å®¢ã‚¢ãƒã‚¿ãƒ¼",
        "faq_question_prefix": "Q{num}.",
        "visualization_chart": "å¯è¦–åŒ–ãƒãƒ£ãƒ¼ãƒˆ",
        "company_search_or_select": "ä¼šç¤¾åã‚’æ¤œç´¢ã¾ãŸã¯é¸æŠã—ã¦ãã ã•ã„ã€‚",
    }
}

# ========================================
# 1-1. Session State ì´ˆê¸°í™” (ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì¶”ê°€)
# ========================================
# â­ ì‚¬ì´ë“œë°” ë²„íŠ¼ì€ ì‚¬ì´ë“œë°” ë¸”ë¡ ì•ˆìœ¼ë¡œ ì´ë™í•´ì•¼ í•¨
# ì—¬ê¸°ì„œëŠ” ì„¸ì…˜ ìƒíƒœë§Œ ì´ˆê¸°í™”

if "language" not in st.session_state:
    st.session_state.language = DEFAULT_LANG
if "is_llm_ready" not in st.session_state:
    st.session_state.is_llm_ready = False
if "llm_init_error_msg" not in st.session_state:
    st.session_state.llm_init_error_msg = ""
if "uploaded_files_state" not in st.session_state:
    st.session_state.uploaded_files_state = None
if "is_rag_ready" not in st.session_state:
    st.session_state.is_rag_ready = False
if "rag_vectorstore" not in st.session_state:
    st.session_state.rag_vectorstore = None
if "rag_messages" not in st.session_state:
    st.session_state.rag_messages = []
if "agent_input" not in st.session_state:
    st.session_state.agent_input = ""
if "last_audio" not in st.session_state:
    st.session_state.last_audio = None
if "simulator_messages" not in st.session_state:
    st.session_state.simulator_messages = []
if "simulator_memory" not in st.session_state:
    st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history")
if "simulator_chain" not in st.session_state:
    st.session_state.simulator_chain = None
if "initial_advice_provided" not in st.session_state:
    st.session_state.initial_advice_provided = False
if "is_chat_ended" not in st.session_state:
    st.session_state.is_chat_ended = False
if "show_delete_confirm" not in st.session_state:
    st.session_state.show_delete_confirm = False
if "customer_query_text_area" not in st.session_state:
    st.session_state.customer_query_text_area = ""
if "agent_response_area_text" not in st.session_state:
    st.session_state.agent_response_area_text = ""
if "reset_agent_response_area" not in st.session_state:
    st.session_state.reset_agent_response_area = False
if "last_transcript" not in st.session_state:
    st.session_state.last_transcript = ""
if "sim_audio_bytes" not in st.session_state:
    st.session_state.sim_audio_bytes = None
if "chat_state" not in st.session_state:
    st.session_state.chat_state = "idle"
    # idle â†’ initial_customer â†’ supervisor_advice â†’ agent_turn â†’ customer_turn â†’ closing
if "openai_client" not in st.session_state:
    st.session_state.openai_client = None
if "openai_init_msg" not in st.session_state:
    st.session_state.openai_init_msg = ""
if "sim_stage" not in st.session_state:
    st.session_state.sim_stage = "WAIT_FIRST_QUERY"
    # WAIT_FIRST_QUERY (ì´ˆê¸° ë¬¸ì˜ ì…ë ¥)
    # AGENT_TURN (ì—ì´ì „íŠ¸ ì‘ë‹µ ì…ë ¥)
    # CUSTOMER_TURN (ê³ ê° ë°˜ì‘ ìƒì„± ìš”ì²­)
    # WAIT_CLOSING_CONFIRMATION_FROM_AGENT (ê³ ê°ì´ ê°ì‚¬, ì—ì´ì „íŠ¸ê°€ ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸° ëŒ€ê¸°)
    # WAIT_CUSTOMER_CLOSING_RESPONSE (ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ë³´ëƒ„, ê³ ê°ì˜ ë§ˆì§€ë§‰ ì‘ë‹µ ëŒ€ê¸°)
    # FINAL_CLOSING_ACTION (ìµœì¢… ì¢…ë£Œ ë²„íŠ¼ ëŒ€ê¸°)
    # CLOSING (ì±„íŒ… ì¢…ë£Œ)
    # â­ ì¶”ê°€: OUTBOUND_CALL_IN_PROGRESS (ì „í™” ë°œì‹  ì§„í–‰ ì¤‘)
if "start_time" not in st.session_state:  # AHT íƒ€ì´ë¨¸ ì‹œì‘ ì‹œê°„
    st.session_state.start_time = None
if "is_solution_provided" not in st.session_state:  # ì†”ë£¨ì…˜ ì œê³µ ì—¬ë¶€ í”Œë˜ê·¸
    st.session_state.is_solution_provided = False
if "transfer_summary_text" not in st.session_state:  # ì´ê´€ ì‹œ ë²ˆì—­ëœ ìš”ì•½
    st.session_state.transfer_summary_text = ""
if "translation_success" not in st.session_state:  # ë²ˆì—­ ì„±ê³µ ì—¬ë¶€ ì¶”ì 
    st.session_state.translation_success = True
if "language_transfer_requested" not in st.session_state:  # ê³ ê°ì˜ ì–¸ì–´ ì´ê´€ ìš”ì²­ ì—¬ë¶€
    st.session_state.language_transfer_requested = False
if "customer_attachment_file" not in st.session_state:  # ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´
    st.session_state.customer_attachment_file = None
if "language_at_transfer" not in st.session_state:  # í˜„ì¬ ì–¸ì–´ì™€ ë¹„êµë¥¼ ìœ„í•œ ë³€ìˆ˜
    st.session_state.language_at_transfer = st.session_state.language
if "language_at_transfer_start" not in st.session_state:  # ë²ˆì—­ ì¬ì‹œë„ë¥¼ ìœ„í•œ ì›ë³¸ ì–¸ì–´
    st.session_state.language_at_transfer_start = st.session_state.language
if "transfer_retry_count" not in st.session_state:
    st.session_state.transfer_retry_count = 0
if "customer_type_sim_select" not in st.session_state:  # FIX: Attribute Error í•´ê²°
    # LANGì´ ì •ì˜ë˜ê¸° ì „ì´ë¯€ë¡œ ê¸°ë³¸ê°’ì„ ì§ì ‘ ì„¤ì •
    default_customer_type = "ê¹Œë‹¤ë¡œìš´ ê³ ê°"  # í•œêµ­ì–´ ê¸°ë³¸ê°’
    if st.session_state.language == "en":
        default_customer_type = "Difficult Customer"
    elif st.session_state.language == "ja":
        default_customer_type = "é›£ã—ã„é¡§å®¢"
    st.session_state.customer_type_sim_select = default_customer_type
if "customer_email" not in st.session_state:  # FIX: customer_email ì´ˆê¸°í™”
    st.session_state.customer_email = ""
if "customer_phone" not in st.session_state:  # FIX: customer_phone ì´ˆê¸°í™”
    st.session_state.customer_phone = ""
if "agent_response_input_box_widget" not in st.session_state:  # FIX: customer_phone ì´ˆê¸°í™”
    st.session_state.agent_response_input_box_widget = ""
if "sim_instance_id" not in st.session_state:  # FIX: DuplicateWidgetID ë°©ì§€ìš© ì¸ìŠ¤í„´ìŠ¤ ID ì´ˆê¸°í™”
    st.session_state.sim_instance_id = str(uuid.uuid4())
if "sim_attachment_context_for_llm" not in st.session_state:
    st.session_state.sim_attachment_context_for_llm = ""
if "realtime_hint_text" not in st.session_state:
    st.session_state.realtime_hint_text = ""
# â­ ì¶”ê°€: ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ
if "sim_call_outbound_summary" not in st.session_state:
    st.session_state.sim_call_outbound_summary = ""
if "sim_call_outbound_target" not in st.session_state:
    st.session_state.sim_call_outbound_target = None
# ----------------------------------------------------------------------
# â­ ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ìƒíƒœ ì¶”ê°€
if "call_sim_stage" not in st.session_state:
    st.session_state.call_sim_stage = "WAITING_CALL"  # WAITING_CALL, RINGING, IN_CALL, CALL_ENDED
if "call_sim_mode" not in st.session_state:
    st.session_state.call_sim_mode = "INBOUND"  # INBOUND or OUTBOUND
if "incoming_phone_number" not in st.session_state:
    st.session_state.incoming_phone_number = "+82 10-1234-5678"
if "is_on_hold" not in st.session_state:
    st.session_state.is_on_hold = False
if "hold_start_time" not in st.session_state:
    st.session_state.hold_start_time = None
if "total_hold_duration" not in st.session_state:
    st.session_state.total_hold_duration = timedelta(0)
if "current_customer_audio_text" not in st.session_state:
    st.session_state.current_customer_audio_text = ""
if "current_agent_audio_text" not in st.session_state:
    st.session_state.current_agent_audio_text = ""
if "agent_response_input_box_widget_call" not in st.session_state:  # ì „í™” íƒ­ ì „ìš© ì…ë ¥ì°½
    st.session_state.agent_response_input_box_widget_call = ""
if "call_initial_query" not in st.session_state:  # ì „í™” íƒ­ ì „ìš© ì´ˆê¸° ë¬¸ì˜
    st.session_state.call_initial_query = ""
if "call_website_url" not in st.session_state:  # ì „í™” íƒ­ ì „ìš© í™ˆí˜ì´ì§€ ì£¼ì†Œ
    st.session_state.call_website_url = ""
# â­ ì¶”ê°€: í†µí™” ìš”ì•½ ë° ì´ˆê¸° ê³ ê° ìŒì„± ì €ì¥ì†Œ
if "call_summary_text" not in st.session_state:
    st.session_state.call_summary_text = ""
if "customer_initial_audio_bytes" not in st.session_state:  # ê³ ê°ì˜ ì²« ìŒì„± (TTS ê²°ê³¼) ì €ì¥
    st.session_state.customer_initial_audio_bytes = None
if "supervisor_policy_context" not in st.session_state:
    # Supervisorê°€ ì—…ë¡œë“œí•œ ì˜ˆì™¸ ì •ì±… í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    st.session_state.supervisor_policy_context = ""
if "agent_policy_attachment_content" not in st.session_state:
    # ì—ì´ì „íŠ¸ê°€ ì—…ë¡œë“œí•œ ì •ì±… íŒŒì¼ ê°ì²´(ë˜ëŠ” ë‚´ìš©)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    st.session_state.agent_policy_attachment_content = ""
if "customer_attachment_b64" not in st.session_state:
    st.session_state.customer_attachment_b64 = ""
if "customer_history_summary" not in st.session_state:
    st.session_state.customer_history_summary = ""
if "customer_avatar" not in st.session_state:
    st.session_state.customer_avatar = {
        "gender": "male",  # ê¸°ë³¸ê°’
        "state": "NEUTRAL",  # ê¸°ë³¸ ì•„ë°”íƒ€ ìƒíƒœ
    }
# â­ ì¶”ê°€: ë¹„ë””ì˜¤ ë™ê¸°í™” ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ
if "current_customer_video" not in st.session_state:
    st.session_state.current_customer_video = None  # í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ê³ ê° ë¹„ë””ì˜¤ ê²½ë¡œ
if "current_customer_video_bytes" not in st.session_state:
    st.session_state.current_customer_video_bytes = None  # í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ê³ ê° ë¹„ë””ì˜¤ ë°”ì´íŠ¸
if "is_video_sync_enabled" not in st.session_state:
    st.session_state.is_video_sync_enabled = True  # ë¹„ë””ì˜¤ ë™ê¸°í™” í™œì„±í™” ì—¬ë¶€
if "video_male_neutral" not in st.session_state:
    st.session_state.video_male_neutral = None  # ë‚¨ì ì¤‘ë¦½ ë¹„ë””ì˜¤ ê²½ë¡œ
if "video_male_happy" not in st.session_state:
    st.session_state.video_male_happy = None
if "video_male_angry" not in st.session_state:
    st.session_state.video_male_angry = None
if "video_male_asking" not in st.session_state:
    st.session_state.video_male_asking = None
if "video_male_sad" not in st.session_state:
    st.session_state.video_male_sad = None
if "video_female_neutral" not in st.session_state:
    st.session_state.video_female_neutral = None  # ì—¬ì ì¤‘ë¦½ ë¹„ë””ì˜¤ ê²½ë¡œ
if "video_female_happy" not in st.session_state:
    st.session_state.video_female_happy = None
if "video_female_angry" not in st.session_state:
    st.session_state.video_female_angry = None
if "video_female_asking" not in st.session_state:
    st.session_state.video_female_asking = None
if "video_female_sad" not in st.session_state:
    st.session_state.video_female_sad = None
# â­ ì¶”ê°€: ì „ì‚¬í•  ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ì„ì‹œ ì €ì¥ì†Œ
if "bytes_to_process" not in st.session_state:
    st.session_state.bytes_to_process = None

# ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
current_lang = st.session_state.get("language", "ko")
if current_lang not in ["ko", "en", "ja"]:
    current_lang = "ko"
L = LANG.get(current_lang, LANG["ko"])

# â­ 2-A. Gemini í‚¤ ì´ˆê¸°í™” (ì˜ëª»ëœ í‚¤ ì”ì¡´ ë°©ì§€)
if "user_gemini_key" in st.session_state and st.session_state["user_gemini_key"].startswith("AIza"):
    pass

# ========================================
# 0. ë©€í‹° ëª¨ë¸ API Key ì•ˆì „ êµ¬ì¡° (Secrets + Env Varë§Œ ì‚¬ìš©)
# ========================================

# 1) ì§€ì›í•˜ëŠ” API ëª©ë¡ ì •ì˜
SUPPORTED_APIS = {
    "openai": {
        "label": "OpenAI API Key",
        "secret_key": "OPENAI_API_KEY",
        "session_key": "user_openai_key",
        "placeholder": "sk-proj-**************************",
    },
    "gemini": {
        "label": "Google Gemini API Key",
        "secret_key": "GEMINI_API_KEY",
        "session_key": "user_gemini_key",
        "placeholder": "AIza***********************************",
    },
    "hyperclova": {
        "label": "Hyperclova API Key",
        "secret_key": "HYPERCLOVA_API_KEY",
        "session_key": "user_hyperclova_key",
        "placeholder": "hyperclova-**************************",
    },
    "nvidia": {
        "label": "NVIDIA NIM API Key",
        "secret_key": "NVIDIA_API_KEY",
        "session_key": "user_nvidia_key",
        "placeholder": "nvapi-**************************",
    },
    "claude": {
        "label": "Anthropic Claude API Key",
        "secret_key": "CLAUDE_API_KEY",
        "session_key": "user_claude_key",
        "placeholder": "sk-ant-api-**************************",
    },
    "groq": {
        "label": "Groq API Key",
        "secret_key": "GROQ_API_KEY",
        "session_key": "user_groq_key",
        "placeholder": "gsk_**************************",
    },
    "hyperclova": {
        "label": "Hyperclova API Key",
        "secret_key": "HYPERCLOVA_API_KEY",
        "session_key": "user_hyperclova_key",
        "placeholder": "hyperclova-**************************",
    },
}

# 2) ì„¸ì…˜ ì´ˆê¸°í™”
for api, cfg in SUPPORTED_APIS.items():
    if cfg["session_key"] not in st.session_state:
        st.session_state[cfg["session_key"]] = ""

if "selected_llm" not in st.session_state:
    st.session_state.selected_llm = "openai_gpt4"


def get_api_key(api):
    cfg = SUPPORTED_APIS[api]

    # â­ 1. Streamlit Secrets (.streamlit/secrets.toml) - ìµœìš°ì„ 
    try:
        if hasattr(st, "secrets") and cfg["secret_key"] in st.secrets:
            return st.secrets[cfg["secret_key"]]
    except Exception:
        pass

    # 2. Environment Variable (os.environ)
    env_key = os.environ.get(cfg["secret_key"])
    if env_key:
        return env_key

    # 3. User Input (Session State - ì œê±°ë¨)
    user_key = st.session_state.get(cfg["session_key"], "")
    if user_key:
        return user_key

    return ""


# ========================================
# 1. Sidebar UI: API Key ì…ë ¥ ì œê±°
# ========================================
# API Key ì…ë ¥ UIëŠ” ì œê±°í•˜ê³ , í™˜ê²½ë³€ìˆ˜ì™€ Streamlit Secretsë§Œ ì‚¬ìš©í•˜ë„ë¡ í•¨.


# ========================================
# 2. LLM í´ë¼ì´ì–¸íŠ¸ ë¼ìš°íŒ… & ì‹¤í–‰
# ========================================
def get_llm_client():
    """ì„ íƒëœ ëª¨ë¸ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ + ëª¨ë¸ì½”ë“œ ë°˜í™˜"""
    model_key = st.session_state.get("selected_llm", "openai_gpt4")

    # --- OpenAI ---
    if model_key.startswith("openai"):
        key = get_api_key("openai")
        if not key: return None, None
        try:
            client = OpenAI(api_key=key)
            model_name = "gpt-4o" if model_key == "openai_gpt4" else "gpt-3.5-turbo"
            return client, ("openai", model_name)
        except Exception:
            return None, None

    # --- Gemini ---
    if model_key.startswith("gemini"):
        key = get_api_key("gemini")
        if not key: return None, None
        try:
            genai.configure(api_key=key)
            model_name = "gemini-2.5-pro" if model_key == "gemini_pro" else "gemini-2.5-flash"
            return genai, ("gemini", model_name)
        except Exception:
            return None, None

    # --- Claude ---
    if model_key.startswith("claude"):
        key = get_api_key("claude")
        if not key: return None, None
        try:
            client = Anthropic(api_key=key)
            model_name = "claude-3-5-sonnet-latest"
            return client, ("claude", model_name)
        except Exception:
            return None, None

    # --- Groq ---
    if model_key.startswith("groq"):
        from groq import Groq
        key = get_api_key("groq")
        if not key: return None, None
        try:
            client = Groq(api_key=key)
            model_name = (
                "llama3-70b-8192"
                if "llama3" in model_key
                else "mixtral-8x7b-32768"
            )
            return client, ("groq", model_name)
        except Exception:
            return None, None

    return None, None


def run_llm(prompt: str) -> str:
    """ì„ íƒëœ LLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (Gemini ìš°ì„ ìˆœìœ„ ë³€ê²½ ì ìš©)"""
    client, info = get_llm_client()

    # Note: infoëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒëœ ì£¼ë ¥ ëª¨ë¸ì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    provider, model_name = info if info else (None, None)

    # Fallback ìˆœì„œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. (Gemini ìš°ì„ )
    llm_attempts = []

    # 1. Geminië¥¼ ìµœìš°ì„  Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    gemini_key = get_api_key("gemini")
    if gemini_key:
        llm_attempts.append(("gemini", gemini_key, "gemini-2.5-pro" if "pro" in model_name else "gemini-2.5-flash"))

    # 2. OpenAIë¥¼ 2ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    openai_key = get_api_key("openai")
    if openai_key:
        llm_attempts.append(("openai", openai_key, "gpt-4o" if "4" in model_name else "gpt-3.5-turbo"))

    # 3. Claudeë¥¼ 3ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    claude_key = get_api_key("claude")
    if claude_key:
        llm_attempts.append(("claude", claude_key, "claude-3-5-sonnet-latest"))

    # 4. Groqë¥¼ 4ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    groq_key = get_api_key("groq")
    if groq_key:
        groq_model = "llama3-70b-8192" if "llama3" in model_name else "mixtral-8x7b-32768"
        llm_attempts.append(("groq", groq_key, groq_model))

    # â­ ìˆœì„œ ì¡°ì •: ì£¼ë ¥ ëª¨ë¸(ì‚¬ìš©ìê°€ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•œ ëª¨ë¸)ì„ ê°€ì¥ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.
    # ë§Œì•½ ì£¼ë ¥ ëª¨ë¸ì´ Fallback ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, ê·¸ ëª¨ë¸ì„ ì²« ìˆœì„œë¡œ ì˜¬ë¦½ë‹ˆë‹¤.
    if provider and provider in [attempt[0] for attempt in llm_attempts]:
        # ì£¼ë ¥ ëª¨ë¸ì„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ì•„ ì œê±°
        primary_attempt = next((attempt for attempt in llm_attempts if attempt[0] == provider), None)
        if primary_attempt:
            llm_attempts.remove(primary_attempt)
            # ì£¼ë ¥ ëª¨ë¸ì´ Geminië‚˜ OpenAIê°€ ì•„ë‹ˆë¼ë©´, Fallback ìˆœì„œì™€ ê´€ê³„ì—†ì´ ê°€ì¥ ë¨¼ì € ì‹œë„í•˜ë„ë¡ ì‚½ì…
            llm_attempts.insert(0, primary_attempt)

    # LLM ìˆœì°¨ ì‹¤í–‰
    for provider, key, model in llm_attempts:
        if not key: continue

        try:
            if provider == "gemini":
                genai.configure(api_key=key)
                gen_model = genai.GenerativeModel(model)
                resp = gen_model.generate_content(prompt)
                return resp.text

            elif provider == "openai":
                o_client = OpenAI(api_key=key)
                resp = o_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.choices[0].message.content

            elif provider == "claude":
                c_client = Anthropic(api_key=key)
                resp = c_client.messages.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.content[0].text

            elif provider == "groq":
                from groq import Groq
                g_client = Groq(api_key=key)
                resp = g_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.choices[0].message.content

        except Exception as e:
            # í•´ë‹¹ APIê°€ ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ APIë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
            print(f"LLM {provider} ({model}) failed: {e}")
            continue

    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ
    return "âŒ ëª¨ë“  LLM API í‚¤ê°€ ì‘ë™í•˜ì§€ ì•Šê±°ë‚˜ í• ë‹¹ëŸ‰ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤."


# ========================================
# 2-A. Whisper / TTS ìš© OpenAI Client ë³„ë„ë¡œ ì´ˆê¸°í™”
# ========================================

def init_openai_audio_client():
    key = get_api_key("openai")
    if not key:
        return None
    try:
        return OpenAI(api_key=key)
    except:
        return None


# â­ ìµœì í™”: LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìºì‹± (ë§¤ë²ˆ ì¬ìƒì„±í•˜ì§€ ì•Šë„ë¡)
# OpenAI í´ë¼ì´ì–¸íŠ¸ ìºì‹±
# â­ ìˆ˜ì •: ì´ˆê¸°í™” ì‹œ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ try-except ì¶”ê°€
if "openai_client" not in st.session_state or st.session_state.openai_client is None:
    try:
        st.session_state.openai_client = init_openai_audio_client()
    except Exception as e:
        st.session_state.openai_client = None
        print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

# LLM ì¤€ë¹„ ìƒíƒœ ìºì‹± (API í‚¤ ë³€ê²½ ì‹œì—ë§Œ ì¬í™•ì¸)
# â­ ìˆ˜ì •: ì´ˆê¸°í™” ì‹œ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ try-except ì¶”ê°€
if "is_llm_ready" not in st.session_state or "llm_ready_checked" not in st.session_state:
    try:
        probe_client, _ = get_llm_client()
        st.session_state.is_llm_ready = probe_client is not None
    except Exception as e:
        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ì•±ì´ ê³„ì† ì‹¤í–‰ë˜ë„ë¡ Falseë¡œ ì„¤ì •
        st.session_state.is_llm_ready = False
        print(f"LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    st.session_state.llm_ready_checked = True

# API í‚¤ ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ì²´í¬
current_api_keys_hash = hashlib.md5(
    f"{get_api_key('openai')}{get_api_key('gemini')}{get_api_key('claude')}{get_api_key('groq')}".encode()
).hexdigest()

if "api_keys_hash" not in st.session_state:
    st.session_state.api_keys_hash = current_api_keys_hash
elif st.session_state.api_keys_hash != current_api_keys_hash:
    # API í‚¤ê°€ ë³€ê²½ëœ ê²½ìš°ë§Œ ì¬í™•ì¸
    # â­ ìˆ˜ì •: ì´ˆê¸°í™” ì‹œ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ try-except ì¶”ê°€
    try:
        probe_client, _ = get_llm_client()
        st.session_state.is_llm_ready = probe_client is not None
    except Exception as e:
        st.session_state.is_llm_ready = False
        print(f"LLM ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    st.session_state.api_keys_hash = current_api_keys_hash
    # OpenAI í´ë¼ì´ì–¸íŠ¸ë„ ì¬ì´ˆê¸°í™”
    try:
        st.session_state.openai_client = init_openai_audio_client()
    except Exception as e:
        st.session_state.openai_client = None
        print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

if st.session_state.openai_client:
    # í‚¤ë¥¼ ì°¾ì•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ëŠ” ìƒì„±ë˜ì—ˆìœ¼ë‚˜, ì‹¤ì œ ì¸ì¦ì€ API í˜¸ì¶œ ì‹œ ì´ë£¨ì–´ì§ (401 ì˜¤ë¥˜ëŠ” ì—¬ê¸°ì„œ ë°œìƒ)
    st.session_state.openai_init_msg = "âœ… OpenAI TTS/Whisper í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ (Key í™•ì¸ë¨)"
else:
    # í‚¤ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
    st.session_state.openai_init_msg = L["openai_missing"]

if not st.session_state.is_llm_ready:
    st.session_state.llm_init_error_msg = L["simulation_no_key_warning"]
else:
    st.session_state.llm_init_error_msg = ""


# ----------------------------------------
# LLM ë²ˆì—­ í•¨ìˆ˜ (Gemini í´ë¼ì´ì–¸íŠ¸ ì˜ì¡´ì„± ì œê±° ë° ê°•í™”)
# ----------------------------------------
def translate_text_with_llm(text_content: str, target_lang_code: str, source_lang_code: str) -> Tuple[str, bool]:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. (ì•ˆì •í™”ëœ í…ìŠ¤íŠ¸ ì¶œë ¥)
    **ìˆ˜ì • ì‚¬í•­:** LLM Fallback ìˆœì„œë¥¼ OpenAI ìš°ì„ ìœ¼ë¡œ ì¡°ì •í•˜ê³ , ì‘ë‹µì´ ë¹„ì–´ìˆì„ ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    
    Returns:
        tuple: (translated_text, is_success) - ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì™€ ì„±ê³µ ì—¬ë¶€
    """
    target_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(target_lang_code, "English")
    source_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(source_lang_code, "English")

    # ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì œ
    system_prompt = (
        f"You are a professional translation AI. Translate the entire following customer support chat history "
        f"from '{source_lang_name}' to '{target_lang_name}'. "
        f"You MUST translate the content to {target_lang_name} ONLY. "
        f"Do not include any mixed languages, the source text, or any introductory/concluding remarks. "
        f"Output ONLY the translated chat history text. "
    )
    prompt = f"Original Chat History:\n\n{text_content}"

    # LLM Fallback ìˆœì„œ: OpenAI -> Gemini -> Claude (OpenAIë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì¡°ì •)
    llm_attempts = [
        ("openai", get_api_key("openai"), "gpt-4o"),  # 1ìˆœìœ„: OpenAI (ê°€ì¥ ì•ˆì •ì )
        ("gemini", get_api_key("gemini"), "gemini-2.5-flash"),  # 2ìˆœìœ„
        ("claude", get_api_key("claude"), "claude-3-5-sonnet-latest"),  # 3ìˆœìœ„
    ]

    last_error = ""

    for provider, key, model_name in llm_attempts:
        if not key: continue

        try:
            translated_text = ""

            if provider == "openai":
                o_client = OpenAI(api_key=key)
                resp = o_client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}],
                    temperature=0.1
                )
                translated_text = resp.choices[0].message.content.strip()

            elif provider == "gemini":
                genai.configure(api_key=key)
                g_model = genai.GenerativeModel(model_name)
                resp = g_model.generate_content(
                    contents=prompt,
                    config=genai.types.GenerateContentConfig(system_instruction=system_prompt, temperature=0.1)
                )
                translated_text = resp.text.strip()

            elif provider == "claude":
                from anthropic import Anthropic
                c_client = Anthropic(api_key=key)
                resp = c_client.messages.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    system=system_prompt
                )
                translated_text = resp.content[0].text.strip()

            # ë²ˆì—­ ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸
            if translated_text and len(translated_text.strip()) > 0:
                return translated_text, True  # ë²ˆì—­ ì„±ê³µ
            else:
                last_error = f"Translation failed: {provider} returned empty response."
                continue  # ë‹¤ìŒ LLM ì‹œë„

        except Exception as e:
            last_error = f"Translation API call failed with {provider} ({model_name}): {e}"  # ëª¨ë¸ëª… ì¶”ê°€
            print(last_error)
            continue  # ë‹¤ìŒ LLM ì‹œë„

    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ, ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ê°€ ê³„ì† ì§„í–‰ë˜ë„ë¡ í•¨
    # (ì˜¤ë¥˜ ë©”ì‹œì§€ ëŒ€ì‹  ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì—¬ ë²ˆì—­ ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥)
    print(f"Translation failed: {last_error or 'No active API key found.'}. Returning original text.")
    return text_content, False  # ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜, ë²ˆì—­ ì‹¤íŒ¨ í‘œì‹œ


# ----------------------------------------
# Realtime Hint Generation (ìš”ì²­ 2 ë°˜ì˜)
# ----------------------------------------
def generate_realtime_hint(current_lang_key: str, is_call: bool = False):
    """í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ì—ê²Œ ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸(í‚¤ì›Œë“œ/ì •ì±…/ì•¡ì…˜)ë¥¼ ì œê³µ"""
    # ì–¸ì–´ í‚¤ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not current_lang_key or current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])
    # ì±„íŒ…/ì „í™” êµ¬ë¶„í•˜ì—¬ ì´ë ¥ ì‚¬ìš©
    if is_call:
        # ì „í™” ì‹œë®¬ë ˆì´í„°ì—ì„œëŠ” í˜„ì¬ CC ì˜ì—­ì— í‘œì‹œëœ í…ìŠ¤íŠ¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ í•¨ê»˜ ì‚¬ìš©
        website_url = st.session_state.get("call_website_url", "").strip()
        website_context = f"\nWebsite URL: {website_url}" if website_url else ""
        history_text = (
            f"Initial Query: {st.session_state.call_initial_query}\n"
            f"Previous Customer Utterance: {st.session_state.current_customer_audio_text}\n"
            f"Previous Agent Utterance: {st.session_state.current_agent_audio_text}{website_context}"
        )
    else:
        history_text = get_chat_history_for_prompt(include_attachment=True)

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    hint_prompt = f"""
You are an AI Supervisor providing an **urgent, internal hint** to a human agent whose AHT is being monitored.
Analyze the conversation history, especially the customer's last message, which might be about complex issues like JR Pass, Universal Studio Japan (USJ), or a complex refund policy.

Provide ONE concise, actionable hint for the agent. The purpose is to save AHT time.

Output MUST be a single paragraph/sentence in {lang_name} containing actionable advice.
DO NOT use markdown headers or titles.
Do NOT direct the agent to check the general website.
Provide an actionable fact or the next specific step (e.g., check policy section, confirm coverage).

Examples of good hints (based on the content):
- Check the official JR Pass site for current exchange rates.
- The 'Universal Express Pass' is non-refundable; clearly cite policy section 3.2.
- Ask for the order confirmation number before proceeding with any action.
- The solution lies in the section of the Klook site titled '~'.

Conversation History:
{history_text}

HINT:
"""
    if not st.session_state.is_llm_ready:
        return "(Mock Hint: LLM Key is missing. Ask the customer for the booking number.)"

    with st.spinner(f"ğŸ’¡ {L['button_request_hint']}..."):
        try:
            return run_llm(hint_prompt).strip()
        except Exception as e:
            return f"âŒ Hint Generation Error. (Try again or check API Key: {e})"


def generate_agent_response_draft(current_lang_key: str) -> str:
    """ê³ ê° ì‘ë‹µì„ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ì—ì´ì „íŠ¸ ì‘ë‹µ ì´ˆì•ˆì„ ìƒì„± (ìš”ì²­ 1 ë°˜ì˜)"""
    # ì–¸ì–´ í‚¤ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not current_lang_key or current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])
    history_text = get_chat_history_for_prompt(include_attachment=True)
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ê³ ê°ì˜ ìµœì‹  ë¬¸ì˜ ë‚´ìš© ì¶”ì¶œ ë° ë¶„ì„ (ê°•í™”)
    latest_customer_message = ""
    initial_customer_query = st.session_state.get('customer_query_text_area', '')
    customer_query_analysis = ""
    
    # ëª¨ë“  ê³ ê° ë©”ì‹œì§€ ìˆ˜ì§‘
    all_customer_messages = []
    if st.session_state.simulator_messages:
        all_customer_messages = [msg["content"] for msg in st.session_state.simulator_messages 
                                if msg.get("role") in ["customer", "customer_rebuttal", "initial_query"]]
    
    # ì´ˆê¸° ë¬¸ì˜ë„ í¬í•¨
    if initial_customer_query and initial_customer_query not in all_customer_messages:
        all_customer_messages.insert(0, initial_customer_query)
    
    if all_customer_messages:
        latest_customer_message = all_customer_messages[-1]
        
        # ë¬¸ì˜ ë‚´ìš©ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        inquiry_keywords = []
        inquiry_text = " ".join(all_customer_messages).lower()
        
        # ì¼ë°˜ì ì¸ ë¬¸ì˜ í‚¤ì›Œë“œ íŒ¨í„´
        important_patterns = [
            r'\b\d{4,}\b',  # ì£¼ë¬¸ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ ë“± ìˆ«ì
            r'\b(ì£¼ë¬¸|order|æ³¨æ–‡)\b',
            r'\b(í™˜ë¶ˆ|refund|è¿”é‡‘)\b',
            r'\b(ì·¨ì†Œ|cancel|ã‚­ãƒ£ãƒ³ã‚»ãƒ«)\b',
            r'\b(ë°°ì†¡|delivery|é…é€)\b',
            r'\b(ë³€ê²½|change|å¤‰æ›´)\b',
            r'\b(ë¬¸ì œ|problem|issue|å•é¡Œ)\b',
            r'\b(ë„ì›€|help|åŠ©ã‘)\b',
        ]
        
        # í•µì‹¬ ë¬¸ì˜ ë‚´ìš© ìš”ì•½
        inquiry_summary = f"""
**CUSTOMER INQUIRY DETAILS:**

Initial Query: "{initial_customer_query if initial_customer_query else 'Not provided'}"

Latest Customer Message: "{latest_customer_message}"

All Customer Messages Context:
{chr(10).join([f"- {msg[:150]}..." if len(msg) > 150 else f"- {msg}" for msg in all_customer_messages[-3:]])}

**YOUR RESPONSE MUST DIRECTLY ADDRESS:**

1. **SPECIFIC ISSUE IDENTIFICATION**: 
   - What EXACT problem or question did the customer mention?
   - Extract and reference specific details: order numbers, dates, product names, locations, error messages, etc.
   - If multiple issues were mentioned, address EACH one specifically

2. **CONCRETE SOLUTION PROVIDED**:
   - Provide STEP-BY-STEP instructions tailored to their EXACT situation
   - Include specific actions they need to take (e.g., "Go to Settings > Account > Order History and click on order #12345")
   - Reference the exact products/services they mentioned
   - If they mentioned a location, reference it in your solution

3. **PERSONALIZATION**:
   - Use the customer's specific words/phrases when appropriate
   - Reference their exact situation (e.g., "Since you mentioned your eSIM isn't activating in Paris...")
   - Acknowledge their specific concern or frustration point

4. **COMPLETENESS**:
   - Answer ALL questions they asked
   - Address ALL problems they mentioned
   - If they asked "why", explain the specific reason for their situation
   - If they asked "how", provide detailed steps for their exact case

**CRITICAL REQUIREMENTS:**
- DO NOT use generic templates like "Thank you for contacting us" without addressing their specific issue
- DO NOT give vague answers like "Please check your settings" - be SPECIFIC about which settings and what to do
- DO NOT ignore specific details they mentioned (order numbers, dates, locations, etc.)
- Your response must read as if it was written SPECIFICALLY for this customer's exact inquiry
- If the customer mentioned "eSIM activation in Paris", your response MUST specifically address eSIM activation and Paris
- If the customer mentioned an order number, your response MUST reference that order number

**EXAMPLE OF GOOD RESPONSE:**
Bad: "Thank you for contacting us. We understand your concern and will help you resolve this issue."
Good: "I understand you're having trouble activating your eSIM in Paris. Let me help you resolve this step by step. First, please check if your phone's APN settings are configured correctly for the Paris network..."

**NOW GENERATE YOUR RESPONSE** following these requirements:
"""
        
        customer_query_analysis = inquiry_summary

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"\n[ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´: {attachment_context}]\n"
    else:
        attachment_context = ""

    # ê³ ê° ìœ í˜• ë° ë°˜ë³µ ë¶ˆë§Œ íŒ¨í„´ ë¶„ì„
    customer_type = st.session_state.get('customer_type_sim_select', 'ì¼ë°˜ì ì¸ ë¬¸ì˜')
    is_difficult_customer = customer_type in ["ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°", "Difficult Customer",
                                              "Highly Dissatisfied Customer", "é›£ã—ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"]

    # ê³ ê° ë©”ì‹œì§€ ìˆ˜ ë° ê°ì • ë¶„ì„
    customer_message_count = sum(
        1 for msg in st.session_state.simulator_messages if msg.get("role") in ["customer", "customer_rebuttal"])
    agent_message_count = sum(1 for msg in st.session_state.simulator_messages if msg.get("role") == "agent_response")

    # ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µë“¤ ì¶”ì¶œ (ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´)
    previous_agent_responses = [msg["content"] for msg in st.session_state.simulator_messages 
                                if msg.get("role") == "agent_response"]
    previous_responses_context = ""
    if previous_agent_responses:
        previous_responses_context = f"\n[ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µë“¤ (ì°¸ê³ ìš©, ë™ì¼í•˜ê²Œ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ):\n"
        for i, prev_resp in enumerate(previous_agent_responses[-3:], 1):  # ìµœê·¼ 3ê°œë§Œ
            prev_resp_preview = prev_resp[:200] + "..." if len(prev_resp) > 200 else prev_resp
            previous_responses_context += f"{i}. {prev_resp_preview}\n"
        previous_responses_context += "]\n"

    # ê³ ê°ì´ ê³„ì† ë”°ì§€ê±°ë‚˜ í™”ë‚´ëŠ” íŒ¨í„´ ê°ì§€ (ê³ ê° ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ë³´ë‹¤ ë§ê±°ë‚˜, ë°˜ë³µì ì¸ ë¶ˆë§Œ í‘œí˜„)
    is_repeating_complaints = False
    if customer_message_count > agent_message_count and customer_message_count >= 2:
        # ë§ˆì§€ë§‰ 2ê°œ ê³ ê° ë©”ì‹œì§€ ë¶„ì„
        recent_customer_messages = [msg["content"].lower() for msg in st.session_state.simulator_messages if
                                    msg.get("role") in ["customer", "customer_rebuttal"]][-2:]
        complaint_keywords = ["ì™œ", "ì´ìœ ", "ì„¤ëª…", "ë§ì´ ì•ˆ", "ì´í•´ê°€ ì•ˆ", "í™”ë‚˜", "ì§œì¦", "ë¶ˆë§Œ", "ì™œ", "why", "reason", "explain",
                              "angry", "frustrated", "complaint", "ãªãœ", "ç†ç”±", "èª¬æ˜", "æ€’ã‚Š", "ä¸æº€"]
        if any(any(keyword in msg for keyword in complaint_keywords) for msg in recent_customer_messages):
            is_repeating_complaints = True

    # ëŒ€ì²˜ë²• í¬ë©”ì´ì…˜ ì¶”ê°€ ì—¬ë¶€ ê²°ì •
    needs_coping_strategy = is_difficult_customer or (is_repeating_complaints and customer_message_count >= 2)

    # ëŒ€ì²˜ë²• ê°€ì´ë“œë¼ì¸ ìƒì„±
    coping_guidance = ""
    if needs_coping_strategy:
        coping_guidance = f"""

[CRITICAL: Handling Difficult Customer Situation]
The customer type is "{customer_type}" and the customer has sent {customer_message_count} messages while the agent has sent {agent_message_count} messages.
The customer may be showing signs of continued frustration or dissatisfaction.

**INCLUDE THE FOLLOWING COPING STRATEGY FORMAT IN YOUR RESPONSE:**

1. **Immediate Acknowledgment** (1-2 sentences):
   - Acknowledge their frustration/specific concern explicitly
   - Show deep empathy and understanding
   - Example formats:
     * "{'ì£„ì†¡í•©ë‹ˆë‹¤. ë¶ˆí¸ì„ ë“œë ¤ ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I sincerely apologize for the inconvenience. I fully understand your situation and frustration.' if current_lang_key == 'en' else 'å¤§å¤‰ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãŠå®¢æ§˜ã®çŠ¶æ³ã¨ã”ä¸ä¾¿ã‚’ååˆ†ã«ç†è§£ã—ã¦ãŠã‚Šã¾ã™ã€‚')}"
     * "{'ê³ ê°ë‹˜ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ì˜ ë“£ê³  ìˆìŠµë‹ˆë‹¤. ì •ë§ ë‹µë‹µí•˜ì…¨ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I hear your concerns clearly. This must have been very frustrating for you.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã”æ„è¦‹ã‚’ã—ã£ã‹ã‚Šã¨å—ã‘æ­¢ã‚ã¦ã„ã¾ã™ã€‚æœ¬å½“ã«ãŠå›°ã‚Šã ã£ãŸã¨æ€ã„ã¾ã™ã€‚')}"

2. **Specific Solution Recap** (2-3 sentences):
   - Clearly restate the solution/step provided previously (if any)
   - Offer a NEW concrete action or alternative solution
   - Be specific and actionable
   - Example formats:
     * "{'ì•ì„œ ì•ˆë‚´ë“œë¦° [êµ¬ì²´ì  í•´ê²°ì±…] ì™¸ì—ë„, [ìƒˆë¡œìš´ ëŒ€ì•ˆ/ì¶”ê°€ ì¡°ì¹˜]ë¥¼ ì§„í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('In addition to the [specific solution] I mentioned earlier, I can also [new alternative/additional action] for you.' if current_lang_key == 'en' else 'å…ˆã»ã©ã”æ¡ˆå†…ã—ãŸ[å…·ä½“çš„è§£æ±ºç­–]ã«åŠ ãˆã¦ã€[æ–°ã—ã„ä»£æ›¿æ¡ˆ/è¿½åŠ æªç½®]ã‚‚é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚')}"
     * "{'í˜¹ì‹œ [êµ¬ì²´ì  ë¬¸ì œì ] ë•Œë¬¸ì— ë¶ˆí¸í•˜ì…¨ë‹¤ë©´, [êµ¬ì²´ì  í•´ê²° ë°©ë²•]ì„ ë°”ë¡œ ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If you are experiencing [specific issue], I can immediately proceed with [specific solution].' if current_lang_key == 'en' else 'ã‚‚ã—[å…·ä½“çš„å•é¡Œ]ã§ã”ä¸ä¾¿ã§ã—ãŸã‚‰ã€[å…·ä½“çš„è§£æ±ºæ–¹æ³•]ã‚’ã™ãã«é€²ã‚ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"

3. **Escalation or Follow-up Offer** (1-2 sentences):
   - Offer to escalate to supervisor/higher level support
   - Promise immediate follow-up within specific time
   - Example formats:
     * "{'ë§Œì•½ ì—¬ì „íˆ ë¶ˆë§Œì´ í•´ì†Œë˜ì§€ ì•Šìœ¼ì‹ ë‹¤ë©´, ì¦‰ì‹œ ìƒê¸‰ ê´€ë¦¬ìì—ê²Œ ì´ê´€í•˜ì—¬ ë” ë‚˜ì€ í•´ê²°ì±…ì„ ì°¾ì•„ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If your concern is still not resolved, I can immediately escalate this to a supervisor to find a better solution.' if current_lang_key == 'en' else 'ã‚‚ã—ã”ä¸æº€ãŒè§£æ¶ˆã•ã‚Œãªã„å ´åˆã¯ã€ã™ãã«ä¸Šç´šç®¡ç†è€…ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ãƒˆã—ã¦ã€ã‚ˆã‚Šè‰¯ã„è§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"
     * "{'24ì‹œê°„ ì´ë‚´ì— [êµ¬ì²´ì  ì¡°ì¹˜/ê²°ê³¼]ë¥¼ í™•ì¸í•˜ì—¬ ê³ ê°ë‹˜ê»˜ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will follow up with you within 24 hours regarding [specific action/result].' if current_lang_key == 'en' else '24æ™‚é–“ä»¥å†…ã«[å…·ä½“çš„æªç½®/çµæœ]ã‚’ç¢ºèªã—ã€ãŠå®¢æ§˜ã«å†åº¦ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚')}"

4. **Closing with Assurance** (1 sentence):
   - Reassure that their concern is being taken seriously
   - Example formats:
     * "{'ê³ ê°ë‹˜ì˜ ëª¨ë“  ë¬¸ì˜ì‚¬í•­ì„ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will prioritize resolving all of your concerns.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã™ã¹ã¦ã®ã”è³ªå•ã‚’æœ€å„ªå…ˆã§å‡¦ç†ã„ãŸã—ã¾ã™ã€‚')}"

**IMPORTANT NOTES:**
- DO NOT repeat the exact same solution that was already provided
- DO NOT sound dismissive or automated
- DO sound genuinely concerned and willing to go the extra mile
- If policy restrictions exist, acknowledge them but still offer alternatives
- Use warm, respectful tone while being firm about what can/cannot be done

**RESPONSE STRUCTURE:**
[Immediate Acknowledgment]
[Specific Solution Recap + New Action]
[Escalation/Follow-up Offer]
[Closing with Assurance]

Now generate the agent's response draft following this structure:
"""

    # ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ì¶”ê°€ ì§€ì‹œì‚¬í•­ (ë” ê°•í™”)
    diversity_instruction = ""
    if previous_agent_responses:
        # ì´ì „ ì‘ë‹µë“¤ì˜ ì£¼ìš” í‚¤ì›Œë“œ/êµ¬ë¬¸ ì¶”ì¶œ (ë°˜ë³µ ë°©ì§€)
        previous_keywords = []
        for prev_resp in previous_agent_responses[-3:]:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (2-3ë‹¨ì–´ êµ¬ë¬¸)
            words = prev_resp.split()[:20]  # ì²˜ìŒ 20ë‹¨ì–´ë§Œ
            for i in range(len(words) - 1):
                if len(words[i]) > 3 and len(words[i+1]) > 3:
                    previous_keywords.append(f"{words[i]} {words[i+1]}")
        
        keywords_warning = ""
        if previous_keywords:
            unique_keywords = list(set(previous_keywords))[:10]  # ìµœëŒ€ 10ê°œë§Œ
            keywords_warning = f"\n- AVOID using these exact phrases from previous responses: {', '.join(unique_keywords[:5])}"
        
        diversity_instruction = f"""
**CRITICAL DIVERSITY REQUIREMENT - STRICTLY ENFORCED:**
- You MUST generate a COMPLETELY DIFFERENT response from ALL previous agent responses shown above
- Use COMPLETELY DIFFERENT wording, phrasing, sentence structures, and vocabulary
- If similar solutions are needed, present them in a COMPLETELY DIFFERENT way or from a COMPLETELY DIFFERENT angle
- Vary your opening sentences, transition phrases, and closing statements - NO REPETITION
- DO NOT copy, paraphrase, or reuse ANY phrases from previous responses - be CREATIVE and UNIQUE while maintaining professionalism
- Consider COMPLETELY different approaches: 
  * If previous responses were formal, try a warmer, more personal tone (or vice versa)
  * If previous responses focused on one aspect, emphasize a COMPLETELY different aspect this time
  * Use different examples, analogies, or explanations
  * Change the order of information presentation
  * Use different sentence lengths and structures
{keywords_warning}
- IMPORTANT: Read ALL previous responses carefully and ensure your response is DISTINCTLY different in style, tone, structure, and content
- If you find yourself writing something similar to a previous response, STOP and rewrite it completely differently
"""

    # ëœë¤ ìš”ì†Œ ì¶”ê°€ë¥¼ ìœ„í•œ ë³€í˜• ì§€ì‹œì‚¬í•­
    variation_approaches = [
        "Start with a different greeting or acknowledgment style",
        "Use a different problem-solving approach or framework",
        "Present information in a different order",
        "Use different examples or analogies",
        "Vary the level of formality or warmth",
        "Focus on different aspects of the solution",
        "Use different transition words and phrases",
        "Change the length and complexity of sentences"
    ]
    selected_approaches = random.sample(variation_approaches, min(3, len(variation_approaches)))
    variation_note = "\n".join([f"- {approach}" for approach in selected_approaches])

    draft_prompt = f"""
You are an AI assistant helping a customer support agent write a professional, tailored response.

**PRIMARY OBJECTIVE:**
Generate a response draft that is SPECIFICALLY tailored to the customer's EXACT inquiry, providing concrete, actionable solutions that directly address their specific situation. The response must read as if it was written personally for this customer's unique case.

**CRITICAL REQUIREMENTS (IN ORDER OF PRIORITY):**
1. **MOST CRITICAL**: Address the customer's SPECIFIC inquiry/question with DETAILED, ACTIONABLE solutions
   - Extract and reference specific details from their message (order numbers, dates, product names, locations, error messages, etc.)
   - Provide step-by-step instructions tailored to their EXACT situation
   - Answer ALL questions they asked completely
   - Address ALL problems they mentioned specifically

2. The response MUST be in {lang_name}

3. Be professional, empathetic, and solution-oriented

4. If the customer asked a question, provide a COMPLETE and SPECIFIC answer - do NOT give vague or generic responses
   - Bad: "Please check your settings"
   - Good: "Please go to Settings > Mobile Network > APN Settings and ensure the APN is set to 'internet'"

5. If the customer mentioned a problem, acknowledge it SPECIFICALLY and provide STEP-BY-STEP solutions
   - Reference their exact problem description
   - Provide solutions that directly address their specific issue

6. Reference specific details from their inquiry (order numbers, dates, products, locations, etc.) if mentioned
   - If they mentioned "order #12345", your response MUST include "order #12345"
   - If they mentioned "Paris", your response should reference Paris specifically
   - If they mentioned "eSIM", address eSIM specifically, not just "SIM card"

7. Keep the tone appropriate for the customer type: {customer_type}

8. Do NOT include any markdown formatting, just plain text

9. {f'**FOLLOW THE COPING STRATEGY FORMAT BELOW**' if needs_coping_strategy else 'Use natural, conversational flow'}

10. **CRITICAL**: Generate a COMPLETELY UNIQUE and VARIED response - avoid repeating ANY similar phrases, structures, or approaches from previous responses

11. **CRITICAL**: Your response must be HIGHLY RELEVANT to the customer's specific inquiry - generic template responses are NOT acceptable
    - DO NOT start with generic greetings without immediately addressing their specific issue
    - DO NOT use placeholder text like "[specific solution]" - provide ACTUAL specific solutions
    - Your response should make the customer feel like you read and understood THEIR specific message

**VARIATION TECHNIQUES TO APPLY:**
{variation_note}

{customer_query_analysis}

**FULL CONVERSATION HISTORY:**
{history_text}
{attachment_context}

**PREVIOUS RESPONSES TO AVOID REPEATING:**
{previous_responses_context if previous_responses_context else "No previous responses to compare against."}

**DIVERSITY REQUIREMENTS:**
{diversity_instruction if diversity_instruction else "This is the first response, so no previous responses to avoid."}

{coping_guidance if needs_coping_strategy else ''}

**YOUR TASK:**
Generate the agent's response draft NOW. The response must:

1. **FIRST**: Read the customer inquiry analysis above CAREFULLY and identify:
   - What is their EXACT problem or question?
   - What specific details did they mention (order numbers, dates, locations, products)?
   - What do they need help with specifically?

2. **SECOND**: Write a response that:
   - Addresses their EXACT problem/question (not a generic version)
   - References the specific details they mentioned
   - Provides concrete, actionable steps tailored to their situation
   - Answers ALL their questions completely
   - Makes them feel understood

3. **THIRD**: Ensure the response is:
   - COMPLETELY DIFFERENT and UNIQUE from any previous responses
   - Professional, empathetic, and solution-oriented
   - Written in {lang_name}
   - Free of markdown formatting

**BEFORE YOU WRITE**: Ask yourself:
- "Does this response address the customer's SPECIFIC inquiry?"
- "Would a generic template response work here?" (If yes, rewrite it to be more specific)
- "Does this response reference specific details from the customer's message?"
- "Would the customer feel like I read and understood THEIR specific message?"

**NOW GENERATE THE RESPONSE:**
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        draft = run_llm(draft_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if draft.startswith("```"):
            lines = draft.split("\n")
            draft = "\n".join(lines[1:-1]) if len(lines) > 2 else draft
        return draft
    except Exception as e:
        return f"âŒ ì‘ë‹µ ì´ˆì•ˆ ìƒì„± ì˜¤ë¥˜: {e}"


# â­ ìƒˆë¡œìš´ í•¨ìˆ˜: ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ ìš”ì•½ ìƒì„±
def generate_outbound_call_summary(customer_query: str, current_lang_key: str, target: str) -> str:
    """
    Simulates an outbound call to a local partner or customer and generates a summary of the outcome.
    """
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # Get the current chat history for context
    history_text = get_chat_history_for_prompt(include_attachment=True)
    if not history_text:
        history_text = f"Initial Customer Query: {customer_query}"

    # Policy context (from supervisor) should be included to guide the outcome
    policy_context = st.session_state.supervisor_policy_context or ""

    summary_prompt = f"""
You are an AI simulating a quick, high-stakes phone call placed by the customer support agent to a '{target}' (either a local partner/vendor or the customer).

The purpose of the call is to resolve a complex, policy-restricted issue (like an exceptional refund for a non-refundable item, or urgent confirmation of an airport transfer change).

Analyze the conversation history, the initial query, and any provided supervisor policy.
Generate a concise summary of the OUTCOME of this simulated phone call.
The summary MUST be professional and strictly in {lang_name}.

[CRITICAL RULE]: For non-refundable items (e.g., Universal Studio Express Pass, non-refundable hotel/transfer), the local partner should only grant an exception IF the customer has provided strong, unavoidable proof (like a flight cancellation notice, doctor's note, or natural disaster notice). If no such proof is evident in the chat history, the outcome should usually be a denial or a request for more proof, but keep the tone professional.
If the customer's query is about Airport Transfer change, the outcome should be: 'Confirmation complete. Change is approved/denied based on partner policy.'

Conversation History:
{history_text}

Supervisor Policy Context (If any):
{policy_context}

Target of Call: {target}

Generate the phone call summary (Outcome ONLY):
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key missing. (Simulated Outcome: The {target} requested the agent to send proof via email.)"

    try:
        summary = run_llm(summary_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if summary.startswith("```"):
            lines = summary.split("\n")
            summary = "\n".join(lines[1:-1]) if len(lines) > 2 else summary
        return summary
    except Exception as e:
        return f"âŒ Phone call simulation error: {e}"


# ========================================
# 3. Whisper / TTS Helper
# ========================================

def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = "audio/webm", lang_code: str = None, auto_detect: bool = True) -> str:
    """
    OpenAI Whisper API ë˜ëŠ” Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•©ë‹ˆë‹¤.
    OpenAIê°€ ì‹¤íŒ¨í•˜ë©´ Geminië¡œ ìë™ fallbackí•©ë‹ˆë‹¤.
    
    Args:
        audio_bytes: ì „ì‚¬í•  ì˜¤ë””ì˜¤ ë°”ì´íŠ¸
        mime_type: ì˜¤ë””ì˜¤ MIME íƒ€ì…
        lang_code: ì–¸ì–´ ì½”ë“œ (ko, en, ja ë“±). Noneì´ê±°ë‚˜ auto_detect=Trueì´ë©´ ìë™ ê°ì§€
        auto_detect: Trueì´ë©´ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ (lang_code ë¬´ì‹œ)
    """
    # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    
    # ì„ì‹œ íŒŒì¼ ì €ì¥ (API í˜¸í™˜ì„±)
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp.write(audio_bytes)
    tmp.flush()
    tmp.close()
    
    # 1ï¸âƒ£ OpenAI Whisper API ì‹œë„
    client = st.session_state.openai_client
    if client is not None:
        try:
            with open(tmp.name, "rb") as f:
                # ì–¸ì–´ ìë™ ê°ì§€ ë˜ëŠ” ì§€ì •ëœ ì–¸ì–´ ì‚¬ìš©
                if auto_detect or lang_code is None:
                    # language íŒŒë¼ë¯¸í„°ë¥¼ ìƒëµí•˜ë©´ Whisperê°€ ìë™ìœ¼ë¡œ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤
                    res = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                    )
                else:
                    whisper_lang = {"ko": "ko", "en": "en", "ja": "ja"}.get(lang_code, "en")
                    res = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                        language=whisper_lang,
                    )
            # res.text ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ res ìì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            result = res.text.strip() if hasattr(res, 'text') else str(res).strip()
            if result:
                try:
                    os.remove(tmp.name)
                except OSError:
                    pass
                return result
        except Exception as e:
            # OpenAI ì‹¤íŒ¨ ì‹œ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  Geminië¡œ fallback
            print(f"OpenAI Whisper failed: {e}")
    
    # 2ï¸âƒ£ Gemini API fallback
    gemini_key = get_api_key("gemini")
    if gemini_key:
        try:
            import base64
            genai.configure(api_key=gemini_key)
            
            # GeminiëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡
            with open(tmp.name, "rb") as f:
                audio_data = f.read()
                audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # Gemini 2.0 Flash ëª¨ë¸ ì‚¬ìš© (ì˜¤ë””ì˜¤ ì§€ì›)
            model = genai.GenerativeModel("gemini-2.0-flash-exp")
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            lang_prompt = ""
            if lang_code:
                lang_map = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}
                lang_prompt = f"ì´ ì˜¤ë””ì˜¤ëŠ” {lang_map.get(lang_code, 'English')}ë¡œ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. "
            
            prompt = f"{lang_prompt}ì´ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•´ì£¼ì„¸ìš”. ì˜¤ì§ ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”."
            
            # GeminiëŠ” íŒŒì¼ ì—…ë¡œë“œ ë°©ì‹ ì‚¬ìš© (Gemini 2.0 FlashëŠ” ì˜¤ë””ì˜¤ ì§€ì›)
            try:
                audio_file = genai.upload_file(path=tmp.name, mime_type=mime_type)
                
                # íŒŒì¼ ì—…ë¡œë“œ í›„ ì ì‹œ ëŒ€ê¸° (ì—…ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°)
                import time
                time.sleep(1)
                
                response = model.generate_content([prompt, audio_file])
                result = response.text.strip() if response.text else ""
                
                # íŒŒì¼ ì‚­ì œ
                try:
                    genai.delete_file(audio_file.name)
                except Exception as del_err:
                    print(f"Failed to delete Gemini file: {del_err}")
            except Exception as upload_err:
                # íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                print(f"Gemini file upload failed: {upload_err}")
                # ëŒ€ì•ˆ: base64 ì¸ì½”ë”©ëœ ì˜¤ë””ì˜¤ë¥¼ ì§ì ‘ ì „ì†¡ (ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ê²½ìš°)
                raise upload_err
            
            if result:
                try:
                    os.remove(tmp.name)
                except OSError:
                    pass
                return result
            else:
                raise Exception("Gemini returned empty result")
        except Exception as e:
            print(f"Gemini transcription failed: {e}")
            # Geminië„ ì‹¤íŒ¨í•œ ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
            try:
                os.remove(tmp.name)
            except OSError:
                pass
            return f"âŒ {L.get('whisper_client_error', 'ì „ì‚¬ ì‹¤íŒ¨')}: OpenAIì™€ Gemini ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({str(e)[:100]})"
    else:
        # ë‘ API ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€
        try:
            os.remove(tmp.name)
        except OSError:
            pass
        return f"âŒ {L.get('openai_missing', 'OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.')} ë˜ëŠ” Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."


def transcribe_audio(audio_bytes, filename="audio.wav"):
    client = st.session_state.openai_client

    # 1ï¸âƒ£ OpenAI Whisper ì‹œë„
    if client:
        try:
            import io
            bio = io.BytesIO(audio_bytes)
            bio.name = filename
            resp = client.audio.transcriptions.create(
                model="whisper-1",
                file=bio,
            )
            return resp.text
        except Exception as e:
            print("Whisper OpenAI failed:", e)

    # 2ï¸âƒ£ Gemini STT fallback
    try:
        genai.configure(api_key=get_api_key("gemini"))
        model = genai.GenerativeModel("gemini-2.5-flash")
        text = model.generate_content("Transcribe this audio:").text
        return text or ""
    except Exception as e:
        print("Gemini STT failed:", e)

    return "âŒ STT not available"


# ========================================
# ë¹„ë””ì˜¤ ë™ê¸°í™” ê´€ë ¨ í•¨ìˆ˜
# ========================================

def analyze_text_for_video_selection(text: str, current_lang_key: str, 
                                     agent_last_response: str = None,
                                     conversation_context: List[Dict] = None) -> Dict[str, Any]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ê°ì • ìƒíƒœì™€ ì œìŠ¤ì²˜ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
    OpenAI/Gemini APIë¥¼ í™œìš©í•œ ì˜ìƒ RAGì˜ í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
    
    â­ Gemini ì œì•ˆ ì ìš©: ê¸´ê¸‰ë„, ë§Œì¡±ë„ ë³€í™”, ì—ì´ì „íŠ¸ ë‹µë³€ ê¸°ë°˜ ì˜ˆì¸¡ ì¶”ê°€
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ê³ ê°ì˜ ì§ˆë¬¸/ì‘ë‹µ)
        current_lang_key: í˜„ì¬ ì–¸ì–´ í‚¤
        agent_last_response: ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ë‹µë³€ (ì„ íƒì , ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ)
        conversation_context: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì , ë§Œì¡±ë„ ë³€í™” ë¶„ì„ìš©)
    
    Returns:
        {
            "emotion": "NEUTRAL" | "HAPPY" | "ANGRY" | "ASKING" | "SAD",
            "gesture": "NONE" | "HAND_WAVE" | "NOD" | "SHAKE_HEAD" | "POINT",
            "urgency": "LOW" | "MEDIUM" | "HIGH",  # â­ ì¶”ê°€: ê¸´ê¸‰ë„
            "satisfaction_delta": -1.0 to 1.0,  # â­ ì¶”ê°€: ë§Œì¡±ë„ ë³€í™” (-1: ê°ì†Œ, 0: ìœ ì§€, 1: ì¦ê°€)
            "confidence": 0.0-1.0
        }
    """
    if not text or not text.strip():
        return {
            "emotion": "NEUTRAL", 
            "gesture": "NONE", 
            "urgency": "LOW",
            "satisfaction_delta": 0.0,
            "confidence": 0.5
        }
    
    L = LANG.get(current_lang_key, LANG["ko"])
    
    # â­ Gemini ì œì•ˆ: ì—ì´ì „íŠ¸ ë‹µë³€ ê¸°ë°˜ ì˜ˆì¸¡ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_info = ""
    if agent_last_response:
        context_info = f"""
ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ë‹µë³€: "{agent_last_response}"

ì—ì´ì „íŠ¸ì˜ ë‹µë³€ì„ ê³ ë ¤í–ˆì„ ë•Œ, ê³ ê°ì´ ì§€ê¸ˆ ë§í•˜ëŠ” ë‚´ìš©ì€ ì–´ë–¤ ê°ì •ì„ ìˆ˜ë°˜í•  ê²ƒì¸ì§€ ì˜ˆì¸¡í•˜ì„¸ìš”.
ì˜ˆë¥¼ ë“¤ì–´:
- ì—ì´ì „íŠ¸ê°€ ì†”ë£¨ì…˜ì„ ì œì‹œí–ˆë‹¤ë©´ â†’ ê³ ê°ì€ HAPPY ë˜ëŠ” ASKING (ì¶”ê°€ ì§ˆë¬¸)
- ì—ì´ì „íŠ¸ê°€ ê±°ì ˆí–ˆë‹¤ë©´ â†’ ê³ ê°ì€ ANGRY ë˜ëŠ” SAD
- ì—ì´ì „íŠ¸ê°€ ì§ˆë¬¸ì„ í–ˆë‹¤ë©´ â†’ ê³ ê°ì€ ASKING (ë‹µë³€) ë˜ëŠ” NEUTRAL
"""
    
    # â­ Gemini ì œì•ˆ: ë§Œì¡±ë„ ë³€í™” ë¶„ì„ ì»¨í…ìŠ¤íŠ¸
    satisfaction_context = ""
    if conversation_context and len(conversation_context) > 1:
        # ìµœê·¼ ëŒ€í™”ì˜ ê°ì • ë³€í™” ì¶”ì 
        recent_emotions = []
        for msg in conversation_context[-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€
            if msg.get("role") == "customer_rebuttal" or msg.get("role") == "customer":
                recent_emotions.append(msg.get("content", ""))
        
        if len(recent_emotions) >= 2:
            satisfaction_context = f"""
ìµœê·¼ ëŒ€í™” íë¦„:
- ì´ì „ ê³ ê° ë©”ì‹œì§€: "{recent_emotions[-2] if len(recent_emotions) >= 2 else ''}"
- í˜„ì¬ ê³ ê° ë©”ì‹œì§€: "{recent_emotions[-1]}"

ë§Œì¡±ë„ ë³€í™”ë¥¼ ë¶„ì„í•˜ì„¸ìš”:
- ì´ì „ë³´ë‹¤ ë” ê¸ì •ì ì´ë©´ satisfaction_delta > 0
- ì´ì „ë³´ë‹¤ ë” ë¶€ì •ì ì´ë©´ satisfaction_delta < 0
- ë¹„ìŠ·í•˜ë©´ satisfaction_delta â‰ˆ 0
"""
    
    # â­ Gemini ì œì•ˆ: ê°œì„ ëœ LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ë‹¤ìŒ ê³ ê°ì˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ê°ì • ìƒíƒœ, ì œìŠ¤ì²˜, ê¸´ê¸‰ë„, ë§Œì¡±ë„ ë³€í™”ë¥¼ íŒë‹¨í•˜ì„¸ìš”.

ê³ ê° í…ìŠ¤íŠ¸: "{text}"
{context_info}
{satisfaction_context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):
{{
    "emotion": "NEUTRAL" | "HAPPY" | "ANGRY" | "ASKING" | "SAD",
    "gesture": "NONE" | "HAND_WAVE" | "NOD" | "SHAKE_HEAD" | "POINT",
    "urgency": "LOW" | "MEDIUM" | "HIGH",
    "satisfaction_delta": -1.0 to 1.0,
    "confidence": 0.0-1.0
}}

ê°ì • íŒë‹¨ ê¸°ì¤€ (ì„¸ë¶„í™”):
- HAPPY: ê¸ì •ì  í‘œí˜„, ê°ì‚¬, ë§Œì¡±, í•´ê²°ë¨ ("ê°ì‚¬í•©ë‹ˆë‹¤", "ì¢‹ì•„ìš”", "ì™„ë²½í•´ìš”", "ì´ì œ ì´í•´í–ˆì–´ìš”")
- ANGRY: ë¶ˆë§Œ, í™”ë‚¨, ê±°ë¶€, ê°•í•œ ë¶€ì • ("í™”ê°€ ë‚˜ìš”", "ë¶ˆê°€ëŠ¥í•´ìš”", "ê±°ì ˆí•©ë‹ˆë‹¤", "ë§ë„ ì•ˆ ë¼ìš”")
- ASKING: ì§ˆë¬¸, ê¶ê¸ˆí•¨, í™•ì¸ ìš”ì²­, ì •ë³´ ìš”êµ¬ ("ì–´ë–»ê²Œ", "ì™œ", "ì•Œë ¤ì£¼ì„¸ìš”", "ì£¼ë¬¸ë²ˆí˜¸ê°€ ë­ì˜ˆìš”?")
- SAD: ìŠ¬í””, ì‹¤ë§, ì¢Œì ˆ ("ìŠ¬í”„ë„¤ìš”", "ì‹¤ë§í–ˆì–´ìš”", "ì•„ì‰½ìŠµë‹ˆë‹¤", "ê·¸ë ‡ë‹¤ë©´ ì–´ì©” ìˆ˜ ì—†ë„¤ìš”")
- NEUTRAL: ì¤‘ë¦½ì  í‘œí˜„, ë‹¨ìˆœ ì •ë³´ ì „ë‹¬ (ê¸°ë³¸ê°’)

ì œìŠ¤ì²˜ íŒë‹¨ ê¸°ì¤€:
- HAND_WAVE: ì¸ì‚¬, í™˜ì˜ ("ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤")
- NOD: ë™ì˜, ê¸ì •, ì´í•´ ("ë„¤", "ë§ì•„ìš”", "ê·¸ë ‡ìŠµë‹ˆë‹¤", "ì•Œê² ìŠµë‹ˆë‹¤")
- SHAKE_HEAD: ë¶€ì •, ê±°ë¶€, ë¶ˆë§Œì¡± ("ì•„ë‹ˆìš”", "ì•ˆ ë©ë‹ˆë‹¤", "ê·¸ê±´ ì•„ë‹ˆì—ìš”")
- POINT: ì„¤ëª…, ì§€ì‹œ, íŠ¹ì • í•­ëª© ì–¸ê¸‰ ("ì—¬ê¸°", "ì´ê²ƒ", "ì €ê²ƒ", "ì£¼ë¬¸ë²ˆí˜¸ëŠ”")
- NONE: íŠ¹ë³„í•œ ì œìŠ¤ì²˜ ì—†ìŒ (ê¸°ë³¸ê°’)

ê¸´ê¸‰ë„ íŒë‹¨ ê¸°ì¤€:
- HIGH: ì¦‰ì‹œ í•´ê²° í•„ìš”, ê¸´ê¸‰í•œ ë¬¸ì œ ("ì§€ê¸ˆ ë‹¹ì¥", "ë°”ë¡œ", "ê¸´ê¸‰", "ì¤‘ìš”í•´ìš”")
- MEDIUM: ë¹ ë¥¸ í•´ê²° ì„ í˜¸, ì¤‘ìš”í•˜ì§€ë§Œ ê¸´ê¸‰í•˜ì§€ ì•ŠìŒ
- LOW: ì¼ë°˜ì ì¸ ë¬¸ì˜, ê¸´ê¸‰í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’)

ë§Œì¡±ë„ ë³€í™” (satisfaction_delta):
- 1.0: ë§¤ìš° ë§Œì¡±, ë¬¸ì œ í•´ê²°ë¨, ê°ì‚¬ í‘œí˜„
- 0.5: ë§Œì¡±, ê¸ì •ì  ë°˜ì‘
- 0.0: ì¤‘ë¦½, ë³€í™” ì—†ìŒ
- -0.5: ë¶ˆë§Œì¡±, ë¶€ì •ì  ë°˜ì‘
- -1.0: ë§¤ìš° ë¶ˆë§Œì¡±, í™”ë‚¨, ê±°ë¶€

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”:"""

    try:
        # LLM í˜¸ì¶œ
        if st.session_state.is_llm_ready:
            response_text = run_llm(prompt)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
                import re
                json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    # ìœ íš¨ì„± ê²€ì‚¬
                    valid_emotions = ["NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD"]
                    valid_gestures = ["NONE", "HAND_WAVE", "NOD", "SHAKE_HEAD", "POINT"]
                    valid_urgencies = ["LOW", "MEDIUM", "HIGH"]
                    
                    emotion = result.get("emotion", "NEUTRAL")
                    gesture = result.get("gesture", "NONE")
                    urgency = result.get("urgency", "LOW")
                    satisfaction_delta = float(result.get("satisfaction_delta", 0.0))
                    confidence = float(result.get("confidence", 0.7))
                    
                    if emotion not in valid_emotions:
                        emotion = "NEUTRAL"
                    if gesture not in valid_gestures:
                        gesture = "NONE"
                    if urgency not in valid_urgencies:
                        urgency = "LOW"
                    
                    # â­ Gemini ì œì•ˆ: ìƒí™©ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
                    context_keywords = []
                    text_lower_for_context = text.lower()
                    
                    # ì£¼ìš” ìƒí™©ë³„ í‚¤ì›Œë“œ ë§¤í•‘
                    if any(word in text_lower_for_context for word in ["ì£¼ë¬¸ë²ˆí˜¸", "order number", "ì£¼ë¬¸ ë²ˆí˜¸"]):
                        context_keywords.append("order_number")
                    if any(word in text_lower_for_context for word in ["í•´ê²°", "ì™„ë£Œ", "ê°ì‚¬", "solution", "resolved"]):
                        if satisfaction_delta > 0.3:
                            context_keywords.append("solution_accepted")
                    if any(word in text_lower_for_context for word in ["ê±°ì ˆ", "ë¶ˆê°€", "ì•ˆ ë©ë‹ˆë‹¤", "denied", "cannot"]):
                        if emotion == "ANGRY":
                            context_keywords.append("policy_denial")
                    
                    return {
                        "emotion": emotion,
                        "gesture": gesture,
                        "urgency": urgency,
                        "satisfaction_delta": max(-1.0, min(1.0, satisfaction_delta)),
                        "context_keywords": context_keywords,  # â­ ì¶”ê°€
                        "confidence": max(0.0, min(1.0, confidence))
                    }
            except json.JSONDecodeError:
                pass
        
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ë¶„ì„
        text_lower = text.lower()
        emotion = "NEUTRAL"
        gesture = "NONE"
        urgency = "LOW"
        satisfaction_delta = 0.0
        
        # ê°ì • í‚¤ì›Œë“œ ë¶„ì„
        if any(word in text_lower for word in ["ê°ì‚¬", "ì¢‹ì•„", "ì™„ë²½", "ë§Œì¡±", "ê³ ë§ˆì›Œ", "í•´ê²°"]):
            emotion = "HAPPY"
            satisfaction_delta = 0.5
        elif any(word in text_lower for word in ["í™”", "ë¶ˆë§Œ", "ê±°ì ˆ", "ë¶ˆê°€ëŠ¥", "ì•ˆ ë©ë‹ˆë‹¤", "ë§ë„ ì•ˆ ë¼"]):
            emotion = "ANGRY"
            satisfaction_delta = -0.5
        elif any(word in text_lower for word in ["ì–´ë–»ê²Œ", "ì™œ", "ì•Œë ¤", "ì§ˆë¬¸", "ê¶ê¸ˆ", "ì£¼ë¬¸ë²ˆí˜¸"]):
            emotion = "ASKING"
        elif any(word in text_lower for word in ["ìŠ¬í”„", "ì‹¤ë§", "ì•„ì‰½", "ê·¸ë ‡ë‹¤ë©´"]):
            emotion = "SAD"
            satisfaction_delta = -0.3
        
        # ê¸´ê¸‰ë„ í‚¤ì›Œë“œ ë¶„ì„
        if any(word in text_lower for word in ["ì§€ê¸ˆ ë‹¹ì¥", "ë°”ë¡œ", "ê¸´ê¸‰", "ì¤‘ìš”í•´ìš”", "ì¦‰ì‹œ"]):
            urgency = "HIGH"
        elif any(word in text_lower for word in ["ë¹¨ë¦¬", "ê°€ëŠ¥í•œ í•œ", "ìµœëŒ€í•œ"]):
            urgency = "MEDIUM"
        
        # ì œìŠ¤ì²˜ í‚¤ì›Œë“œ ë¶„ì„
        if any(word in text_lower for word in ["ì•ˆë…•", "ë°˜ê°‘", "ì¸ì‚¬"]):
            gesture = "HAND_WAVE"
        elif any(word in text_lower for word in ["ë„¤", "ë§ì•„", "ê·¸ë˜", "ë™ì˜", "ì•Œê² ìŠµë‹ˆë‹¤"]):
            gesture = "NOD"
            if emotion == "HAPPY":
                satisfaction_delta = 0.3
        elif any(word in text_lower for word in ["ì•„ë‹ˆ", "ì•ˆ ë©ë‹ˆë‹¤", "ê±°ì ˆ"]):
            gesture = "SHAKE_HEAD"
            satisfaction_delta = -0.2
        elif any(word in text_lower for word in ["ì—¬ê¸°", "ì´ê²ƒ", "ì €ê²ƒ", "ì´ê±°", "ì£¼ë¬¸ë²ˆí˜¸"]):
            gesture = "POINT"
        
        # â­ Gemini ì œì•ˆ: ìƒí™©ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„)
        context_keywords = []
        if any(word in text_lower for word in ["ì£¼ë¬¸ë²ˆí˜¸", "order number", "ì£¼ë¬¸ ë²ˆí˜¸"]):
            context_keywords.append("order_number")
        if any(word in text_lower for word in ["í•´ê²°", "ì™„ë£Œ", "ê°ì‚¬", "solution"]):
            if satisfaction_delta > 0.3:
                context_keywords.append("solution_accepted")
        if any(word in text_lower for word in ["ê±°ì ˆ", "ë¶ˆê°€", "ì•ˆ ë©ë‹ˆë‹¤"]):
            if emotion == "ANGRY":
                context_keywords.append("policy_denial")
        
        return {
            "emotion": emotion,
            "gesture": gesture,
            "urgency": urgency,
            "satisfaction_delta": satisfaction_delta,
            "context_keywords": context_keywords,  # â­ ì¶”ê°€
            "confidence": 0.6  # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ì€ ë‚®ì€ ì‹ ë¢°ë„
        }
    
    except Exception as e:
        print(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "emotion": "NEUTRAL", 
            "gesture": "NONE", 
            "urgency": "LOW",
            "satisfaction_delta": 0.0,
            "context_keywords": [],  # â­ ì¶”ê°€
            "confidence": 0.5
        }


def get_video_path_by_avatar(gender: str, emotion: str, is_speaking: bool = False, 
                             gesture: str = "NONE", context_keywords: List[str] = None) -> str:
    """
    ê³ ê° ì•„ë°”íƒ€ ì •ë³´(ì„±ë³„, ê°ì • ìƒíƒœ, ì œìŠ¤ì²˜, ìƒí™©)ì— ë”°ë¼ ì ì ˆí•œ ë¹„ë””ì˜¤ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    OpenAI/Gemini ê¸°ë°˜ ì˜ìƒ RAG: LLMì´ ë¶„ì„í•œ ê°ì •/ì œìŠ¤ì²˜ì— ë”°ë¼ ë¹„ë””ì˜¤ í´ë¦½ì„ ì„ íƒí•©ë‹ˆë‹¤.
    
    â­ Gemini ì œì•ˆ: ìƒí™©ë³„ ë¹„ë””ì˜¤ í´ë¦½ íŒ¨í„´ í™•ì¥ (ì˜ˆ: male_asking_order_number.mp4)
    
    Args:
        gender: "male" ë˜ëŠ” "female"
        emotion: "NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD"
        is_speaking: ë§í•˜ëŠ” ì¤‘ì¸ì§€ ì—¬ë¶€
        gesture: "NONE", "HAND_WAVE", "NOD", "SHAKE_HEAD", "POINT"
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["order_number", "solution_accepted", "policy_denial"])
    
    Returns:
        ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
    """
    # ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì‚¬ìš©ìê°€ ì„¤ì •í•œ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ì´ ì €ì¥ëœ ìœ„ì¹˜)
    video_base_dir = os.path.join(DATA_DIR, "videos")
    os.makedirs(video_base_dir, exist_ok=True)
    
    # â­ Gemini ì œì•ˆ: ìš°ì„ ìˆœìœ„ -1 - ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì¶”ì²œ ë¹„ë””ì˜¤ (ê°€ì¥ ìš°ì„ )
    if context_keywords:
        db_recommended = get_recommended_video_from_database(emotion, gesture, context_keywords)
        if db_recommended:
            return db_recommended
    else:
        db_recommended = get_recommended_video_from_database(emotion, gesture, [])
        if db_recommended:
            return db_recommended
    
    # â­ Gemini ì œì•ˆ: ìš°ì„ ìˆœìœ„ 0 - ìƒí™©ë³„ ë¹„ë””ì˜¤ í´ë¦½ (ê°€ì¥ êµ¬ì²´ì )
    if context_keywords:
        for keyword in context_keywords:
            # ìƒí™©ë³„ íŒŒì¼ëª… íŒ¨í„´ ì‹œë„ (ì˜ˆ: male_asking_order_number.mp4)
            context_filename = f"{gender}_{emotion.lower()}_{keyword}"
            if is_speaking:
                context_filename += "_speaking"
            context_filename += ".mp4"
            context_path = os.path.join(video_base_dir, context_filename)
            if os.path.exists(context_path):
                return context_path
            
            # ì„¸ì…˜ ìƒíƒœì—ì„œë„ í™•ì¸
            context_video_key = f"video_{gender}_{emotion.lower()}_{keyword}"
            if context_video_key in st.session_state and st.session_state[context_video_key]:
                video_path = st.session_state[context_video_key]
                if os.path.exists(video_path):
                    return video_path
    
    # ìš°ì„ ìˆœìœ„ 1: ì œìŠ¤ì²˜ê°€ ìˆëŠ” ê²½ìš° ì œìŠ¤ì²˜ë³„ ë¹„ë””ì˜¤ ì‹œë„
    if gesture != "NONE" and gesture:
        gesture_video_key = f"video_{gender}_{emotion.lower()}_{gesture.lower()}"
        if gesture_video_key in st.session_state and st.session_state[gesture_video_key]:
            video_path = st.session_state[gesture_video_key]
            if os.path.exists(video_path):
                return video_path
        
        # ì œìŠ¤ì²˜ë³„ íŒŒì¼ëª… íŒ¨í„´ ì‹œë„
        gesture_filename = f"{gender}_{emotion.lower()}_{gesture.lower()}"
        if is_speaking:
            gesture_filename += "_speaking"
        gesture_filename += ".mp4"
        gesture_path = os.path.join(video_base_dir, gesture_filename)
        if os.path.exists(gesture_path):
            return gesture_path
    
    # ìš°ì„ ìˆœìœ„ 2: ê°ì • ìƒíƒœë³„ ë¹„ë””ì˜¤ (ì œìŠ¤ì²˜ ì—†ì´)
    video_key = f"video_{gender}_{emotion.lower()}"
    if is_speaking:
        video_key += "_speaking"
    
    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ë¹„ë””ì˜¤ ê²½ë¡œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if video_key in st.session_state and st.session_state[video_key]:
        video_path = st.session_state[video_key]
        if os.path.exists(video_path):
            return video_path
    
    # ê¸°ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ëª… íŒ¨í„´ ì‹œë„
    video_filename = f"{gender}_{emotion.lower()}"
    if is_speaking:
        video_filename += "_speaking"
    video_filename += ".mp4"
    
    video_path = os.path.join(video_base_dir, video_filename)
    if os.path.exists(video_path):
        return video_path
    
    # ìš°ì„ ìˆœìœ„ 3: ê¸°ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ ì‹œë„ (ì¤‘ë¦½ ìƒíƒœ)
    default_video = os.path.join(video_base_dir, f"{gender}_neutral.mp4")
    if os.path.exists(default_video):
        return default_video
    
    # ìš°ì„ ìˆœìœ„ 4: ì„¸ì…˜ ìƒíƒœì—ì„œ ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ í™•ì¸
    if "current_customer_video" in st.session_state and st.session_state.current_customer_video:
        return st.session_state.current_customer_video
    
    return None


# â­ Gemini ì œì•ˆ: ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í•¨ìˆ˜
def load_video_mapping_database() -> Dict[str, Any]:
    """ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    if os.path.exists(VIDEO_MAPPING_DB_FILE):
        try:
            with open(VIDEO_MAPPING_DB_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {"mappings": [], "feedback_history": []}
    return {"mappings": [], "feedback_history": []}


def save_video_mapping_database(db_data: Dict[str, Any]):
    """ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(VIDEO_MAPPING_DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(db_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}")


def add_video_mapping_feedback(
    customer_text: str,
    selected_video_path: str,
    emotion: str,
    gesture: str,
    context_keywords: List[str],
    user_rating: int,  # 1-5 ì ìˆ˜
    user_comment: str = ""
) -> None:
    """
    â­ Gemini ì œì•ˆ: ì‚¬ìš©ì í”¼ë“œë°±ì„ ë¹„ë””ì˜¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        customer_text: ê³ ê°ì˜ í…ìŠ¤íŠ¸
        selected_video_path: ì„ íƒëœ ë¹„ë””ì˜¤ ê²½ë¡œ
        emotion: ë¶„ì„ëœ ê°ì •
        gesture: ë¶„ì„ëœ ì œìŠ¤ì²˜
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ
        user_rating: ì‚¬ìš©ì í‰ê°€ (1-5)
        user_comment: ì‚¬ìš©ì ì½”ë©˜íŠ¸ (ì„ íƒì )
    """
    db_data = load_video_mapping_database()
    
    feedback_entry = {
        "timestamp": datetime.now().isoformat(),
        "customer_text": customer_text[:200],  # ìµœëŒ€ 200ì
        "selected_video": os.path.basename(selected_video_path) if selected_video_path else None,
        "video_path": selected_video_path,
        "emotion": emotion,
        "gesture": gesture,
        "context_keywords": context_keywords,
        "user_rating": user_rating,
        "user_comment": user_comment[:500] if user_comment else "",  # ìµœëŒ€ 500ì
        "is_natural_match": user_rating >= 4  # 4ì  ì´ìƒì´ë©´ ìì—°ìŠ¤ëŸ¬ìš´ ë§¤ì¹­ìœ¼ë¡œ ê°„ì£¼
    }
    
    db_data["feedback_history"].append(feedback_entry)
    
    # ë§¤í•‘ ê·œì¹™ ì—…ë°ì´íŠ¸ (í‰ê°€ê°€ ë†’ì€ ê²½ìš°)
    if user_rating >= 4:
        mapping_key = f"{emotion}_{gesture}_{'_'.join(context_keywords) if context_keywords else 'none'}"
        
        # ê¸°ì¡´ ë§¤í•‘ ì°¾ê¸°
        existing_mapping = None
        for mapping in db_data["mappings"]:
            if mapping.get("key") == mapping_key:
                existing_mapping = mapping
                break
        
        if existing_mapping:
            # ê¸°ì¡´ ë§¤í•‘ ì—…ë°ì´íŠ¸ (í‰ê·  ì ìˆ˜ ê³„ì‚°)
            total_rating = existing_mapping.get("total_rating", 0) + user_rating
            count = existing_mapping.get("count", 0) + 1
            existing_mapping["total_rating"] = total_rating
            existing_mapping["count"] = count
            existing_mapping["avg_rating"] = total_rating / count
            existing_mapping["last_updated"] = datetime.now().isoformat()
        else:
            # ìƒˆ ë§¤í•‘ ì¶”ê°€
            db_data["mappings"].append({
                "key": mapping_key,
                "emotion": emotion,
                "gesture": gesture,
                "context_keywords": context_keywords,
                "recommended_video": os.path.basename(selected_video_path) if selected_video_path else None,
                "video_path": selected_video_path,
                "total_rating": user_rating,
                "count": 1,
                "avg_rating": float(user_rating),
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat()
            })
    
    save_video_mapping_database(db_data)


def get_recommended_video_from_database(
    emotion: str,
    gesture: str,
    context_keywords: List[str]
) -> str:
    """
    â­ Gemini ì œì•ˆ: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¶”ì²œ ë¹„ë””ì˜¤ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        emotion: ê°ì • ìƒíƒœ
        gesture: ì œìŠ¤ì²˜
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ
    
    Returns:
        ì¶”ì²œ ë¹„ë””ì˜¤ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
    """
    db_data = load_video_mapping_database()
    
    mapping_key = f"{emotion}_{gesture}_{'_'.join(context_keywords) if context_keywords else 'none'}"
    
    # ì •í™•í•œ ë§¤ì¹­ ì°¾ê¸°
    for mapping in db_data["mappings"]:
        if mapping.get("key") == mapping_key and mapping.get("avg_rating", 0) >= 4.0:
            video_path = mapping.get("video_path")
            if video_path and os.path.exists(video_path):
                return video_path
    
    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ê°ì •ê³¼ ì œìŠ¤ì²˜ë§Œ)
    partial_key = f"{emotion}_{gesture}_none"
    for mapping in db_data["mappings"]:
        if mapping.get("key") == partial_key and mapping.get("avg_rating", 0) >= 4.0:
            video_path = mapping.get("video_path")
            if video_path and os.path.exists(video_path):
                return video_path
    
    return None


def render_synchronized_video(text: str, audio_bytes: bytes, gender: str, emotion: str, 
                               role: str = "customer", autoplay: bool = True,
                               gesture: str = "NONE", context_keywords: List[str] = None):
    """
    TTS ì˜¤ë””ì˜¤ì™€ ë™ê¸°í™”ëœ ë¹„ë””ì˜¤ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    
    â­ Gemini ì œì•ˆ: í”¼ë“œë°± í‰ê°€ ê¸°ëŠ¥ ì¶”ê°€
    
    Args:
        text: ë§í•˜ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©
        audio_bytes: TTSë¡œ ìƒì„±ëœ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸
        gender: ê³ ê° ì„±ë³„ ("male" ë˜ëŠ” "female")
        emotion: ê°ì • ìƒíƒœ ("NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD")
        role: ì—­í•  ("customer" ë˜ëŠ” "agent")
        autoplay: ìë™ ì¬ìƒ ì—¬ë¶€
        gesture: ì œìŠ¤ì²˜ (ì„ íƒì )
        context_keywords: ìƒí™©ë³„ í‚¤ì›Œë“œ (ì„ íƒì )
    """
    if role == "customer":
        is_speaking = True
        if context_keywords is None:
            context_keywords = []
        
        # â­ Gemini ì œì•ˆ: ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì¶”ì²œ ë¹„ë””ì˜¤ ìš°ì„  ì‚¬ìš©
        video_path = get_video_path_by_avatar(gender, emotion, is_speaking, gesture, context_keywords)
        
        if video_path and os.path.exists(video_path):
            try:
                with open(video_path, "rb") as f:
                    video_bytes = f.read()
                
                # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ë¥¼ í•¨ê»˜ ì¬ìƒ
                # Streamlitì˜ st.videoëŠ” ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ìˆëŠ” ë¹„ë””ì˜¤ë¥¼ ì§€ì›í•©ë‹ˆë‹¤
                # ì—¬ê¸°ì„œëŠ” ë¹„ë””ì˜¤ë§Œ í‘œì‹œí•˜ê³ , ì˜¤ë””ì˜¤ëŠ” ë³„ë„ë¡œ ì¬ìƒí•©ë‹ˆë‹¤
                st.video(video_bytes, format="video/mp4", autoplay=autoplay, loop=False, muted=False)
                
                # ì˜¤ë””ì˜¤ë„ í•¨ê»˜ ì¬ìƒ (ë™ê¸°í™”)
                if audio_bytes:
                    st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
                
                # â­ Gemini ì œì•ˆ: ì‚¬ìš©ì í”¼ë“œë°± í‰ê°€ UI ì¶”ê°€ (ì±„íŒ…/ì´ë©”ì¼ íƒ­ìš©)
                if not autoplay:  # ìë™ ì¬ìƒì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ í”¼ë“œë°± UI í‘œì‹œ
                    st.markdown("---")
                    st.markdown("**ğŸ’¬ ë¹„ë””ì˜¤ ë§¤ì¹­ í‰ê°€**")
                    st.caption("ì´ ë¹„ë””ì˜¤ê°€ ê³ ê°ì˜ í…ìŠ¤íŠ¸ì™€ ê°ì •ì— ìì—°ìŠ¤ëŸ½ê²Œ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆê¹Œ?")
                    
                    feedback_key = f"video_feedback_chat_{st.session_state.get('sim_instance_id', 'default')}_{hash(text) % 10000}"
                    
                    col_rating, col_comment = st.columns([2, 3])
                    with col_rating:
                        rating = st.slider(
                            "í‰ê°€ ì ìˆ˜ (1-5ì )",
                            min_value=1,
                            max_value=5,
                            value=3,
                            key=f"{feedback_key}_rating",
                            help="1ì : ë§¤ìš° ë¶€ìì—°ìŠ¤ëŸ¬ì›€, 5ì : ë§¤ìš° ìì—°ìŠ¤ëŸ¬ì›€"
                        )
                    
                    with col_comment:
                        comment = st.text_input(
                            "ì˜ê²¬ (ì„ íƒì‚¬í•­)",
                            key=f"{feedback_key}_comment",
                            placeholder="ì˜ˆ: ë¹„ë””ì˜¤ê°€ í…ìŠ¤íŠ¸ì™€ ì˜ ë§ì•˜ìŠµë‹ˆë‹¤"
                        )
                    
                    if st.button("í”¼ë“œë°± ì œì¶œ", key=f"{feedback_key}_submit"):
                        # í”¼ë“œë°±ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        add_video_mapping_feedback(
                            customer_text=text[:200],
                            selected_video_path=video_path,
                            emotion=emotion,
                            gesture=gesture,
                            context_keywords=context_keywords,
                            user_rating=rating,
                            user_comment=comment
                        )
                        st.success(f"âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ì ìˆ˜: {rating}/5)")
                        st.info("ğŸ’¡ ì´ í”¼ë“œë°±ì€ í–¥í›„ ë¹„ë””ì˜¤ ì„ íƒ ì •í™•ë„ë¥¼ ê°œì„ í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.")
                
                return True
            except Exception as e:
                st.warning(f"ë¹„ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                # ë¹„ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨ ì‹œ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                if audio_bytes:
                    st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
                return False
        else:
            # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
            if audio_bytes:
                st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
            return False
    else:
        # ì—ì´ì „íŠ¸ëŠ” ë¹„ë””ì˜¤ ì—†ì´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
        if audio_bytes:
            st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
        return False


def generate_virtual_human_video(text: str, audio_bytes: bytes, gender: str, emotion: str, 
                                 provider: str = "hyperclova") -> bytes:
    """
    ê°€ìƒ íœ´ë¨¼ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ì— ë§ëŠ” ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    âš ï¸ ì£¼ì˜: OpenAI/Gemini APIë§Œìœ¼ë¡œëŠ” ì…ëª¨ì–‘ ë™ê¸°í™” ë¹„ë””ì˜¤ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
    ê°€ìƒ íœ´ë¨¼ ë¹„ë””ì˜¤ ìƒì„±ì€ ë³„ë„ì˜ ê°€ìƒ íœ´ë¨¼ API (ì˜ˆ: Hyperclova)ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    
    í˜„ì¬ëŠ” ë¯¸ë¦¬ ì¤€ë¹„ëœ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    
    Args:
        text: ë§í•˜ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš©
        audio_bytes: TTSë¡œ ìƒì„±ëœ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸
        gender: ê³ ê° ì„±ë³„ ("male" ë˜ëŠ” "female")
        emotion: ê°ì • ìƒíƒœ ("NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD")
        provider: ê°€ìƒ íœ´ë¨¼ ì œê³µì ("hyperclova", "other")
    
    Returns:
        ìƒì„±ëœ ë¹„ë””ì˜¤ ë°”ì´íŠ¸ (ì—†ìœ¼ë©´ None)
    """
    # ê°€ìƒ íœ´ë¨¼ API í‚¤ í™•ì¸
    if provider == "hyperclova":
        api_key = get_api_key("hyperclova")
        if not api_key:
            return None
        
        # TODO: Hyperclova API ì—°ë™ êµ¬í˜„ (ë³„ë„ API í•„ìš”)
        # OpenAI/Gemini APIë§Œìœ¼ë¡œëŠ” ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, ì‹¤ì œ ê°€ìƒ íœ´ë¨¼ APIê°€ í•„ìš”í•©ë‹ˆë‹¤.
        # ì˜ˆì‹œ êµ¬ì¡°:
        # response = requests.post(
        #     "https://api.hyperclova.com/virtual-human/generate",
        #     headers={"Authorization": f"Bearer {api_key}"},
        #     json={
        #         "text": text,
        #         "audio": base64.b64encode(audio_bytes).decode(),
        #         "gender": gender,
        #         "emotion": emotion
        #     }
        # )
        # return response.content
    
    # ë‹¤ë¥¸ ì œê³µìë„ ì—¬ê¸°ì— ì¶”ê°€ ê°€ëŠ¥
    # elif provider == "other":
    #     ...
    
    return None


def get_virtual_human_config() -> Dict[str, Any]:
    """
    ê°€ìƒ íœ´ë¨¼ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ê°€ìƒ íœ´ë¨¼ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    return {
        "enabled": st.session_state.get("virtual_human_enabled", False),
        "provider": st.session_state.get("virtual_human_provider", "hyperclova"),
        "api_key": get_api_key("hyperclova") if st.session_state.get("virtual_human_provider", "hyperclova") == "hyperclova" else None
    }


# ì—­í• ë³„ TTS ìŒì„± ìŠ¤íƒ€ì¼ ì„¤ì •
TTS_VOICES = {
    "customer_male": {
        "gender": "male",
        "voice": "alloy"  # Male voice
    },
    "customer_female": {
        "gender": "female",
        "voice": "nova"  # Female voice
    },
    "customer": {
        "gender": "male",
        "voice": "alloy"  # Default male voice (fallback)
    },
    "agent": {
        "gender": "female",
        "voice": "shimmer"  # Distinct Female, Professional/Agent
    },
    "supervisor": {
        "gender": "female",
        "voice": "nova"  # Another Distinct Female, Informative/Supervisor
    }
}


def synthesize_tts(text: str, lang_key: str, role: str = "agent"):
    # lang_key ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not lang_key or lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"  # ìµœì¢… ê¸°ë³¸ê°’
    
    L = LANG.get(lang_key, LANG["ko"])  # ì•ˆì „í•œ ì ‘ê·¼
    client = st.session_state.openai_client
    if client is None:
        return None, L.get("openai_missing", "OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # â­ ìˆ˜ì •: ê³ ê° ì—­í• ì¸ ê²½ìš° ì„±ë³„ì— ë”°ë¼ ìŒì„± ì„ íƒ
    if role == "customer":
        customer_gender = st.session_state.customer_avatar.get("gender", "male")
        if customer_gender == "female":
            voice_key = "customer_female"
        else:
            voice_key = "customer_male"
        
        if voice_key in TTS_VOICES:
            voice_name = TTS_VOICES[voice_key]["voice"]
        else:
            voice_name = TTS_VOICES["customer"]["voice"]  # Fallback
    elif role in TTS_VOICES:
        voice_name = TTS_VOICES[role]["voice"]
    else:
        voice_name = TTS_VOICES["agent"]["voice"]  # Default fallback

    try:
        # â­ ìˆ˜ì •: í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œì„ ì œê±°í•˜ì—¬ ì „ì²´ ë¬¸ì˜ê°€ ì¬ìƒë˜ë„ë¡ í•¨
        # OpenAI TTSëŠ” ìµœëŒ€ 4096ìë¥¼ ì§€ì›í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë” ê¸´ í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬ ê°€ëŠ¥
        # ê³ ê°ì˜ ë¬¸ì˜ë¥¼ ëê¹Œì§€ ë‹¤ ë“¤ì–´ì•¼ ì›í™œí•œ ì‘ëŒ€ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬
        # ë§Œì•½ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ (ì˜ˆ: 10000ì ì´ìƒ) ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ,
        # ì¼ë°˜ì ì¸ ê³ ê° ë¬¸ì˜ëŠ” 4096ì ì´ë‚´ì´ë¯€ë¡œ ì „ì²´ë¥¼ ì²˜ë¦¬
        
        # tts-1 ëª¨ë¸ ì‚¬ìš© (ì•ˆì •ì„±)
        resp = client.audio.speech.create(
            model="tts-1",
            voice=voice_name,
            input=text
            # format="mp3"ì€ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.
        )
        return resp.read(), L["tts_status_success"]

    except Exception as e:
        return None, f"{L['tts_status_error']}: {e}"


# ----------------------------------------
# TTS Helper
# ----------------------------------------

def render_tts_button(text, lang_key, role="customer", prefix="", index: int = -1):
    # lang_key ê²€ì¦ ë° ê¸°ë³¸ê°’ ì²˜ë¦¬
    if not lang_key or lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"  # ìµœì¢… ê¸°ë³¸ê°’
    
    L = LANG.get(lang_key, LANG["ko"])  # ì•ˆì „í•œ ì ‘ê·¼

    # â­ ìˆ˜ì •: index=-1ì¸ ê²½ìš°, UUIDë¥¼ ì‚¬ìš©í•˜ì—¬ safe_key ìƒì„±
    if index == -1:
        # ì´ê´€ ìš”ì•½ì²˜ëŸ¼ ì¸ë±ìŠ¤ê°€ ê³ ì •ë˜ì§€ ì•ŠëŠ” ê²½ìš°, í…ìŠ¤íŠ¸ í•´ì‹œì™€ ì„¸ì…˜ ì¸ìŠ¤í„´ìŠ¤ IDë¥¼ ì¡°í•©
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        session_id_part = st.session_state.get('sim_instance_id', 'default_session')
        # â­ ìˆ˜ì •: ì´ê´€ ìš”ì•½ì˜ ê²½ìš° ì•ˆì •ì ì¸ í‚¤ë¥¼ ìƒì„± (time.time_ns() ì œê±°í•˜ì—¬ ë§¤ë²ˆ ê°™ì€ í‚¤ ìƒì„±)
        # ì–¸ì–´ ì½”ë“œë„ ì¶”ê°€í•˜ì—¬ ì´ê´€ í›„ ì–¸ì–´ ë³€ê²½ ì‹œì—ë„ ê³ ìœ ì„± ë³´ì¥
        lang_code = st.session_state.get('language', lang_key)
        safe_key = f"{prefix}_SUMMARY_{session_id_part}_{lang_code}_{content_hash}"
    else:
        # ëŒ€í™” ë¡œê·¸ì²˜ëŸ¼ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        safe_key = f"{prefix}_{index}_{content_hash}"

    # ì¬ìƒ ë²„íŠ¼ì„ ëˆ„ë¥¼ ë•Œë§Œ TTS ìš”ì²­
    if st.button(L["button_listen_audio"], key=safe_key):
        if not st.session_state.openai_client:
            st.error(L["openai_missing"])
            return  # í‚¤ ì—†ìœ¼ë©´ ì¢…ë£Œ

        with st.spinner(L["tts_status_generating"]):
            try:
                audio_bytes, msg = synthesize_tts(text, lang_key, role=role)
                if audio_bytes:
                    # â­ st.audio í˜¸ì¶œ ì‹œ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì¬ìƒ ì‹œê°„ì„ í™•ë³´
                    # Streamlit ë¬¸ì„œ: autoplayëŠ” ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì—†ì´ëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                    try:
                        st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                        st.success(msg)
                        # â­ ìˆ˜ì •: ì¬ìƒì´ ì‹œì‘ë  ì¶©ë¶„í•œ ì‹œê°„ì„ í™•ë³´í•˜ê¸° ìœ„í•´ ëŒ€ê¸° ì‹œê°„ì„ 3ì´ˆë¡œ ëŠ˜ë¦¼
                        time.sleep(3)
                    except Exception as e:
                        st.warning(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜: {e}. ì˜¤ë””ì˜¤ íŒŒì¼ì€ ìƒì„±ë˜ì—ˆì§€ë§Œ ìë™ ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                        st.success(msg)
                else:
                    st.error(msg)
                    time.sleep(1)  # ì—ëŸ¬ ë°œìƒ ì‹œë„ ì ì‹œ ëŒ€ê¸°
            except Exception as e:
                # TTS API í˜¸ì¶œ ìì²´ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ (ë„¤íŠ¸ì›Œí¬ ë“±)
                st.error(f"âŒ TTS ìƒì„± ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                time.sleep(1)

            # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ í›„, ë¶ˆí•„ìš”í•œ ì¬ì‹¤í–‰ì„ ë§‰ê¸° ìœ„í•´ ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ
            return
        # [ì¤‘ëµ: TTS Helper ë]


# ========================================
# 4. ë¡œì»¬ ìŒì„± ê¸°ë¡ Helper
# ========================================

def load_voice_records() -> List[Dict[str, Any]]:
    return _load_json(VOICE_META_FILE, [])


def save_voice_records(records: List[Dict[str, Any]]):
    _save_json(VOICE_META_FILE, records)


def save_audio_record_local(
        audio_bytes: bytes,
        filename: str,
        transcript_text: str,
        mime_type: str = "audio/webm",
        meta: Dict[str, Any] = None,
) -> str:
    records = load_voice_records()
    rec_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    ext = filename.split(".")[-1] if "." in filename else "webm"
    audio_filename = f"{rec_id}.{ext}"
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "wb") as f:
        f.write(audio_bytes)

    rec = {
        "id": rec_id,
        "created_at": ts,
        "filename": filename,
        "audio_filename": audio_filename,
        "size": len(audio_bytes),
        "transcript": transcript_text,
        "mime_type": mime_type,
        "language": st.session_state.language,
        "meta": meta or {},
    }
    records.insert(0, rec)
    save_voice_records(records)
    return rec_id


def delete_audio_record_local(rec_id: str) -> bool:
    records = load_voice_records()
    idx = next((i for i, r in enumerate(records) if r.get("id") == rec_id), None)
    if idx is None:
        return False
    rec = records.pop(idx)
    audio_filename = rec.get("audio_filename")
    if audio_filename:
        audio_path = os.path.join(AUDIO_DIR, audio_filename)
        try:
            os.remove(audio_path)
        except FileNotFoundError:
            pass
    save_voice_records(records)
    return True


def get_audio_bytes_local(rec_id: str):
    records = load_voice_records()
    rec = next((r for r in records if r.get("id") == rec_id), None)
    if not rec:
        raise FileNotFoundError("record not found")
    audio_filename = rec["audio_filename"]
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "rb") as f:
        b = f.read()
    return b, rec


# ========================================
# 5. ë¡œì»¬ ì‹œë®¬ë ˆì´ì…˜ ì´ë ¥ Helper (ìš”ì²­ 4 ë°˜ì˜)
# ========================================

def load_simulation_histories_local(lang_key: str) -> List[Dict[str, Any]]:
    histories = _load_json(SIM_META_FILE, [])
    # í˜„ì¬ ì–¸ì–´ì™€ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ì´ë ¥ë§Œ í•„í„°ë§
    return [
        h for h in histories
        if h.get("language_key") == lang_key and (isinstance(h.get("messages"), list) or h.get("summary"))
    ]


def generate_chat_summary(messages: List[Dict[str, Any]], initial_query: str, customer_type: str,
                          current_lang_key: str) -> Dict[str, Any]:
    """ì±„íŒ… ë‚´ìš©ì„ AIë¡œ ìš”ì•½í•˜ì—¬ ì£¼ìš” ì •ë³´ì™€ ì ìˆ˜ë¥¼ ì¶”ì¶œ (ìš”ì²­ 4)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ëŒ€í™” ë‚´ìš© ì¶”ì¶œ
    conversation_text = f"Initial Query: {initial_query}\n\n"
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role in ["customer", "customer_rebuttal", "phone_exchange"]:
            conversation_text += f"Customer: {content}\n"
        elif role == "agent_response" or role == "agent":
            conversation_text += f"Agent: {content}\n"
        # supervisor ë©”ì‹œì§€ëŠ” LLMì— ì „ë‹¬í•˜ì§€ ì•Šì•„ ì—­í•  í˜¼ë™ ë°©ì§€

    # í° êµí™˜ ë¡œê·¸ëŠ” ì´ë¯¸ "Agent: ... | Customer: ..." í˜•íƒœë¡œ ê¸°ë¡ë˜ë¯€ë¡œ,
    # generate_summary_for_call í•¨ìˆ˜ì—ì„œ ë³„ë„ë¡œ ì²˜ë¦¬í•  í•„ìš” ì—†ì´,
    # ì—¬ê¸°ì„œëŠ” ë²”ìš© ì±„íŒ… ìš”ì•½ ë¡œì§ì„ ë”°ë¥´ë„ë¡ ë©”ì‹œì§€ë¥¼ ì •ì œí•©ë‹ˆë‹¤.

    summary_prompt = f"""
You are an AI analyst summarizing a customer support conversation.

Analyze the conversation and provide a structured summary in JSON format (ONLY JSON, no markdown).

Extract and score:
1. Main inquiry topic (what the customer asked about)
2. Key responses provided by the agent (list of max 3 core actions/solutions)
3. Customer sentiment score (0-100, where 0=very negative, 50=neutral, 100=very positive)
4. Customer satisfaction score (0-100, based on final response)
5. Customer characteristics:
   - Language preference (if mentioned)
   - Cultural background hints (if any)
   - Location/region (if mentioned, but anonymize specific addresses)
   - Communication style (formal/casual, brief/detailed)
6. Privacy-sensitive information (anonymize: names, emails, phone numbers, specific addresses)
   - Extract patterns only (e.g., "email provided", "phone number provided", "resides in Asia region")

Output format (JSON only):
{{
  "main_inquiry": "brief description of main issue",
  "key_responses": ["response 1", "response 2"],
  "customer_sentiment_score": 75,
  "customer_satisfaction_score": 80,
  "customer_characteristics": {{
    "language": "ko/en/ja or unknown",
    "cultural_hints": "brief description or unknown",
    "region": "general region or unknown",
    "communication_style": "formal/casual/brief/detailed"
  }},
  "privacy_info": {{
    "has_email": true/false,
    "has_phone": true/false,
    "has_address": true/false,
    "region_hint": "general region or unknown"
  }},
  "summary": "overall conversation summary in {lang_name}"
}}

Conversation:
{conversation_text}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        # Fallback summary
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Customer inquiry about: {initial_query[:100]}"
        }

    try:
        summary_text = run_llm(summary_prompt).strip()
        # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if "```json" in summary_text:
            summary_text = summary_text.split("```json")[1].split("```")[0].strip()
        elif "```" in summary_text:
            summary_text = summary_text.split("```")[1].split("```")[0].strip()

        import json
        summary_data = json.loads(summary_text)
        return summary_data
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback
        st.warning(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Customer inquiry about: {initial_query[:100]}"
        }
        # Fallback on error
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Error generating summary: {str(e)}"
        }


def save_simulation_history_local(initial_query: str, customer_type: str, messages: List[Dict[str, Any]],
                                  is_chat_ended: bool, attachment_context: str, is_call: bool = False):
    """AI ìš”ì•½ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì´ë ¥ì„ ì €ì¥ (ìš”ì²­ 4 ë°˜ì˜)"""
    histories = _load_json(SIM_META_FILE, [])
    doc_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    # AI ìš”ì•½ ìƒì„± (ì±„íŒ… ì¢…ë£Œ ì‹œ ë˜ëŠ” ì¶©ë¶„í•œ ëŒ€í™”ê°€ ìˆì„ ë•Œ)
    summary_data = None
    if is_chat_ended or len(messages) > 4 or is_call:  # ì „í™” í†µí™”ëŠ” ë°”ë¡œ ìš”ì•½ ì‹œë„
        summary_data = generate_chat_summary(messages, initial_query, customer_type, st.session_state.language)

    # ìš”ì•½ ë°ì´í„°ê°€ ìƒì„±ëœ ê²½ìš°ì—ë§Œ ì €ì¥ (ìš”ì•½ ì¤‘ì‹¬ ì €ì¥)
    if summary_data:
        # ìš”ì•½ ë°ì´í„°ì— ì´ˆê¸° ë¬¸ì˜ì™€ í•µì‹¬ ì •ë³´ í¬í•¨
        data = {
            "id": doc_id,
            "initial_query": initial_query,  # ì´ˆê¸° ë¬¸ì˜ëŠ” ìœ ì§€
            "customer_type": customer_type,
            "messages": [],  # ì „ì²´ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ìš”ì•½ë§Œ ì €ì¥)
            "summary": summary_data,  # AI ìš”ì•½ ë°ì´í„° (ì£¼ìš” ì €ì¥ ë‚´ìš©)
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",  # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸
            "is_call": is_call,  # ì „í™” ì—¬ë¶€ í”Œë˜ê·¸
        }
    else:
        # ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° (ì§„í–‰ ì¤‘ì¸ ëŒ€í™”), ìµœì†Œí•œì˜ ì •ë³´ë§Œ ì €ì¥
        data = {
            "id": doc_id,
            "initial_query": initial_query,
            "customer_type": customer_type,
            "messages": messages[:10] if len(messages) > 10 else messages,  # ìµœê·¼ 10ê°œë§Œ ì €ì¥
            "summary": None,  # ìš”ì•½ ì—†ìŒ
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",
            "is_call": is_call,
        }

    # ê¸°ì¡´ ì´ë ¥ì— ì¶”ê°€ (ìµœì‹ ìˆœ)
    histories.insert(0, data)
    # ë„ˆë¬´ ë§ì€ ì´ë ¥ ë°©ì§€ (ì˜ˆ: 100ê°œë¡œ ì¦ê°€ - ìš”ì•½ë§Œ ì €ì¥í•˜ë¯€ë¡œ ìš©ëŸ‰ ë¶€ë‹´ ì ìŒ)
    _save_json(SIM_META_FILE, histories[:100])
    return doc_id


def delete_all_history_local():
    _save_json(SIM_META_FILE, [])


# ========================================
# DB ì €ì¥ ê¸°ëŠ¥ (Word/PPTX/PDF)
# ========================================
def export_history_to_word(histories: List[Dict[str, Any]], filename: str = None, lang: str = "ko") -> str:
    """ì´ë ¥ì„ Word íŒŒì¼ë¡œ ì €ì¥"""
    if not IS_DOCX_AVAILABLE:
        raise ImportError("Word ì €ì¥ì„ ìœ„í•´ python-docxê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install python-docx")
    
    # ì–¸ì–´ ì„¤ì • í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    if lang not in ["ko", "en", "ja"]:
        lang = "ko"
    L = LANG.get(lang, LANG["ko"])
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
    filepath = os.path.join(DATA_DIR, filename)
    
    doc = DocxDocument()
    
    # ì œëª© ì¶”ê°€
    title = doc.add_heading(L.get("download_history_title", "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½"), 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ê° ì´ë ¥ ì¶”ê°€
    for i, hist in enumerate(histories, 1):
        # ì´ë ¥ ì œëª©
        doc.add_heading(f'{L.get("download_history_number", "ì´ë ¥ #")}{i}', level=1)
        
        # ê¸°ë³¸ ì •ë³´
        doc.add_paragraph(f'ID: {hist.get("id", "N/A")}')
        doc.add_paragraph(f'{L.get("date_label", "ë‚ ì§œ")}: {hist.get("timestamp", "N/A")}')
        doc.add_paragraph(f'{L.get("download_initial_inquiry", "ì´ˆê¸° ë¬¸ì˜")}: {hist.get("initial_query", "N/A")}')
        doc.add_paragraph(f'{L.get("customer_type_label", "ê³ ê° ìœ í˜•")}: {hist.get("customer_type", "N/A")}')
        doc.add_paragraph(f'{L.get("language_label", "ì–¸ì–´")}: {hist.get("language_key", "N/A")}')
        
        summary = hist.get('summary', {})
        if summary:
            # ìš”ì•½ ì„¹ì…˜
            doc.add_heading(L.get("download_summary", "ìš”ì•½"), level=2)
            doc.add_paragraph(f'{L.get("download_main_inquiry", "ì£¼ìš” ë¬¸ì˜")}: {summary.get("main_inquiry", "N/A")}')
            doc.add_paragraph(f'{L.get("download_key_response", "í•µì‹¬ ì‘ë‹µ")}: {", ".join(summary.get("key_responses", []))}')
            doc.add_paragraph(f'{L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜")}: {summary.get("customer_sentiment_score", "N/A")}/100')
            doc.add_paragraph(f'{L.get("customer_satisfaction_score_label", "ê³ ê° ë§Œì¡±ë„ ì ìˆ˜")}: {summary.get("customer_satisfaction_score", "N/A")}/100')
            
            # ê³ ê° íŠ¹ì„±
            characteristics = summary.get('customer_characteristics', {})
            doc.add_heading(L.get("download_customer_characteristics", "ê³ ê° íŠ¹ì„±"), level=2)
            doc.add_paragraph(f'{L.get("language_label", "ì–¸ì–´")}: {characteristics.get("language", "N/A")}')
            doc.add_paragraph(f'{L.get("download_cultural_background", "ë¬¸í™”ì  ë°°ê²½")}: {characteristics.get("cultural_hints", "N/A")}')
            doc.add_paragraph(f'{L.get("region_label", "ì§€ì—­")}: {characteristics.get("region", "N/A")}')
            doc.add_paragraph(f'{L.get("download_communication_style", "ì†Œí†µ ìŠ¤íƒ€ì¼")}: {characteristics.get("communication_style", "N/A")}')
            
            # ê°œì¸ì •ë³´ ìš”ì•½
            privacy = summary.get('privacy_info', {})
            doc.add_heading(L.get("download_privacy_summary", "ê°œì¸ì •ë³´ ìš”ì•½"), level=2)
            doc.add_paragraph(f'{L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_email") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            doc.add_paragraph(f'{L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_phone") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            doc.add_paragraph(f'{L.get("download_address_provided", "ì£¼ì†Œ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_address") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            doc.add_paragraph(f'{L.get("download_region_hint", "ì§€ì—­ íŒíŠ¸")}: {privacy.get("region_hint", "N/A")}')
            
            # ì „ì²´ ìš”ì•½
            doc.add_paragraph(f'{L.get("download_overall_summary", "ì „ì²´ ìš”ì•½")}: {summary.get("summary", "N/A")}')
        
        # êµ¬ë¶„ì„ 
        if i < len(histories):
            doc.add_paragraph('-' * 80)
    
    doc.save(filepath)
    return filepath


def export_history_to_pptx(histories: List[Dict[str, Any]], filename: str = None, lang: str = "ko") -> str:
    """ì´ë ¥ì„ PPTX íŒŒì¼ë¡œ ì €ì¥"""
    if not IS_PPTX_AVAILABLE:
        raise ImportError("PPTX ì €ì¥ì„ ìœ„í•´ python-pptxê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install python-pptx")
    
    # ì–¸ì–´ ì„¤ì • í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    if lang not in ["ko", "en", "ja"]:
        lang = "ko"
    L = LANG.get(lang, LANG["ko"])
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx"
    filepath = os.path.join(DATA_DIR, filename)
    
    prs = Presentation()
    prs.slide_width = Inches(10)
    prs.slide_height = Inches(7.5)
    
    # ì œëª© ìŠ¬ë¼ì´ë“œ
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    title = slide.shapes.title
    subtitle = slide.placeholders[1]
    title.text = L.get("download_history_title", "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½")
    subtitle.text = f"{L.get('download_created_date', 'ìƒì„±ì¼')}: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    
    # ê° ì´ë ¥ì— ëŒ€í•´ ìŠ¬ë¼ì´ë“œ ìƒì„±
    for i, hist in enumerate(histories, 1):
        # ì œëª© ë° ë‚´ìš© ë ˆì´ì•„ì›ƒ
        bullet_slide_layout = prs.slide_layouts[1]
        slide = prs.slides.add_slide(bullet_slide_layout)
        shapes = slide.shapes
        
        title_shape = shapes.title
        body_shape = shapes.placeholders[1]
        
        title_shape.text = f"{L.get('download_history_number', 'ì´ë ¥ #')}{i}"
        
        tf = body_shape.text_frame
        tf.text = f"ID: {hist.get('id', 'N/A')}"
        
        p = tf.add_paragraph()
        p.text = f"{L.get('date_label', 'ë‚ ì§œ')}: {hist.get('timestamp', 'N/A')}"
        p.level = 0
        
        p = tf.add_paragraph()
        p.text = f"{L.get('download_initial_inquiry', 'ì´ˆê¸° ë¬¸ì˜')}: {hist.get('initial_query', 'N/A')}"
        p.level = 0
        
        p = tf.add_paragraph()
        p.text = f"{L.get('customer_type_label', 'ê³ ê° ìœ í˜•')}: {hist.get('customer_type', 'N/A')}"
        p.level = 0
        
        summary = hist.get('summary', {})
        if summary:
            p = tf.add_paragraph()
            p.text = f"{L.get('download_main_inquiry', 'ì£¼ìš” ë¬¸ì˜')}: {summary.get('main_inquiry', 'N/A')}"
            p.level = 0
            
            p = tf.add_paragraph()
            p.text = f"{L.get('sentiment_score_label', 'ê³ ê° ê°ì • ì ìˆ˜')}: {summary.get('customer_sentiment_score', 'N/A')}/100"
            p.level = 0
            
            p = tf.add_paragraph()
            p.text = f"{L.get('customer_satisfaction_score_label', 'ê³ ê° ë§Œì¡±ë„ ì ìˆ˜')}: {summary.get('customer_satisfaction_score', 'N/A')}/100"
            p.level = 0
    
    prs.save(filepath)
    return filepath


def export_history_to_pdf(histories: List[Dict[str, Any]], filename: str = None, lang: str = "ko") -> str:
    """ì´ë ¥ì„ PDF íŒŒì¼ë¡œ ì €ì¥ (í•œê¸€/ì¼ë³¸ì–´ ì¸ì½”ë”© ì§€ì› ê°•í™”)"""
    if not IS_REPORTLAB_AVAILABLE:
        raise ImportError("PDF ì €ì¥ì„ ìœ„í•´ reportlabì´ í•„ìš”í•©ë‹ˆë‹¤: pip install reportlab")
    
    # ì–¸ì–´ ì„¤ì • í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    if lang not in ["ko", "en", "ja"]:
        lang = "ko"
    L = LANG.get(lang, LANG["ko"])
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
    filepath = os.path.join(DATA_DIR, filename)
    
    # â­ ê°œì„ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ ì§€ì› ê°•í™” - ë‘˜ ë‹¤ ë“±ë¡í•˜ì—¬ í˜¼í•© ì‚¬ìš© ê°€ëŠ¥
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    
    # í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ ìƒíƒœ
    korean_font_registered = False
    japanese_font_registered = False
    korean_font_name = 'KoreanFont'
    japanese_font_name = 'JapaneseFont'
    
    def register_font(font_name: str, font_path: str) -> bool:
        """í°íŠ¸ë¥¼ ë“±ë¡í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        try:
            if font_path.endswith('.ttf'):
                # TTF íŒŒì¼ ë“±ë¡
                font = TTFont(font_name, font_path)
                pdfmetrics.registerFont(font)
                if font_name in pdfmetrics.getRegisteredFontNames():
                    return True
            elif font_path.endswith('.ttc'):
                # TTC íŒŒì¼ ì²˜ë¦¬ (ì—¬ëŸ¬ ì„œë¸Œí°íŠ¸ ì‹œë„)
                for subfont_idx in range(8):  # ì„œë¸Œí°íŠ¸ ì¸ë±ìŠ¤ í™•ëŒ€ (0-7)
                    try:
                        font = TTFont(font_name, font_path, subfontIndex=subfont_idx)
                        pdfmetrics.registerFont(font)
                        if font_name in pdfmetrics.getRegisteredFontNames():
                            return True
                    except Exception:
                        continue
            return False
        except Exception as e:
            print(f"âš ï¸ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ({font_name}, {font_path}): {e}")
            return False
    
    try:
        # ìš´ì˜ì²´ì œë³„ í°íŠ¸ ê²½ë¡œ ì„¤ì •
        import platform
        system = platform.system()
        
        if system == 'Windows':
            # Windows ê¸°ë³¸ í•œê¸€ í°íŠ¸ ê²½ë¡œ (ìš°ì„ ìˆœìœ„ ìˆœ)
            korean_font_paths = [
                "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”• (TTF)
                "C:/Windows/Fonts/malgunsl.ttf",  # ë§‘ì€ ê³ ë”• (TTF, ëŒ€ì²´)
                "C:/Windows/Fonts/NanumGothic.ttf",  # ë‚˜ëˆ”ê³ ë”•
                "C:/Windows/Fonts/NanumBarunGothic.ttf",  # ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•
                "C:/Windows/Fonts/NanumGothicBold.ttf",  # ë‚˜ëˆ”ê³ ë”• ë³¼ë“œ
                "C:/Windows/Fonts/gulim.ttc",  # êµ´ë¦¼ (TTC)
                "C:/Windows/Fonts/batang.ttc",  # ë°”íƒ• (TTC)
                "C:/Windows/Fonts/malgun.ttc",  # ë§‘ì€ ê³ ë”• (TTC)
                "C:/Windows/Fonts/NanumGothic.ttc",  # ë‚˜ëˆ”ê³ ë”• (TTC)
            ]
            
            # Windows ì¼ë³¸ì–´ í°íŠ¸ ê²½ë¡œ (í•œì ì§€ì› ê°•í™”)
            japanese_font_paths = [
                "C:/Windows/Fonts/msgothic.ttc",  # MS Gothic (ì¼ë³¸ì–´ í•œì ì§€ì›)
                "C:/Windows/Fonts/msmincho.ttc",  # MS Mincho (ì¼ë³¸ì–´ í•œì ì§€ì›)
                "C:/Windows/Fonts/meiryo.ttc",  # Meiryo (ì¼ë³¸ì–´)
                "C:/Windows/Fonts/yuanti.ttc",  # Microsoft YaHei (ì¤‘êµ­ì–´/ì¼ë³¸ì–´ í•œì ì§€ì›)
                "C:/Windows/Fonts/notosanscjksc-regular.otf",  # Noto Sans CJK (í•œì¤‘ì¼ í†µí•©)
            ]
        elif system == 'Darwin':  # macOS
            korean_font_paths = [
                "/System/Library/Fonts/Supplemental/AppleGothic.ttf",
                "/Library/Fonts/AppleGothic.ttf",
                "/System/Library/Fonts/AppleGothic.ttc",
            ]
            japanese_font_paths = [
                "/System/Library/Fonts/Supplemental/AppleGothic.ttf",  # í•œì¤‘ì¼ í†µí•©
                "/System/Library/Fonts/Hiragino Sans GB.ttc",
                "/System/Library/Fonts/STHeiti Light.ttc",
            ]
        else:  # Linux
            korean_font_paths = [
                "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
                "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            ]
            japanese_font_paths = [
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",  # í•œì¤‘ì¼ í†µí•©
                "/usr/share/fonts/truetype/takao/TakaoGothic.ttf",
            ]
        
        # í•œê¸€ í°íŠ¸ ë“±ë¡ (ëª¨ë“  ê²½ë¡œ ì‹œë„)
        for font_path in korean_font_paths:
            if os.path.exists(font_path):
                if register_font(korean_font_name, font_path):
                    korean_font_registered = True
                    print(f"âœ… í•œê¸€ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                    break
        
        # ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ (í•œê¸€ê³¼ ë…ë¦½ì ìœ¼ë¡œ ë“±ë¡ - ë‘˜ ë‹¤ ì‚¬ìš© ê°€ëŠ¥)
        for font_path in japanese_font_paths:
            if os.path.exists(font_path):
                if register_font(japanese_font_name, font_path):
                    japanese_font_registered = True
                    print(f"âœ… ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                    break
        
        # í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ê²½ê³ 
        if not korean_font_registered and not japanese_font_registered:
            print("âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDFì—ì„œ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print(f"   ì‹œìŠ¤í…œ: {system}")
            print("   ë“±ë¡ëœ í°íŠ¸ ëª©ë¡:", pdfmetrics.getRegisteredFontNames())
            if system == 'Windows':
                print("   í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”: C:/Windows/Fonts/")
            elif system == 'Darwin':
                print("   í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”: /System/Library/Fonts/")
            else:
                print("   í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”: /usr/share/fonts/")
            
    except Exception as e:
        error_msg = str(e)
        print(f"âš ï¸ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨: {error_msg}")
        korean_font_registered = False
        japanese_font_registered = False
    
    doc = SimpleDocTemplate(filepath, pagesize=A4)
    story = []
    styles = getSampleStyleSheet()
    
    # â­ ê°œì„ : í…ìŠ¤íŠ¸ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ í°íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” ìŠ¤íƒ€ì¼ ìƒì„± í•¨ìˆ˜
    def get_multilingual_style(base_style_name, default_font=None, **kwargs):
        """ë‹¤êµ­ì–´ ì§€ì› ìŠ¤íƒ€ì¼ ìƒì„± (í•œê¸€/ì¼ë³¸ì–´/ì˜ì–´)"""
        base_style = styles[base_style_name]
        style_kwargs = {
            'parent': base_style,
            **kwargs
        }
        
        # ê¸°ë³¸ í°íŠ¸ ì„¤ì • (í•œê¸€ ìš°ì„ , ì—†ìœ¼ë©´ ì¼ë³¸ì–´, ì—†ìœ¼ë©´ ê¸°ë³¸)
        registered_fonts = pdfmetrics.getRegisteredFontNames()
        if default_font and default_font in registered_fonts:
            style_kwargs['fontName'] = default_font
        elif korean_font_registered and korean_font_name in registered_fonts:
            style_kwargs['fontName'] = korean_font_name
        elif japanese_font_registered and japanese_font_name in registered_fonts:
            style_kwargs['fontName'] = japanese_font_name
        elif not korean_font_registered and not japanese_font_registered:
            print("âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return ParagraphStyle(f'Multilingual{base_style_name}', **style_kwargs)
    
    # ì œëª© ìŠ¤íƒ€ì¼ (í•œê¸€ í°íŠ¸ ìš°ì„  ì‚¬ìš©)
    title_style = get_multilingual_style(
        'Heading1',
        fontSize=24,
        textColor=black,
        spaceAfter=30,
        alignment=1,  # ì¤‘ì•™ ì •ë ¬
        default_font=korean_font_name if korean_font_registered else japanese_font_name
    )
    
    # ì¼ë°˜ í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
    normal_style = get_multilingual_style('Normal')
    heading1_style = get_multilingual_style('Heading1')
    heading2_style = get_multilingual_style('Heading2')
    
    # â­ ê°œì„ : í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ê³  ì ì ˆí•œ í°íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    def safe_text(text, detect_font=True):
        """í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ PDFì— í‘œì‹œ (í•œê¸€/ì¼ë³¸ì–´/í•œì ì§€ì› ê°•í™”)
        
        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            detect_font: í…ìŠ¤íŠ¸ ë‚´ìš©ì— ë”°ë¼ í°íŠ¸ë¥¼ ìë™ ì„ íƒí• ì§€ ì—¬ë¶€
        
        Returns:
            (ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸, ì¶”ì²œ í°íŠ¸ëª…) íŠœí”Œ
        """
        if text is None:
            return ("N/A", None)
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜ (UTF-8 ì¸ì½”ë”© ëª…ì‹œì  ì²˜ë¦¬)
        text_str = None
        if isinstance(text, bytes):
            # ë°”ì´íŠ¸ ë¬¸ìì—´ì¸ ê²½ìš° UTF-8ë¡œ ë””ì½”ë”© ì‹œë„
            try:
                text_str = text.decode('utf-8', errors='replace')
            except:
                try:
                    # UTF-8 ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
                    text_str = text.decode('cp949', errors='replace')  # í•œêµ­ì–´ Windows ì¸ì½”ë”©
                except:
                    try:
                        text_str = text.decode('shift_jis', errors='replace')  # ì¼ë³¸ì–´ ì¸ì½”ë”©
                    except:
                        try:
                            text_str = text.decode('euc-kr', errors='replace')  # í•œêµ­ì–´ EUC-KR
                        except:
                            text_str = text.decode('latin-1', errors='replace')
        else:
            text_str = str(text)
        
        # None ì²´í¬
        if text_str is None:
            return ("N/A", None)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFC í˜•ì‹ìœ¼ë¡œ í†µì¼) - í•œê¸€/ì¼ë³¸ì–´ ë¬¸ì ì •í™•ë„ í–¥ìƒ
        try:
            import unicodedata
            text_str = unicodedata.normalize('NFC', text_str)
        except:
            pass
        
        # íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ (HTML ì—”í‹°í‹°ë¡œ ë³€í™˜) - ReportLab ParagraphëŠ” HTMLì„ ì§€ì›
        # í•˜ì§€ë§Œ &ëŠ” ë¨¼ì € ì²˜ë¦¬í•´ì•¼ í•¨
        text_str = text_str.replace('&', '&amp;')
        text_str = text_str.replace('<', '&lt;')
        text_str = text_str.replace('>', '&gt;')
        text_str = text_str.replace('"', '&quot;')
        text_str = text_str.replace("'", '&#39;')
        
        # í°íŠ¸ ì„ íƒ ë¡œì§ (í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„)
        recommended_font = None
        if detect_font:
            try:
                # ìœ ë‹ˆì½”ë“œ ë²”ìœ„ í™•ì¸
                # í•œê¸€: AC00-D7AF (ì™„ì„±í˜•), 1100-11FF (ìëª¨)
                # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜: 3040-309F, ê°€íƒ€ì¹´ë‚˜: 30A0-30FF, í•œì: 4E00-9FFF
                has_korean = any('\uAC00' <= char <= '\uD7AF' or '\u1100' <= char <= '\u11FF' for char in text_str)
                has_japanese = any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FFF' for char in text_str)
                
                registered_fonts = pdfmetrics.getRegisteredFontNames()
                
                if has_korean and korean_font_registered and korean_font_name in registered_fonts:
                    recommended_font = korean_font_name
                elif has_japanese and japanese_font_registered and japanese_font_name in registered_fonts:
                    recommended_font = japanese_font_name
                elif has_korean or has_japanese:
                    # í•œê¸€/ì¼ë³¸ì–´ ë¬¸ìê°€ ìˆì§€ë§Œ ì ì ˆí•œ í°íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
                    if korean_font_registered and korean_font_name in registered_fonts:
                        recommended_font = korean_font_name
                    elif japanese_font_registered and japanese_font_name in registered_fonts:
                        recommended_font = japanese_font_name
                    else:
                        print(f"âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ í°íŠ¸ê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        print(f"   í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {text_str[:50]}")
                        print(f"   ë“±ë¡ëœ í°íŠ¸: {registered_fonts}")
            except Exception as check_error:
                # í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                pass
        
        return (text_str, recommended_font)
    
    # Paragraph ìƒì„± í—¬í¼ í•¨ìˆ˜ (í°íŠ¸ ìë™ ì„ íƒ)
    def create_paragraph(text, style, auto_font=True):
        """í…ìŠ¤íŠ¸ì™€ ìŠ¤íƒ€ì¼ë¡œ Paragraph ìƒì„± (í°íŠ¸ ìë™ ì„ íƒ)"""
        text_str, recommended_font = safe_text(text, detect_font=auto_font)
        
        # ì¶”ì²œ í°íŠ¸ê°€ ìˆê³  ìŠ¤íƒ€ì¼ì— í°íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°
        if recommended_font and auto_font:
            # ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ ìƒì„± (í°íŠ¸ í¬í•¨)
            from reportlab.lib.styles import ParagraphStyle
            style_with_font = ParagraphStyle(
                name=f'{style.name}_with_font',
                parent=style,
                fontName=recommended_font
            )
            return Paragraph(text_str, style_with_font)
        
        return Paragraph(text_str, style)
    
    # ì œëª© ì¶”ê°€
    title_text, _ = safe_text(L.get("download_history_title", "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½"))
    story.append(Paragraph(title_text, title_style))
    story.append(Spacer(1, 0.2*inch))
    
    # ê° ì´ë ¥ ì¶”ê°€
    for i, hist in enumerate(histories, 1):
        # ì´ë ¥ ì œëª©
        heading_text, _ = safe_text(f'{L.get("download_history_number", "ì´ë ¥ #")}{i}')
        story.append(Paragraph(heading_text, heading1_style))
        story.append(Spacer(1, 0.1*inch))
        
        # ê¸°ë³¸ ì •ë³´ (í°íŠ¸ ìë™ ì„ íƒ)
        id_text, _ = safe_text(f'ID: {hist.get("id", "N/A")}')
        story.append(create_paragraph(id_text, normal_style))
        
        timestamp_text, _ = safe_text(f'{L.get("date_label", "ë‚ ì§œ")}: {hist.get("timestamp", "N/A")}')
        story.append(create_paragraph(timestamp_text, normal_style))
        
        query_text, _ = safe_text(f'{L.get("download_initial_inquiry", "ì´ˆê¸° ë¬¸ì˜")}: {hist.get("initial_query", "N/A")}')
        story.append(create_paragraph(query_text, normal_style))
        
        customer_type_text, _ = safe_text(f'{L.get("customer_type_label", "ê³ ê° ìœ í˜•")}: {hist.get("customer_type", "N/A")}')
        story.append(create_paragraph(customer_type_text, normal_style))
        
        language_text, _ = safe_text(f'{L.get("language_label", "ì–¸ì–´")}: {hist.get("language_key", "N/A")}')
        story.append(create_paragraph(language_text, normal_style))
        
        summary = hist.get('summary', {})
        if summary:
            story.append(Spacer(1, 0.1*inch))
            summary_title, _ = safe_text(L.get("download_summary", "ìš”ì•½"))
            story.append(Paragraph(summary_title, heading2_style))
            
            main_inquiry_text, _ = safe_text(f'{L.get("download_main_inquiry", "ì£¼ìš” ë¬¸ì˜")}: {summary.get("main_inquiry", "N/A")}')
            story.append(create_paragraph(main_inquiry_text, normal_style))
            
            key_responses = summary.get("key_responses", [])
            if isinstance(key_responses, list):
                responses_list = []
                for r in key_responses:
                    r_text, _ = safe_text(r)
                    responses_list.append(r_text)
                responses_text = ", ".join(responses_list)
            else:
                responses_text, _ = safe_text(key_responses)
            responses_para_text, _ = safe_text(f'{L.get("download_key_response", "í•µì‹¬ ì‘ë‹µ")}: {responses_text}')
            story.append(create_paragraph(responses_para_text, normal_style))
            
            sentiment_text, _ = safe_text(f'{L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜")}: {summary.get("customer_sentiment_score", "N/A")}/100')
            story.append(create_paragraph(sentiment_text, normal_style))
            
            satisfaction_text, _ = safe_text(f'{L.get("customer_satisfaction_score_label", "ê³ ê° ë§Œì¡±ë„ ì ìˆ˜")}: {summary.get("customer_satisfaction_score", "N/A")}/100')
            story.append(create_paragraph(satisfaction_text, normal_style))
            
            characteristics = summary.get('customer_characteristics', {})
            story.append(Spacer(1, 0.1*inch))
            char_title, _ = safe_text(L.get("download_customer_characteristics", "ê³ ê° íŠ¹ì„±"))
            story.append(Paragraph(char_title, heading2_style))
            
            lang_char_text, _ = safe_text(f'{L.get("language_label", "ì–¸ì–´")}: {characteristics.get("language", "N/A")}')
            story.append(create_paragraph(lang_char_text, normal_style))
            
            cultural_text, _ = safe_text(f'{L.get("download_cultural_background", "ë¬¸í™”ì  ë°°ê²½")}: {characteristics.get("cultural_hints", "N/A")}')
            story.append(create_paragraph(cultural_text, normal_style))
            
            region_text, _ = safe_text(f'{L.get("region_label", "ì§€ì—­")}: {characteristics.get("region", "N/A")}')
            story.append(create_paragraph(region_text, normal_style))
            
            comm_style_text, _ = safe_text(f'{L.get("download_communication_style", "ì†Œí†µ ìŠ¤íƒ€ì¼")}: {characteristics.get("communication_style", "N/A")}')
            story.append(create_paragraph(comm_style_text, normal_style))
            
            privacy = summary.get('privacy_info', {})
            story.append(Spacer(1, 0.1*inch))
            privacy_title, _ = safe_text(L.get("download_privacy_summary", "ê°œì¸ì •ë³´ ìš”ì•½"))
            story.append(Paragraph(privacy_title, heading2_style))
            
            email_text, _ = safe_text(f'{L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_email") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            story.append(create_paragraph(email_text, normal_style))
            
            phone_text, _ = safe_text(f'{L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_phone") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            story.append(create_paragraph(phone_text, normal_style))
            
            address_text, _ = safe_text(f'{L.get("download_address_provided", "ì£¼ì†Œ ì œê³µ")}: {L.get("download_yes", "ì˜ˆ") if privacy.get("has_address") else L.get("download_no", "ì•„ë‹ˆì˜¤")}')
            story.append(create_paragraph(address_text, normal_style))
            
            region_hint_text, _ = safe_text(f'{L.get("download_region_hint", "ì§€ì—­ íŒíŠ¸")}: {privacy.get("region_hint", "N/A")}')
            story.append(create_paragraph(region_hint_text, normal_style))
            
            full_summary_text, _ = safe_text(f'{L.get("download_overall_summary", "ì „ì²´ ìš”ì•½")}: {summary.get("summary", "N/A")}')
            story.append(create_paragraph(full_summary_text, normal_style))
        
        # êµ¬ë¶„ì„ 
        if i < len(histories):
            story.append(Spacer(1, 0.2*inch))
            divider_text, _ = safe_text('-' * 80)
            story.append(Paragraph(divider_text, normal_style))
            story.append(Spacer(1, 0.2*inch))
    
    # PDF ë¹Œë“œ (UTF-8 ì¸ì½”ë”© ëª…ì‹œ, í°íŠ¸ ì„œë¸Œì…‹íŒ… ê°•í™”)
    try:
        # í°íŠ¸ ë“±ë¡ ìƒíƒœ í™•ì¸ ë° ê²½ê³ 
        registered_fonts = pdfmetrics.getRegisteredFontNames()
        print(f"ğŸ“‹ ë“±ë¡ëœ í°íŠ¸ ëª©ë¡: {registered_fonts}")
        
        if not korean_font_registered and not japanese_font_registered:
            print("âš ï¸ ê²½ê³ : í•œê¸€/ì¼ë³¸ì–´ í°íŠ¸ê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFì—ì„œ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("   ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•:")
            import platform
            system = platform.system()
            if system == 'Windows':
                print("   1. Windows í°íŠ¸ í´ë”(C:/Windows/Fonts/)ì— í•œê¸€ í°íŠ¸ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
                print("   2. ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰")
                print("   3. ë§‘ì€ ê³ ë”•(malgun.ttf) ë˜ëŠ” ë‚˜ëˆ”ê³ ë”•(NanumGothic.ttf) ì„¤ì¹˜ í™•ì¸")
            elif system == 'Darwin':
                print("   1. macOS ì‹œìŠ¤í…œ í°íŠ¸(/System/Library/Fonts/) í™•ì¸")
                print("   2. AppleGothic í°íŠ¸ ì„¤ì¹˜ í™•ì¸")
            else:
                print("   1. Linux ì‹œìŠ¤í…œ í°íŠ¸(/usr/share/fonts/) í™•ì¸")
                print("   2. Noto Sans CJK ë˜ëŠ” Nanum í°íŠ¸ ì„¤ì¹˜ í™•ì¸")
        else:
            if korean_font_registered:
                print(f"âœ… í•œê¸€ í°íŠ¸ ë“±ë¡ í™•ì¸: {korean_font_name} in {registered_fonts}")
            if japanese_font_registered:
                print(f"âœ… ì¼ë³¸ì–´ í°íŠ¸ ë“±ë¡ í™•ì¸: {japanese_font_name} in {registered_fonts}")
            print("âœ… í•œê¸€/ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë©ë‹ˆë‹¤.")
        
        # PDF ë¹Œë“œ ì‹¤í–‰ (í°íŠ¸ ì„œë¸Œì…‹íŒ… ìë™ ì ìš©)
        doc.build(story)
        print(f"âœ… PDF ìƒì„± ì™„ë£Œ: {filepath}")
        print(f"   íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath) / 1024:.2f} KB")
        
    except Exception as e:
        # ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì¬ì‹œë„
        error_msg = str(e)
        print(f"âš ï¸ PDF ë¹Œë“œ ì˜¤ë¥˜: {error_msg}")
        
        # í°íŠ¸ ê´€ë ¨ ì˜¤ë¥˜ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´ ì œê³µ
        if 'font' in error_msg.lower() or 'encoding' in error_msg.lower():
            print("   í°íŠ¸/ì¸ì½”ë”© ì˜¤ë¥˜ë¡œ ë³´ì…ë‹ˆë‹¤. í°íŠ¸ ë“±ë¡ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            registered_fonts = pdfmetrics.getRegisteredFontNames()
            print(f"   ë“±ë¡ëœ í°íŠ¸: {registered_fonts}")
            if korean_font_registered:
                print(f"   - í•œê¸€ í°íŠ¸: ë“±ë¡ë¨ ({korean_font_name})")
            else:
                print(f"   - í•œê¸€ í°íŠ¸: ë“±ë¡ë˜ì§€ ì•ŠìŒ")
            if japanese_font_registered:
                print(f"   - ì¼ë³¸ì–´ í°íŠ¸: ë“±ë¡ë¨ ({japanese_font_name})")
            else:
                print(f"   - ì¼ë³¸ì–´ í°íŠ¸: ë“±ë¡ë˜ì§€ ì•ŠìŒ")
        
        # ì¬ì‹œë„ (ë‹¨ìˆœ ì¬ì‹œë„ëŠ” ìœ„í—˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´)
        raise Exception(f"PDF ìƒì„± ì‹¤íŒ¨: {error_msg}")
    
    return filepath


# ========================================
# 6. RAG Helper (FAISS)
# ========================================
# RAG ê´€ë ¨ í•¨ìˆ˜ëŠ” ì‹œë®¬ë ˆì´í„°ì™€ ë¬´ê´€í•˜ë¯€ë¡œ ê¸°ì¡´ ì½”ë“œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

def load_documents(files) -> List[Document]:
    docs: List[Document] = []
    for f in files:
        name = f.name
        lower = name.lower()
        if lower.endswith(".pdf"):
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
            tmp.write(f.read())
            tmp.flush()
            tmp.close()
            loader = PyPDFLoader(tmp.name)
            file_docs = loader.load()
            for d in file_docs:
                d.metadata["source"] = name
            docs.extend(file_docs)
            try:
                os.remove(tmp.name)
            except OSError:
                pass
        elif lower.endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
        elif lower.endswith(".html") or lower.endswith(".htm"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
    return docs


def split_documents(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " ", ""],
    )
    return splitter.split_documents(docs)


def get_embedding_model():
    if get_api_key("openai"):
        try:
            return OpenAIEmbeddings(model="text-embedding-3-small")
        except:
            pass
    if get_api_key("gemini"):
        try:
            return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        except:
            pass
    return None


def get_embedding_function():
    """
    RAG ì„ë² ë”©ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ê²°ì •í•©ë‹ˆë‹¤.
    API í‚¤ ìœ íš¨ì„± ìˆœì„œ: OpenAI (ì‚¬ìš©ì ì„¤ì • ì‹œ) -> Gemini -> NVIDIA -> HuggingFace (fallback)
    API ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì´ë™í•˜ë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    # 1. OpenAI ì„ë² ë”© ì‹œë„ (ì‚¬ìš©ìê°€ ìœ íš¨í•œ í‚¤ë¥¼ ì„¤ì •í–ˆì„ ê²½ìš°)
    openai_key = get_api_key("openai")
    if openai_key:
        try:
            st.info("ğŸ”¹ RAG: OpenAI Embedding ì‚¬ìš© ì¤‘")
            return OpenAIEmbeddings(openai_api_key=openai_key)
        except Exception as e:
            st.warning(f"OpenAI ì„ë² ë”© ì‹¤íŒ¨ â†’ Geminië¡œ Fallback: {e}")

    # 2. Gemini ì„ë² ë”© ì‹œë„
    gemini_key = get_api_key("gemini")
    if IS_GEMINI_EMBEDDING_AVAILABLE and gemini_key:
        try:
            st.info("ğŸ”¹ RAG: Gemini Embedding ì‚¬ìš© ì¤‘")
            # â­ ìˆ˜ì •: ëª¨ë¸ ì´ë¦„ í˜•ì‹ì„ 'models/model-name'ìœ¼ë¡œ ìˆ˜ì •
            return GoogleGenerativeAIEmbeddings(google_api_key=gemini_key, model="models/text-embedding-004")
        except Exception as e:
            st.warning(f"Gemini ì„ë² ë”© ì‹¤íŒ¨ â†’ NVIDIAë¡œ Fallback: {e}")

    # 3. NVIDIA ì„ë² ë”© ì‹œë„
    nvidia_key = get_api_key("nvidia")
    if IS_NVIDIA_EMBEDDING_AVAILABLE and nvidia_key:
        try:
            st.info("ğŸ”¹ RAG: NVIDIA Embedding ì‚¬ìš© ì¤‘")
            # NIM ëª¨ë¸ ì‚¬ìš© (ì‹¤ì œ í‚¤ê°€ ìœ íš¨í•´ì•¼ í•¨)
            return NVIDIAEmbeddings(api_key=nvidia_key, model="ai-embed-qa-4")
        except Exception as e:
            st.warning(f"NVIDIA ì„ë² ë”© ì‹¤íŒ¨ â†’ HuggingFace Fallback: {e}")

    # 4. HuggingFace Embeddings (Local Fallback)
    try:
        st.info("ğŸ”¹ RAG: Local HuggingFace Embedding ì‚¬ìš© ì¤‘")
        # ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    except Exception as e:
        st.warning(f"ìµœì¢… Fallback ì„ë² ë”© ì‹¤íŒ¨: {e}")

    st.error("âŒ RAG ì„ë² ë”© ì‹¤íŒ¨: ì‚¬ìš© ê°€ëŠ¥í•œ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    return None


def build_rag_index(files):
    # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    if not files: return None, 0

    # ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‹œë„í•˜ëŠ” ê³¼ì •ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ try-exceptë¡œ ê°ìŒ‰ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception as e:
        st.error(f"RAG ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, 0

    if embeddings is None:
        # ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ë„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŒì„ ì•Œë¦¼
        error_msg = L["rag_embed_error_none"]

        # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ êµ¬ì„± (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)
        if not get_api_key("openai"):
            error_msg += f"\n- {L['rag_embed_error_openai']}"
        if not get_api_key("gemini"):
            error_msg += f"\n- {L['rag_embed_error_gemini']}"
        if not get_api_key("nvidia"):
            error_msg += f"\n- {L['rag_embed_error_nvidia']}"

        st.error(error_msg)
        return None, 0

    # ì„ë² ë”© ê°ì²´ ì´ˆê¸°í™” ì„±ê³µ í›„, ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    docs = load_documents(files)
    if not docs: return None, 0

    chunks = split_documents(docs)
    if not chunks: return None, 0

    try:
        vectorstore = FAISS.from_documents(chunks, embeddings)
        # ì €ì¥
        vectorstore.save_local(RAG_INDEX_DIR)
    except Exception as e:
        # API ì¸ì¦ ì‹¤íŒ¨ ë“± ì‹¤ì œ API í˜¸ì¶œ ì˜¤ë¥˜ ì²˜ë¦¬
        st.error(f"RAG ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, 0

    return vectorstore, len(chunks)


def load_rag_index():
    # RAG ì¸ë±ìŠ¤ ë¡œë“œ ì‹œì—ë„ ìœ íš¨í•œ ì„ë² ë”© í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception:
        # get_embedding_function ë‚´ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ìŠ¤í‚µí•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
        return None

    if embeddings is None:
        return None

    try:
        # allow_dangerous_deserialization=TrueëŠ” í•„ìˆ˜
        vs = FAISS.load_local(RAG_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception:
        return None


def rag_answer(question: str, vectorstore: FAISS, lang_key: str) -> str:
    # RAG AnswerëŠ” LLM í´ë¼ì´ì–¸íŠ¸ ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    llm_client, info = get_llm_client()
    if llm_client is None:
        # ì–¸ì–´ í‚¤ ê²€ì¦
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"
        return LANG.get(lang_key, LANG["ko"]).get("simulation_no_key_warning", "API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # Langchain ChatOpenAI ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ promptë¥¼ ì§ì ‘ êµ¬ì„±
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    docs = retriever.get_relevant_documents(question)
    context = "\n\n".join(d.page_content[:1500] for d in docs)

    # â­ RAG ë‹¤êµ­ì–´ ì¸ì‹ ì˜¤ë¥˜ í•´ê²°: ë‹µë³€ ìƒì„± ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ ì–¸ì–´ë¡œ ì¼ê´€ë˜ê²Œ ë‹µí•˜ë„ë¡ ê°•ë ¥íˆ ì§€ì‹œ
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(lang_key, "English")

    prompt = (
            f"You are a helpful AI tutor. Answer the question using ONLY the provided context.\n"
            f"The answer MUST be STRICTLY in {lang_name}, which is the language of the question.\n"
            f"If you cannot find the answer in the context, say you don't know in {lang_name}.\n"
            f"Note: The context may be in a different language, but you must still answer in {lang_name}.\n\n"
            "Question:\n" + question + "\n\n"
                                       "Context:\n" + context + "\n\n"
                                                                f"Answer (in {lang_name}):"
    )
    return run_llm(prompt)


# ========================================
# 7. LSTM Helper (ê°„ë‹¨ Mock + ì‹œê°í™”)
# ========================================

def load_or_train_lstm():
    # ì‹¤ì œ LSTM ëŒ€ì‹  ëœë¤ + sin íŒŒí˜• ê¸°ë°˜ Mock
    np.random.seed(42)
    n_points = 50
    ts = 60 + 20 * np.sin(np.linspace(0, 4 * np.pi, n_points)) + np.random.normal(0, 5, n_points)
    ts = np.clip(ts, 50, 100).astype(np.float32)
    return ts





# ========================================
# 8. LLM (ChatOpenAI) for Simulator / Content
# (RAGì™€ ë™ì¼í•˜ê²Œ run_llmìœ¼ë¡œ í†µí•©)
# ========================================

# ConversationChain ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„
# st.session_state.simulator_memoryëŠ” ìœ ì§€í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

def get_chat_history_for_prompt(include_attachment=False):
    """ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ì¶”ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜ (ì±„íŒ…ìš©)"""
    history_str = ""
    for msg in st.session_state.simulator_messages:
        role = msg["role"]
        content = msg["content"]
        if role == "customer" or role == "customer_rebuttal":
            history_str += f"Customer: {content}\n"
        elif role == "agent_response":
            history_str += f"Agent: {content}\n"
        # supervisor ë©”ì‹œì§€ëŠ” LLMì— ì „ë‹¬í•˜ì§€ ì•Šì•„ ì—­í•  í˜¼ë™ ë°©ì§€
    return history_str


def generate_customer_reaction(current_lang_key: str, is_call: bool = False) -> str:
    """
    ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ì„ ìƒì„±í•˜ëŠ” LLM í˜¸ì¶œ (ì±„íŒ… ì „ìš©)
    **ìˆ˜ì • ì‚¬í•­:** ì—ì´ì „íŠ¸ ì •ë³´ ìš”ì²­ ì‹œ í•„ìˆ˜ ì •ë³´ (ì£¼ë¬¸ë²ˆí˜¸, eSIM, ìë…€ ë§Œ ë‚˜ì´, ì·¨ì†Œ ì‚¬ìœ ) ì œê³µ ì˜ë¬´ë¥¼ ê°•í™”í•¨.
    """
    history_text = get_chat_history_for_prompt()
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG.get(current_lang_key, LANG["ko"])

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        # LLMì—ê²Œ ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì—ì´ì „íŠ¸ì—ê²Œ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    next_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

Read the following conversation and respond naturally in {lang_name}.

Conversation so far:
{history_text}

RULES:
1. You are only the customer. Do not write as the agent.
2. **[CRITICAL: Mandatory Information Submission for Problem Resolution]** If the agent requested any of the following critical information, you MUST provide it:
    - Order/Booking Number (e.g., ABC123, 123456)
    - eSIM related details (e.g., Host device compatibility, local status/location, time of activation)
    - Child-related product details (e.g., Child's Date of Birth or Current Age)
    - Exception/Refund Reason (e.g., flight cancellation/delay, illness, local natural disaster)
    - **If you are a difficult customer and the agent requests this information, you MUST still provide it, but you may express frustration or impatience while doing so.**
3. **[Crucial Rule for Repetition/New Inquiry]** After the agent has provided an attempt at a solution or answer:
    - If you are still confused or the problem is not fully solved, you MUST state the remaining confusion/problem clearly and briefly. DO NOT REPEAT THE INITIAL QUERY. Focus only on the unresolved aspect or the new inquiry.
4. **[CRITICAL: Solution Acknowledgment]** If the agent provided a clear and accurate solution/confirmation:
    - You MUST respond with appreciation and satisfaction, like "{L_local['customer_positive_response']}" or similar positive acknowledgment. This applies even if you are a difficult customer.
5. If the agent's LAST message was the closing confirmation: "{L_local['customer_closing_confirm']}"
    - If you have NO additional questions: You MUST reply with "{L_local['customer_no_more_inquiries']}".
    - If you DO have additional questions: You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS IMMEDIATELY. DO NOT just repeat that you have an additional question.
6. Do NOT repeat your initial message or previous responses unless necessary.
7. Output ONLY the customer's next message.
"""
    try:
        reaction = run_llm(next_prompt)

        # â­ LLMì´ ì‘ë‹µí–ˆì§€ë§Œ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš°, ê¸ì • ì¢…ë£Œ ë¬¸êµ¬ë¥¼ ë°˜í™˜
        if not reaction or len(reaction.strip()) < 5:
            print("LLM returned insufficient response. Using positive closing fallback.")
            return L_local['customer_positive_response']

        return reaction.strip()
    except Exception as e:
        # â­ LLM í˜¸ì¶œ ìì²´ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ (API í‚¤, í• ë‹¹ëŸ‰) ê¸ì • ì¢…ë£Œ ë¬¸êµ¬ë¥¼ ê°•ì œ ë°˜í™˜
        print(f"LLM Customer Reaction generation failed: {e}. Falling back to positive closing.")
        return L_local['customer_positive_response']  # ê°•ì œ ì•ˆì „ì¥ì¹˜


def summarize_history_with_ai(current_lang_key: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ë¥¼ ì •ë¦¬í•˜ì—¬ LLMì— ì „ë‹¬í•˜ê³  ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ë°›ëŠ” í•¨ìˆ˜."""
    # ì „í™” ë¡œê·¸ëŠ” 'phone_exchange' ì—­í• ì„ ê°€ì§€ê±°ë‚˜, 'initial_query'ì— í¬í•¨ë˜ì–´ ìˆìŒ

    # 1. ë¡œê·¸ ì¶”ì¶œ
    conversation_text = ""
    initial_query = st.session_state.get("call_initial_query", "N/A")
    website_url = st.session_state.get("call_website_url", "").strip()
    if initial_query and initial_query != "N/A":
        conversation_text += f"Initial Query: {initial_query}\n"
    if website_url:
        conversation_text += f"Website URL: {website_url}\n"

    for msg in st.session_state.simulator_messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "phone_exchange":
            # phone_exchangeëŠ” "Agent: ... | Customer: ..." í˜•íƒœë¡œ ì´ë¯¸ ì •ë¦¬ë˜ì–´ ìˆìŒ
            conversation_text += f"{content}\n"

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    summary_prompt = f"""
You are an AI Analyst specialized in summarizing customer phone calls. 
Analyze the full conversation log below, identify the main issue, the steps taken by the agent, and the customer's sentiment.

Provide a concise, easy-to-read summary of the key exchange STRICTLY in {lang_name}.

--- Conversation Log ---
{conversation_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return "LLM Keyê°€ ì—†ì–´ ìš”ì•½ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤."

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}"


def generate_customer_reaction_for_call(current_lang_key: str, last_agent_response: str) -> str:
    """ì „í™” ì‹œë®¬ë ˆì´í„° ì „ìš© ê³ ê° ë°˜ì‘ ìƒì„± (ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µ ì¤‘ì‹¬)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]
    
    # â­ ì¶”ê°€: ê³ ê° ì„±ë³„ ë° ê°ì • ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
    customer_gender = st.session_state.customer_avatar.get("gender", "male")
    customer_emotion = st.session_state.customer_avatar.get("state", "NEUTRAL")
    
    # ê°ì • ìƒíƒœì— ë”°ë¥¸ í†¤ ì„¤ì •
    emotion_tone_map = {
        "HAPPY": "friendly, positive, and satisfied",
        "ASKING": "slightly frustrated, questioning, and seeking clarification",
        "ANGRY": "angry, frustrated, and demanding",
        "SAD": "sad, depressed, and disappointed",
        "NEUTRAL": "neutral, calm, and polite"
    }
    emotion_tone = emotion_tone_map.get(customer_emotion, "neutral, calm, and polite")
    
    gender_pronoun = "she" if customer_gender == "female" else "he"
    
    # â­ ì¶”ê°€: ì—ì´ì „íŠ¸ê°€ ì¢…ë£Œ í™•ì¸ ì§ˆë¬¸ì„ í–ˆëŠ”ì§€ í™•ì¸
    closing_msg = L_local['customer_closing_confirm']
    is_closing_question = closing_msg in last_agent_response or any(
        phrase in last_agent_response.lower() 
        for phrase in ["ë‹¤ë¥¸ ë¬¸ì˜", "ì¶”ê°€ ë¬¸ì˜", "ë‹¤ë¥¸ ë„ì›€", "anything else", "other questions"]
    )
    
    # â­ ìˆ˜ì •: ì´ˆê¸° ë¬¸ì˜ë¥¼ ì™„ì „íˆ ì œê±°í•˜ê³  ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µì—ë§Œ ì§‘ì¤‘
    # ìµœê·¼ ëŒ€í™” ì´ë ¥ë§Œ ì¶”ì¶œ (ìµœëŒ€ 3-4ê°œ êµí™˜ë§Œ)
    recent_exchanges = []
    for msg in reversed(st.session_state.simulator_messages):  # ì—­ìˆœìœ¼ë¡œ ìµœê·¼ ê²ƒë¶€í„°
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        if role == "phone_exchange":
            recent_exchanges.insert(0, content)  # ì•ì— ì‚½ì…í•˜ì—¬ ìˆœì„œ ìœ ì§€
            if len(recent_exchanges) >= 3:  # ìµœê·¼ 3ê°œë§Œ
                break
        elif role == "agent":
            recent_exchanges.insert(0, f"Agent: {content}")
            if len(recent_exchanges) >= 3:
                break
    
    # ìµœê·¼ ëŒ€í™” ì´ë ¥ (ìˆëŠ” ê²½ìš°ë§Œ)
    recent_history = "\n".join(recent_exchanges) if recent_exchanges else "(No previous exchanges)"
    
    website_url = st.session_state.get("call_website_url", "").strip()
    website_context = f"\nWebsite URL: {website_url}" if website_url else ""
    
    # â­ ìˆ˜ì •: ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µë§Œ ê°•ì¡° (ì´ˆê¸° ë¬¸ì˜ ì™„ì „ ì œê±°)
    last_agent_text = last_agent_response.strip() if last_agent_response else "None"
    
    history_text = f"""[Recent Conversation Context - For Reference Only]
{recent_history}{website_context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ YOUR TASK: Respond ONLY to the Agent's message below
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Agent just said: "{last_agent_text}"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IMPORTANT: 
- Respond DIRECTLY to what the agent JUST SAID above
- DO NOT repeat your initial query
- DO NOT refer to old conversation unless agent asks
- Keep your response short and conversational
- Your emotional state: {customer_emotion} - respond with {emotion_tone} tone
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""

    # â­ ì¶”ê°€: ì¢…ë£Œ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
    if is_closing_question:
        call_prompt = f"""
You are a CUSTOMER in a phone call. You are a {customer_gender} customer. Respond naturally in {lang_name}.

Your current emotional state: {customer_emotion}
Your response tone should be: {emotion_tone}

{history_text}

The agent just asked: "{last_agent_text}"

CRITICAL RULES FOR CLOSING CONFIRMATION:
1. If you have NO additional questions and the conversation is resolved:
   - You MUST reply with: "{L_local['customer_no_more_inquiries']}"
2. If you DO have additional questions or the issue is NOT fully resolved:
   - You MUST reply with: "{L_local['customer_has_additional_inquiries']}" AND immediately state your additional question
3. Your response MUST be ONLY one of the two options above, in {lang_name}.
4. Output ONLY the customer's response (must be one of the two rule options).

Your response (respond to the closing confirmation question):
"""
    else:
        call_prompt = f"""
You are a CUSTOMER in a phone call. You are a {customer_gender} customer. Respond naturally in {lang_name}.

Your current emotional state: {customer_emotion}
Your response tone should be: {emotion_tone}

{history_text}

RULES:
1. Respond ONLY to what the agent JUST SAID: "{last_agent_text}"
2. If agent asked a question â†’ Answer it
3. If agent requested information â†’ Provide it
4. If agent gave a solution â†’ Acknowledge based on your emotional state ({customer_emotion})
5. Keep your response short (1-2 sentences max)
6. DO NOT repeat your initial query
7. DO NOT mention old conversation
8. IMPORTANT: Match your tone to your emotional state ({customer_emotion}) - be {emotion_tone}

Your response (respond ONLY to the agent's message above, with {emotion_tone} tone):
"""
    try:
        reaction = run_llm(call_prompt)
        reaction_text = reaction.strip()
        
        # â­ ì¶”ê°€: ì¢…ë£Œ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ê²€ì¦ ë° ê°•ì œ ì ìš©
        if is_closing_question:
            if L_local['customer_no_more_inquiries'] in reaction_text:
                return L_local['customer_no_more_inquiries']
            elif L_local['customer_has_additional_inquiries'] in reaction_text:
                return reaction_text  # ì¶”ê°€ ë¬¸ì˜ ë‚´ìš© í¬í•¨ ê°€ëŠ¥
            else:
                # LLMì´ ê·œì¹™ì„ ë”°ë¥´ì§€ ì•Šìœ¼ë©´, ëŒ€í™”ê°€ í•´ê²°ëœ ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê³  ì¢…ë£Œ ì‘ë‹µ ë°˜í™˜
                return L_local['customer_no_more_inquiries']
        
        return reaction_text
    except Exception as e:
        return f"âŒ ê³ ê° ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}"


def generate_customer_reaction_for_first_greeting(current_lang_key: str, agent_greeting: str, initial_query: str) -> str:
    """ì „í™” ì‹œë®¬ë ˆì´í„° ì „ìš©: ì²« ì¸ì‚¬ë§ì— ëŒ€í•œ ê³ ê°ì˜ ë§ì¶¤í˜• ë°˜ì‘ ìƒì„± (ì´ˆê¸° ë¬¸ì˜ ê³ ë ¤)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]
    
    # â­ ì¶”ê°€: ê³ ê° ì„±ë³„ ë° ê°ì • ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
    customer_gender = st.session_state.customer_avatar.get("gender", "male")
    customer_emotion = st.session_state.customer_avatar.get("state", "NEUTRAL")
    
    # ê°ì • ìƒíƒœì— ë”°ë¥¸ í†¤ ì„¤ì •
    emotion_tone_map = {
        "HAPPY": "friendly, positive, and satisfied",
        "ASKING": "slightly frustrated, questioning, and seeking clarification",
        "ANGRY": "angry, frustrated, and demanding",
        "SAD": "sad, depressed, and disappointed",
        "NEUTRAL": "neutral, calm, and polite"
    }
    emotion_tone = emotion_tone_map.get(customer_emotion, "neutral, calm, and polite")
    
    website_url = st.session_state.get("call_website_url", "").strip()
    website_context = f"\nWebsite URL: {website_url}" if website_url else ""
    
    agent_greeting_text = agent_greeting.strip() if agent_greeting else "None"
    initial_query_text = initial_query.strip() if initial_query else "None"
    
    call_prompt = f"""
You are a CUSTOMER in a phone call. You are a {customer_gender} customer. Respond naturally in {lang_name}.

Your current emotional state: {customer_emotion}
Your response tone should be: {emotion_tone}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ YOUR SITUATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

You called because: "{initial_query_text}"

The agent just greeted you and said: "{agent_greeting_text}"
{website_context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOUR TASK: Respond to the agent's greeting in a way that:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Acknowledge the agent's greeting naturally
2. Briefly mention your inquiry/concern: "{initial_query_text}"
3. Show that you're ready to discuss your issue
4. Keep it conversational and natural (1-2 sentences max)
5. DO NOT be overly formal - this is a phone call, be natural
6. IMPORTANT: Match your tone to your emotional state ({customer_emotion}) - be {emotion_tone}

Example good responses (adjust tone based on your emotional state):
- If {customer_emotion}: [Respond with {emotion_tone} tone]
- "Hello, thank you. I'm calling because [brief mention of issue]..."
- "Hi, yes. I need help with [your issue]..."
- "Thank you. I have a question about [your issue]..."

Your response (respond naturally to the greeting and briefly mention your inquiry, with {emotion_tone} tone):
"""
    try:
        reaction = run_llm(call_prompt)
        return reaction.strip()
    except Exception as e:
        return f"âŒ ê³ ê° ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}"


def summarize_history_for_call(call_logs: List[Dict[str, str]], initial_query: str, current_lang_key: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ë³¸ì„ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ë¡œê·¸ ì¬êµ¬ì„± (phone_exchange ì—­í• ë§Œ ì‚¬ìš©)
    full_log_text = f"--- Initial Customer Query ---\nCustomer: {initial_query}\n"
    for log in call_logs:
        if log["role"] == "phone_exchange":
            full_log_text += f"{log['content']}\n"
        elif log["role"] == "agent" and "content" in log:
            # ìµœì´ˆ ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ì€ ì—¬ê¸°ì— í¬í•¨
            full_log_text += f"Agent (Greeting): {log['content']}\n"

    summary_prompt = f"""
You are an AI Supervisor. Analyze the following telephone support conversation log.
Provide a concise, neutral summary of the key issue, the steps taken by the agent, and the final outcome.
The summary MUST be STRICTLY in {lang_name}.

--- Conversation Log ---
{full_log_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key is missing. Cannot generate summary. Log length: {len(full_log_text.splitlines())}"

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ Summary Generation Error: {e}"


def generate_customer_closing_response(current_lang_key: str) -> str:
    """ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ê³ ê°ì˜ ìµœì¢… ë‹µë³€ ìƒì„± (ì±„íŒ…ìš©)"""
    history_text = get_chat_history_for_prompt()
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG.get(current_lang_key, LANG["ko"])  # â­ ìˆ˜ì •: í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ íŒ©

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ì˜ ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ì¸ì§€ í™•ì¸ (í”„ë¡¬í”„íŠ¸ì— í¬í•¨)
    closing_msg = L_local['customer_closing_confirm']

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    final_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

The agent's final message was the closing confirmation: "{closing_msg}".
You MUST respond to this confirmation based on the overall conversation.

Conversation history:
{history_text}

RULES:
1. If the conversation seems resolved and you have NO additional questions:
    - You MUST reply with "{L_local['customer_no_more_inquiries']}".
2. If the conversation is NOT fully resolved and you DO have additional questions (or the agent provided a cancellation denial that you want to appeal):
    - You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS. DO NOT just repeat that you have an additional question.
3. Your reply MUST be ONLY one of the two options above, in {lang_name}.
4. Output ONLY the customer's next message (must be one of the two rule options).
"""
    try:
        reaction = run_llm(final_prompt)
        # LLMì˜ ì¶œë ¥ì´ ê·œì¹™ì„ ë”°ë¥´ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê°•ì œ ì ìš©
        reaction_text = reaction.strip()
        # "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤"ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìƒì„¸ ë‚´ìš© í¬í•¨ ê°€ì •)
        if L_local['customer_no_more_inquiries'] in reaction_text:
            return L_local['customer_no_more_inquiries']
        elif L_local['customer_has_additional_inquiries'] in reaction_text:
            return reaction_text
        else:
            # LLMì´ ê·œì¹™ì„ ì–´ê²¼ì„ ê²½ìš°, "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆë‹¤"ê³  ê°€ì •í•˜ê³  ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ë„˜ê¹€
            return L_local['customer_has_additional_inquiries']
    except Exception as e:
        st.error(f"ê³ ê° ìµœì¢… ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}")
        return L_local['customer_has_additional_inquiries']  # ì˜¤ë¥˜ ì‹œ ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ë„


# ----------------------------------------
# Initial Advice/Draft Generation (ì´ê´€ í›„ ì¬ì‚¬ìš©) (ìš”ì²­ 4 ë°˜ì˜)
# ----------------------------------------
def generate_agent_first_greeting(lang_key: str, initial_query: str) -> str:
    """ì „í™” í†µí™” ì‹œì‘ ì‹œ ì—ì´ì „íŠ¸ì˜ ì²« ì¸ì‚¬ë§ì„ ìƒì„± (ì„ì‹œ í•¨ìˆ˜)"""
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"
    L_local = LANG.get(lang_key, LANG["ko"])
    # ë¬¸ì˜ ë‚´ìš©ì˜ ì²« 10ìë§Œ ì‚¬ìš© (too long)
    topic = initial_query.strip()[:15].replace('\n', ' ')
    if len(initial_query.strip()) > 15:
        topic += "..."

    if lang_key == 'ko':
        return f"ì•ˆë…•í•˜ì„¸ìš”, {topic} ê´€ë ¨ ë¬¸ì˜ ì£¼ì…¨ì£ ? ìƒë‹´ì› 000ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    elif lang_key == 'en':
        return f"Hello, thank you for calling. I see you're calling about {topic}. My name is 000. How may I help you today?"
    elif lang_key == 'ja':
        return f"ãŠé›»è©±ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚{topic}ã®ä»¶ã§ã™ã­ã€‚æ‹…å½“ã®000ã¨ç”³ã—ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹?"
    return "Hello, how may I help you?"


def detect_text_language(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•©ë‹ˆë‹¤.
    Returns: "ko", "en", "ja" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "ko")
    """
    if not text or not text.strip():
        return "ko"  # ê¸°ë³¸ê°’
    
    try:
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì¼ë³¸ì–´ ë¬¸ì(íˆë¼ê°€ë‚˜, ê°€íƒ€ì¹´ë‚˜, í•œì)ê°€ ë§ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¼ë³¸ì–´
        japanese_chars = sum(1 for char in text if '\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF')
        if japanese_chars > len(text) * 0.1:  # 10% ì´ìƒ ì¼ë³¸ì–´ ë¬¸ì
            return "ja"
        
        # ì˜ì–´ ë¬¸ì ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ ì˜ì–´
        english_chars = sum(1 for char in text if char.isascii() and char.isalpha())
        if english_chars > len(text) * 0.7:  # 70% ì´ìƒ ì˜ì–´ ë¬¸ì
            return "en"
        
        # LLMì„ ì‚¬ìš©í•œ ì •í™•í•œ ì–¸ì–´ ê°ì§€ ì‹œë„ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‹œí•˜ê³  íœ´ë¦¬ìŠ¤í‹± ê²°ê³¼ ì‚¬ìš©)
        if st.session_state.is_llm_ready:
            try:
                detection_prompt = f"""Detect the language of the following text. Respond with ONLY one word: "ko" (Korean), "en" (English), or "ja" (Japanese).

Text: {text[:200]}

Language:"""
                detected = run_llm(detection_prompt).strip().lower()
                # ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‚¬ìš©
                if detected and detected not in ["âŒ", "error", "failed"] and detected in ["ko", "en", "ja"]:
                    return detected
            except Exception as e:
                # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹± ê²°ê³¼ ì‚¬ìš©
                print(f"Language detection LLM call failed: {e}")
                pass
    except Exception as e:
        # ì „ì²´ í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        print(f"Language detection error: {e}")
        return "ko"
    
    # ê¸°ë³¸ê°’: í•œêµ­ì–´
    return "ko"


def analyze_customer_profile(customer_query: str, current_lang_key: str = None) -> Dict[str, Any]:
    """ì‹ ê·œ ê³ ê°ì˜ ë¬¸ì˜ì‚¬í•­ê³¼ ë§íˆ¬ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ê°ì„±í–¥ ì ìˆ˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì‚° (ìš”ì²­ 4)"""
    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    try:
        detected_lang = detect_text_language(customer_query)
    except Exception as e:
        print(f"Language detection failed in analyze_customer_profile: {e}")
        detected_lang = "ko"  # ê¸°ë³¸ê°’ ì‚¬ìš©
    
    # current_lang_keyê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©
    lang_key_to_use = current_lang_key if current_lang_key else detected_lang
    # lang_key_to_useê°€ ìœ íš¨í•œì§€ í™•ì¸
    if lang_key_to_use not in ["ko", "en", "ja"]:
        lang_key_to_use = "ko"  # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
    
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[lang_key_to_use]

    analysis_prompt = f"""
You are an AI analyst analyzing a customer's inquiry to determine their profile and sentiment.

Analyze the following customer inquiry and provide a structured analysis in JSON format (ONLY JSON, no markdown).

Analyze:
1. Customer gender (male/female/unknown - analyze based on name, language patterns, or cultural hints)
2. Customer sentiment score (0-100, where 0=very negative/angry, 50=neutral, 100=very positive/happy)
3. Communication style (formal/casual, brief/detailed, polite/direct)
4. Urgency level (low/medium/high)
5. Customer type prediction (normal/difficult/very_dissatisfied)
6. Language and cultural hints (if any)
7. Key concerns or pain points

Output format (JSON only):
{{
  "gender": "male",
  "sentiment_score": 45,
  "communication_style": "brief, direct, slightly frustrated",
  "urgency_level": "high",
  "predicted_customer_type": "difficult",
  "cultural_hints": "unknown",
  "key_concerns": ["issue 1", "issue 2"],
  "tone_analysis": "brief description of tone"
}}

Customer Inquiry:
{customer_query}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        return {
            "gender": "unknown",
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": "Unable to analyze"
        }

    try:
        analysis_text = run_llm(analysis_prompt).strip()
        # JSON ì¶”ì¶œ
        if "```json" in analysis_text:
            analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
        elif "```" in analysis_text:
            analysis_text = analysis_text.split("```")[1].split("```")[0].strip()

        import json
        analysis_data = json.loads(analysis_text)
        return analysis_data
    except Exception as e:
        return {
            "gender": "unknown",
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": f"Analysis error: {str(e)}"
        }


def find_similar_cases(customer_query: str, customer_profile: Dict[str, Any], current_lang_key: str,
                       limit: int = 5) -> List[Dict[str, Any]]:
    """ì €ì¥ëœ ìš”ì•½ ë°ì´í„°ì—ì„œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì•„ ë°˜í™˜ (ìš”ì²­ 4)"""
    histories = load_simulation_histories_local(current_lang_key)

    if not histories:
        return []

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
           and not h.get("is_call", False)  # ì „í™” ì´ë ¥ ì œì™¸
    ]

    if not cases_with_summary:
        return []

    # ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ + ì ìˆ˜ ìœ ì‚¬ë„)
    similar_cases = []
    query_lower = customer_query.lower()
    customer_sentiment = customer_profile.get("sentiment_score", 50)
    customer_style = customer_profile.get("communication_style", "")

    for case in cases_with_summary:
        summary = case.get("summary", {})
        main_inquiry = summary.get("main_inquiry", "").lower()
        case_sentiment = summary.get("customer_sentiment_score", 50)
        case_satisfaction = summary.get("customer_satisfaction_score", 50)

        # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        similarity_score = 0

        # 1. ë¬¸ì˜ ë‚´ìš© ìœ ì‚¬ë„ (í‚¤ì›Œë“œ ë§¤ì¹­)
        query_words = set(query_lower.split())
        inquiry_words = set(main_inquiry.split())
        if query_words and inquiry_words:
            word_overlap = len(query_words & inquiry_words) / len(query_words | inquiry_words)
            similarity_score += word_overlap * 40

        # 2. ê°ì • ì ìˆ˜ ìœ ì‚¬ë„
        sentiment_diff = abs(customer_sentiment - case_sentiment)
        sentiment_similarity = max(0, 1 - (sentiment_diff / 100)) * 30
        similarity_score += sentiment_similarity

        # 3. ë§Œì¡±ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì¼€ì´ìŠ¤)
        satisfaction_bonus = (case_satisfaction / 100) * 30
        similarity_score += satisfaction_bonus

        if similarity_score > 30:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            similar_cases.append({
                "case": case,
                "similarity_score": similarity_score,
                "summary": summary
            })

    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similar_cases.sort(key=lambda x: x["similarity_score"], reverse=True)
    return similar_cases[:limit]


def visualize_customer_profile_scores(customer_profile: Dict[str, Any], current_lang_key: str):
    """ê³ ê° í”„ë¡œí•„ ì ìˆ˜ë¥¼ ì‹œê°í™” (ê°ì • ì ìˆ˜, ê¸´ê¸‰ë„)"""
    if not IS_PLOTLY_AVAILABLE:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    sentiment_score = customer_profile.get("sentiment_score", 50)
    urgency_map = {"low": 25, "medium": 50, "high": 75}
    urgency_level = customer_profile.get("urgency_level", "medium")
    urgency_score = urgency_map.get(urgency_level.lower(), 50)

    # ê²Œì´ì§€ ì°¨íŠ¸ ìƒì„±
    fig = make_subplots(
        rows=1, cols=2,
        specs=[[{"type": "indicator"}, {"type": "indicator"}]],
        subplot_titles=(
            L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜"),
            L.get("urgency_score_label", "ê¸´ê¸‰ë„ ì ìˆ˜")
        )
    )

    # ê°ì • ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number+delta",
            value=sentiment_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("sentiment_score_label", "ê°ì • ì ìˆ˜")},
            delta={'reference': 50},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 33], 'color': "lightgray"},
                    {'range': [33, 66], 'color': "gray"},
                    {'range': [66, 100], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 90
                }
            }
        ),
        row=1, col=1
    )

    # ê¸´ê¸‰ë„ ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number",
            value=urgency_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("urgency_score_label", "ê¸´ê¸‰ë„")},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkred"},
                'steps': [
                    {'range': [0, 50], 'color': "lightgreen"},
                    {'range': [50, 75], 'color': "yellow"},
                    {'range': [75, 100], 'color': "lightcoral"}
                ],
            }
        ),
        row=1, col=2
    )

    fig.update_layout(height=300, margin=dict(l=20, r=20, t=50, b=20))
    return fig


def visualize_similarity_cases(similar_cases: List[Dict[str, Any]], current_lang_key: str):
    """ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ì²œì„ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not similar_cases:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    case_labels = []
    similarity_scores = []
    sentiment_scores = []
    satisfaction_scores = []

    for idx, similar_case in enumerate(similar_cases, 1):
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]
        case_labels.append(f"Case {idx}")
        similarity_scores.append(similarity)
        sentiment_scores.append(summary.get("customer_sentiment_score", 50))
        satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))

    # ìœ ì‚¬ë„ ì°¨íŠ¸
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=(
            L.get("similarity_chart_title", "ìœ ì‚¬ ì¼€ì´ìŠ¤ ìœ ì‚¬ë„"),
            L.get("scores_comparison_title",
                  "ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ")
        ),
        vertical_spacing=0.15
    )

    # ìœ ì‚¬ë„ ë°” ì°¨íŠ¸
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=similarity_scores,
            name=L.get("similarity_score_label", "ìœ ì‚¬ë„"),
            marker_color='lightblue',
            text=[f"{s:.1f}%" for s in similarity_scores],
            textposition='outside'
        ),
        row=1, col=1
    )

    # ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=sentiment_scores,
            name=L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
            marker_color='lightcoral'
        ),
        row=2, col=1
    )

    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=satisfaction_scores,
            name=L.get("satisfaction_score_label", "ë§Œì¡±ë„"),
            marker_color='lightgreen'
        ),
        row=2, col=1
    )

    fig.update_layout(
        height=600,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20),
        barmode='group'
    )
    fig.update_yaxes(title_text="ì ìˆ˜", row=2, col=1)
    fig.update_yaxes(title_text="ìœ ì‚¬ë„ (%)", row=1, col=1)

    return fig


def visualize_case_trends(histories: List[Dict[str, Any]], current_lang_key: str):
    """ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ íŠ¸ë Œë“œë¥¼ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not histories:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
    ]

    if not cases_with_summary:
        return None

    # ë‚ ì§œë³„ë¡œ ì •ë ¬
    cases_with_summary.sort(key=lambda x: x.get("timestamp", ""))

    dates = []
    sentiment_scores = []
    satisfaction_scores = []

    for case in cases_with_summary:
        summary = case.get("summary", {})
        timestamp = case.get("timestamp", "")
        try:
            dt = datetime.fromisoformat(timestamp)
            dates.append(dt)
            sentiment_scores.append(summary.get("customer_sentiment_score", 50))
            satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))
        except Exception:
            continue

    if not dates:
        return None

    # íŠ¸ë Œë“œ ë¼ì¸ ì°¨íŠ¸
    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=dates,
        y=sentiment_scores,
        mode='lines+markers',
        name=L.get("sentiment_trend_label", "ê°ì • ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightcoral', width=2),
        marker=dict(size=6)
    ))

    fig.add_trace(go.Scatter(
        x=dates,
        y=satisfaction_scores,
        mode='lines+markers',
        name=L.get("satisfaction_trend_label", "ë§Œì¡±ë„ ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightgreen', width=2),
        marker=dict(size=6)
    ))

    fig.update_layout(
        title=L.get("case_trends_title", "ê³¼ê±° ì¼€ì´ìŠ¤ ì ìˆ˜ ì¶”ì´"),
        xaxis_title=L.get("date_label", "ë‚ ì§œ"),
        yaxis_title=L.get("score_label", "ì ìˆ˜ (0-100)"),
        height=400,
        hovermode='x unified',
        legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
    )

    return fig


def visualize_customer_characteristics(summary: Dict[str, Any], current_lang_key: str):
    """ê³ ê° íŠ¹ì„±ì„ ì‹œê°í™” (ì–¸ì–´, ë¬¸í™”ê¶Œ, ì§€ì—­ ë“±)"""
    if not IS_PLOTLY_AVAILABLE or not summary:
        return None

    # ì–¸ì–´ í‚¤ ê²€ì¦
    if current_lang_key not in ["ko", "en", "ja"]:
        current_lang_key = st.session_state.get("language", "ko")
        if current_lang_key not in ["ko", "en", "ja"]:
            current_lang_key = "ko"
    L = LANG.get(current_lang_key, LANG["ko"])

    characteristics = summary.get("customer_characteristics", {})
    privacy_info = summary.get("privacy_info", {})

    # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
    labels = []
    values = []

    # ì–¸ì–´ ì •ë³´
    language = characteristics.get("language", "unknown")
    if language != "unknown":
        labels.append(L.get("language_label", "ì–¸ì–´"))
        lang_map = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}
        values.append(lang_map.get(language, language))

    # ê°œì¸ì •ë³´ ì œê³µ ì—¬ë¶€
    if privacy_info.get("has_email"):
        labels.append(L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ"))
        values.append("Yes")
    if privacy_info.get("has_phone"):
        labels.append(L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ"))
        values.append("Yes")

    # ì§€ì—­ ì •ë³´
    region = privacy_info.get("region_hint", characteristics.get("region", "unknown"))
    if region != "unknown":
        labels.append(L.get("region_label", "ì§€ì—­"))
        values.append(region)

    if not labels:
        return None

    # íŒŒì´ ì°¨íŠ¸
    fig = go.Figure(data=[go.Pie(
        labels=labels,
        values=[1] * len(labels),
        hole=0.4,
        marker_colors=px.colors.qualitative.Set3[:len(labels)]
    )])

    fig.update_layout(
        title=L.get("customer_characteristics_title",
                    "ê³ ê° íŠ¹ì„± ë¶„í¬"),
        height=300,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20)
    )

    return fig


def generate_guideline_from_past_cases(customer_query: str, customer_profile: Dict[str, Any],
                                       similar_cases: List[Dict[str, Any]], current_lang_key: str) -> str:
    """ê³¼ê±° ìœ ì‚¬ ì¼€ì´ìŠ¤ì˜ ì„±ê³µì ì¸ í•´ê²° ë°©ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¼ì¸ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    if not similar_cases:
        return ""

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìš”ì•½
    past_cases_text = ""
    for idx, similar_case in enumerate(similar_cases, 1):
        case = similar_case["case"]
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]

        past_cases_text += f"""
[Case {idx}] (Similarity: {similarity:.1f}%)
- Inquiry: {summary.get('main_inquiry', 'N/A')}
- Customer Sentiment: {summary.get('customer_sentiment_score', 50)}/100
- Customer Satisfaction: {summary.get('customer_satisfaction_score', 50)}/100
- Key Responses: {', '.join(summary.get('key_responses', [])[:3])}
- Summary: {summary.get('summary', 'N/A')[:200]}
"""

    guideline_prompt = f"""
You are an AI Customer Support Supervisor analyzing past successful cases to provide guidance.

Based on the following similar past cases and their successful resolution strategies, provide actionable guidelines for handling the current customer inquiry.

Current Customer Inquiry:
{customer_query}

Current Customer Profile:
- Gender: {customer_profile.get('gender', 'unknown')}
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}

Similar Past Cases (Successful Resolutions):
{past_cases_text}

Provide a concise guideline in {lang_name} that:
1. Identifies what worked well in similar past cases
2. Suggests specific approaches based on successful patterns
3. Warns about potential pitfalls based on past experiences
4. Recommends response strategies that led to high customer satisfaction

Guideline (in {lang_name}):
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        guideline = run_llm(guideline_prompt).strip()
        return guideline
    except Exception as e:
        return f"ê°€ì´ë“œë¼ì¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"


def _generate_initial_advice(customer_query, customer_type_display, customer_email, customer_phone, current_lang_key,
                             customer_attachment_file):
    """Supervisor ê°€ì´ë“œë¼ì¸ê³¼ ì´ˆì•ˆì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì €ì¥ëœ ë°ì´í„° í™œìš©)"""
    # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    try:
        detected_lang = detect_text_language(customer_query)
    except Exception as e:
        print(f"Language detection failed in _generate_initial_advice: {e}")
        detected_lang = current_lang_key if current_lang_key else "ko"
    
    # ê°ì§€ëœ ì–¸ì–´ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë˜, current_lang_keyê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µë˜ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
    lang_key_to_use = detected_lang if detected_lang else current_lang_key
    # lang_key_to_useê°€ ìœ íš¨í•œì§€ í™•ì¸
    if lang_key_to_use not in ["ko", "en", "ja"]:
        lang_key_to_use = current_lang_key if current_lang_key else "ko"
    
    # ì–¸ì–´ í‚¤ ê²€ì¦
    if lang_key_to_use not in ["ko", "en", "ja"]:
        lang_key_to_use = st.session_state.get("language", "ko")
        if lang_key_to_use not in ["ko", "en", "ja"]:
            lang_key_to_use = "ko"
    L = LANG.get(lang_key_to_use, LANG["ko"])
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[lang_key_to_use]

    contact_info_block = ""
    if customer_email or customer_phone:
        contact_info_block = (
            f"\n\n[Customer contact info for reference (DO NOT use these in your reply draft!)]"
            f"\n- Email: {customer_email or 'N/A'}"
            f"\n- Phone: {customer_phone or 'N/A'}"
        )

    attachment_block = ""
    if customer_attachment_file:
        file_name = customer_attachment_file.name
        attachment_block = f"\n\n[ATTACHMENT NOTE]: {L['attachment_info_llm'].format(filename=file_name)}"

    # ê³ ê° í”„ë¡œí•„ ë¶„ì„ (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
    customer_profile = analyze_customer_profile(customer_query, lang_key_to_use)

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì°¾ê¸° (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
    similar_cases = find_similar_cases(customer_query, customer_profile, lang_key_to_use, limit=5)

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ìƒì„± (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
    past_cases_guideline = ""
    if similar_cases:
        past_cases_guideline = generate_guideline_from_past_cases(
            customer_query, customer_profile, similar_cases, lang_key_to_use
        )

    # ê³ ê° í”„ë¡œí•„ ì •ë³´
    gender_display = customer_profile.get('gender', 'unknown')
    profile_block = f"""
[Customer Profile Analysis]
- Gender: {gender_display}
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency Level: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}
- Key Concerns: {', '.join(customer_profile.get('key_concerns', []))}
- Tone: {customer_profile.get('tone_analysis', 'unknown')}
"""

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ë¸”ë¡
    past_cases_block = ""
    if past_cases_guideline:
        past_cases_block = f"""
[Guidelines Based on {len(similar_cases)} Similar Past Cases]
{past_cases_guideline}
"""
    elif similar_cases:
        past_cases_block = f"""
[Note: Found {len(similar_cases)} similar past cases, but unable to generate detailed guidelines.
Consider reviewing past cases manually for patterns.]
"""

    # Output ALL text (guidelines and draft) STRICTLY in {lang_name}. <--- ê°•ë ¥í•œ ì–¸ì–´ ê°•ì œ ì§€ì‹œ
    initial_prompt = f"""
Output ALL text (guidelines and draft) STRICTLY in {lang_name}.

You are an AI Customer Support Supervisor. Your role is to analyze the following customer inquiry
from a **{st.session_state.customer_type_sim_select}** and provide:

1) A detailed **response guideline for the human agent** (step-by-step).
2) A **ready-to-send draft reply** in {lang_name}.

[FORMAT]
- Use the exact markdown headers:
  - "### {L['simulation_advice_header']}"
  - "### {L['simulation_draft_header']}"

[CRITICAL GUIDELINE RULES]
1. **Initial Information Collection (Req 3):** The first step in the guideline MUST be to request the necessary initial diagnostic information (e.g., device compatibility, local status/location, order number) BEFORE attempting to troubleshoot or solve the problem.
2. **Empathy for Difficult Customers (Req 5):** If the customer type is 'Difficult Customer' or 'Highly Dissatisfied Customer', the guideline MUST emphasize extreme politeness, empathy, and apologies, even if the policy (e.g., no refund) must be enforced.
3. **24-48 Hour Follow-up (Req 6):** If the issue cannot be solved immediately or requires confirmation from a local partner/supervisor, the guideline MUST state the procedure:
   - Acknowledge the issue.
   - Inform the customer they will receive a definite answer within 24 or 48 hours.
   - Request the customer's email or phone number for follow-up contact. (Use provided contact info if available)
4. **Past Cases Learning:** If past cases guidelines are provided, incorporate successful strategies from those cases into your recommendations.

Customer Inquiry:
{customer_query}
{contact_info_block}
{attachment_block}
{profile_block}
{past_cases_block}
"""
    if not st.session_state.is_llm_ready:
        mock_text = (
            f"### {L['simulation_advice_header']}\n\n"
            f"- (Mock) {st.session_state.customer_type_sim_select} ìœ í˜• ê³ ê° ì‘ëŒ€ ê°€ì´ë“œì…ë‹ˆë‹¤. (ìš”ì²­ 3, 5, 6 ë°˜ì˜)\n\n"
            f"### {L['simulation_draft_header']}\n\n"
            f"(Mock) ì—ì´ì „íŠ¸ ì‘ëŒ€ ì´ˆì•ˆì´ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤ã€‚\n\n"
        )
        return mock_text
    else:
        with st.spinner(L["response_generating"]):
            try:
                return run_llm(initial_prompt)
            except Exception as e:
                st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return f"âŒ AI Advice Generation Error: {e}"


# ========================================
# 9. ì‚¬ì´ë“œë°”
# ========================================

with st.sidebar:
    # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    if "language" not in st.session_state:
        st.session_state.language = "ko"
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    
    # íšŒì‚¬ ëª©ë¡ ì´ˆê¸°í™” (íšŒì‚¬ ì •ë³´ íƒ­ì—ì„œ ì‚¬ìš©)
    if "company_language_priority" not in st.session_state:
        st.session_state.company_language_priority = {
            "default": ["ko", "en", "ja"],
            "companies": {}
        }
    
    st.markdown("---")
    
    # ì–¸ì–´ ì„ íƒ
    if "language" not in st.session_state:
        st.session_state.language = "ko"
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    
    lang_priority = st.session_state.company_language_priority["default"]
    
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=lang_priority,
        index=lang_priority.index(st.session_state.language) if st.session_state.language in lang_priority else 0,
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
    )

    # ğŸ”¹ ì–¸ì–´ ë³€ê²½ ê°ì§€
    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        # ì±„íŒ…/ì „í™” ê³µí†µ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.simulator_messages = []
        # â­ ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        try:
            if hasattr(st.session_state, 'simulator_memory') and st.session_state.simulator_memory is not None:
                st.session_state.simulator_memory.clear()
        except Exception:
            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ ìƒì„±
            try:
                st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history")
            except Exception:
                pass  # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        st.session_state.initial_advice_provided = False
        st.session_state.is_chat_ended = False
        # â­ ìˆ˜ì •: ìœ„ì ¯ì´ ìƒì„±ëœ í›„ì—ëŠ” session_stateë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í”Œë˜ê·¸ ì‚¬ìš©
        st.session_state.reset_agent_response_area = True
        st.session_state.customer_query_text_area = ""
        st.session_state.last_transcript = ""
        st.session_state.sim_audio_bytes = None
        st.session_state.sim_stage = "WAIT_FIRST_QUERY"
        st.session_state.customer_attachment_file = []  # ì–¸ì–´ ë³€ê²½ ì‹œ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
        st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
        # ì „í™” ì‹œë®¬ë ˆì´í„° ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.call_sim_stage = "WAITING_CALL"
        st.session_state.call_sim_mode = "INBOUND"
        st.session_state.is_on_hold = False
        st.session_state.total_hold_duration = timedelta(0)
        st.session_state.hold_start_time = None
        st.session_state.current_customer_audio_text = ""
        st.session_state.current_agent_audio_text = ""
        st.session_state.agent_response_input_box_widget_call = ""
        st.session_state.call_initial_query = ""
        # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.sim_call_outbound_summary = ""
        st.session_state.sim_call_outbound_target = None
        # â­ ì–¸ì–´ ë³€ê²½ ì‹œ ì¬ì‹¤í–‰ - ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•´ í”Œë˜ê·¸ ì‚¬ìš©
        if "language_changed" not in st.session_state or not st.session_state.language_changed:
            st.session_state.language_changed = True
        else:
            # ì´ë¯¸ í•œ ë²ˆ ì¬ì‹¤í–‰í–ˆìœ¼ë©´ í”Œë˜ê·¸ ì´ˆê¸°í™”
            st.session_state.language_changed = False

    # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])

    st.title(L["sidebar_title"])
    st.markdown("---")

    # â­ API Key ì„¤ì • ì„¹ì…˜ ì¶”ê°€
    st.subheader("ğŸ”‘ API Key ì„¤ì •")
    
    # LLM ì„ íƒ
    llm_options = {
        "openai_gpt4": "OpenAI GPT-4",
        "openai_gpt35": "OpenAI GPT-3.5",
        "gemini_pro": "Google Gemini Pro",
        "gemini_flash": "Google Gemini Flash",
        "claude": "Anthropic Claude",
        "groq": "Groq",
        "nvidia": "NVIDIA NIM"
    }
    
    current_llm = st.session_state.get("selected_llm", "openai_gpt4")
    selected_llm = st.selectbox(
        "LLM ëª¨ë¸ ì„ íƒ",
        options=list(llm_options.keys()),
        format_func=lambda x: llm_options[x],
        index=list(llm_options.keys()).index(current_llm) if current_llm in llm_options else 0,
        key="sidebar_llm_select"
    )
    if selected_llm != current_llm:
        st.session_state.selected_llm = selected_llm
    
    # API Key ë§¤í•‘
    api_key_map = {
        "openai_gpt4": "openai",
        "openai_gpt35": "openai",
        "gemini_pro": "gemini",
        "gemini_flash": "gemini",
        "claude": "claude",
        "groq": "groq",
        "nvidia": "nvidia"
    }
    
    api_name = api_key_map.get(selected_llm, "openai")
    api_config = SUPPORTED_APIS.get(api_name, {})
    
    if api_config:
        # í˜„ì¬ API Key í™•ì¸
        current_key = get_api_key(api_name)
        if not current_key:
            # ìˆ˜ë™ ì…ë ¥ í•„ë“œ
            session_key = api_config.get("session_key", "")
            manual_key = st.text_input(
                api_config.get("label", "API Key"),
                value=st.session_state.get(session_key, ""),
                type="password",
                placeholder=api_config.get("placeholder", "API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”"),
                key=f"manual_api_key_{selected_llm}"
            )
            if manual_key and manual_key != st.session_state.get(session_key, ""):
                st.session_state[session_key] = manual_key
        else:
            st.success(f"âœ… {api_config.get('label', 'API Key')} ì„¤ì •ë¨")
    
    st.markdown("---")

    # â­ ê¸°ëŠ¥ ì„ íƒ - ê¸°ë³¸ê°’ì„ AI ì±— ì‹œë®¬ë ˆì´í„°ë¡œ ì„¤ì •
    if "feature_selection" not in st.session_state:
        st.session_state.feature_selection = L["sim_tab_chat_email"]

    # â­ í•µì‹¬ ê¸°ëŠ¥ê³¼ ë”ë³´ê¸° ê¸°ëŠ¥ ë¶„ë¦¬ (íšŒì‚¬ ì •ë³´ ë° FAQ ì¶”ê°€)
    core_features = [L["sim_tab_chat_email"], L["sim_tab_phone"], L["company_info_tab"]]
    other_features = [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["voice_rec_header"]]
    
    # ëª¨ë“  ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•© (í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥í•˜ë„ë¡)
    all_features = core_features + other_features
    
    # í˜„ì¬ ì„ íƒëœ ê¸°ëŠ¥
    current_selection = st.session_state.get("feature_selection", L["sim_tab_chat_email"])
    
    # í˜„ì¬ ì„ íƒì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    try:
        current_index = all_features.index(current_selection) if current_selection in all_features else 0
    except (ValueError, AttributeError):
        current_index = 0
    
    # â­ í•˜ë‚˜ì˜ í†µí•©ëœ ì„ íƒ ë¡œì§ (í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥) - ì„¤ëª… ì œê±°
    selected_feature = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        all_features,
        index=current_index,
        key="unified_feature_selection",
        label_visibility="hidden"
    )
    
    # ì„ íƒëœ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸
    if selected_feature != current_selection:
        st.session_state.feature_selection = selected_feature
    
    feature_selection = st.session_state.get("feature_selection", L["sim_tab_chat_email"])

# ë©”ì¸ íƒ€ì´í‹€
# â­ L ë³€ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì‚¬ì´ë“œë°”ì—ì„œ ì´ë¯¸ ì •ì˜ë¨)
if "language" not in st.session_state:
    st.session_state.language = "ko"
# ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
current_lang = st.session_state.get("language", "ko")
if current_lang not in ["ko", "en", "ja"]:
    current_lang = "ko"
L = LANG.get(current_lang, LANG["ko"])

# â­ íƒ€ì´í‹€ê³¼ ì„¤ëª…ì„ í•œ ì¤„ë¡œ ê°„ê²°í•˜ê²Œ í‘œì‹œ
feature_selection = st.session_state.get("feature_selection", L["sim_tab_chat_email"])
if feature_selection == L["sim_tab_chat_email"]:
    st.markdown(f"### ğŸ“§ {L['sim_tab_chat_email']}")
    st.caption(L['sim_tab_chat_email_desc'])
elif feature_selection == L["sim_tab_phone"]:
    st.markdown(f"### ğŸ“ {L['sim_tab_phone']}")
    st.caption(L['sim_tab_phone_desc'])
elif feature_selection == L["rag_tab"]:
    st.markdown(f"### ğŸ“š {L['rag_tab']}")
    st.caption(L['rag_tab_desc'])
elif feature_selection == L["content_tab"]:
    st.markdown(f"### ğŸ“ {L['content_tab']}")
    st.caption(L['content_tab_desc'])
elif feature_selection == L["lstm_tab"]:
    st.markdown(f"### ğŸ“Š {L['lstm_tab']}")
    st.caption(L['lstm_tab_desc'])
elif feature_selection == L["voice_rec_header"]:
    st.markdown(f"### ğŸ¤ {L['voice_rec_header']}")
    st.caption(L['voice_rec_header_desc'])
elif feature_selection == L["company_info_tab"]:
    # ê³µë°± ì¶•ì†Œ: ì œëª©ê³¼ ì„¤ëª…ì„ í•œ ì¤„ë¡œ ê°„ê²°í•˜ê²Œ í‘œì‹œ
    st.markdown(f"#### ğŸ“‹ {L['company_info_tab']}")
    st.caption(L['company_info_tab_desc'])

# ========================================
# 10. ê¸°ëŠ¥ë³„ í˜ì´ì§€
# ========================================

# -------------------- Company Info & FAQ Tab --------------------
if feature_selection == L["company_info_tab"]:
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    
    # FAQ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    faq_data = load_faq_database()
    companies = list(faq_data.get("companies", {}).keys())
    
    # íšŒì‚¬ëª… ê²€ìƒ‰ ì…ë ¥ (ìƒë‹¨ì— ë°°ì¹˜) - ì…ë ¥ë€ì€ ê¸€ë¡œë²Œ ê¸°ì—… ì˜ë¬¸ëª… ê³ ë ¤í•˜ì—¬ ì›ë˜ í¬ê¸° ìœ ì§€
    col_search_header, col_search_input, col_search_btn = st.columns([0.5, 1.2, 0.2])
    with col_search_header:
        st.write(f"**{L['search_company']}**")
    with col_search_input:
        company_search_input = st.text_input(
            "",
            placeholder=L["company_search_placeholder"],
            key="company_search_input",
            value=st.session_state.get("searched_company", ""),
            label_visibility="collapsed"
        )
    with col_search_btn:
        search_button = st.button(f"ğŸ” {L['company_search_button']}", key="company_search_btn", type="primary", use_container_width=True)
    
    # ê²€ìƒ‰ëœ íšŒì‚¬ ì •ë³´ ì €ì¥
    searched_company = st.session_state.get("searched_company", "")
    searched_company_data = st.session_state.get("searched_company_data", None)
    
    # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹œ LLMìœ¼ë¡œ íšŒì‚¬ ì •ë³´ ìƒì„±
    if search_button and company_search_input:
        with st.spinner(f"{company_search_input} {L['generating_company_info']}"):
            generated_data = generate_company_info_with_llm(company_search_input, current_lang)
            st.session_state.searched_company = company_search_input
            st.session_state.searched_company_data = generated_data
            searched_company = company_search_input
            searched_company_data = generated_data
            
            # ìƒì„±ëœ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if company_search_input not in faq_data.get("companies", {}):
                faq_data.setdefault("companies", {})[company_search_input] = {
                    f"info_{current_lang}": generated_data.get("company_info", ""),
                    "info_ko": generated_data.get("company_info", ""),
                    "info_en": "",
                    "info_ja": "",
                    "popular_products": generated_data.get("popular_products", []),
                    "trending_topics": generated_data.get("trending_topics", []),
                    "faqs": generated_data.get("faqs", [])
                }
                save_faq_database(faq_data)
    
    # ê²€ìƒ‰ëœ íšŒì‚¬ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë°ì´í„° ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ì¡´ íšŒì‚¬ ì„ íƒ
    if searched_company and searched_company_data:
        display_company = searched_company
        display_data = searched_company_data
        # ë°ì´í„°ë² ì´ìŠ¤ì—ë„ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
        if display_company in faq_data.get("companies", {}):
            faq_data["companies"][display_company].update({
                f"info_{current_lang}": display_data.get("company_info", ""),
                "popular_products": display_data.get("popular_products", []),
                "trending_topics": display_data.get("trending_topics", []),
                "faqs": display_data.get("faqs", [])
            })
            save_faq_database(faq_data)
    elif companies:
        display_company = st.selectbox(
            L["select_company"],
            options=companies,
            key="company_select_display"
        )
        company_db_data = faq_data["companies"][display_company]
        display_data = {
            "company_info": company_db_data.get(f"info_{current_lang}", company_db_data.get("info_ko", "")),
            "popular_products": company_db_data.get("popular_products", []),
            "trending_topics": company_db_data.get("trending_topics", []),
            "faqs": company_db_data.get("faqs", [])
        }
    else:
        display_company = None
        display_data = None
    
    # íƒ­ ìƒì„± (FAQ ê²€ìƒ‰ íƒ­ ì œê±°, FAQ íƒ­ì— í†µí•©) - ê³µë°± ì¶•ì†Œ
    tab1, tab2, tab3 = st.tabs([
        L["company_info"], 
        L["company_faq"], 
        L["button_add_company"]
    ])
    
    # íƒ­ 1: íšŒì‚¬ ì†Œê°œ ë° ì‹œê°í™”
    with tab1:
        if display_company and display_data:
            # ì œëª©ì„ ë” ê°„ê²°í•˜ê²Œ í‘œì‹œ
            st.markdown(f"#### {display_company} - {L['company_info']}")
            
            # íšŒì‚¬ ì†Œê°œ í‘œì‹œ
            if display_data.get("company_info"):
                st.markdown(display_data["company_info"])
            
            # ì‹œê°í™” ì°¨íŠ¸ í‘œì‹œ
            if display_data.get("popular_products") or display_data.get("trending_topics"):
                charts = visualize_company_data(
                    {
                        "popular_products": display_data.get("popular_products", []),
                        "trending_topics": display_data.get("trending_topics", [])
                    },
                    current_lang
                )
                
                if charts:
                    # ë§‰ëŒ€ ê·¸ë˜í”„ í‘œì‹œ - ê³µë°± ì¶•ì†Œ
                    st.markdown(f"#### ğŸ“Š {L['visualization_chart']}")
                    col1_bar, col2_bar = st.columns(2)
                    
                    if "products_bar" in charts:
                        with col1_bar:
                            st.plotly_chart(charts["products_bar"], use_container_width=True)
                    
                    if "topics_bar" in charts:
                        with col2_bar:
                            st.plotly_chart(charts["topics_bar"], use_container_width=True)
                    
                    # ì„ í˜• ê·¸ë˜í”„ í‘œì‹œ
                    col1_line, col2_line = st.columns(2)
                    
                    if "products_line" in charts:
                        with col1_line:
                            st.plotly_chart(charts["products_line"], use_container_width=True)
                    
                    if "topics_line" in charts:
                        with col2_line:
                            st.plotly_chart(charts["topics_line"], use_container_width=True)
            
            # ì¸ê¸° ìƒí’ˆ ëª©ë¡ (ì´ë¯¸ì§€ í¬í•¨) - ê³µë°± ì¶•ì†Œ
            if display_data.get("popular_products"):
                st.markdown(f"#### {L['popular_products']}")
                # ìƒí’ˆì„ ê·¸ë¦¬ë“œ í˜•íƒœë¡œ í‘œì‹œ
                product_cols = st.columns(min(3, len(display_data["popular_products"])))
                for idx, product in enumerate(display_data["popular_products"]):
                    product_text = product.get(f"text_{current_lang}", product.get("text_ko", ""))
                    product_score = product.get("score", 0)
                    product_image_url = product.get("image_url", "")
                    
                    with product_cols[idx % len(product_cols)]:
                        # ì´ë¯¸ì§€ í‘œì‹œ - ìƒí’ˆëª… ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì´ë¯¸ì§€ ê²€ìƒ‰
                        if not product_image_url:
                            # ëª¨ë“  ì–¸ì–´ ë²„ì „ì˜ ìƒí’ˆëª…ì„ í™•ì¸í•˜ì—¬ ì´ë¯¸ì§€ URL ìƒì„±
                            # ìš°ì„ ìˆœìœ„: í˜„ì¬ ì–¸ì–´ > í•œêµ­ì–´ > ì˜ì–´ > ì¼ë³¸ì–´
                            image_found = False
                            for lang_key in [current_lang, "ko", "en", "ja"]:
                                check_text = product.get(f"text_{lang_key}", "")
                                if check_text:
                                    check_url = get_product_image_url(check_text)
                                    if check_url:
                                        product_image_url = check_url
                                        image_found = True
                                        break
                            
                            # ëª¨ë“  ì–¸ì–´ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
                            if not image_found:
                                product_image_url = get_product_image_url(product_text)
                        
                        # ì´ë¯¸ì§€ í‘œì‹œ ì‹œë„ (ë¡œì»¬ íŒŒì¼ ë° URL ëª¨ë‘ ì§€ì›)
                        image_displayed = False
                        if product_image_url:
                            try:
                                # ë¡œì»¬ íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
                                if os.path.exists(product_image_url):
                                    st.image(product_image_url, caption=product_text[:30], use_container_width=True)
                                    image_displayed = True
                                # URLì¸ ê²½ìš°
                                elif product_image_url.startswith("http://") or product_image_url.startswith("https://"):
                                    try:
                                        # HEAD ìš”ì²­ìœ¼ë¡œ ì´ë¯¸ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (íƒ€ì„ì•„ì›ƒ 2ì´ˆ)
                                        response = requests.head(product_image_url, timeout=2, allow_redirects=True)
                                        if response.status_code == 200:
                                            st.image(product_image_url, caption=product_text[:30], use_container_width=True)
                                            image_displayed = True
                                        else:
                                            image_displayed = False
                                    except Exception:
                                        # HEAD ìš”ì²­ ì‹¤íŒ¨ ì‹œì—ë„ ì´ë¯¸ì§€ í‘œì‹œ ì‹œë„ (ì¼ë¶€ ì„œë²„ëŠ” HEADë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ)
                                        try:
                                            st.image(product_image_url, caption=product_text[:30], use_container_width=True)
                                            image_displayed = True
                                        except Exception:
                                            image_displayed = False
                                else:
                                    # ê¸°íƒ€ ê²½ë¡œ ì‹œë„
                                    try:
                                        st.image(product_image_url, caption=product_text[:30], use_container_width=True)
                                        image_displayed = True
                                    except Exception:
                                        image_displayed = False
                            except Exception as img_error:
                                # ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨
                                image_displayed = False
                        
                        # ì´ë¯¸ì§€ í‘œì‹œ ì‹¤íŒ¨ ì‹œ ì´ëª¨ì§€ ì¹´ë“œ í‘œì‹œ
                        if not image_displayed:
                            product_emoji = "ğŸ«" if "í‹°ì¼“" in product_text or "ticket" in product_text.lower() else \
                                          "ğŸ¢" if "í…Œë§ˆíŒŒí¬" in product_text or "theme" in product_text.lower() or "ë””ì¦ˆë‹ˆ" in product_text or "ìœ ë‹ˆë²„ì…œ" in product_text or "ìŠ¤íŠœë””ì˜¤" in product_text else \
                                          "âœˆï¸" if "í•­ê³µ" in product_text or "flight" in product_text.lower() else \
                                          "ğŸ¨" if "í˜¸í…”" in product_text or "hotel" in product_text.lower() else \
                                          "ğŸ”" if "ìŒì‹" in product_text or "food" in product_text.lower() else \
                                          "ğŸŒ" if "ì—¬í–‰" in product_text or "travel" in product_text.lower() or "ì‚¬íŒŒë¦¬" in product_text else \
                                          "ğŸ“¦"
                            st.markdown(
                                f"""
                                <div style='text-align: center; padding: 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                                border-radius: 10px; color: white; min-height: 200px; display: flex; flex-direction: column; justify-content: center;'>
                                    <h1 style='font-size: 64px; margin: 0;'>{product_emoji}</h1>
                                    <p style='font-size: 16px; margin-top: 15px; font-weight: bold;'>{product_text[:25]}</p>
                                </div>
                                """, 
                                unsafe_allow_html=True
                            )
                        
                        st.write(f"**{product_text}**")
                        st.caption(f"{L.get('popularity', 'ì¸ê¸°ë„')}: {product_score}")
                        st.markdown("---")
            
            # í™”ì œì˜ ì†Œì‹ ëª©ë¡ (ìƒì„¸ ë‚´ìš© í¬í•¨) - ê³µë°± ì¶•ì†Œ
            if display_data.get("trending_topics"):
                st.markdown(f"#### {L['trending_topics']}")
                for idx, topic in enumerate(display_data["trending_topics"], 1):
                    topic_text = topic.get(f"text_{current_lang}", topic.get("text_ko", ""))
                    topic_score = topic.get("score", 0)
                    topic_detail = topic.get(f"detail_{current_lang}", topic.get("detail_ko", ""))
                    
                    with st.expander(f"{idx}. **{topic_text}** ({L.get('trend_score', 'í™”ì œë„')}: {topic_score})"):
                        if topic_detail:
                            st.write(topic_detail)
                        else:
                            # ìƒì„¸ ë‚´ìš©ì´ ì—†ìœ¼ë©´ LLMìœ¼ë¡œ ìƒì„±
                            if display_company:
                                try:
                                    # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸
                                    detail_prompts = {
                                        "ko": f"{display_company}ì˜ '{topic_text}'ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©ì„ 200ì ì´ìƒ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                                        "en": f"Please write detailed content of at least 200 characters about '{topic_text}' from {display_company}.",
                                        "ja": f"{display_company}ã®ã€Œ{topic_text}ã€ã«é–¢ã™ã‚‹è©³ç´°å†…å®¹ã‚’200æ–‡å­—ä»¥ä¸Šã§ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                                    }
                                    detail_prompt = detail_prompts.get(current_lang, detail_prompts["ko"])
                                    generated_detail = run_llm(detail_prompt)
                                    if generated_detail and not generated_detail.startswith("âŒ"):
                                        st.write(generated_detail)
                                        # ìƒì„±ëœ ìƒì„¸ ë‚´ìš©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                                        if display_company in faq_data.get("companies", {}):
                                            topic_idx = idx - 1
                                            if topic_idx < len(faq_data["companies"][display_company].get("trending_topics", [])):
                                                faq_data["companies"][display_company]["trending_topics"][topic_idx][f"detail_{current_lang}"] = generated_detail
                                                save_faq_database(faq_data)
                                    else:
                                        st.write(L.get("generating_detail", "ìƒì„¸ ë‚´ìš©ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."))
                                except Exception as e:
                                    st.write(L.get("checking_additional_info", "ìƒì„¸ ë‚´ìš©: {topic}ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.").format(topic=topic_text))
                            else:
                                st.write(L.get("checking_additional_info", "ìƒì„¸ ë‚´ìš©: {topic}ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.").format(topic=topic_text))
        else:
            st.info(L["company_search_or_select"])
    
    # íƒ­ 2: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ) - ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨
    with tab2:
        if display_company and display_data:
            # ì œëª©ì„ ë” ê°„ê²°í•˜ê²Œ í‘œì‹œ
            st.markdown(f"#### {display_company} - {L['company_faq']}")
            
            # FAQ ê²€ìƒ‰ ê¸°ëŠ¥ (íƒ­ ë‚´ë¶€ì— í†µí•©) - ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€, ê³µë°± ì¶•ì†Œ
            col_search_faq, col_btn_faq = st.columns([3.5, 1])
            with col_search_faq:
                faq_search_query = st.text_input(
                    L["faq_search_placeholder"],
                    key="faq_search_in_tab",
                    placeholder=L.get("faq_search_placeholder_extended", L["faq_search_placeholder"])
                )
            with col_btn_faq:
                faq_search_btn = st.button(L["button_search_faq"], key="faq_search_btn_in_tab")
            
            faqs = display_data.get("faqs", [])
            popular_products = display_data.get("popular_products", [])
            trending_topics = display_data.get("trending_topics", [])
            company_info = display_data.get("company_info", "")
            
            # ê²€ìƒ‰ ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
            matched_products = []
            matched_topics = []
            matched_info = False
            
            # ê²€ìƒ‰ì–´ê°€ ìˆìœ¼ë©´ í™•ì¥ëœ ê²€ìƒ‰ (FAQ, ìƒí’ˆ, í™”ì œ ì†Œì‹, íšŒì‚¬ ì†Œê°œ ëª¨ë‘ ê²€ìƒ‰)
            if faq_search_query and faq_search_btn:
                query_lower = faq_search_query.lower()
                filtered_faqs = []
                
                # 1. FAQ ê²€ìƒ‰ (ê¸°ë³¸ FAQ + ìƒí’ˆëª… ê´€ë ¨ FAQ)
                for faq in faqs:
                    question = faq.get(f"question_{current_lang}", faq.get("question_ko", ""))
                    answer = faq.get(f"answer_{current_lang}", faq.get("answer_ko", ""))
                    if query_lower in question.lower() or query_lower in answer.lower():
                        filtered_faqs.append(faq)
                
                # 2. ìƒí’ˆëª…ìœ¼ë¡œ FAQ ê²€ìƒ‰ (ìƒí’ˆëª…ì´ ê²€ìƒ‰ì–´ì™€ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ëŠ” ê²½ìš°)
                # ê²€ìƒ‰ì–´ê°€ ìƒí’ˆëª…ì— í¬í•¨ë˜ë©´ í•´ë‹¹ ìƒí’ˆê³¼ ê´€ë ¨ëœ FAQë¥¼ ì°¾ì•„ì„œ í‘œì‹œ
                for product in popular_products:
                    product_text = product.get(f"text_{current_lang}", product.get("text_ko", ""))
                    product_text_lower = product_text.lower()
                    
                    # ê²€ìƒ‰ì–´ê°€ ìƒí’ˆëª…ì— í¬í•¨ë˜ëŠ” ê²½ìš°
                    if query_lower in product_text_lower:
                        # í•´ë‹¹ ìƒí’ˆëª…ì´ FAQ ì§ˆë¬¸/ë‹µë³€ì— í¬í•¨ëœ ê²½ìš° ì°¾ê¸°
                        product_related_faqs = []
                        for faq in faqs:
                            question = faq.get(f"question_{current_lang}", faq.get("question_ko", ""))
                            answer = faq.get(f"answer_{current_lang}", faq.get("answer_ko", ""))
                            # ìƒí’ˆëª…ì´ FAQì— ì–¸ê¸‰ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€
                            if product_text_lower in question.lower() or product_text_lower in answer.lower():
                                if faq not in filtered_faqs:
                                    filtered_faqs.append(faq)
                                    product_related_faqs.append(faq)
                        
                        # ìƒí’ˆëª…ì´ ë§¤ì¹­ë˜ì—ˆì§€ë§Œ ê´€ë ¨ FAQê°€ ì—†ëŠ” ê²½ìš°, ìƒí’ˆ ì •ë³´ë§Œ í‘œì‹œ
                        if not product_related_faqs:
                            matched_products.append(product)
                
                # 2. ì¸ê¸° ìƒí’ˆ ê²€ìƒ‰
                for product in popular_products:
                    product_text = product.get(f"text_{current_lang}", product.get("text_ko", ""))
                    if query_lower in product_text.lower():
                        matched_products.append(product)
                
                # 3. í™”ì œì˜ ì†Œì‹ ê²€ìƒ‰
                for topic in trending_topics:
                    topic_text = topic.get(f"text_{current_lang}", topic.get("text_ko", ""))
                    if query_lower in topic_text.lower():
                        matched_topics.append(topic)
                
                # 4. íšŒì‚¬ ì†Œê°œ ê²€ìƒ‰
                if query_lower in company_info.lower():
                    matched_info = True
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                if filtered_faqs or matched_products or matched_topics or matched_info:
                    # ë§¤ì¹­ëœ ìƒí’ˆ í‘œì‹œ (FAQê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
                    if matched_products and not filtered_faqs:
                        st.subheader(f"ğŸ” {L.get('related_products', 'ê´€ë ¨ ìƒí’ˆ')} ({len(matched_products)}{L.get('items', 'ê°œ')})")
                        st.info(L.get("no_faq_for_product", "í•´ë‹¹ ìƒí’ˆê³¼ ê´€ë ¨ëœ FAQë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒí’ˆ ì •ë³´ë§Œ í‘œì‹œë©ë‹ˆë‹¤."))
                        for idx, product in enumerate(matched_products, 1):
                            product_text = product.get(f"text_{current_lang}", product.get("text_ko", ""))
                            product_score = product.get("score", 0)
                            st.write(f"â€¢ **{product_text}** ({L.get('popularity', 'ì¸ê¸°ë„')}: {product_score})")
                        st.markdown("---")
                    
                    # ë§¤ì¹­ëœ í™”ì œ ì†Œì‹ í‘œì‹œ
                    if matched_topics:
                        st.subheader(f"ğŸ” {L.get('related_trending_news', 'ê´€ë ¨ í™”ì œ ì†Œì‹')} ({len(matched_topics)}{L.get('items', 'ê°œ')})")
                        for idx, topic in enumerate(matched_topics, 1):
                            topic_text = topic.get(f"text_{current_lang}", topic.get("text_ko", ""))
                            topic_score = topic.get("score", 0)
                            st.write(f"â€¢ **{topic_text}** ({L.get('trend_score', 'í™”ì œë„')}: {topic_score})")
                        st.markdown("---")
                    
                    # ë§¤ì¹­ëœ íšŒì‚¬ ì†Œê°œ í‘œì‹œ
                    if matched_info:
                        st.subheader(f"ğŸ” {L.get('related_company_info', 'ê´€ë ¨ íšŒì‚¬ ì†Œê°œ ë‚´ìš©')}")
                        # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ë¶€ë¶„ ê°•ì¡°í•˜ì—¬ í‘œì‹œ
                        info_lower = company_info.lower()
                        query_pos = info_lower.find(query_lower)
                        if query_pos != -1:
                            start = max(0, query_pos - 100)
                            end = min(len(company_info), query_pos + len(query_lower) + 100)
                            snippet = company_info[start:end]
                            if start > 0:
                                snippet = "..." + snippet
                            if end < len(company_info):
                                snippet = snippet + "..."
                            # ê²€ìƒ‰ì–´ ê°•ì¡°
                            highlighted = snippet.replace(
                                query_lower, 
                                f"**{query_lower}**"
                            )
                            st.write(highlighted)
                        st.markdown("---")
                    
                    # FAQ ê²°ê³¼
                    faqs = filtered_faqs
                else:
                    faqs = []
            
            # FAQ ëª©ë¡ í‘œì‹œ
            if faqs:
                if faq_search_query and faq_search_btn:
                    st.subheader(f"ğŸ” {L.get('related_faq', 'ê´€ë ¨ FAQ')} ({len(faqs)}{L.get('items', 'ê°œ')})")
                else:
                    st.subheader(f"{L['company_faq']} ({len(faqs)}{L.get('items', 'ê°œ')})")
                for idx, faq in enumerate(faqs, 1):
                    question = faq.get(f"question_{current_lang}", faq.get("question_ko", ""))
                    answer = faq.get(f"answer_{current_lang}", faq.get("answer_ko", ""))
                    with st.expander(f"{L['faq_question_prefix'].format(num=idx)} {question}"):
                        st.write(f"**{L['faq_answer']}:** {answer}")
            else:
                if faq_search_query and faq_search_btn:
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ ë©”ì‹œì§€ í‘œì‹œ (ìœ„ì—ì„œ ì´ë¯¸ ê´€ë ¨ ìƒí’ˆ/ì†Œì‹ ë“±ì´ í‘œì‹œë˜ì—ˆì„ ìˆ˜ ìˆìŒ)
                    if not (matched_products or matched_topics or matched_info):
                        st.info(L["no_faq_results"])
                else:
                    st.info(L.get("no_faq_for_company", f"{display_company}ì˜ FAQê°€ ì—†ìŠµë‹ˆë‹¤.").format(company=display_company))
        else:
            st.info(L.get("no_company_selected", "íšŒì‚¬ëª…ì„ ê²€ìƒ‰í•˜ê±°ë‚˜ ì„ íƒí•´ì£¼ì„¸ìš”."))
    
    # íƒ­ 3: ê³ ê° ë¬¸ì˜ ì¬í™•ì¸ (ì—ì´ì „íŠ¸ìš©)
    with tab3:
        # ì œëª©ê³¼ ì„¤ëª…ì„ í•œ ì¤„ë¡œ ê°„ê²°í•˜ê²Œ í‘œì‹œ
        st.markdown(f"#### {L['customer_inquiry_review']}")
        st.caption(L.get("customer_inquiry_review_desc", "ì—ì´ì „íŠ¸ê°€ ìƒì‚¬ë“¤ì—ê²Œ ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì¬í™•ì¸í•˜ê³ , AI ë‹µì•ˆ ë° íŒíŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤."))
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if "generated_ai_answer" not in st.session_state:
            st.session_state.generated_ai_answer = None
        if "generated_hint" not in st.session_state:
            st.session_state.generated_hint = None
        
        # íšŒì‚¬ ì„ íƒ (ì„ íƒì‚¬í•­)
        selected_company_for_inquiry = None
        if companies:
            all_option = L.get("all_companies", "ì „ì²´")
            selected_company_for_inquiry = st.selectbox(
                f"{L['select_company']} ({L.get('optional', 'ì„ íƒì‚¬í•­')})",
                options=[all_option] + companies,
                key="inquiry_company_select"
            )
            if selected_company_for_inquiry == all_option:
                selected_company_for_inquiry = None
        
        # ê³ ê° ë¬¸ì˜ ë‚´ìš© ì…ë ¥
        customer_inquiry = st.text_area(
            L["inquiry_question_label"],
            placeholder=L["inquiry_question_placeholder"],
            key="customer_inquiry_input",
            height=150
        )
        
        # ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ
        uploaded_file = st.file_uploader(
            L.get("inquiry_attachment_label", "ğŸ“ ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ (ì‚¬ì§„/ìŠ¤í¬ë¦°ìƒ·)"),
            type=["png", "jpg", "jpeg", "pdf"],
            key="customer_inquiry_attachment",
            help=L.get("inquiry_attachment_help", "íŠ¹íˆ ì·¨ì†Œ ë¶ˆê°€ ì—¬í–‰ìƒí’ˆì˜ ë¹„í–‰ê¸° ì§€ì—°, ì—¬ê¶Œ ì´ìŠˆ ë“± ë¶ˆê°€í”¼í•œ ì‚¬ìœ ì˜ ê²½ìš°, ë°˜ë“œì‹œ ì‚¬ì§„ì´ë‚˜ ìŠ¤í¬ë¦°ìƒ·ì„ ì²¨ë¶€í•´ì£¼ì„¸ìš”.")
        )
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì €ì¥
        attachment_info = ""
        uploaded_file_info = None
        file_content_extracted = ""
        file_content_translated = ""
        
        if uploaded_file is not None:
            file_name = uploaded_file.name
            file_type = uploaded_file.type
            file_size = len(uploaded_file.getvalue())
            st.success(L.get("inquiry_attachment_uploaded", "âœ… ì²¨ë¶€ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {filename}").format(filename=file_name))
            
            # íŒŒì¼ ì •ë³´ ì €ì¥
            uploaded_file_info = {
                "name": file_name,
                "type": file_type,
                "size": file_size
            }
            
            # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ (PDF, TXT, ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš°)
            if file_name.lower().endswith(('.pdf', '.txt', '.png', '.jpg', '.jpeg')):
                try:
                    with st.spinner(L.get("extracting_file_content", "íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘...")):
                        if file_name.lower().endswith('.pdf'):
                            import tempfile
                            import os
                            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
                            tmp.write(uploaded_file.getvalue())
                            tmp.flush()
                            tmp.close()
                            try:
                                loader = PyPDFLoader(tmp.name)
                                file_docs = loader.load()
                                file_content_extracted = "\n".join([doc.page_content for doc in file_docs])
                            finally:
                                try:
                                    os.remove(tmp.name)
                                except:
                                    pass
                        elif file_name.lower().endswith('.txt'):
                            uploaded_file.seek(0)  # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ì´ë™
                            file_content_extracted = uploaded_file.read().decode("utf-8", errors="ignore")
                        elif file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                            # ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ìš° OCRì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            uploaded_file.seek(0)
                            image_bytes = uploaded_file.getvalue()
                            image_base64 = base64.b64encode(image_bytes).decode('utf-8')
                            
                            # Gemini Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            ocr_prompt = """ì´ ì´ë¯¸ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”. 
ì´ë¯¸ì§€ì— í•œêµ­ì–´, ì¼ë³¸ì–´, ì˜ì–´ ë“± ì–´ë–¤ ì–¸ì–´ì˜ í…ìŠ¤íŠ¸ê°€ ìˆë“  ëª¨ë‘ ì¶”ì¶œí•˜ê³ , 
í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ì™€ ìˆœì„œë¥¼ ìœ ì§€í•´ì£¼ì„¸ìš”. 
ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ "í…ìŠ¤íŠ¸ ì—†ìŒ"ì´ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ì¶”ì¶œëœ í…ìŠ¤íŠ¸:"""
                            
                            try:
                                # Gemini Vision API í˜¸ì¶œ
                                gemini_key = get_api_key("gemini")
                                if gemini_key:
                                    import google.generativeai as genai
                                    genai.configure(api_key=gemini_key)
                                    model = genai.GenerativeModel('gemini-2.0-flash-exp')
                                    
                                    # ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ í•¨ê»˜ ì „ì†¡
                                    response = model.generate_content([
                                        {
                                            "mime_type": file_type,
                                            "data": image_bytes
                                        },
                                        ocr_prompt
                                    ])
                                    file_content_extracted = response.text if response.text else ""
                                else:
                                    # Gemini í‚¤ê°€ ì—†ìœ¼ë©´ LLMì— base64 ì´ë¯¸ì§€ë¥¼ ì „ì†¡í•˜ì—¬ OCR ìš”ì²­
                                    ocr_llm_prompt = f"""{ocr_prompt}

ì´ë¯¸ì§€ëŠ” base64ë¡œ ì¸ì½”ë”©ë˜ì–´ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”."""
                                    # LLMì´ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
                                    file_content_extracted = ""
                                    st.info(L.get("ocr_requires_manual", "ì´ë¯¸ì§€ OCRì„ ìœ„í•´ì„œëŠ” Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”."))
                            except Exception as ocr_error:
                                error_msg = L.get("ocr_error", "ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {error}")
                                st.warning(error_msg.format(error=str(ocr_error)))
                                file_content_extracted = ""
                        
                        # íŒŒì¼ ë‚´ìš©ì´ ì¶”ì¶œëœ ê²½ìš° ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ (ì¼ë³¸ì–´/ì˜ì–´ ë²„ì „ì—ì„œ í•œêµ­ì–´ íŒŒì¼ ë²ˆì—­)
                        if file_content_extracted and current_lang in ["ja", "en"]:
                            # í•œêµ­ì–´ ë‚´ìš©ì¸ì§€ í™•ì¸í•˜ê³  ë²ˆì—­
                            with st.spinner(L.get("detecting_language", "ì–¸ì–´ ê°ì§€ ì¤‘...")):
                                # ì–¸ì–´ ê°ì§€ í”„ë¡¬í”„íŠ¸ (í˜„ì¬ ì–¸ì–´ì— ë§ì¶¤)
                                detect_prompts = {
                                    "ja": f"""æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªã‚’æ¤œå‡ºã—ã¦ãã ã•ã„ã€‚éŸ“å›½èªã€æ—¥æœ¬èªã€è‹±èªã®ã„ãšã‚Œã‹ã§ç­”ãˆã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{file_content_extracted[:500]}

è¨€èª:""",
                                    "en": f"""Detect the language of the following text. Answer with only one of: Korean, Japanese, or English.

Text:
{file_content_extracted[:500]}

Language:""",
                                    "ko": f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•´ì£¼ì„¸ìš”. í•œêµ­ì–´, ì¼ë³¸ì–´, ì˜ì–´ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{file_content_extracted[:500]}

ì–¸ì–´:"""
                                }
                                detect_prompt = detect_prompts.get(current_lang, detect_prompts["ko"])
                                detected_lang = run_llm(detect_prompt).strip().lower()
                                
                                # í•œêµ­ì–´ë¡œ ê°ì§€ëœ ê²½ìš° í˜„ì¬ ì–¸ì–´ë¡œ ë²ˆì—­
                                if "í•œêµ­ì–´" in detected_lang or "korean" in detected_lang or "ko" in detected_lang:
                                    with st.spinner(L.get("translating_content", "íŒŒì¼ ë‚´ìš© ë²ˆì—­ ì¤‘...")):
                                        # ë²ˆì—­ í”„ë¡¬í”„íŠ¸ (í˜„ì¬ ì–¸ì–´ì— ë§ì¶¤)
                                        translate_prompts = {
                                            "ja": f"""æ¬¡ã®éŸ“å›½èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚åŸæ–‡ã®æ„å‘³ã¨ãƒˆãƒ¼ãƒ³ã‚’æ­£ç¢ºã«ç¶­æŒã—ãªãŒã‚‰ã€è‡ªç„¶ãªæ—¥æœ¬èªã§ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

éŸ“å›½èªãƒ†ã‚­ã‚¹ãƒˆ:
{file_content_extracted}

æ—¥æœ¬èªç¿»è¨³:""",
                                            "en": f"""Please translate the following Korean text into English. Maintain the exact meaning and tone of the original text while translating into natural English.

Korean text:
{file_content_extracted}

English translation:"""
                                        }
                                        translate_prompt = translate_prompts.get(current_lang)
                                        if translate_prompt:
                                            file_content_translated = run_llm(translate_prompt)
                                            if file_content_translated and not file_content_translated.startswith("âŒ"):
                                                st.info(L.get("file_translated", "âœ… íŒŒì¼ ë‚´ìš©ì´ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤."))
                                            else:
                                                file_content_translated = ""
                except Exception as e:
                    error_msg = L.get("file_extraction_error", "íŒŒì¼ ë‚´ìš© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")
                    st.warning(error_msg.format(error=str(e)))
            
            # ì–¸ì–´ë³„ íŒŒì¼ ì •ë³´ í…ìŠ¤íŠ¸ ìƒì„±
            file_content_to_include = file_content_translated if file_content_translated else file_content_extracted
            content_section = ""
            if file_content_to_include:
                content_section = f"\n\n[íŒŒì¼ ë‚´ìš©]\n{file_content_to_include[:2000]}"  # ìµœëŒ€ 2000ìë§Œ í¬í•¨
                if len(file_content_to_include) > 2000:
                    content_section += "\n...(ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë¨)"
            
            attachment_info_by_lang = {
                "ko": f"\n\n[ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´]\n- íŒŒì¼ëª…: {file_name}\n- íŒŒì¼ íƒ€ì…: {file_type}\n- íŒŒì¼ í¬ê¸°: {file_size} bytes\n- ì°¸ê³ : ê³ ê°ì´ {file_name} íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì€ ë¹„í–‰ê¸° ì§€ì—°, ì—¬ê¶Œ ì´ìŠˆ, ì§ˆë³‘ ë“± ë¶ˆê°€í”¼í•œ ì‚¬ìœ ë¡œ ì¸í•œ ì·¨ì†Œ ë¶ˆê°€ ì—¬í–‰ìƒí’ˆ ê´€ë ¨ ì¦ë¹™ ìë£Œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”.{content_section}",
                "en": f"\n\n[Customer Attachment Information]\n- File name: {file_name}\n- File type: {file_type}\n- File size: {file_size} bytes\n- Note: The customer has attached the file {file_name}. This file may be evidence related to non-refundable travel products due to unavoidable reasons such as flight delays, passport issues, illness, etc. Please refer to the file content when responding.{content_section}",
                "ja": f"\n\n[é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±]\n- ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}\n- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {file_type}\n- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size} bytes\n- å‚è€ƒ: é¡§å®¢ãŒ{file_name}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜ã—ã¾ã—ãŸã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€é£›è¡Œæ©Ÿã®é…å»¶ã€ãƒ‘ã‚¹ãƒãƒ¼ãƒˆã®å•é¡Œã€ç—…æ°—ãªã©ã‚„ã‚€ã‚’å¾—ãªã„ç†ç”±ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒ³ã‚»ãƒ«ä¸å¯ã®æ—…è¡Œå•†å“ã«é–¢é€£ã™ã‚‹è¨¼æ‹ è³‡æ–™ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å‚ç…§ã—ã¦å¯¾å¿œã—ã¦ãã ã•ã„ã€‚{content_section}"
            }
            attachment_info = attachment_info_by_lang.get(current_lang, attachment_info_by_lang["ko"])
            
            # ì´ë¯¸ì§€ íŒŒì¼ì¸ ê²½ìš° ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
            if file_type and file_type.startswith("image/"):
                st.image(uploaded_file, caption=file_name, use_container_width=True)
        
        col_ai_answer, col_hint = st.columns(2)
        
        # AI ë‹µì•ˆ ìƒì„±
        with col_ai_answer:
            if st.button(L["button_generate_ai_answer"], key="generate_ai_answer_btn", type="primary"):
                if customer_inquiry:
                    with st.spinner(L["generating_ai_answer"]):
                        # íšŒì‚¬ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨í•˜ì—¬ ë‹µì•ˆ ìƒì„±
                        company_context = ""
                        if selected_company_for_inquiry and selected_company_for_inquiry in faq_data.get("companies", {}):
                            company_data = get_company_info_faq(selected_company_for_inquiry, current_lang)
                            company_info_label = L.get("company_info", "íšŒì‚¬ ì •ë³´")
                            company_context = f"\n\n{company_info_label}: {company_data.get('info', '')}"
                            # ê´€ë ¨ FAQë„ í¬í•¨
                            related_faqs = company_data.get("faqs", [])[:5]  # ìƒìœ„ 5ê°œë§Œ
                            if related_faqs:
                                faq_label = L.get("company_faq", "ìì£¼ ë‚˜ì˜¤ëŠ” ì§ˆë¬¸")
                                faq_context = f"\n\n{faq_label}:\n"
                                for faq in related_faqs:
                                    q = faq.get(f"question_{current_lang}", faq.get("question_ko", ""))
                                    a = faq.get(f"answer_{current_lang}", faq.get("answer_ko", ""))
                                    faq_context += f"Q: {q}\nA: {a}\n"
                                company_context += faq_context
                        
                        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸
                        lang_prompts_inquiry = {
                            "ko": f"""ë‹¤ìŒ ê³ ê° ë¬¸ì˜ì— ëŒ€í•œ ì „ë¬¸ì ì´ê³  ì¹œì ˆí•œ ë‹µì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ê³ ê° ë¬¸ì˜: {customer_inquiry}
{company_context}
{attachment_info if attachment_info else ""}

ë‹µì•ˆì€ ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
1. ê³ ê°ì˜ ë¬¸ì˜ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€
2. í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ì •ë³´ë‚˜ ì•ˆë‚´
3. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤
4. ì²¨ë¶€ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°, í•´ë‹¹ íŒŒì¼ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”. íŠ¹íˆ ì·¨ì†Œ ë¶ˆê°€ ì—¬í–‰ìƒí’ˆì˜ ë¹„í–‰ê¸° ì§€ì—°, ì—¬ê¶Œ ì´ìŠˆ ë“± ë¶ˆê°€í”¼í•œ ì‚¬ìœ ì˜ ê²½ìš°, ì²¨ë¶€ëœ ì¦ë¹™ ìë£Œë¥¼ í™•ì¸í•˜ê³  ì ì ˆíˆ ëŒ€ì‘í•˜ì„¸ìš”.

ë‹µì•ˆ:""",
                            "en": f"""Please write a professional and friendly answer to the following customer inquiry.

Customer Inquiry: {customer_inquiry}
{company_context}
{attachment_info if attachment_info else ""}

The answer should include:
1. Clear answer to the customer's inquiry
2. Additional information or guidance if needed
3. Friendly and professional tone
4. If there is an attachment, please reference the file content in your response. For non-refundable travel products with unavoidable reasons (flight delays, passport issues, etc.), review the attached evidence and respond appropriately.

Answer:""",
                            "ja": f"""æ¬¡ã®é¡§å®¢å•ã„åˆã‚ã›ã«å¯¾ã™ã‚‹å°‚é–€çš„ã§è¦ªåˆ‡ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

é¡§å®¢å•ã„åˆã‚ã›: {customer_inquiry}
{company_context}
{attachment_info if attachment_info else ""}

å›ç­”ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™:
1. é¡§å®¢ã®å•ã„åˆã‚ã›ã«å¯¾ã™ã‚‹æ˜ç¢ºãªå›ç­”
2. å¿…è¦ã«å¿œã˜ã¦è¿½åŠ æƒ…å ±ã‚„æ¡ˆå†…
3. è¦ªåˆ‡ã§å°‚é–€çš„ãªãƒˆãƒ¼ãƒ³
4. æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã€ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’å‚ç…§ã—ã¦å¯¾å¿œã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«ä¸å¯ã®æ—…è¡Œå•†å“ã§ã€é£›è¡Œæ©Ÿã®é…å»¶ã€ãƒ‘ã‚¹ãƒãƒ¼ãƒˆã®å•é¡Œãªã©ã‚„ã‚€ã‚’å¾—ãªã„ç†ç”±ãŒã‚ã‚‹å ´åˆã¯ã€æ·»ä»˜ã•ã‚ŒãŸè¨¼æ‹ è³‡æ–™ã‚’ç¢ºèªã—ã€é©åˆ‡ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚

å›ç­”:"""
                        }
                        prompt = lang_prompts_inquiry.get(current_lang, lang_prompts_inquiry["ko"])
                        
                        ai_answer = run_llm(prompt)
                        st.session_state.generated_ai_answer = ai_answer
                        st.success(f"âœ… {L.get('ai_answer_generated', 'AI ë‹µì•ˆì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.')}")
                else:
                    st.warning(L.get("warning_enter_inquiry", "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."))
        
        # ì‘ëŒ€ íŒíŠ¸ ìƒì„±
        with col_hint:
            if st.button(L["button_generate_hint"], key="generate_hint_btn", type="primary"):
                if customer_inquiry:
                    with st.spinner(L["generating_hint"]):
                        # íšŒì‚¬ ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨í•˜ì—¬ íŒíŠ¸ ìƒì„±
                        company_context = ""
                        if selected_company_for_inquiry and selected_company_for_inquiry in faq_data.get("companies", {}):
                            company_data = get_company_info_faq(selected_company_for_inquiry, current_lang)
                            company_info_label = L.get("company_info", "íšŒì‚¬ ì •ë³´")
                            company_context = f"\n\n{company_info_label}: {company_data.get('info', '')}"
                        
                        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸
                        lang_prompts_hint = {
                            "ko": f"""ë‹¤ìŒ ê³ ê° ë¬¸ì˜ì— ëŒ€í•œ ì‘ëŒ€ íŒíŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ê³ ê° ë¬¸ì˜: {customer_inquiry}
{company_context}
{attachment_info if attachment_info else ""}

ì‘ëŒ€ íŒíŠ¸ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
1. ê³ ê° ë¬¸ì˜ì˜ í•µì‹¬ í¬ì¸íŠ¸
2. ì‘ëŒ€ ì‹œ ì£¼ì˜ì‚¬í•­
3. ê¶Œì¥ ì‘ëŒ€ ë°©ì‹
4. ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
5. ì²¨ë¶€ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°, í•´ë‹¹ íŒŒì¼ì„ í™•ì¸í•˜ê³  ì¦ë¹™ ìë£Œë¡œ í™œìš©í•˜ì„¸ìš”. íŠ¹íˆ ì·¨ì†Œ ë¶ˆê°€ ì—¬í–‰ìƒí’ˆì˜ ê²½ìš°, ì²¨ë¶€ëœ ì‚¬ì§„ì´ë‚˜ ìŠ¤í¬ë¦°ìƒ·ì„ í†µí•´ ë¶ˆê°€í”¼í•œ ì‚¬ìœ ë¥¼ í™•ì¸í•˜ê³  ì ì ˆí•œ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì„¸ìš”.

ì‘ëŒ€ íŒíŠ¸:""",
                            "en": f"""Please write response hints for the following customer inquiry.

Customer Inquiry: {customer_inquiry}
{company_context}
{attachment_info if attachment_info else ""}

Response hints should include:
1. Key points of the customer inquiry
2. Precautions when responding
3. Recommended response method
4. Items that need additional confirmation (if any)
5. If there is an attachment, review the file and use it as evidence. For non-refundable travel products, verify unavoidable reasons through attached photos or screenshots and take appropriate action.

Response Hints:""",
                            "ja": f"""æ¬¡ã®é¡§å®¢å•ã„åˆã‚ã›ã«å¯¾ã™ã‚‹å¯¾å¿œãƒ’ãƒ³ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

é¡§å®¢å•ã„åˆã‚ã›: {customer_inquiry}
{company_context}
{attachment_info if attachment_info else ""}

å¯¾å¿œãƒ’ãƒ³ãƒˆã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™:
1. é¡§å®¢å•ã„åˆã‚ã›ã®æ ¸å¿ƒãƒã‚¤ãƒ³ãƒˆ
2. å¯¾å¿œæ™‚ã®æ³¨æ„äº‹é …
3. æ¨å¥¨å¯¾å¿œæ–¹æ³•
4. è¿½åŠ ç¢ºèªãŒå¿…è¦ãªäº‹é …ï¼ˆã‚ã‚‹å ´åˆï¼‰
5. æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã€ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã€è¨¼æ‹ è³‡æ–™ã¨ã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«ä¸å¯ã®æ—…è¡Œå•†å“ã®å ´åˆã€æ·»ä»˜ã•ã‚ŒãŸå†™çœŸã‚„ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’é€šã˜ã¦ã‚„ã‚€ã‚’å¾—ãªã„ç†ç”±ã‚’ç¢ºèªã—ã€é©åˆ‡ãªæªç½®ã‚’å–ã£ã¦ãã ã•ã„ã€‚

å¯¾å¿œãƒ’ãƒ³ãƒˆ:"""
                        }
                        prompt = lang_prompts_hint.get(current_lang, lang_prompts_hint["ko"])
                        
                        hint = run_llm(prompt)
                        st.session_state.generated_hint = hint
                        st.success(f"âœ… {L.get('hint_generated', 'ì‘ëŒ€ íŒíŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.')}")
                else:
                    st.warning(L.get("warning_enter_inquiry", "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."))
        
        # ìƒì„±ëœ ê²°ê³¼ í‘œì‹œ
        if st.session_state.get("generated_ai_answer"):
            st.markdown("---")
            st.subheader(L["ai_answer_header"])
            
            answer_text = st.session_state.generated_ai_answer
            
            # ë‹µì•ˆì„ ì„ íƒ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ (í°íŠ¸ í¬ê¸° í™•ëŒ€)
            import html as html_escape
            answer_escaped = html_escape.escape(answer_text)
            st.markdown(f"""
            <div style="font-size: 18px; line-height: 1.8; padding: 20px; background-color: #f8f9fa; border-radius: 5px; border: 1px solid #dee2e6;">
            <pre style="white-space: pre-wrap; word-wrap: break-word; font-family: 'Malgun Gothic', 'ë§‘ì€ ê³ ë”•', 'Noto Sans JP', sans-serif; margin: 0; font-size: 18px; color: #212529;">{answer_escaped}</pre>
            </div>
            """, unsafe_allow_html=True)
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€ (ë” ì•ˆì •ì ì¸ ë³µì‚¬ ë°©ë²•)
            col_copy, col_download = st.columns(2)
            with col_copy:
                st.info(L.get("copy_instruction", "ğŸ’¡ ìœ„ í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ê³  Ctrl+C (Mac: Cmd+C)ë¡œ ë³µì‚¬í•˜ì„¸ìš”."))
            with col_download:
                st.download_button(
                    label=f"ğŸ“¥ {L.get('button_download_answer', 'ë‹µì•ˆ ë‹¤ìš´ë¡œë“œ')}",
                    data=answer_text.encode('utf-8'),
                    file_name=f"ai_answer_{st.session_state.get('copy_answer_id', 0)}.txt",
                    mime="text/plain",
                    key="download_answer_btn"
                )
        
        if st.session_state.get("generated_hint"):
            st.markdown("---")
            st.subheader(L["hint_header"])
            
            hint_text = st.session_state.generated_hint
            
            # íŒíŠ¸ë¥¼ ì„ íƒ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ (í°íŠ¸ í¬ê¸° í™•ëŒ€)
            import html as html_escape
            hint_escaped = html_escape.escape(hint_text)
            st.markdown(f"""
            <div style="font-size: 18px; line-height: 1.8; padding: 20px; background-color: #f8f9fa; border-radius: 5px; border: 1px solid #dee2e6;">
            <pre style="white-space: pre-wrap; word-wrap: break-word; font-family: 'Malgun Gothic', 'ë§‘ì€ ê³ ë”•', 'Noto Sans JP', sans-serif; margin: 0; font-size: 18px; color: #212529;">{hint_escaped}</pre>
            </div>
            """, unsafe_allow_html=True)
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€ (ë” ì•ˆì •ì ì¸ ë³µì‚¬ ë°©ë²•)
            col_copy_hint, col_download_hint = st.columns(2)
            with col_copy_hint:
                st.info(L.get("copy_instruction", "ğŸ’¡ ìœ„ í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ê³  Ctrl+C (Mac: Cmd+C)ë¡œ ë³µì‚¬í•˜ì„¸ìš”."))
            with col_download_hint:
                st.download_button(
                    label=f"ğŸ“¥ {L.get('button_download_hint', 'íŒíŠ¸ ë‹¤ìš´ë¡œë“œ')}",
                    data=hint_text.encode('utf-8'),
                    file_name=f"response_hint_{st.session_state.get('copy_hint_id', 0)}.txt",
                    mime="text/plain",
                    key="download_hint_btn"
                )
        
        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.session_state.get("generated_ai_answer") or st.session_state.get("generated_hint"):
            if st.button(f"ğŸ”„ {L.get('button_reset', 'ìƒˆë¡œ ì‹œì‘')}", key="reset_inquiry_btn"):
                st.session_state.generated_ai_answer = None
                st.session_state.generated_hint = None

# -------------------- Voice Record Tab --------------------

# -------------------- Voice Record Tab --------------------
if feature_selection == L["voice_rec_header"]:
    # ... (ê¸°ì¡´ ìŒì„± ê¸°ë¡ íƒ­ ë¡œì§ ìœ ì§€)
    st.header(L["voice_rec_header"])
    st.caption(L["record_help"])

    col_rec, col_list = st.columns([1, 1])

    # ë…¹ìŒ/ì—…ë¡œë“œ + ì „ì‚¬ + ì €ì¥
    with col_rec:
        st.subheader(L["rec_header"])
        audio_file = st.file_uploader(
            L["uploaded_file"],
            type=["wav", "mp3", "m4a", "webm", "ogg"],
            key="voice_rec_uploader",
        )
        audio_bytes = None
        audio_mime = "audio/webm"

        if audio_file is not None:
            audio_bytes = audio_file.getvalue()
            audio_mime = audio_file.type or "audio/webm"

        # ì¬ìƒ
        # Streamlit ë¬¸ì„œ: bytes, íŒŒì¼ ê²½ë¡œ, URL ëª¨ë‘ ì§€ì›
        if audio_bytes:
            try:
                # MIME íƒ€ì…ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê³  ê¸°ë³¸ê°’ ì„¤ì •
                valid_formats = ["audio/wav", "audio/mp3", "audio/mpeg", "audio/webm", "audio/ogg", "audio/m4a"]
                if audio_mime not in valid_formats:
                    # MIME íƒ€ì…ì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ íŒŒì¼ í™•ì¥ìë¡œ ì¶”ì •
                    audio_mime = "audio/wav"  # ê¸°ë³¸ê°’
                st.audio(audio_bytes, format=audio_mime)
            except Exception as e:
                st.error(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                # ê¸°ë³¸ í¬ë§·ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    st.audio(audio_bytes, format="audio/wav")
                except:
                    st.error("ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì¬ìƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì „ì‚¬ ë²„íŠ¼
        if audio_bytes and st.button(L["transcribe_btn"]):
            if st.session_state.openai_client is None:
                st.error(L["openai_missing"])
            else:
                with st.spinner(L["transcribing"]):
                    # ìë™ ì–¸ì–´ ê°ì§€ ì‚¬ìš© (ì…ë ¥ ì–¸ì–´ì™€ ê´€ê³„ì—†ì´ ì •í™•í•œ ì „ì‚¬)
                    text = transcribe_bytes_with_whisper(
                        audio_bytes, audio_mime, lang_code=None, auto_detect=True
                    )
                    st.session_state.last_transcript = text
                    snippet = text[:50].replace("\n", " ")
                    if len(text) > 50:
                        snippet += "..."
                    if text.startswith("âŒ"):
                        st.error(text)
                    else:
                        st.success(f"{L['transcript_result']} **{snippet}**")

        st.text_area(
            L["transcript_text"],
            value=st.session_state.last_transcript,
            height=150,
            key="voice_rec_transcript_area",
        )

        if audio_bytes and st.button(L["save_btn"]):
            try:
                ext = audio_mime.split("/")[-1] if "/" in audio_mime else "webm"
                filename = f"record_{int(time.time())}.{ext}"
                save_audio_record_local(
                    audio_bytes,
                    filename,
                    st.session_state.last_transcript,
                    mime_type=audio_mime,
                )
                st.success(L["saved_success"])
                st.session_state.last_transcript = ""
            except Exception as e:
                st.error(f"{L['error']} {e}")

    # ì €ì¥ëœ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
    with col_list:
        st.subheader(L["rec_list_title"])
        try:
            records = load_voice_records()
        except Exception as e:
            st.error(f"read error: {e}")
            records = []

        if not records:
            st.info(L["no_records"])
        else:
            for rec in records:
                rec_id = rec["id"]
                created_at = rec.get("created_at")
                try:
                    dt = datetime.fromisoformat(created_at)
                    created_str = dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    created_str = str(created_at)

                transcript_snippet = (rec.get("transcript") or "")[:50].replace("\n", " ")
                if len(rec.get("transcript") or "") > 50:
                    transcript_snippet += "..."

                with st.expander(f"[{created_str}] {transcript_snippet}"):
                    st.write(f"**{L['transcript_text']}:** {rec.get('transcript') or 'N/A'}")
                    st.caption(
                        f"**Size:** {rec.get('size')} bytes | **File:** {rec.get('audio_filename')}"
                    )

                    col_p, col_r, col_d = st.columns([2, 1, 1])

                    if col_p.button(L["playback"], key=f"play_{rec_id}"):
                        try:
                            b, info = get_audio_bytes_local(rec_id)
                            mime = info.get("mime_type", "audio/webm")
                            # Streamlit ë¬¸ì„œ: bytes ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
                            # MIME íƒ€ì… ê²€ì¦
                            valid_formats = ["audio/wav", "audio/mp3", "audio/mpeg", "audio/webm", "audio/ogg", "audio/m4a"]
                            if mime not in valid_formats:
                                mime = "audio/wav"  # ê¸°ë³¸ê°’
                            st.audio(b, format=mime, autoplay=False)
                        except Exception as e:
                            st.error(f"{L['gcs_playback_fail']}: {e}")

                    if col_r.button(L["retranscribe"], key=f"re_{rec_id}"):
                        if st.session_state.openai_client is None:
                            st.error(L["openai_missing"])
                        else:
                            with st.spinner(L["transcribing"]):
                                try:
                                    b, info = get_audio_bytes_local(rec_id)
                                    mime = info.get("mime_type", "audio/webm")
                                    # ìë™ ì–¸ì–´ ê°ì§€ ì‚¬ìš© (ì…ë ¥ ì–¸ì–´ì™€ ê´€ê³„ì—†ì´ ì •í™•í•œ ì „ì‚¬)
                                    new_text = transcribe_bytes_with_whisper(
                                        b, mime, lang_code=None, auto_detect=True
                                    )
                                    records = load_voice_records()
                                    for r in records:
                                        if r["id"] == rec_id:
                                            r["transcript"] = new_text
                                            break
                                    save_voice_records(records)
                                    st.success(L["retranscribe"] + " " + L["saved_success"])
                                except Exception as e:
                                    st.error(f"{L['error']} {e}")

                    if col_d.button(L["delete"], key=f"del_{rec_id}"):
                        if st.session_state.get(f"confirm_del_{rec_id}", False):
                            ok = delete_audio_record_local(rec_id)
                            if ok:
                                st.success(L["delete_success"])
                            else:
                                st.error(L["delete_fail"])
                            st.session_state[f"confirm_del_{rec_id}"] = False
                        else:
                            st.session_state[f"confirm_del_{rec_id}"] = True
                            st.warning(L["delete_confirm_rec"])
                            st.write("sim_stage:", st.session_state.get("sim_stage"))
                            st.write("is_llm_ready:", st.session_state.get("is_llm_ready"))

# -------------------- Simulator (Chat/Email) Tab --------------------
elif feature_selection == L["sim_tab_chat_email"]:
    # ... (ê¸°ì¡´ ì±„íŒ…/ì´ë©”ì¼ ì‹œë®¬ë ˆì´í„° ë¡œì§ ìœ ì§€)
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])

    current_lang = st.session_state.language
    L = LANG[current_lang]  # ë‹¤ì‹œ L ì—…ë°ì´íŠ¸

    # =========================
    # 0. ì „ì²´ ì´ë ¥ ì‚­ì œ
    # =========================
    col_del, _ = st.columns([1, 4])
    with col_del:
        if st.button(L["delete_history_button"], key="trigger_delete_hist"):
            st.session_state.show_delete_confirm = True

    if st.session_state.show_delete_confirm:
        with st.container():
            st.warning(L["delete_confirm_message"])
            c_yes, c_no = st.columns(2)
            if c_yes.button(L["delete_confirm_yes"], key="confirm_del_yes"):
                with st.spinner(L["deleting_history_progress"]):
                    delete_all_history_local()
                    st.session_state.simulator_messages = []
                    st.session_state.simulator_memory.clear()
                    st.session_state.show_delete_confirm = False
                    st.session_state.is_chat_ended = False
                    st.session_state.sim_stage = "WAIT_FIRST_QUERY"
                    st.session_state.customer_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
                    st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                    st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
                    st.success(L["delete_success"])
            if c_no.button(L["delete_confirm_no"], key="confirm_del_no"):
                st.session_state.show_delete_confirm = False

    # =========================
    # 1. ì´ì „ ì´ë ¥ ë¡œë“œ (ê²€ìƒ‰/í•„í„°ë§ ê¸°ëŠ¥ ê°œì„ )
    # =========================
    with st.expander(L["history_expander_title"]):
        # Always load all available histories for the current language (sorted by recency)
        histories = load_simulation_histories_local(current_lang)

        # ì „ì²´ í†µê³„ ë° íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        cases_with_summary = [
            h for h in histories
            if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
               and not h.get("is_call", False)  # ì „í™” ì´ë ¥ ì œì™¸
        ]

        if cases_with_summary:
            st.markdown("---")
            st.subheader("ğŸ“ˆ ê³¼ê±° ì¼€ì´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ")

            # íŠ¸ë Œë“œ ì°¨íŠ¸ í‘œì‹œ
            trend_chart = visualize_case_trends(histories, current_lang)
            if trend_chart:
                st.plotly_chart(trend_chart, use_container_width=True)
            else:
                # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                avg_sentiment = np.mean(
                    [h["summary"].get("customer_sentiment_score", 50) for h in cases_with_summary if h.get("summary")])
                avg_satisfaction = np.mean(
                    [h["summary"].get("customer_satisfaction_score", 50) for h in cases_with_summary if
                     h.get("summary")])
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("í‰ê·  ê°ì • ì ìˆ˜", f"{avg_sentiment:.1f}/100", f"ì´ {len(cases_with_summary)}ê±´")
                with col2:
                    st.metric("í‰ê·  ë§Œì¡±ë„", f"{avg_satisfaction:.1f}/100", f"ì´ {len(cases_with_summary)}ê±´")

            st.markdown("---")

        # â­ ê²€ìƒ‰ í¼ ì œê±° ë° ë…ë¦½ëœ ìœ„ì ¯ ì‚¬ìš©
        col_search, col_btn = st.columns([4, 1])

        with col_search:
            # st.text_inputì€ Enter í‚¤ ì…ë ¥ ì‹œ ì•±ì„ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.
            search_query = st.text_input(L["search_history_label"], key="sim_hist_search_input_new")

        with col_btn:
            # ê²€ìƒ‰ ë²„íŠ¼: ëˆ„ë¥´ë©´ ì•±ì„ ê°•ì œ ì¬ì‹¤í–‰í•˜ì—¬ ê²€ìƒ‰/í•„í„°ë§ ë¡œì§ì„ ë‹¤ì‹œ íƒ€ë„ë¡ í•©ë‹ˆë‹¤.
            st.markdown("<br>", unsafe_allow_html=True)  # Align button vertically
            search_clicked = st.button(L["history_search_button"], key="apply_search_btn_new")

        # ë‚ ì§œ ë²”ìœ„ í•„í„°
        today = datetime.now().date()
        date_range_value = [today - timedelta(days=7), today]
        dr = st.date_input(
            L["date_range_label"],
            value=date_range_value,
            key="sim_hist_date_range_actual",
        )

        # --- Filtering Logic ---
        current_search_query = search_query.strip()

        if histories:
            start_date = min(dr)
            end_date = max(dr)

            filtered = []
            for h in histories:
                # ì „í™” ì´ë ¥ì€ ì œì™¸ (ì±„íŒ…/ì´ë©”ì¼ íƒ­ì´ë¯€ë¡œ)
                if h.get("is_call", False):
                    continue

                ok_search = True
                if current_search_query:
                    q = current_search_query.lower()
                    # ê²€ìƒ‰ ëŒ€ìƒ: ì´ˆê¸° ë¬¸ì˜, ê³ ê° ìœ í˜•, ìš”ì•½ ë°ì´í„°
                    text = (h["initial_query"] + " " + h["customer_type"]).lower()

                    # ìš”ì•½ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš”ì•½ ë‚´ìš©ë„ ê²€ìƒ‰ ëŒ€ìƒì— í¬í•¨
                    summary = h.get("summary")
                    if summary and isinstance(summary, dict):
                        summary_text = summary.get("main_inquiry", "") + " " + summary.get("summary", "")
                        text += " " + summary_text.lower()

                    # Check if query matches in initial query, customer type, or summary
                    if q not in text:
                        ok_search = False

                ok_date = True
                ts = h.get("timestamp")
                if ts:
                    try:
                        d = datetime.fromisoformat(ts).date()
                        # Apply date filtering
                        if not (start_date <= d <= end_date):
                            ok_date = False
                    except Exception:
                        pass  # Ignore histories with invalid timestamp

                if ok_search and ok_date:
                    filtered.append(h)
        else:
            filtered = []

        # Determine the list for display (â­ ìš”ì²­ ì‚¬í•­: ê²€ìƒ‰ì–´/í•„í„°ê°€ ì—†ìœ¼ë©´ ìµœê·¼ 10ê±´ë§Œ í‘œì‹œ)
        is_searching_or_filtering = bool(current_search_query) or dr != date_range_value

        if not is_searching_or_filtering:
            # ê²€ìƒ‰/í•„í„° ì¡°ê±´ì´ ì—†ìœ¼ë©´, ì „ì²´ ì´ë ¥ ì¤‘ ìµœì‹  10ê±´ë§Œ í‘œì‹œ
            filtered_for_display = filtered[:10]  # í•„í„°ë§ëœ ëª©ë¡(ì „í™” ì œì™¸) ì¤‘ 10ê°œ
        else:
            # ê²€ìƒ‰/í•„í„° ì¡°ê±´ì´ ìˆìœ¼ë©´, í•„í„°ë§ëœ ëª¨ë“  ê²°ê³¼ë¥¼ í‘œì‹œ
            filtered_for_display = filtered

        # --- Display Logic ---

        if filtered_for_display:
            def _label(h):
                try:
                    t = datetime.fromisoformat(h["timestamp"])
                    t_str = t.strftime("%m-%d %H:%M")
                except Exception:
                    t_str = h.get("timestamp", "")

                # ìš”ì•½ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš”ì•½ ì •ë³´ í‘œì‹œ, ì—†ìœ¼ë©´ ì´ˆê¸° ë¬¸ì˜ í‘œì‹œ
                summary = h.get("summary")
                if summary and isinstance(summary, dict):
                    main_inquiry = summary.get("main_inquiry", h["initial_query"][:30])
                    sentiment = summary.get("customer_sentiment_score", 50)
                    satisfaction = summary.get("customer_satisfaction_score", 50)
                    q = main_inquiry[:30].replace("\n", " ")
                    # ì²¨ë¶€ íŒŒì¼ ì—¬ë¶€ í‘œì‹œ ì¶”ê°€
                    attachment_icon = "ğŸ“" if h.get("attachment_context") else ""
                    # ìš”ì•½ ë°ì´í„° í‘œì‹œ (ê°ì •/ë§Œì¡±ë„ ì ìˆ˜ í¬í•¨)
                    return f"[{t_str}] {attachment_icon} {h['customer_type']} | ê°ì •:{sentiment} ë§Œì¡±:{satisfaction} - {q}..."
                else:
                    q = h["initial_query"][:30].replace("\n", " ")
                    attachment_icon = "ğŸ“" if h.get("attachment_context") else ""
                    return f"[{t_str}] {attachment_icon} {h['customer_type']} - {q}..."


            options_map = {_label(h): h for h in filtered_for_display}

            # Show a message indicating what is displayed if filters were applied
            if is_searching_or_filtering:
                st.caption(f"ğŸ” ì´ {len(filtered_for_display)}ê°œ ì´ë ¥ ê²€ìƒ‰ë¨ (ì „í™” ì´ë ¥ ì œì™¸)")
            else:
                st.caption(f"â­ ìµœê·¼ {len(filtered_for_display)}ê°œ ì´ë ¥ í‘œì‹œ ì¤‘ (ì „í™” ì´ë ¥ ì œì™¸)")

            sel_key = st.selectbox(L["history_selectbox_label"], options=list(options_map.keys()))

            if st.button(L["history_load_button"], key="load_hist_btn"):
                h = options_map[sel_key]
                st.session_state.customer_query_text_area = h["initial_query"]

                # ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆê³  ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì†Œí•œì˜ ë©”ì‹œì§€ ì¬êµ¬ì„±
                if not h.get("messages") and h.get("summary"):
                    summary = h["summary"]
                    # ìš”ì•½ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±
                    reconstructed_messages = [
                        {"role": "customer", "content": h["initial_query"]}
                    ]
                    # ìš”ì•½ì—ì„œ í•µì‹¬ ì‘ë‹µ ì¶”ê°€
                    if summary.get("key_responses"):
                        for response in summary.get("key_responses", [])[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                            reconstructed_messages.append({"role": "agent_response", "content": response})
                    # ìš”ì•½ ì •ë³´ë¥¼ supervisor ë©”ì‹œì§€ë¡œ ì¶”ê°€
                    summary_text = f"**ìš”ì•½ëœ ìƒë‹´ ì´ë ¥**\n\n"
                    summary_text += f"ì£¼ìš” ë¬¸ì˜: {summary.get('main_inquiry', 'N/A')}\n"
                    summary_text += f"ê³ ê° ê°ì • ì ìˆ˜: {summary.get('customer_sentiment_score', 50)}/100\n"
                    summary_text += f"ê³ ê° ë§Œì¡±ë„: {summary.get('customer_satisfaction_score', 50)}/100\n"
                    summary_text += f"\nì „ì²´ ìš”ì•½:\n{summary.get('summary', 'N/A')}"
                    reconstructed_messages.append({"role": "supervisor", "content": summary_text})
                    st.session_state.simulator_messages = reconstructed_messages

                    # ìš”ì•½ ë°ì´í„° ì‹œê°í™”
                    st.markdown("---")
                    st.subheader("ğŸ“Š ë¡œë“œëœ ì¼€ì´ìŠ¤ ë¶„ì„")

                    # ìš”ì•½ ë°ì´í„°ë¥¼ í”„ë¡œí•„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    loaded_profile = {
                        "sentiment_score": summary.get("customer_sentiment_score", 50),
                        "urgency_level": "medium",  # ê¸°ë³¸ê°’
                        "predicted_customer_type": h.get("customer_type", "normal")
                    }

                    # í”„ë¡œí•„ ì ìˆ˜ ì°¨íŠ¸
                    profile_chart = visualize_customer_profile_scores(loaded_profile, current_lang)
                    if profile_chart:
                        st.plotly_chart(profile_chart, use_container_width=True)
                    else:
                        # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric(L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
                                      f"{summary.get('customer_sentiment_score', 50)}/100")
                        with col2:
                            st.metric(L.get("urgency_score_label", "ê¸´ê¸‰ë„"), f"50/100")
                        with col3:
                            st.metric(L.get("customer_type_label", "ê³ ê° ìœ í˜•"), h.get("customer_type", "normal"))

                    # ê³ ê° íŠ¹ì„± ì‹œê°í™”
                    if summary.get("customer_characteristics") or summary.get("privacy_info"):
                        characteristics_chart = visualize_customer_characteristics(summary, current_lang)
                        if characteristics_chart:
                            st.plotly_chart(characteristics_chart, use_container_width=True)
                else:
                    # ê¸°ì¡´ ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    st.session_state.simulator_messages = h.get("messages", [])

                st.session_state.initial_advice_provided = True
                st.session_state.is_chat_ended = h.get("is_chat_ended", False)
                st.session_state.sim_attachment_context_for_llm = h.get("attachment_context", "")  # ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
                st.session_state.customer_attachment_file = []  # ë¡œë“œëœ ì´ë ¥ì—ëŠ” íŒŒì¼ ê°ì²´ ëŒ€ì‹  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë§Œ ì‚¬ìš©
                st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”

                # ìƒíƒœ ë³µì›
                if st.session_state.is_chat_ended:
                    st.session_state.sim_stage = "CLOSING"
                else:
                    messages = st.session_state.simulator_messages
                    last_role = messages[-1]["role"] if messages else None
                    if last_role == "agent_response":
                        st.session_state.sim_stage = "CUSTOMER_TURN"
                    elif last_role == "customer_rebuttal":
                        st.session_state.sim_stage = "AGENT_TURN"
                    elif last_role == "supervisor" and messages and messages[-1]["content"] == L[
                        "customer_closing_confirm"]:
                        st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"
                    else:
                        st.session_state.sim_stage = "AGENT_TURN"

                st.session_state.simulator_memory.clear()  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        else:
            st.info(L["no_history_found"])

    # =========================
    # AHT íƒ€ì´ë¨¸ (í™”ë©´ ìµœìƒë‹¨)
    # =========================
    if st.session_state.sim_stage not in ["WAIT_FIRST_QUERY", "CLOSING", "idle"]:
        elapsed_placeholder = st.empty()

        if st.session_state.start_time is not None:
            # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ í˜ì´ì§€ ë¡œë“œ ì‹œë§ˆë‹¤ í˜„ì¬ ì‹œê°„ ê³„ì‚°
            elapsed_time = datetime.now() - st.session_state.start_time
            total_seconds = elapsed_time.total_seconds()

            # Hold ì‹œê°„ ì œì™¸ (ì±„íŒ…/ì´ë©”ì¼ì€ Hold ì—†ìŒ, ì „í™” íƒ­ê³¼ ë¡œì§ í†µì¼ ìœ„í•´ ìœ ì§€)
            # total_seconds -= st.session_state.total_hold_duration.total_seconds()

            # ì‹œê°„ í˜•ì‹ í¬ë§·íŒ…
            minutes = int(total_seconds // 60)
            seconds = int(total_seconds % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # ê²½ê³  ê¸°ì¤€
            if total_seconds > 900:  # 15ë¶„
                delta_str = L["timer_info_risk"]
                delta_color = "inverse"
            elif total_seconds > 600:  # 10ë¶„
                delta_str = L["timer_info_warn"]
                delta_color = "off"
            else:
                delta_str = L["timer_info_ok"]
                delta_color = "normal"

            elapsed_placeholder.metric(
                L["timer_metric"],
                time_str,
                delta=delta_str,
                delta_color=delta_color
            )

            # â­ ìˆ˜ì •: 3ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰í•˜ì—¬ AHT ì‹¤ì‹œê°„ì„± í™•ë³´
            if seconds % 3 == 0 and total_seconds < 1000:
                time.sleep(1)

        st.markdown("---")

    # =========================
    # 2. LLM ì¤€ë¹„ ì²´í¬ & ì±„íŒ… ì¢…ë£Œ ìƒíƒœ
    # =========================
    if not st.session_state.is_llm_ready:
        st.warning(L["simulation_no_key_warning"])

    if st.session_state.sim_stage == "CLOSING":
        st.success(L["survey_sent_confirm"])
        st.info(L["new_simulation_ready"])
        
        # â­ ì¶”ê°€: í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
        st.markdown("---")
        st.markdown("**ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ**")
        download_col1, download_col2, download_col3 = st.columns(3)
        
        # í˜„ì¬ ì„¸ì…˜ì˜ ì´ë ¥ì„ ìƒì„±
        current_session_history = None
        if st.session_state.simulator_messages:
            try:
                customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
                current_session_summary = generate_chat_summary(
                    st.session_state.simulator_messages,
                    st.session_state.customer_query_text_area,
                    customer_type_display,
                    st.session_state.language
                )
                current_session_history = [{
                    "id": f"session_{st.session_state.sim_instance_id}",
                    "timestamp": datetime.now().isoformat(),
                    "initial_query": st.session_state.customer_query_text_area,
                    "customer_type": customer_type_display,
                    "language_key": st.session_state.language,
                    "messages": st.session_state.simulator_messages,
                    "summary": current_session_summary,
                    "is_chat_ended": True,
                    "attachment_context": st.session_state.sim_attachment_context_for_llm
                }]
            except Exception as e:
                st.warning(f"ì´ë ¥ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤ì„ ì§ì ‘ í‘œì‹œ
        if current_session_history:
            # í˜„ì¬ ì–¸ì–´ ê°€ì ¸ì˜¤ê¸°
            current_lang = st.session_state.get("language", "ko")
            if current_lang not in ["ko", "en", "ja"]:
                current_lang = "ko"
            
            with download_col1:
                try:
                    filepath_word = export_history_to_word(current_session_history, lang=current_lang)
                    with open(filepath_word, "rb") as f:
                        st.download_button(
                            label=L.get("download_history_word", "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (Word)"),
                            data=f.read(),
                            file_name=os.path.basename(filepath_word),
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            key="download_word_file"
                        )
                except Exception as e:
                    st.error(f"Word ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col2:
                try:
                    filepath_pptx = export_history_to_pptx(current_session_history, lang=current_lang)
                    with open(filepath_pptx, "rb") as f:
                        st.download_button(
                            label=L.get("download_history_pptx", "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PPTX)"),
                            data=f.read(),
                            file_name=os.path.basename(filepath_pptx),
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                            key="download_pptx_file"
                        )
                except Exception as e:
                    st.error(f"PPTX ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col3:
                try:
                    filepath_pdf = export_history_to_pdf(current_session_history, lang=current_lang)
                    with open(filepath_pdf, "rb") as f:
                        st.download_button(
                            label=L.get("download_history_pdf", "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PDF)"),
                            data=f.read(),
                            file_name=os.path.basename(filepath_pdf),
                            mime="application/pdf",
                            key="download_pdf_file"
                        )
                except Exception as e:
                    st.error(f"PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        else:
            st.warning("ë‹¤ìš´ë¡œë“œí•  ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        
        if st.button(L["new_simulation_button"], key="new_simulation_btn"):
            # ì´ˆê¸°í™” ë¡œì§
            st.session_state.simulator_messages = []
            st.session_state.simulator_memory.clear()
            st.session_state.initial_advice_provided = False
            st.session_state.is_chat_ended = False
            st.session_state.agent_response_area_text = ""
            st.session_state.customer_query_text_area = ""
            st.session_state.last_transcript = ""
            st.session_state.sim_audio_bytes = None
            st.session_state.sim_stage = "WAIT_FIRST_QUERY"
            st.session_state.customer_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.start_time = None
            # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""
            st.session_state.sim_call_outbound_target = None
        # st.stop()

    # =========================
    # 5-A. ì „í™” ë°œì‹  ì§„í–‰ ì¤‘ (OUTBOUND_CALL_IN_PROGRESS)
    # =========================
    elif st.session_state.sim_stage == "OUTBOUND_CALL_IN_PROGRESS":
        # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
        current_lang = st.session_state.get("language", "ko")
        if current_lang not in ["ko", "en", "ja"]:
            current_lang = "ko"
        L = LANG.get(current_lang, LANG["ko"])
        target = st.session_state.get("sim_call_outbound_target", "ëŒ€ìƒ")
        st.warning(L["call_outbound_loading"])

        # LLM í˜¸ì¶œ ë° ìš”ì•½ ìƒì„±
        with st.spinner(L["call_outbound_loading"]):
            # 1. LLM í˜¸ì¶œí•˜ì—¬ í†µí™” ìš”ì•½ ìƒì„±
            summary = generate_outbound_call_summary(
                st.session_state.customer_query_text_area,
                st.session_state.language,
                target
            )

            # 2. ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì „í™” ì‹œë„) ì¶”ê°€
            st.session_state.simulator_messages.append(
                {"role": "system_end", "content": L["call_outbound_system_msg"].format(target=target)}
            )

            # 3. ìš”ì•½ ë©”ì‹œì§€ (ê²°ê³¼) ì¶”ê°€
            summary_markdown = f"### {L['call_outbound_summary_header']}\n\n{summary}"
            st.session_state.simulator_messages.append(
                {"role": "supervisor", "content": summary_markdown}
            )

            # 4. Agent Turnìœ¼ë¡œ ë³µê·€
            st.session_state.sim_stage = "AGENT_TURN"
            st.session_state.sim_call_outbound_summary = summary_markdown  # Save for display/reference
            st.session_state.sim_call_outbound_target = None  # Reset target

            # 5. ì´ë ¥ ì €ì¥ (ì „í™” ë°œì‹  í›„ ìƒíƒœ ì €ì¥)
            customer_type_display = st.session_state.get("customer_type_sim_select", "")
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display + f" (Outbound Call to {target})",
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )

        st.success(f"âœ… {L['call_outbound_simulation_header']}ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ í™•ì¸í•˜ê³  ê³ ê°ì—ê²Œ íšŒì‹ í•˜ì„¸ìš”.")

    # ========================================
    # 3. ì´ˆê¸° ë¬¸ì˜ ì…ë ¥ (WAIT_FIRST_QUERY)
    # ========================================
    if st.session_state.sim_stage == "WAIT_FIRST_QUERY":
        customer_query = st.text_area(
            L["customer_query_label"],
            key="customer_query_text_area",
            height=150,
            placeholder=L["initial_query_sample"],
        )

        # --- í•„ìˆ˜ ì…ë ¥ í•„ë“œ (ìš”ì²­ 3 ë°˜ì˜: UI í…ìŠ¤íŠ¸ ë³€ê²½) ---
        customer_email = st.text_input(
            L["customer_email_label"],
            key="customer_email_input",
            value=st.session_state.customer_email,
        )
        customer_phone = st.text_input(
            L["customer_phone_label"],
            key="customer_phone_input",
            value=st.session_state.customer_phone,
        )
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.customer_email = customer_email
        st.session_state.customer_phone = customer_phone
        # --------------------------------------------------

        customer_type_options = L["customer_type_options"]
        # st.session_state.customer_type_sim_selectëŠ” ì´ë¯¸ ì´ˆê¸°í™”ë¨
        default_idx = customer_type_options.index(
            st.session_state.customer_type_sim_select) if st.session_state.customer_type_sim_select in customer_type_options else 0

        # SelectboxëŠ” ìì²´ì ìœ¼ë¡œ ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ, ì—¬ê¸°ì— valueë¥¼ ì„¤ì •í•  í•„ìš” ì—†ìŒ
        st.session_state.customer_type_sim_select = st.selectbox(
            L["customer_type_label"],
            customer_type_options,
            index=default_idx,
            key="customer_type_sim_select_widget",
        )

        # --- ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë” ì¶”ê°€ ---
        customer_attachment_widget = st.file_uploader(
            L["attachment_label"],
            type=["png", "jpg", "jpeg", "pdf"],
            key="customer_attachment_file_uploader",
            help=L["attachment_placeholder"],
            accept_multiple_files=False  # ì±„íŒ…/ì´ë©”ì¼ì€ ë‹¨ì¼ íŒŒì¼ë§Œ í—ˆìš©
        )

        # íŒŒì¼ ì •ë³´ ì €ì¥ ë° LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        if customer_attachment_widget:
            st.session_state.customer_attachment_file = customer_attachment_widget
            st.session_state.sim_attachment_context_for_llm = L["attachment_status_llm"].format(
                filename=customer_attachment_widget.name, filetype=customer_attachment_widget.type
            )
        else:
            st.session_state.customer_attachment_file = None
            st.session_state.sim_attachment_context_for_llm = ""
        # --------------------------

        if st.button(L["button_simulate"], key=f"btn_simulate_initial_{st.session_state.sim_instance_id}"):  # ê³ ìœ  í‚¤ ì‚¬ìš©
            if not customer_query.strip():
                st.warning(L["simulation_warning_query"])
                # st.stop()

            # --- í•„ìˆ˜ ì…ë ¥ í•„ë“œ ê²€ì¦ (ìš”ì²­ 3 ë°˜ì˜: ê²€ì¦ ë¡œì§ ì¶”ê°€) ---
            if not st.session_state.customer_email.strip() or not st.session_state.customer_phone.strip():
                st.error(L["error_mandatory_contact"])
                # st.stop()
            # ------------------------------------------

            # ì´ˆê¸° ìƒíƒœ ë¦¬ì…‹
            st.session_state.simulator_messages = []
            st.session_state.simulator_memory.clear()
            st.session_state.is_chat_ended = False
            st.session_state.initial_advice_provided = False
            st.session_state.is_solution_provided = False  # ì†”ë£¨ì…˜ í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state.language_transfer_requested = False  # ì–¸ì–´ ìš”ì²­ í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state.transfer_summary_text = ""  # ì´ê´€ ìš”ì•½ ë¦¬ì…‹
            st.session_state.start_time = None  # AHT íƒ€ì´ë¨¸ ì´ˆê¸°í™” (ì²« ê³ ê° ë°˜ì‘ í›„ ì‹œì‘)
            st.session_state.sim_instance_id = str(uuid.uuid4())  # ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ID í• ë‹¹
            # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""
            st.session_state.sim_call_outbound_target = None

            # 1) ê³ ê° ì²« ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.simulator_messages.append(
                {"role": "customer", "content": customer_query}
            )

            # 2) Supervisor ê°€ì´ë“œ + ì´ˆì•ˆ ìƒì„±
            # ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
            try:
                detected_lang = detect_text_language(customer_query)
                # ê°ì§€ëœ ì–¸ì–´ê°€ ìœ íš¨í•œì§€ í™•ì¸
                if detected_lang not in ["ko", "en", "ja"]:
                    detected_lang = current_lang
                else:
                    # ì–¸ì–´ê°€ ê°ì§€ë˜ì—ˆê³  í˜„ì¬ ì–¸ì–´ì™€ ë‹¤ë¥´ë©´ ìë™ìœ¼ë¡œ ì–¸ì–´ ì„¤ì • ì—…ë°ì´íŠ¸
                    if detected_lang != current_lang:
                        st.session_state.language = detected_lang
                        st.info(f"ğŸŒ ì…ë ¥ ì–¸ì–´ê°€ ê°ì§€ë˜ì–´ ì–¸ì–´ ì„¤ì •ì´ '{detected_lang}'ë¡œ ìë™ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"Language detection failed: {e}")
                detected_lang = current_lang  # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±
            
            # ê³ ê° í”„ë¡œí•„ ë¶„ì„ (ì‹œê°í™”ë¥¼ ìœ„í•´ ë¨¼ì € ìˆ˜í–‰, ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
            customer_profile = analyze_customer_profile(customer_query, detected_lang)
            similar_cases = find_similar_cases(customer_query, customer_profile, detected_lang, limit=5)

            # ì‹œê°í™” ì°¨íŠ¸ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“Š ê³ ê° í”„ë¡œí•„ ë¶„ì„")

            # ê³ ê° í”„ë¡œí•„ ì ìˆ˜ ì°¨íŠ¸ (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
            profile_chart = visualize_customer_profile_scores(customer_profile, detected_lang)
            if profile_chart:
                st.plotly_chart(profile_chart, use_container_width=True)
            else:
                # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    gender_display = customer_profile.get("gender", "unknown")
                    if gender_display == "male":
                        gender_display = "ë‚¨ì"
                    elif gender_display == "female":
                        gender_display = "ì—¬ì"
                    else:
                        gender_display = "ì•Œ ìˆ˜ ì—†ìŒ"
                    st.metric(
                        "ì„±ë³„",
                        gender_display
                    )
                with col2:
                    st.metric(
                        L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
                        f"{customer_profile.get('sentiment_score', 50)}/100"
                    )
                with col3:
                    urgency_map = {"low": 25, "medium": 50, "high": 75}
                    urgency_score = urgency_map.get(customer_profile.get("urgency_level", "medium").lower(), 50)
                    st.metric(
                        L.get("urgency_score_label", "ê¸´ê¸‰ë„"),
                        f"{urgency_score}/100"
                    )
                with col4:
                    st.metric(
                        L.get("customer_type_label", "ê³ ê° ìœ í˜•"),
                        customer_profile.get("predicted_customer_type", "normal")
                    )

            # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì‹œê°í™”
            if similar_cases:
                st.markdown("---")
                st.subheader("ğŸ” ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ì²œ")
                similarity_chart = visualize_similarity_cases(similar_cases, detected_lang)
                if similarity_chart:
                    st.plotly_chart(similarity_chart, use_container_width=True)

                # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìš”ì•½ í‘œì‹œ
                with st.expander(f"ğŸ’¡ {len(similar_cases)}ê°œ ìœ ì‚¬ ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´"):
                    for idx, similar_case in enumerate(similar_cases, 1):
                        case = similar_case["case"]
                        summary = similar_case["summary"]
                        similarity = similar_case["similarity_score"]
                        st.markdown(f"### ì¼€ì´ìŠ¤ {idx} (ìœ ì‚¬ë„: {similarity:.1f}%)")
                        st.markdown(f"**ë¬¸ì˜ ë‚´ìš©:** {summary.get('main_inquiry', 'N/A')}")
                        st.markdown(f"**ê°ì • ì ìˆ˜:** {summary.get('customer_sentiment_score', 50)}/100")
                        st.markdown(f"**ë§Œì¡±ë„ ì ìˆ˜:** {summary.get('customer_satisfaction_score', 50)}/100")
                        if summary.get("key_responses"):
                            st.markdown("**í•µì‹¬ ì‘ë‹µ:**")
                            for response in summary.get("key_responses", [])[:3]:
                                st.markdown(f"- {response[:100]}...")
                        st.markdown("---")

            # ì´ˆê¸° ì¡°ì–¸ ìƒì„± (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
            text = _generate_initial_advice(
                customer_query,
                st.session_state.customer_type_sim_select,
                st.session_state.customer_email,
                st.session_state.customer_phone,
                detected_lang,  # ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©
                st.session_state.customer_attachment_file
            )
            st.session_state.simulator_messages.append({"role": "supervisor", "content": text})

            st.session_state.initial_advice_provided = True
            save_simulation_history_local(
                customer_query,
                st.session_state.customer_type_sim_select,
                st.session_state.simulator_messages,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
                is_chat_ended=False,
            )
            st.session_state.sim_stage = "AGENT_TURN"

    # =========================
    # 4. ëŒ€í™” ë¡œê·¸ í‘œì‹œ (ê³µí†µ)
    # =========================
    
    # í”¼ë“œë°± ì €ì¥ ì½œë°± í•¨ìˆ˜
    def save_feedback(index):
        """ì—ì´ì „íŠ¸ ì‘ë‹µì— ëŒ€í•œ ê³ ê° í”¼ë“œë°±ì„ ì €ì¥"""
        feedback_key = f"feedback_{st.session_state.sim_instance_id}_{index}"
        if feedback_key in st.session_state:
            feedback_value = st.session_state[feedback_key]
            # ë©”ì‹œì§€ì— í”¼ë“œë°± ì •ë³´ ì €ì¥
            if index < len(st.session_state.simulator_messages):
                st.session_state.simulator_messages[index]["feedback"] = feedback_value
    
    for idx, msg in enumerate(st.session_state.simulator_messages):
        role = msg["role"]
        content = msg["content"]
        avatar = {"customer": "ğŸ™‹", "supervisor": "ğŸ¤–", "agent_response": "ğŸ§‘â€ğŸ’»", "customer_rebuttal": "âœ¨",
                  "system_end": "ğŸ“Œ", "system_transfer": "ğŸ“Œ"}.get(role, "ğŸ’¬")
        tts_role = "customer" if role.startswith("customer") or role == "customer_rebuttal" else (
            "agent" if role == "agent_response" else "supervisor")

        with st.chat_message(role, avatar=avatar):
            st.markdown(content)
            # ì¸ë±ìŠ¤ë¥¼ render_tts_buttonì— ì „ë‹¬í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±ì— ì‚¬ìš©
            render_tts_button(content, st.session_state.language, role=tts_role, prefix=f"{role}_", index=idx)
            
            # â­ ì—ì´ì „íŠ¸ ì‘ë‹µì— ëŒ€í•œ í”¼ë“œë°± ìœ„ì ¯ ì¶”ê°€
            if role == "agent_response":
                feedback_key = f"feedback_{st.session_state.sim_instance_id}_{idx}"
                # ê¸°ì¡´ í”¼ë“œë°± ê°’ ê°€ì ¸ì˜¤ê¸°
                existing_feedback = msg.get("feedback", None)
                if existing_feedback is not None:
                    st.session_state[feedback_key] = existing_feedback
                
                # í”¼ë“œë°± ìœ„ì ¯ í‘œì‹œ
                st.feedback(
                    "thumbs",
                    key=feedback_key,
                    disabled=existing_feedback is not None,
                    on_change=save_feedback,
                    args=[idx],
                )

            # â­ [ìƒˆë¡œìš´ ë¡œì§] ê³ ê° ì²¨ë¶€ íŒŒì¼ ë Œë”ë§ (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ ê²½ìš°)
            if idx == 0 and role == "customer" and st.session_state.customer_attachment_b64:
                mime = st.session_state.customer_attachment_mime or "image/png"
                data_url = f"data:{mime};base64,{st.session_state.customer_attachment_b64}"

                # ì´ë¯¸ì§€ íŒŒì¼ë§Œ í‘œì‹œ (PDF ë“±ì€ ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
                if mime.startswith("image/"):
                    st.image(data_url, caption=f"ì²¨ë¶€ëœ ì¦ê±°ë¬¼ ({st.session_state.customer_attachment_file.name})",
                             use_column_width=True)
                elif mime == "application/pdf":
                    # PDF íŒŒì¼ì¼ ê²½ìš°, íŒŒì¼ ì´ë¦„ê³¼ í•¨ê»˜ ë‹¤ìš´ë¡œë“œ ë§í¬ ë˜ëŠ” ê²½ê³  í‘œì‹œ
                    st.warning(
                        f"ì²¨ë¶€ëœ PDF íŒŒì¼ ({st.session_state.customer_attachment_file.name})ì€ í˜„ì¬ ì¸ë¼ì¸ ë¯¸ë¦¬ë³´ê¸°ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ì´ê´€ ìš”ì•½ í‘œì‹œ (ì´ê´€ í›„ì—ë§Œ) - ë£¨í”„ ë°–ìœ¼ë¡œ ì´ë™í•˜ì—¬ í•œ ë²ˆë§Œ í‘œì‹œ
    if st.session_state.transfer_summary_text or (st.session_state.language != st.session_state.language_at_transfer_start and st.session_state.language_at_transfer_start):
                st.markdown("---")
                st.markdown(f"**{L['transfer_summary_header']}**")
                st.info(L["transfer_summary_intro"])

                # ë²ˆì—­ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° í™•ì¸ (ë²ˆì—­ ì„±ê³µ ì—¬ë¶€ í”Œë˜ê·¸ ì‚¬ìš©)
                is_translation_failed = not st.session_state.get("translation_success", True) or not st.session_state.transfer_summary_text

                if is_translation_failed:
                    # ë²ˆì—­ ì‹¤íŒ¨ ì‹œì—ë„ ì›ë³¸ í…ìŠ¤íŠ¸ê°€ í‘œì‹œë˜ë¯€ë¡œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ì´ ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ í‘œì‹œ
                    # (ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ì§€ ì•Šì•„ë„ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ê³„ì† ì§„í–‰ ê°€ëŠ¥)
                    if st.session_state.transfer_summary_text:
                        st.info(st.session_state.transfer_summary_text)
                    # ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ ì¶”ê°€ (ì„ íƒì )
                    if st.button(L.get("button_retry_translation", "ë²ˆì—­ ë‹¤ì‹œ ì‹œë„"),
                                 key=f"btn_retry_translation_{st.session_state.sim_instance_id}"):  # ê³ ìœ  í‚¤ ì‚¬ìš©
                        # ì¬ì‹œë„ ë¡œì§ ì‹¤í–‰
                        with st.spinner(L.get("transfer_loading", "ë²ˆì—­ ì¤‘...")):
                            source_lang = st.session_state.language_at_transfer_start
                            target_lang = st.session_state.language

                            # ì´ì „ ëŒ€í™” ë‚´ìš© ì¬ê°€ê³µ
                            history_text = get_chat_history_for_prompt(include_attachment=False)
                            for msg in st.session_state.simulator_messages:
                                role = "Customer" if msg["role"].startswith("customer") or msg[
                                    "role"] == "initial_query" else "Agent"
                                if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                                   "customer_closing_response"]:
                                    history_text += f"{role}: {msg['content']}\n"

                            # â­ ìˆ˜ì •: ë¨¼ì € í•µì‹¬ í¬ì¸íŠ¸ë§Œ ìš”ì•½í•œ í›„ ë²ˆì—­
                            lang_name_source = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(source_lang, "Korean")
                            summary_prompt = f"""
You are an AI assistant that summarizes customer service conversations. 
Extract ONLY the key points from the conversation below. Keep it concise and focused on:
1. Customer's main inquiry/question
2. Key information provided by the agent
3. Important decisions or outcomes
4. Any unresolved issues

Write the summary in {lang_name_source}. Maximum 200 words. Be brief and to the point.

--- Conversation ---
{history_text}
---

Key Points Summary:
"""
                            
                            # ìš”ì•½ ìƒì„±
                            summarized_text = ""
                            if st.session_state.is_llm_ready:
                                try:
                                    summarized_text = run_llm(summary_prompt).strip()
                                except Exception as e:
                                    print(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨, ì „ì²´ ëŒ€í™” ì‚¬ìš©: {e}")
                                    summarized_text = history_text  # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì „ì²´ ëŒ€í™” ì‚¬ìš©
                            else:
                                summarized_text = history_text  # LLMì´ ì—†ìœ¼ë©´ ì „ì²´ ëŒ€í™” ì‚¬ìš©
                            
                            translated_summary, is_success = translate_text_with_llm(summarized_text, target_lang, source_lang)
                            st.session_state.transfer_summary_text = translated_summary
                            st.session_state.translation_success = is_success
                            st.session_state.transfer_retry_count += 1


                else:
                    # [ìˆ˜ì • 2] ë²ˆì—­ ì„±ê³µ ì‹œ ë‚´ìš© í‘œì‹œ ë° TTS ë²„íŠ¼ ì¶”ê°€
                    st.markdown(st.session_state.transfer_summary_text)
            # â­ ìˆ˜ì •: ì´ê´€ ìš”ì•½ì˜ ê²½ìš° ì•ˆì •ì ì¸ í‚¤ë¥¼ ìƒì„±í•˜ë„ë¡ ìˆ˜ì • (ì„¸ì…˜ IDì™€ ì–¸ì–´ ì½”ë“œ ì¡°í•©)
                    render_tts_button(
                        st.session_state.transfer_summary_text,
                        st.session_state.language,
                        role="agent",
                        prefix="trans_summary_tts",
                        index=-1  # ê³ ìœ  ì„¸ì…˜ ID ê¸°ë°˜ì˜ í‚¤ë¥¼ ìƒì„±í•˜ë„ë¡ ì§€ì‹œ
                    )
                st.markdown("---")

    # =========================
    # 5. ì—ì´ì „íŠ¸ ì…ë ¥ ë‹¨ê³„ (AGENT_TURN)
    # =========================
    if st.session_state.sim_stage == "AGENT_TURN":
        st.markdown(f"### {L['agent_response_header']}")

        # --- ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸ ì˜ì—­ ---
        hint_cols = st.columns([4, 1])
        with hint_cols[0]:
            st.info(L["hint_placeholder"] + st.session_state.realtime_hint_text)

        with hint_cols[1]:
            # íŒíŠ¸ ìš”ì²­ ë²„íŠ¼
            if st.button(L["button_request_hint"], key=f"btn_request_hint_{st.session_state.sim_instance_id}"):
                with st.spinner(L["response_generating"]):
                    # ì±„íŒ…/ì´ë©”ì¼ íƒ­ì´ë¯€ë¡œ is_call=False
                    hint = generate_realtime_hint(current_lang, is_call=False)
                    st.session_state.realtime_hint_text = hint

        # --- ì–¸ì–´ ì´ê´€ ìš”ì²­ ê°•ì¡° í‘œì‹œ ---
        if st.session_state.language_transfer_requested:
            st.error("ğŸš¨ ê³ ê°ì´ ì–¸ì–´ ì „í™˜(ì´ê´€)ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì‘ëŒ€í•˜ê±°ë‚˜ ì´ê´€ì„ ì§„í–‰í•˜ì„¸ìš”ã€‚")

        # --- ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì¬í‘œì‹œ ---
        if st.session_state.sim_attachment_context_for_llm:
            st.info(
                f"ğŸ“ ìµœì´ˆ ë¬¸ì˜ ì‹œ ì²¨ë¶€ëœ íŒŒì¼ ì •ë³´:\n\n{st.session_state.sim_attachment_context_for_llm.replace('[ATTACHMENT STATUS]', '').strip()}")

        # --- AI ì‘ë‹µ ì´ˆì•ˆ ìƒì„± ë²„íŠ¼ (ìš”ì²­ 1 ë°˜ì˜) ---
        if st.button(L["button_generate_draft"], key=f"btn_generate_ai_draft_{st.session_state.sim_instance_id}"):
            if not st.session_state.is_llm_ready:
                st.warning(L["simulation_no_key_warning"])
            else:
                with st.spinner(L["draft_generating"]):
                    # ì´ˆì•ˆ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
                    ai_draft = generate_agent_response_draft(current_lang)
                    if ai_draft and not ai_draft.startswith("âŒ"):
                        st.session_state.agent_response_area_text = ai_draft
                        st.success(L["draft_success"])
                    else:
                        st.error(ai_draft if ai_draft else L.get("draft_error", "ì‘ë‹µ ì´ˆì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."))

        # --- ì „í™” ë°œì‹  ë²„íŠ¼ ì¶”ê°€ (ìš”ì²­ 2 ë°˜ì˜) ---
        st.markdown("---")
        st.subheader(L["button_call_outbound"])
        call_cols = st.columns(2)

        with call_cols[0]:
            if st.button(L["button_call_outbound_to_provider"], key="btn_call_outbound_partner", use_container_width=True):
                # ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜: í˜„ì§€ ì—…ì²´
                st.session_state.sim_call_outbound_target = "í˜„ì§€ ì—…ì²´/íŒŒíŠ¸ë„ˆ"
                st.session_state.sim_stage = "OUTBOUND_CALL_IN_PROGRESS"

        with call_cols[1]:
            if st.button(L["button_call_outbound_to_customer"], key="btn_call_outbound_customer", use_container_width=True):
                # ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜: ê³ ê°
                st.session_state.sim_call_outbound_target = "ê³ ê°"
                st.session_state.sim_stage = "OUTBOUND_CALL_IN_PROGRESS"

        st.markdown("---")
        # --- ì „í™” ë°œì‹  ë²„íŠ¼ ì¶”ê°€ ë ---

        st.markdown("### ğŸš¨ Supervisor ì •ì±…/ì§€ì‹œ ì‚¬í•­ ì—…ë¡œë“œ (ì˜ˆì™¸ ì²˜ë¦¬ ë°©ì¹¨)")

        # --- Supervisor ì •ì±… ì—…ë¡œë” ì¶”ê°€ ---
        supervisor_attachment_widget = st.file_uploader(
            "Supervisor ì§€ì‹œ ì‚¬í•­/ìŠ¤í¬ë¦°ìƒ· ì—…ë¡œë“œ (ì˜ˆì™¸ ì •ì±… í¬í•¨)",
            type=["png", "jpg", "jpeg", "pdf", "txt"],
            key="supervisor_policy_uploader",
            help="ë¹„í–‰ê¸° ì§€ì—°, ì§ˆë³‘ ë“± ì˜ˆì™¸ì  ìƒí™©ì— ëŒ€í•œ Supervisorì˜ ìµœì‹  ì§€ì‹œ ì‚¬í•­ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
            accept_multiple_files=False
        )

        # íŒŒì¼ ì •ë³´ ì €ì¥ ë° LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        if supervisor_attachment_widget:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ë˜ëŠ” PDF/ì´ë¯¸ì§€ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì»¨í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì—¬ policy_contextì— ì €ì¥í•´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” íŒŒì¼ ì´ë¦„ê³¼ íƒ€ì…ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ê³ , LLMì´ ì´ê²ƒì´ 'ì˜ˆì™¸ ì •ì±…'ì„ì„ ì•Œë„ë¡ ìœ ë„
            file_name = supervisor_attachment_widget.name
            st.session_state.supervisor_policy_context = f"[Supervisor Policy Attached] Filename: {file_name}, Filetype: {supervisor_attachment_widget.type}. This file contains a CRITICAL, temporary policy update regarding exceptions (e.g., flight delays, illness, natural disasters). Analyze and prioritize this policy in the response."
            st.success(f"âœ… Supervisor ì •ì±… íŒŒì¼: **{file_name}**ì´(ê°€) ì‘ëŒ€ ê°€ì´ë“œì— ë°˜ì˜ë©ë‹ˆë‹¤.")
        elif st.session_state.supervisor_policy_context:
            st.info("â­ í˜„ì¬ ì ìš© ì¤‘ì¸ Supervisor ì •ì±…ì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.session_state.supervisor_policy_context = ""

        # --- ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë” (ë‹¤ì¤‘ íŒŒì¼ í—ˆìš©) ---
        agent_attachment_files = st.file_uploader(
            L["agent_attachment_label"],
            type=["png", "jpg", "jpeg", "pdf"],
            key="agent_attachment_file_uploader",
            help=L["agent_attachment_placeholder"],
            accept_multiple_files=True
        )

        if agent_attachment_files:
            st.session_state.agent_attachment_file = [
                {"name": f.name, "type": f.type, "size": f.size} for f in agent_attachment_files
            ]
            file_names = ", ".join([f["name"] for f in
                                    st.session_state.agent_attachment_file])  # ìˆ˜ì •: file_infos ëŒ€ì‹  st.session_state.agent_attachment_file ì‚¬ìš©
            st.info(f"âœ… {len(agent_attachment_files)}ê°œ ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ: {file_names}")
        else:
            st.session_state.agent_attachment_file = []

        # --- ì…ë ¥ í•„ë“œ ë° ë²„íŠ¼ ---
        col_mic, col_text = st.columns([1, 2])

        # --- ë§ˆì´í¬ ë…¹ìŒ ---
        with col_mic:
            mic_audio = mic_recorder(
                start_prompt=L["button_mic_input"],
                stop_prompt=L["button_mic_stop"],
                just_once=False,
                format="wav",
                use_container_width=True,
                key="sim_mic_recorder",
            )

        if mic_audio and mic_audio.get("bytes"):
            st.session_state.sim_audio_bytes = mic_audio["bytes"]
            # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
            current_lang = st.session_state.get("language", "ko")
            if current_lang not in ["ko", "en", "ja"]:
                current_lang = "ko"
            L = LANG.get(current_lang, LANG["ko"])
            st.info(L["recording_complete_press_transcribe"])

        if st.session_state.sim_audio_bytes:
            col_audio, col_transcribe, col_del = st.columns([3, 1, 1])

            # 1. ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´
            # Streamlit ë¬¸ì„œ: bytes ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
            with col_audio:
                try:
                    st.audio(st.session_state.sim_audio_bytes, format="audio/wav", autoplay=False)
                except Exception as e:
                    st.error(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")

            # 2. ë…¹ìŒ ì‚­ì œ ë²„íŠ¼ (ì¶”ê°€ ìš”ì²­ ë°˜ì˜)
            with col_del:
                st.markdown("<br>", unsafe_allow_html=True)  # ë²„íŠ¼ ìˆ˜ì§ ì •ë ¬
                if st.button(L["delete_mic_record"], key="btn_delete_sim_audio_call"):
                    # ì˜¤ë””ì˜¤ ë° ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
                    st.session_state.sim_audio_bytes = None
                    st.session_state.last_transcript = ""
                    # â­ ìˆ˜ì •: ìœ„ì ¯ì´ ìƒì„±ëœ í›„ì—ëŠ” session_stateë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í”Œë˜ê·¸ ì‚¬ìš©
                    st.session_state.reset_agent_response_area = True
                    st.success("ë…¹ìŒì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë…¹ìŒí•´ ì£¼ì„¸ìš”.")

            # 3. ì „ì‚¬(Whisper) ë²„íŠ¼ (ê¸°ì¡´ ë¡œì§ ëŒ€ì²´)
            col_tr, _ = st.columns([1, 2])
            if col_tr.button(L["transcribe_btn"], key="sim_transcribe_btn"):
                if st.session_state.sim_audio_bytes is None:
                    st.warning("ë¨¼ì € ë§ˆì´í¬ë¡œ ë…¹ìŒì„ ì™„ë£Œí•˜ì„¸ìš”.")
                else:
                    # â­ ìˆ˜ì •: OpenAI ë˜ëŠ” Gemini API í‚¤ ì²´í¬
                    has_openai = st.session_state.openai_client is not None
                    has_gemini = bool(get_api_key("gemini"))
                    
                    if not has_openai and not has_gemini:
                        st.error(L["whisper_client_error"] + " (OpenAI ë˜ëŠ” Gemini API Key í•„ìš”)")
                    else:
                        with st.spinner(L["whisper_processing"]):
                            # transcribe_bytes_with_whisper í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
                            # ìë™ ì–¸ì–´ ê°ì§€ ì‚¬ìš© (ì…ë ¥ ì–¸ì–´ì™€ ê´€ê³„ì—†ì´ ì •í™•í•œ ì „ì‚¬)
                            transcribed_text = transcribe_bytes_with_whisper(
                                st.session_state.sim_audio_bytes,
                                "audio/wav",
                                lang_code=None,
                                auto_detect=True,
                            )
                            if transcribed_text.startswith("âŒ"):
                                st.error(transcribed_text)
                                st.session_state.last_transcript = ""
                            else:
                                st.session_state.last_transcript = transcribed_text.strip()
                                # â­ ìˆ˜ì •: ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì°½ì˜ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ì— ë°˜ì˜
                                st.session_state.agent_response_area_text = transcribed_text.strip()
                                st.session_state.agent_response_input_box_widget = transcribed_text.strip()

                                snippet = transcribed_text[:50].replace("\n", " ")
                                if len(transcribed_text) > 50:
                                    snippet += "..."
                                st.success(L["whisper_success"] + f"\n\n**ì¸ì‹ ë‚´ìš©:** *{snippet}*")

        col_text, col_button = st.columns([4, 1])

        # --- ì…ë ¥ í•„ë“œ ë° ë²„íŠ¼ ---
        with col_text:
            # â­ ìˆ˜ì •: ìœ„ì ¯ ìƒì„± ì „ì— ì´ˆê¸°í™” í”Œë˜ê·¸ë¥¼ í™•ì¸í•˜ì—¬ ê°’ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
            if st.session_state.get("reset_agent_response_area", False):
                st.session_state.agent_response_area_text = ""
                st.session_state.reset_agent_response_area = False
            
            # st.text_areaì˜ ê°’ì„ ì½ì–´ ì„¸ì…˜ ìƒíƒœë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ëŠ” on_changeë¥¼ ì œê±°í•˜ê³ 
            # st.text_area ìœ„ì ¯ ìì²´ì˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ send_clicked ì‹œ ìµœì‹  ê°’ì„ ì½ë„ë¡ í•©ë‹ˆë‹¤.
            # (Streamlit ê¸°ë³¸ ë™ì‘: ë²„íŠ¼ í´ë¦­ ì‹œ ìœ„ì ¯ì˜ ìµœì¢… ê°’ì´ ì„¸ì…˜ ìƒíƒœì— ë°˜ì˜ë¨)
            # â­ ìˆ˜ì •: keyë¥¼ agent_response_area_textë¡œ í†µì¼í•˜ì—¬ ì„¸ì…˜ ìƒíƒœì™€ ë™ê¸°í™”
            agent_response_input = st.text_area(
                L["agent_response_placeholder"],
                value=st.session_state.agent_response_area_text,
                key="agent_response_area_text",  # ì„¸ì…˜ ìƒíƒœ í‚¤ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •í•˜ì—¬ ë™ê¸°í™” ë³´ì¥
                height=150,
            )

            # ì†”ë£¨ì…˜ ì œê³µ ì²´í¬ë°•ìŠ¤
            st.session_state.is_solution_provided = st.checkbox(
                L["solution_check_label"],
                value=st.session_state.is_solution_provided,
                key="solution_checkbox_widget",
            )

        with col_button:
            send_clicked = st.button(L["send_response_button"], key="send_agent_response_btn")

        if send_clicked:
            # â­ ìˆ˜ì •: st.session_state.agent_response_area_textì—ì„œ ìµœì‹  ì…ë ¥ê°’ì„ ê°€ì ¸ì˜´ (keyì™€ ë™ì¼)
            agent_response = st.session_state.agent_response_area_text.strip()

            if not agent_response:
                st.warning(L["empty_response_warning"])
                # st.stop()

            # AHT íƒ€ì´ë¨¸ ì‹œì‘
            if st.session_state.start_time is None and len(st.session_state.simulator_messages) >= 1:
                st.session_state.start_time = datetime.now()

            # --- ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬ (ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬) ---
            final_response_content = agent_response
            if st.session_state.agent_attachment_file:
                file_infos = st.session_state.agent_attachment_file
                file_names = ", ".join([f["name"] for f in file_infos])
                attachment_msg = L["agent_attachment_status"].format(
                    filename=file_names, filetype=f"ì´ {len(file_infos)}ê°œ íŒŒì¼"
                )
                final_response_content = f"{agent_response}\n\n---\n{attachment_msg}"

            # ë¡œê·¸ ì—…ë°ì´íŠ¸
            st.session_state.simulator_messages.append(
                {"role": "agent_response", "content": final_response_content}
            )

            # â­ ì¶”ê°€: ì—ì´ì „íŠ¸ ì‘ë‹µì— ë©”ì¼ ëì¸ì‚¬ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            email_closing_patterns = [
                "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì—°ë½", "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì—°ë½",
                "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´", "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´",
                "ì–¸ì œë“ ì§€ ì—°ë½", "ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì„¸ìš”",
                "additional inquiries", "any additional questions", "any further questions",
                "feel free to contact", "please feel free to contact",
                "please don't hesitate to contact", "don't hesitate to contact",
                "please let me know", "let me know", "let me know if",
                "please let me know so", "let me know so",
                "if you have any questions", "if you have any further questions",
                "if you need any assistance", "if you need further assistance",
                "if you encounter any issues", "if you still have", "if you remain unclear",
                "I can assist further", "I can help further", "I can assist",
                "so I can assist", "so I can help", "so I can assist further",
                "è¿½åŠ ã®ã”è³ªå•", "è¿½åŠ ã®ãŠå•ã„åˆã‚ã›", "ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰", "ãŠå•ã„åˆã‚ã›ãŒã”ã–ã„ã¾ã—ãŸã‚‰"
            ]
            is_email_closing_in_response = any(pattern.lower() in final_response_content.lower() for pattern in email_closing_patterns)
            if is_email_closing_in_response:
                st.session_state.has_email_closing = True  # í”Œë˜ê·¸ ì„¤ì •

            # ì…ë ¥ì°½/ì˜¤ë””ì˜¤/ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            # â­ ìˆ˜ì •: ìœ„ì ¯ì´ ìƒì„±ëœ í›„ì—ëŠ” session_stateë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ,
            # rerun í›„ ìœ„ì ¯ì´ ë‹¤ì‹œ ìƒì„±ë  ë•Œ ì´ˆê¸°ê°’ì´ ì ìš©ë˜ë„ë¡ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            st.session_state.sim_audio_bytes = None
            st.session_state.agent_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.language_transfer_requested = False
            st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""  # ì „í™” ë°œì‹  ìš”ì•½ ì´ˆê¸°í™”

            # â­ ìˆ˜ì •: agent_response_area_textëŠ” rerun í›„ ìœ„ì ¯ì´ ë‹¤ì‹œ ìƒì„±ë  ë•Œ ì´ˆê¸°í™”ë˜ë„ë¡
            # í”Œë˜ê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ìœ„ì ¯ ìƒì„± ì „ì— ì´ í”Œë˜ê·¸ë¥¼ í™•ì¸í•˜ì—¬ ê°’ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
            st.session_state.reset_agent_response_area = True
            
            # â­ ìˆ˜ì •: ì‘ë‹µ ì „ì†¡ ì‹œ ë°”ë¡œ ê³ ê° ë°˜ì‘ ìë™ ìƒì„±
            if st.session_state.is_llm_ready:
                # LLMì´ ì¤€ë¹„ëœ ê²½ìš° ë°”ë¡œ ê³ ê° ë°˜ì‘ ìƒì„±
                with st.spinner(L["generating_customer_response"]):
                    customer_response = generate_customer_reaction(st.session_state.language, is_call=False)
                
                # ê³ ê° ë°˜ì‘ì„ ë©”ì‹œì§€ì— ì¶”ê°€
                st.session_state.simulator_messages.append(
                    {"role": "customer", "content": customer_response}
                )
                
                # â­ ì¶”ê°€: ë©”ì¼ ëì¸ì‚¬ê°€ í¬í•¨ëœ ê²½ìš° ê³ ê° ì‘ë‹µ í™•ì¸ ë° ì„¤ë¬¸ ì¡°ì‚¬ ë²„íŠ¼ í™œì„±í™”
                if st.session_state.get("has_email_closing", False):
                    # ê³ ê°ì˜ ê¸ì • ë°˜ì‘ í™•ì¸
                    positive_keywords = [
                        "No, that will be all", "no more", "ì—†ìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤", "Thank you", "ã‚ã‚ŠãŒã¨ã†",
                        "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤", "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤", "no additional", "è¿½åŠ ã®è³ªå•ã¯ã‚ã‚Šã¾ã›ã‚“",
                        "ì•Œê² ìŠµë‹ˆë‹¤", "ì•Œê² ì–´ìš”", "ok", "okay", "ë„¤", "yes", "ì¢‹ìŠµë‹ˆë‹¤", "good", "fine", "ê´œì°®ìŠµë‹ˆë‹¤"
                    ]
                    is_positive = any(keyword.lower() in customer_response.lower() for keyword in positive_keywords)
                    
                    if is_positive or L.get('customer_no_more_inquiries', '') in customer_response:
                        # ì„¤ë¬¸ ì¡°ì‚¬ ë²„íŠ¼ í™œì„±í™”ë¥¼ ìœ„í•´ WAIT_CUSTOMER_CLOSING_RESPONSE ë‹¨ê³„ë¡œ ì´ë™
                        st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"
                        st.rerun()
            else:
                # LLMì´ ì—†ëŠ” ê²½ìš° í”Œë˜ê·¸ ì„¤ì •í•˜ì—¬ CUSTOMER_TURN ë‹¨ê³„ì—ì„œ ìˆ˜ë™ ìƒì„± ê°€ëŠ¥í•˜ë„ë¡
                st.session_state.need_customer_response = True
            
            # â­ ìˆ˜ì •: ê³ ê° ë°˜ì‘ ìƒì„± í›„ CUSTOMER_TURN ë‹¨ê³„ë¡œ ì´ë™í•˜ê³  UI ì—…ë°ì´íŠ¸
            st.session_state.sim_stage = "CUSTOMER_TURN"
            st.rerun()
            

        # --- ì–¸ì–´ ì´ê´€ ë²„íŠ¼ ---
        st.markdown("---")
        st.markdown(f"**{L['transfer_header']}**")
        transfer_cols = st.columns(len(LANG) - 1)

        languages = list(LANG.keys())
        languages.remove(current_lang)


        def transfer_session(target_lang: str, current_messages: List[Dict[str, str]]):
            """ì–¸ì–´ ì´ê´€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  ì„¸ì…˜ ì–¸ì–´ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""

            # API í‚¤ ì²´í¬ëŠ” run_llm ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ Gemini í‚¤ë¥¼ ìš”êµ¬í•¨
            if not get_api_key("gemini"):
                st.error(LANG[current_lang]["simulation_no_key_warning"].replace('API Key', 'Gemini API Key'))
                # st.stop()
                return

            current_lang_at_start = st.session_state.language  # Source language

            # AHT íƒ€ì´ë¨¸ ì¤‘ì§€
            st.session_state.start_time = None

            # 1. ë¡œë”© ì‹œì‘ (ì‹œê°„ ì–‘í•´ ë©”ì‹œì§€ ì‹œë®¬ë ˆì´ì…˜)
            with st.spinner(L["transfer_loading"]):
                # ì‹¤ì œ ëŒ€ê¸° ì‹œê°„ 5~10ì´ˆ (3~10ë¶„ ì‹œë®¬ë ˆì´ì…˜)
                time.sleep(np.random.uniform(5, 10))

                # 2. ëŒ€í™” ê¸°ë¡ì„ ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¡œ ê°€ê³µ
                history_text = ""
                for msg in current_messages:
                    role = "Customer" if msg["role"].startswith("customer") or msg[
                        "role"] == "initial_query" else "Agent"
                    if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                       "customer_closing_response"]:
                        history_text += f"{role}: {msg['content']}\n"

                # â­ ìˆ˜ì •: ë¨¼ì € í•µì‹¬ í¬ì¸íŠ¸ë§Œ ìš”ì•½í•œ í›„ ë²ˆì—­
                # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
                lang_name_source = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(current_lang_at_start, "Korean")
                summary_prompt = f"""
You are an AI assistant that summarizes customer service conversations. 
Extract ONLY the key points from the conversation below. Keep it concise and focused on:
1. Customer's main inquiry/question
2. Key information provided by the agent
3. Important decisions or outcomes
4. Any unresolved issues

Write the summary in {lang_name_source}. Maximum 200 words. Be brief and to the point.

--- Conversation ---
{history_text}
---

Key Points Summary:
"""
                
                # ìš”ì•½ ìƒì„±
                summarized_text = ""
                if st.session_state.is_llm_ready:
                    try:
                        summarized_text = run_llm(summary_prompt).strip()
                    except Exception as e:
                        print(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨, ì „ì²´ ëŒ€í™” ì‚¬ìš©: {e}")
                        summarized_text = history_text  # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì „ì²´ ëŒ€í™” ì‚¬ìš©
                else:
                    summarized_text = history_text  # LLMì´ ì—†ìœ¼ë©´ ì „ì²´ ëŒ€í™” ì‚¬ìš©

                # 3. LLM ë²ˆì—­ ì‹¤í–‰ (ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­)
                translated_summary, is_success = translate_text_with_llm(summarized_text, target_lang,
                                                             current_lang_at_start)  # Use current_lang_at_start as source

                # 4. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.transfer_summary_text = translated_summary
                st.session_state.translation_success = is_success
                st.session_state.language_at_transfer = target_lang  # Save destination language
                st.session_state.language_at_transfer_start = current_lang_at_start  # Save source language for retry
                st.session_state.language = target_lang  # Language switch

                # --- ê¸°ì¡´ ê°€ì´ë“œë¼ì¸ ì‚­ì œ ë° ìƒˆ ê°€ì´ë“œë¼ì¸ ìƒì„± (ì–¸ì–´ í†µì¼ì„± í™•ë³´) ---
                # 1. ê¸°ì¡´ Supervisor Advice ë©”ì‹œì§€ ì‚­ì œ
                st.session_state.simulator_messages = [
                    msg for msg in st.session_state.simulator_messages
                    if msg['role'] != 'supervisor'
                ]

                # 2. ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ê°€ì´ë“œë¼ì¸/ì´ˆì•ˆ ì¬ìƒì„±
                new_advice = _generate_initial_advice(
                    st.session_state.customer_query_text_area,
                    st.session_state.customer_type_sim_select,
                    st.session_state.customer_email,
                    st.session_state.customer_phone,
                    target_lang,  # ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ìƒì„±
                    st.session_state.customer_attachment_file
                )
                st.session_state.simulator_messages.append({"role": "supervisor", "content": new_advice})
                # -------------------------------------------------------------------

                st.session_state.is_solution_provided = False  # ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ìœ„í•´ í”Œë˜ê·¸ ë¦¬ì…‹
                st.session_state.language_transfer_requested = False  # í”Œë˜ê·¸ ë¦¬ì…‹
                st.session_state.sim_stage = "AGENT_TURN"

                # 5. ì´ë ¥ ì €ì¥
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                save_simulation_history_local(
                    st.session_state.customer_query_text_area,
                    customer_type_display + f" (Transferred from {current_lang_at_start} to {target_lang})",
                    st.session_state.simulator_messages,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                    is_chat_ended=False,
                )

            # 6. UI ì¬ì‹¤í–‰ (ì–¸ì–´ ë³€ê²½ ì ìš©)
            st.success(f"âœ… {LANG[target_lang]['transfer_summary_header']}ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")


        for i, target_lang in enumerate(languages):
            button_label_key = f"transfer_to_{target_lang}"
            button_label = L.get(button_label_key, f"Transfer to {target_lang.capitalize()} Team")

            if transfer_cols[i].button(button_label, key=f"btn_transfer_{target_lang}"):
                transfer_session(target_lang, st.session_state.simulator_messages)

        st.markdown("---")

    # --- Language Transfer Buttons End ---

    # =========================
    # 6. ê³ ê° ë°˜ì‘ ìƒì„± ë‹¨ê³„ (CUSTOMER_TURN)
    # =========================
    elif st.session_state.sim_stage == "CUSTOMER_TURN":
        # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
        current_lang = st.session_state.get("language", "ko")
        if current_lang not in ["ko", "en", "ja"]:
            current_lang = "ko"
        L = LANG.get(current_lang, LANG["ko"])
        customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
        st.info(L["customer_turn_info"])

        # 1. ê³ ê° ë°˜ì‘ ìƒì„±
        # ì´ë¯¸ ê³ ê° ë°˜ì‘ì´ ìƒì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        last_customer_message = None
        for msg in reversed(st.session_state.simulator_messages):
            if msg.get("role") == "customer" and msg.get("content"):
                last_customer_message = msg.get("content", "")
                break
        
        if last_customer_message is None:
            # ê³ ê° ë°˜ì‘ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìƒì„±
            with st.spinner(L["generating_customer_response"]):
                customer_response = generate_customer_reaction(st.session_state.language, is_call=False)

            # 2. ëŒ€í™” ë¡œê·¸ ì—…ë°ì´íŠ¸
            st.session_state.simulator_messages.append(
                {"role": "customer", "content": customer_response}
            )
            
            # 3. ìƒì„± ì§í›„ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
            positive_closing_phrases = [L["customer_positive_response"], L["customer_no_more_inquiries"]]
            is_positive_closing = any(phrase in customer_response for phrase in positive_closing_phrases)
            
            # ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
            if L["customer_positive_response"] in customer_response:
                if st.session_state.is_solution_provided:
                    st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
                else:
                    st.session_state.sim_stage = "AGENT_TURN"
            elif is_positive_closing:
                if L['customer_no_more_inquiries'] in customer_response:
                    st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
                else:
                    if st.session_state.is_solution_provided:
                        st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
                    else:
                        st.session_state.sim_stage = "AGENT_TURN"
            elif customer_response.startswith(L["customer_escalation_start"]):
                st.session_state.sim_stage = "ESCALATION_REQUIRED"
            else:
                # ê³ ê°ì´ ì¶”ê°€ ì§ˆë¬¸í•˜ê±°ë‚˜ ì •ë³´ ì œê³µí•œ ê²½ìš° -> ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ì´ë™
                st.session_state.sim_stage = "AGENT_TURN"
            
            # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ rerun
            st.rerun()
        else:
            customer_response = last_customer_message

        # 3. ì¢…ë£Œ ì¡°ê±´ ê²€í†  (ì´ë¯¸ ê³ ê° ë°˜ì‘ì´ ìˆëŠ” ê²½ìš°)
        positive_closing_phrases = [L["customer_positive_response"], L["customer_no_more_inquiries"]]
        is_positive_closing = any(phrase in customer_response for phrase in positive_closing_phrases)

        # â­ ì¶”ê°€: ë©”ì¼ ì‘ëŒ€ ì¢…ë£Œ ë¬¸êµ¬ í™•ì¸ (í”Œë˜ê·¸ ë˜ëŠ” ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ì‘ë‹µ í™•ì¸)
        # ë¨¼ì € í”Œë˜ê·¸ í™•ì¸ (ì—ì´ì „íŠ¸ ì‘ë‹µ ì „ì†¡ ì‹œ ì„¤ì •ë¨)
        is_email_closing = st.session_state.get("has_email_closing", False)
        
        # í”Œë˜ê·¸ê°€ ì—†ìœ¼ë©´ ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ì‘ë‹µì—ì„œ ì§ì ‘ í™•ì¸
        if not is_email_closing:
            last_agent_response = None
            for msg in reversed(st.session_state.simulator_messages):
                if msg.get("role") == "agent_response" and msg.get("content"):
                    last_agent_response = msg.get("content", "")
                    break
            
            # ë©”ì¼ ëì¸ì‚¬ ë¬¸êµ¬ íŒ¨í„´ (ë‹¤êµ­ì–´ ì§€ì›) - ë” í¬ê´„ì ì¸ íŒ¨í„´ ì¶”ê°€
            email_closing_patterns = [
                "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì—°ë½", "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì—°ë½",
                "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´", "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´",
                "ì–¸ì œë“ ì§€ ì—°ë½", "ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì„¸ìš”",
                "additional inquiries", "any additional questions", "any further questions",
                "feel free to contact", "please feel free to contact",
                "please don't hesitate to contact", "don't hesitate to contact",
                "please let me know", "let me know", "let me know if",
                "please let me know so", "let me know so",
                "if you have any questions", "if you have any further questions",
                "if you need any assistance", "if you need further assistance",
                "if you encounter any issues", "if you still have", "if you remain unclear",
                "I can assist further", "I can help further", "I can assist",
                "so I can assist", "so I can help", "so I can assist further",
                "è¿½åŠ ã®ã”è³ªå•", "è¿½åŠ ã®ãŠå•ã„åˆã‚ã›", "ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰", "ãŠå•ã„åˆã‚ã›ãŒã”ã–ã„ã¾ã—ãŸã‚‰"
            ]
            
            if last_agent_response:
                is_email_closing = any(pattern.lower() in last_agent_response.lower() for pattern in email_closing_patterns)
                if is_email_closing:
                    st.session_state.has_email_closing = True  # í”Œë˜ê·¸ ì—…ë°ì´íŠ¸

        # â­ ìˆ˜ì •: ë©”ì¼ ëì¸ì‚¬ê°€ í¬í•¨ëœ ê²½ìš°, ê³ ê°ì˜ ê¸ì • ë°˜ì‘ì´ë‚˜ "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤" ë‹µë³€ì„ ì¸ì‹í•˜ë©´ ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë²„íŠ¼ ìë™ í™œì„±í™”
        if is_email_closing:
            # ê³ ê°ì˜ ê¸ì • ë°˜ì‘ ë˜ëŠ” "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤" ë‹µë³€ í™•ì¸
            no_more_keywords = [
                L['customer_no_more_inquiries'],
                "No, that will be all",
                "no more",
                "ì—†ìŠµë‹ˆë‹¤",
                "ê°ì‚¬í•©ë‹ˆë‹¤",
                "Thank you",
                "ã‚ã‚ŠãŒã¨ã†",
                "è¿½åŠ  ë¬¸ì˜ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤",
                "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤",
                "no additional",
                "è¿½åŠ ã®è³ªå•ã¯ã‚ã‚Šã¾ã›ã‚“",
                "ì•Œê² ìŠµë‹ˆë‹¤",
                "ì•Œê² ì–´ìš”",
                "ok",
                "okay",
                "ë„¤",
                "yes"
            ]
            has_no_more_inquiry = any(keyword.lower() in customer_response.lower() for keyword in no_more_keywords)
            
            # ê¸ì • ë°˜ì‘ í‚¤ì›Œë“œ ì¶”ê°€ (ë” í¬ê´„ì ì¸ ì¸ì‹)
            positive_keywords = [
                "ì•Œê² ìŠµë‹ˆë‹¤", "ì•Œê² ì–´ìš”", "ë„¤", "yes", "ok", "okay", "ê°ì‚¬í•©ë‹ˆë‹¤", "thank you", "ã‚ã‚ŠãŒã¨ã†",
                "ì¢‹ìŠµë‹ˆë‹¤", "good", "fine", "ê´œì°®ìŠµë‹ˆë‹¤", "ì•Œê² ìŠµë‹ˆë‹¤ ê°ì‚¬í•©ë‹ˆë‹¤"
            ]
            is_positive_response = any(keyword.lower() in customer_response.lower() for keyword in positive_keywords)
            
            # ê¸ì • ë°˜ì‘ì´ ìˆê±°ë‚˜ "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤" ë‹µë³€ì´ ìˆìœ¼ë©´ ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë²„íŠ¼ í™œì„±í™”
            if is_positive_closing or has_no_more_inquiry or L['customer_no_more_inquiries'] in customer_response or is_positive_response:
                # ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ê°€ ì•„ì§ ì¶”ê°€ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
                agent_closing_added = False
                for msg in reversed(st.session_state.simulator_messages):
                    if msg.get("role") == "agent_response":
                        agent_msg_content = msg.get("content", "")
                        if "ê°ì‚¬" in agent_msg_content or "Thank you" in agent_msg_content or "ã‚ã‚ŠãŒã¨ã†" in agent_msg_content:
                            agent_closing_added = True
                        break
                
                if not agent_closing_added:
                    # ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ ë©”ì‹œì§€ ì „ì†¡
                    agent_name = st.session_state.get("agent_name", "000")
                    if current_lang == "ko":
                        agent_closing_msg = f"ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› {agent_name}ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì„¸ìš”."
                    elif current_lang == "en":
                        agent_closing_msg = f"Thank you for contacting us. This was {agent_name}. Have a great day!"
                    else:  # ja
                        agent_closing_msg = f"ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚æ‹…å½“ã¯{agent_name}ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"
                    
                    # ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    st.session_state.simulator_messages.append(
                        {"role": "agent_response", "content": agent_closing_msg}
                    )
                
                # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë²„íŠ¼ í™œì„±í™”ë¥¼ ìœ„í•´ WAIT_CUSTOMER_CLOSING_RESPONSE ë‹¨ê³„ë¡œ ì´ë™
                # (ì‹¤ì œë¡œëŠ” ê³ ê° ì‘ë‹µì´ ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì„¤ë¬¸ ì¡°ì‚¬ ë²„íŠ¼ í‘œì‹œ)
                st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"
                st.rerun()
            else:
                # ë©”ì¼ ëì¸ì‚¬ê°€ ìˆì§€ë§Œ ê³ ê°ì´ ì¶”ê°€ ì§ˆë¬¸ì„ í•œ ê²½ìš°
                st.session_state.sim_stage = "AGENT_TURN"
                st.rerun()
        # â­ ìˆ˜ì •: ê³ ê°ì´ "ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤"ë¼ê³  ë‹µë³€í–ˆì„ ë•Œ, ì†”ë£¨ì…˜ì´ ì œê³µëœ ê²½ìš°ì—ë§Œ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ í™•ì¸ ë‹¨ê³„ë¡œ ì´ë™
        # ì •í™•í•œ ë¬¸ìì—´ ë¹„êµê°€ ì•„ë‹Œ í¬í•¨ ì—¬ë¶€ë¡œ í™•ì¸ (LLM ì‘ë‹µì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        elif L["customer_positive_response"] in customer_response:
            # ì†”ë£¨ì…˜ì´ ì œê³µëœ ê²½ìš°ì—ë§Œ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ í™•ì¸ ë‹¨ê³„ë¡œ ì´ë™
            if st.session_state.is_solution_provided:
                st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
            else:
                # ì†”ë£¨ì…˜ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ì§€
                st.session_state.sim_stage = "AGENT_TURN"
        elif is_positive_closing:
            # ê¸ì • ì¢…ë£Œ ì‘ë‹µ ì²˜ë¦¬
            if L['customer_no_more_inquiries'] in customer_response:
                # â­ ìˆ˜ì •: "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ë‹µë³€ ì‹œ ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ë¥¼ í•œ í›„ ì¢…ë£Œí•˜ë„ë¡ ë³€ê²½
                # ë°”ë¡œ ì¢…ë£Œí•˜ì§€ ì•Šê³  WAIT_CLOSING_CONFIRMATION_FROM_AGENT ë‹¨ê³„ë¡œ ì´ë™í•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ í›„ ì¢…ë£Œ
                st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
            else:
                # "ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤"ì™€ ìœ ì‚¬í•œ ê¸ì • ì‘ë‹µì¸ ê²½ìš°, ì†”ë£¨ì…˜ ì œê³µ ì—¬ë¶€ í™•ì¸
                if st.session_state.is_solution_provided:
                    st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
                else:
                    st.session_state.sim_stage = "AGENT_TURN"


        # â­ ìˆ˜ì •: ê³ ê°ì´ ì•„ì§ ì†”ë£¨ì…˜ì— ë§Œì¡±í•˜ì§€ ì•Šê±°ë‚˜ ì¶”ê°€ ì§ˆë¬¸ì„ í•œ ê²½ìš° (ì¼ë°˜ì ì¸ í„´)
        elif customer_response.startswith(L["customer_escalation_start"]):
            st.session_state.sim_stage = "ESCALATION_REQUIRED"  # ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”
        else:
            # ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ì§€ (ê³ ê°ì´ ì¶”ê°€ ì§ˆë¬¸í•˜ê±°ë‚˜ ì •ë³´ ì œê³µ)
            st.session_state.sim_stage = "AGENT_TURN"

        st.session_state.is_solution_provided = False  # ì¢…ë£Œ ë‹¨ê³„ ì§„ì… í›„ í”Œë˜ê·¸ ë¦¬ì…‹

        # ì´ë ¥ ì €ì¥ (ì¢…ë£Œë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì €ì¥)
        # â­ ìˆ˜ì •: "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ë‹µë³€ ì‹œì—ëŠ” ì´ë¯¸ ì´ë ¥ ì €ì¥ì„ í–ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì €ì¥ ë°©ì§€
        if st.session_state.sim_stage != "CLOSING":
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )

        st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
        
        # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™í–ˆìœ¼ë¯€ë¡œ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ rerun
        st.rerun()


    # =========================
    # 7. ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ëŒ€ê¸° (WAIT_CLOSING_CONFIRMATION_FROM_AGENT)
    # =========================
    elif st.session_state.sim_stage == "WAIT_CLOSING_CONFIRMATION_FROM_AGENT":
        # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
        current_lang = st.session_state.get("language", "ko")
        if current_lang not in ["ko", "en", "ja"]:
            current_lang = "ko"
        L = LANG.get(current_lang, LANG["ko"])
        st.success(L["customer_positive_solution_reaction"])

        col_chat_end, col_email_end = st.columns(2)  # ë²„íŠ¼ì„ ë‚˜ë€íˆ ë°°ì¹˜

        # [1] ì±„íŒ… - ì¶”ê°€ ë¬¸ì˜ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸° ë²„íŠ¼
        with col_chat_end:
            # [ìˆ˜ì • 1] ë‹¤êµ­ì–´ ë ˆì´ë¸” ì‚¬ìš©
            if st.button(L["send_closing_confirm_button"],
                         key=f"btn_send_closing_confirm_{st.session_state.sim_instance_id}"):
                # â­ ìˆ˜ì •: ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ë¥¼ í¬í•¨í•œ ì¢…ë£Œ ë©”ì‹œì§€ ì „ì†¡
                # ì–¸ì–´ë³„ ê°ì‚¬ ì¸ì‚¬ ë©”ì‹œì§€ ìƒì„±
                agent_name = st.session_state.get("agent_name", "000")
                if current_lang == "ko":
                    closing_msg = f"ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› {agent_name}ì˜€ìŠµë‹ˆë‹¤. {L['customer_closing_confirm']} ì¦ê±°ìš´ í•˜ë£¨ ë˜ì„¸ìš”."
                elif current_lang == "en":
                    closing_msg = f"Thank you for contacting us. This was {agent_name}. {L['customer_closing_confirm']} Have a great day!"
                else:  # ja
                    closing_msg = f"ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚æ‹…å½“ã¯{agent_name}ã§ã—ãŸã€‚{L['customer_closing_confirm']} è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"

                # ì—ì´ì „íŠ¸ ì‘ë‹µìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡
                st.session_state.simulator_messages.append(
                    {"role": "agent_response", "content": closing_msg}
                )

                # [ì¶”ê°€] TTS ë²„íŠ¼ ë Œë”ë§ì„ ìœ„í•´ sleep/rerun ê°•ì œ
                time.sleep(0.1)
                st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"
                st.rerun()

        # [2] ì´ë©”ì¼ - ìƒë‹´ ì¢…ë£Œ ë²„íŠ¼ (ì¦‰ì‹œ ì¢…ë£Œ)
        with col_email_end:
            # [ìˆ˜ì • 1] ë‹¤êµ­ì–´ ë ˆì´ë¸” ì‚¬ìš©
            if st.button(L["button_email_end_chat"], key=f"btn_email_end_chat_{st.session_state.sim_instance_id}"):
                # AHT íƒ€ì´ë¨¸ ì •ì§€
                st.session_state.start_time = None

                # [ìˆ˜ì • 1] ë‹¤êµ­ì–´ ë ˆì´ë¸” ì‚¬ìš©
                end_msg = L["prompt_survey"]
                st.session_state.simulator_messages.append(
                    {"role": "system_end", "content": "(ì‹œìŠ¤í…œ: ì´ë©”ì¼ ìƒë‹´ ì¢…ë£Œ) " + end_msg}
                )

                # [ì¶”ê°€] TTS ë²„íŠ¼ ë Œë”ë§ì„ ìœ„í•´ sleep/rerun ê°•ì œ
                time.sleep(0.1)
                st.session_state.is_chat_ended = True
                st.session_state.sim_stage = "CLOSING"  # ë°”ë¡œ CLOSINGìœ¼ë¡œ ì „í™˜

    # =========================
    # 8. ê³ ê° ìµœì¢… ì‘ë‹µ ìƒì„± ë° ì²˜ë¦¬ (WAIT_CUSTOMER_CLOSING_RESPONSE)
    # =========================
    elif st.session_state.sim_stage == "WAIT_CUSTOMER_CLOSING_RESPONSE":
        # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
        current_lang = st.session_state.get("language", "ko")
        if current_lang not in ["ko", "en", "ja"]:
            current_lang = "ko"
        L = LANG.get(current_lang, LANG["ko"])
        customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
        
        # â­ ì¶”ê°€: ë©”ì¼ ì‘ëŒ€ ì¢…ë£Œ ë¬¸êµ¬ í™•ì¸ (ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ì‘ë‹µì— "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì„¸ìš”" ê°™ì€ ë¬¸êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸)
        last_agent_response = None
        for msg in reversed(st.session_state.simulator_messages):
            if msg.get("role") == "agent_response" and msg.get("content"):
                last_agent_response = msg.get("content", "")
                break
        
        # ë©”ì¼ ëì¸ì‚¬ ë¬¸êµ¬ íŒ¨í„´ (ë‹¤êµ­ì–´ ì§€ì›) - ë” í¬ê´„ì ì¸ íŒ¨í„´ ì¶”ê°€
        email_closing_patterns = [
            "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì—°ë½",
            "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ì—°ë½",
            "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½",
            "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½",
            "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´",
            "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´",
            "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´",
            "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ë©´",
            "ì–¸ì œë“ ì§€ ì—°ë½",
            "ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì„¸ìš”",
            "ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤",
            "additional inquiries",
            "any additional questions",
            "any further questions",
            "feel free to contact",
            "please feel free to contact",
            "please don't hesitate to contact",
            "don't hesitate to contact",
            "è¿½åŠ ã®ã”è³ªå•",
            "è¿½åŠ ã®ãŠå•ã„åˆã‚ã›",
            "ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰",
            "ãŠå•ã„åˆã‚ã›ãŒã”ã–ã„ã¾ã—ãŸã‚‰"
        ]
        
        is_email_closing = False
        if last_agent_response:
            is_email_closing = any(pattern.lower() in last_agent_response.lower() for pattern in email_closing_patterns)
        
        # â­ ìˆ˜ì •: ì´ë¯¸ ê³ ê° ì‘ë‹µì´ ìƒì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        last_customer_message = None
        for msg in reversed(st.session_state.simulator_messages):
            if msg.get("role") == "customer_rebuttal":
                last_customer_message = msg.get("content", "")
                break
            # â­ ì¶”ê°€: customer ì—­í• ì˜ ë©”ì‹œì§€ë„ í™•ì¸ (ë©”ì¼ ëì¸ì‚¬ê°€ í¬í•¨ëœ ê²½ìš° CUSTOMER_TURNì—ì„œ ì´ë¯¸ ê³ ê° ì‘ë‹µì´ ìƒì„±ë˜ì—ˆì„ ìˆ˜ ìˆìŒ)
            elif msg.get("role") == "customer" and is_email_closing:
                last_customer_message = msg.get("content", "")
                break
        
        # ê³ ê° ì‘ë‹µì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ìƒì„±
        if last_customer_message is None:
            # ê³ ê° ë‹µë³€ ìë™ ìƒì„± (LLM Key ê²€ì¦ í¬í•¨)
            if not st.session_state.is_llm_ready:
                st.warning(L["llm_key_missing_customer_response"])
                if st.button(L["customer_generate_response_button"], key="btn_generate_final_response"):
                    st.session_state.sim_stage = "AGENT_TURN"
                    st.rerun()
                st.stop()
            
            # LLMì´ ì¤€ë¹„ëœ ê²½ìš° ê³ ê° ì‘ë‹µ ìƒì„±
            st.info(L["agent_confirmed_additional_inquiry"])
            with st.spinner(L["generating_customer_response"]):
                final_customer_reaction = generate_customer_closing_response(st.session_state.language)

            # ë¡œê·¸ ê¸°ë¡
            st.session_state.simulator_messages.append(
                {"role": "customer_rebuttal", "content": final_customer_reaction}
            )
            last_customer_message = final_customer_reaction
        
        # ê³ ê° ì‘ë‹µì— ë”°ë¼ ì²˜ë¦¬ (ìƒì„± ì§í›„ ë˜ëŠ” ì´ë¯¸ ìˆëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬)
        if last_customer_message is None:
            # ê³ ê° ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° (ì´ë¯¸ ìƒì„±í–ˆëŠ”ë°ë„ Noneì¸ ê²½ìš°ëŠ” ì—ëŸ¬)
            st.warning(L["customer_response_generation_failed"])
        else:
            final_customer_reaction = last_customer_message
            
            # (A) "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ê²½ë¡œ -> ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ í›„ ë²„íŠ¼ í‘œì‹œ
            # ë” ìœ ì—°í•œ ë§¤ì¹­ì„ ìœ„í•´ í‚¤ì›Œë“œ ì²´í¬ ì¶”ê°€
            no_more_keywords = [
                L['customer_no_more_inquiries'],
                "No, that will be all",
                "no more",
                "ì—†ìŠµë‹ˆë‹¤",
                "ê°ì‚¬í•©ë‹ˆë‹¤",
                "çµæ§‹ã§ã™",
                "ã‚ã‚ŠãŒã¨ã†",
                "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤",
                "ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤",
                "no additional",
                "è¿½åŠ ã®è³ªå•ã¯ã‚ã‚Šã¾ã›ã‚“"
            ]
            has_no_more_inquiry = any(keyword.lower() in final_customer_reaction.lower() for keyword in no_more_keywords)
            
            # â­ ì¶”ê°€: ë©”ì¼ ëì¸ì‚¬ê°€ í¬í•¨ëœ ê²½ìš°, ê³ ê°ì˜ ê¸ì • ë°˜ì‘ì´ë‚˜ "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ ì—†ìŠµë‹ˆë‹¤" ë‹µë³€ì„ ì¸ì‹í•˜ë©´ ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë²„íŠ¼ ìë™ í™œì„±í™”
            # ê¸ì • ë°˜ì‘ í‚¤ì›Œë“œ ì¶”ê°€
            positive_keywords = [
                "ì•Œê² ìŠµë‹ˆë‹¤", "ì•Œê² ì–´ìš”", "ë„¤", "yes", "ok", "okay", "ê°ì‚¬í•©ë‹ˆë‹¤", "thank you", "ã‚ã‚ŠãŒã¨ã†"
            ]
            is_positive_response = any(keyword.lower() in final_customer_reaction.lower() for keyword in positive_keywords)
            
            if is_email_closing and (has_no_more_inquiry or L['customer_no_more_inquiries'] in final_customer_reaction or is_positive_response):
                # ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ê°€ ì•„ì§ ì¶”ê°€ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
                agent_closing_added = False
                for msg in reversed(st.session_state.simulator_messages):
                    if msg.get("role") == "agent_response":
                        agent_msg_content = msg.get("content", "")
                        if "ê°ì‚¬" in agent_msg_content or "Thank you" in agent_msg_content or "ã‚ã‚ŠãŒã¨ã†" in agent_msg_content:
                            agent_closing_added = True
                        break
                
                if not agent_closing_added:
                    # ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ ë©”ì‹œì§€ ì „ì†¡
                    agent_name = st.session_state.get("agent_name", "000")
                    if current_lang == "ko":
                        agent_closing_msg = f"ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› {agent_name}ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì„¸ìš”."
                    elif current_lang == "en":
                        agent_closing_msg = f"Thank you for contacting us. This was {agent_name}. Have a great day!"
                    else:  # ja
                        agent_closing_msg = f"ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚æ‹…å½“ã¯{agent_name}ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"
                    
                    # ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    st.session_state.simulator_messages.append(
                        {"role": "agent_response", "content": agent_closing_msg}
                    )
                
                # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë²„íŠ¼ í‘œì‹œ
                st.markdown("---")
                st.success(L["no_more_inquiries_confirmed"])
                st.markdown(f"### {L['consultation_end_header']}")
                st.info(L["click_survey_button_to_end"])
                st.markdown("---")
                
                # ë²„íŠ¼ì„ ì¤‘ì•™ì— í¬ê²Œ í‘œì‹œ
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    end_chat_button = st.button(
                        L["sim_end_chat_button"], 
                        key="btn_final_end_chat_email_closing", 
                        use_container_width=True, 
                        type="primary"
                    )
                
                if end_chat_button:
                    # AHT íƒ€ì´ë¨¸ ì •ì§€
                    st.session_state.start_time = None

                    # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë©”ì‹œì§€ ì¶”ê°€
                    end_msg = L["prompt_survey"]
                    st.session_state.simulator_messages.append(
                        {"role": "system_end", "content": end_msg}
                    )

                    # ì±„íŒ… ì¢…ë£Œ ì²˜ë¦¬
                    st.session_state.is_chat_ended = True
                    st.session_state.sim_stage = "CLOSING"
                    
                    # ì´ë ¥ ì €ì¥
                    save_simulation_history_local(
                        st.session_state.customer_query_text_area, customer_type_display,
                        st.session_state.simulator_messages, is_chat_ended=True,
                        attachment_context=st.session_state.sim_attachment_context_for_llm,
                    )
                    
                    st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
                    st.rerun()  # ë²„íŠ¼ í´ë¦­ í›„ UI ì—…ë°ì´íŠ¸
            # ë©”ì¼ ëì¸ì‚¬ê°€ í¬í•¨ëœ ê²½ìš° ì—¬ê¸°ì„œ ì²˜ë¦¬ ì™„ë£Œ, ë‹¤ë¥¸ ë¡œì§ì€ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
            elif L['customer_no_more_inquiries'] in final_customer_reaction or has_no_more_inquiry:
                # â­ ìˆ˜ì •: ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ê°€ ì•„ì§ ì¶”ê°€ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
                agent_closing_added = False
                for msg in reversed(st.session_state.simulator_messages):
                    if msg.get("role") == "agent_response":
                        # ì´ë¯¸ ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                        agent_msg_content = msg.get("content", "")
                        if "ê°ì‚¬" in agent_msg_content or "Thank you" in agent_msg_content or "ã‚ã‚ŠãŒã¨ã†" in agent_msg_content:
                            agent_closing_added = True
                        break
                
                if not agent_closing_added:
                    # â­ ì¶”ê°€: ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ ë©”ì‹œì§€ ì „ì†¡
                    agent_name = st.session_state.get("agent_name", "000")
                    if current_lang == "ko":
                        agent_closing_msg = f"ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› {agent_name}ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì„¸ìš”."
                    elif current_lang == "en":
                        agent_closing_msg = f"Thank you for contacting us. This was {agent_name}. Have a great day!"
                    else:  # ja
                        agent_closing_msg = f"ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚æ‹…å½“ã¯{agent_name}ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"
                    
                    # ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    st.session_state.simulator_messages.append(
                        {"role": "agent_response", "content": agent_closing_msg}
                    )
                
                # â­ ìˆ˜ì •: í˜„ì¬ ë‹¨ê³„ì—ì„œ ë°”ë¡œ ë²„íŠ¼ í‘œì‹œ (FINAL_CLOSING_ACTIONìœ¼ë¡œ ì´ë™í•˜ì§€ ì•ŠìŒ)
                st.markdown("---")
                st.success(L["no_more_inquiries_confirmed"])
                st.markdown(f"### {L['consultation_end_header']}")
                st.info(L["click_survey_button_to_end"])
                st.markdown("---")
                
                # ë²„íŠ¼ì„ ì¤‘ì•™ì— í¬ê²Œ í‘œì‹œ
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    end_chat_button = st.button(
                        L["sim_end_chat_button"], 
                        key="btn_final_end_chat_in_wait", 
                        use_container_width=True, 
                        type="primary"
                    )
                
                if end_chat_button:
                    # AHT íƒ€ì´ë¨¸ ì •ì§€
                    st.session_state.start_time = None

                    # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë©”ì‹œì§€ ì¶”ê°€
                    end_msg = L["prompt_survey"]
                    st.session_state.simulator_messages.append(
                        {"role": "system_end", "content": end_msg}
                    )

                    # ì±„íŒ… ì¢…ë£Œ ì²˜ë¦¬
                    st.session_state.is_chat_ended = True
                    st.session_state.sim_stage = "CLOSING"
                    
                    # ì´ë ¥ ì €ì¥
                    save_simulation_history_local(
                        st.session_state.customer_query_text_area, customer_type_display,
                        st.session_state.simulator_messages, is_chat_ended=True,
                        attachment_context=st.session_state.sim_attachment_context_for_llm,
                    )
                    
                    st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
                    st.rerun()  # ë²„íŠ¼ í´ë¦­ í›„ UI ì—…ë°ì´íŠ¸
            # (B) "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤" ê²½ë¡œ -> AGENT_TURNìœ¼ë¡œ ë³µê·€
            elif L['customer_has_additional_inquiries'] in final_customer_reaction:
                st.session_state.sim_stage = "AGENT_TURN"
                save_simulation_history_local(
                    st.session_state.customer_query_text_area, customer_type_display,
                    st.session_state.simulator_messages, is_chat_ended=False,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                )
                st.session_state.realtime_hint_text = ""
                st.rerun()  # AGENT_TURNìœ¼ë¡œ ì´ë™ í›„ UI ì—…ë°ì´íŠ¸
            else:
                # ê³ ê° ì‘ë‹µì´ ìƒì„±ë˜ì—ˆì§€ë§Œ ì¡°ê±´ì— ë§ì§€ ì•ŠëŠ” ê²½ìš°ì—ë„ ë²„íŠ¼ í‘œì‹œ
                # (ê¸°ë³¸ì ìœ¼ë¡œ "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤"ë¡œ ê°„ì£¼)
                # â­ ìˆ˜ì •: fallback ê²½ë¡œì—ì„œë„ ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ ë©”ì‹œì§€ ì¶”ê°€
                agent_closing_added = False
                for msg in reversed(st.session_state.simulator_messages):
                    if msg.get("role") == "agent_response":
                        # ì´ë¯¸ ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ê°€ ìˆëŠ”ì§€ í™•ì¸
                        agent_msg_content = msg.get("content", "")
                        if "ê°ì‚¬" in agent_msg_content or "Thank you" in agent_msg_content or "ã‚ã‚ŠãŒã¨ã†" in agent_msg_content:
                            agent_closing_added = True
                        break
                
                if not agent_closing_added:
                    # â­ ì¶”ê°€: ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ ë©”ì‹œì§€ ì „ì†¡
                    agent_name = st.session_state.get("agent_name", "000")
                    if current_lang == "ko":
                        agent_closing_msg = f"ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› {agent_name}ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì„¸ìš”."
                    elif current_lang == "en":
                        agent_closing_msg = f"Thank you for contacting us. This was {agent_name}. Have a great day!"
                    else:  # ja
                        agent_closing_msg = f"ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚æ‹…å½“ã¯{agent_name}ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"
                    
                    # ì—ì´ì „íŠ¸ ê°ì‚¬ ì¸ì‚¬ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€
                    st.session_state.simulator_messages.append(
                        {"role": "agent_response", "content": agent_closing_msg}
                    )
                
                st.markdown("---")
                st.success(L["no_more_inquiries_confirmed"])
                st.markdown(f"### {L['consultation_end_header']}")
                st.info(L["click_survey_button_to_end"])
                st.markdown("---")
                
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    end_chat_button = st.button(
                        L["sim_end_chat_button"], 
                        key="btn_final_end_chat_fallback", 
                        use_container_width=True, 
                        type="primary"
                    )
                
                if end_chat_button:
                    # AHT íƒ€ì´ë¨¸ ì •ì§€
                    st.session_state.start_time = None
                    
                    # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë©”ì‹œì§€ ì¶”ê°€
                    end_msg = L["prompt_survey"]
                    st.session_state.simulator_messages.append(
                        {"role": "system_end", "content": end_msg}
                    )
                    
                    # ì±„íŒ… ì¢…ë£Œ ì²˜ë¦¬
                    st.session_state.is_chat_ended = True
                    st.session_state.sim_stage = "CLOSING"
                    
                    # ì´ë ¥ ì €ì¥
                    save_simulation_history_local(
                        st.session_state.customer_query_text_area, customer_type_display,
                        st.session_state.simulator_messages, is_chat_ended=True,
                        attachment_context=st.session_state.sim_attachment_context_for_llm,
                    )
                    
                    st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
                    st.rerun()  # ë²„íŠ¼ í´ë¦­ í›„ UI ì—…ë°ì´íŠ¸

    # =========================
    # 9. ìµœì¢… ì¢…ë£Œ í–‰ë™ (FINAL_CLOSING_ACTION)
    # =========================
    elif st.session_state.sim_stage == "FINAL_CLOSING_ACTION":
        # ì–¸ì–´ í‚¤ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
        current_lang = st.session_state.get("language", "ko")
        if current_lang not in ["ko", "en", "ja"]:
            current_lang = "ko"
        L = LANG.get(current_lang, LANG["ko"])
        
        # â­ ìˆ˜ì •: ëª…í™•í•œ ì•ˆë‚´ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë²„íŠ¼ í‘œì‹œ
        st.markdown("---")
        st.success(L["no_more_inquiries_confirmed"])
        st.markdown(f"### {L['consultation_end_header']}")
        st.info(L["click_survey_button_to_end"])
        st.markdown("---")
        
        # ë²„íŠ¼ì„ ì¤‘ì•™ì— í¬ê²Œ í‘œì‹œ
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            end_chat_button = st.button(
                L["sim_end_chat_button"], 
                key="btn_final_end_chat", 
                use_container_width=True, 
                type="primary"
            )
        
        if end_chat_button:
            # AHT íƒ€ì´ë¨¸ ì •ì§€
            st.session_state.start_time = None

            # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë©”ì‹œì§€ ì¶”ê°€
            end_msg = L["prompt_survey"]
            st.session_state.simulator_messages.append(
                {"role": "system_end", "content": end_msg}
            )

            # ì±„íŒ… ì¢…ë£Œ ì²˜ë¦¬
            st.session_state.is_chat_ended = True
            st.session_state.sim_stage = "CLOSING"
            
            # ì´ë ¥ ì €ì¥
            customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=True,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )
            
            st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”

# ========================================
# ì „í™” ì‹œë®¬ë ˆì´í„° ë¡œì§
# ========================================

elif feature_selection == L["sim_tab_phone"]:
    st.header(L["phone_header"])
    st.markdown(L["simulator_desc"])

    current_lang = st.session_state.language
    L = LANG[current_lang]



    # ========================================
    # AHT íƒ€ì´ë¨¸ (IN_CALL ìƒíƒœì—ì„œë§Œ ë™ì‘)
    # ========================================
    if st.session_state.call_sim_stage == "IN_CALL":
        # AHT íƒ€ì´ë¨¸ ê³„ì‚° ë¡œì§
        col_timer, col_duration = st.columns([1, 4])

        if st.session_state.start_time is not None:
            now = datetime.now()

            # Hold ì¤‘ì´ë¼ë©´, Hold ìƒíƒœê°€ ëœ ì´í›„ì˜ ì‹œê°„ì„ í˜„ì¬ total_hold_durationì— ë”í•˜ì§€ ì•ŠìŒ (Resume ì‹œ ì •ì‚°)
            if st.session_state.is_on_hold and st.session_state.hold_start_time:
                # Hold ì¤‘ì´ì§€ë§Œ AHT íƒ€ì´ë¨¸ëŠ” ê³„ì† í˜ëŸ¬ê°€ì•¼ í•˜ë¯€ë¡œ, Hold ì‹œê°„ì€ ì œì™¸í•˜ì§€ ì•Šê³  ìµœì¢… AHT ê³„ì‚°ì—ë§Œ ì‚¬ìš©
                elapsed_time_total = now - st.session_state.start_time
            else:
                elapsed_time_total = now - st.session_state.start_time

            # â­ AHTëŠ” í†µí™” ì‹œì‘ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ì´ ê²½ê³¼ ì‹œê°„ì…ë‹ˆë‹¤.
            total_seconds = elapsed_time_total.total_seconds()
            total_seconds = max(0, total_seconds)  # ìŒìˆ˜ ë°©ì§€

            # ì‹œê°„ í˜•ì‹ í¬ë§·íŒ…
            minutes = int(total_seconds // 60)
            seconds = int(total_seconds % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # ê²½ê³  ê¸°ì¤€
            if total_seconds > 900:  # 15ë¶„
                delta_str = L["timer_info_risk"]
                delta_color = "inverse"
            elif total_seconds > 600:  # 10ë¶„
                delta_str = L["timer_info_warn"]
                delta_color = "off"
            else:
                delta_str = L["timer_info_ok"]
                delta_color = "normal"

                with col_timer:
                    # AHT íƒ€ì´ë¨¸ í‘œì‹œ
                    st.metric(L["timer_metric"], time_str, delta=delta_str, delta_color=delta_color)

                # â­ ìˆ˜ì •: AHT íƒ€ì´ë¨¸ ì‹¤ì‹œê°„ ê°±ì‹ ì„ ìœ„í•œ ê°•ì œ ì¬ì‹¤í–‰ ë¡œì§ ì¶”ê°€
                # í†µí™” ì¤‘ì´ê³ , Hold ìƒíƒœê°€ ì•„ë‹ ë•Œë§Œ 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸í•˜ì—¬ ì‹¤ì‹œê°„ì„±ì„ í™•ë³´
                if not st.session_state.is_on_hold and total_seconds < 1000:
                    time.sleep(1)

        # ========================================
        # í™”ë©´ êµ¬ë¶„ (ì• ë‹ˆë©”ì´ì…˜ / CC)
        # ========================================
    col_video, col_cc = st.columns([1, 2])

    with col_video:
        st.subheader(f"ğŸ“º {L['customer_video_simulation']}")

        if st.session_state.call_sim_stage == "WAITING_CALL":
            st.info("í†µí™” ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")

        elif st.session_state.call_sim_stage == "CALL_ENDED":
            st.info("í†µí™” ì¢…ë£Œ")

        else:
            # â­ ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì˜µì…˜ ì¶”ê°€ (ë¡œì»¬ ê²½ë¡œ ì§€ì›)
            # í•­ìƒ í¼ì³ì§„ ìƒíƒœë¡œ í‘œì‹œí•˜ì—¬ ë¹„ë””ì˜¤ë¥¼ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í•¨
            with st.expander(L["video_upload_expander"], expanded=True):
                # ë¹„ë””ì˜¤ ë™ê¸°í™” í™œì„±í™” ì—¬ë¶€
                st.session_state.is_video_sync_enabled = st.checkbox(
                    L["video_sync_enable"],
                    value=st.session_state.is_video_sync_enabled,
                    key="video_sync_checkbox"
                )
                
                # OpenAI/Gemini ê¸°ë°˜ ì˜ìƒ RAG ì„¤ëª…
                st.markdown("---")
                st.markdown(f"**{L['video_rag_title']}**")
                st.success(L["video_rag_desc"])
                
                # ê°€ìƒ íœ´ë¨¼ ê¸°ìˆ ì€ í˜„ì¬ ë¹„í™œì„±í™” (OpenAI/Gemini ê¸°ë°˜ ì˜ìƒ RAG ì‚¬ìš©)
                st.session_state.virtual_human_enabled = False
                
                # ì„±ë³„ ë° ê°ì • ìƒíƒœë³„ ë¹„ë””ì˜¤ ì—…ë¡œë“œ
                st.markdown(f"**{L['video_gender_emotion_setting']}**")
                col_gender_video, col_emotion_video = st.columns(2)
                
                with col_gender_video:
                    video_gender = st.radio(L["video_gender_label"], [L["video_gender_male"], L["video_gender_female"]], key="video_gender_select", horizontal=True)
                    gender_key = "male" if video_gender == L["video_gender_male"] else "female"
                
                with col_emotion_video:
                    video_emotion = st.selectbox(
                        L["video_emotion_label"],
                        ["NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD"],
                        key="video_emotion_select"
                    )
                    emotion_key = video_emotion.lower()
                
                # í•´ë‹¹ ì¡°í•©ì˜ ë¹„ë””ì˜¤ ì—…ë¡œë“œ
                video_key = f"video_{gender_key}_{emotion_key}"
                uploaded_video = st.file_uploader(
                    L["video_upload_label"].format(gender=video_gender, emotion=video_emotion),
                    type=["mp4", "webm", "ogg"],
                    key=f"customer_video_uploader_{gender_key}_{emotion_key}"
                )
                
                # â­ Gemini ì œì•ˆ: ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì§ì ‘ ì €ì¥ (íŒŒì¼ ì €ì¥ì€ ì˜µì…˜)
                upload_key = f"last_uploaded_video_{gender_key}_{emotion_key}"
                video_bytes_key = f"video_bytes_{gender_key}_{emotion_key}"  # ë°”ì´íŠ¸ ë°ì´í„° ì €ì¥ í‚¤
                
                if uploaded_video is not None:
                    # íŒŒì¼ì´ ìƒˆë¡œ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ (íŒŒì¼ëª…ìœ¼ë¡œ ë¹„êµ)
                    current_upload_name = uploaded_video.name if hasattr(uploaded_video, 'name') else None
                    last_upload_info = st.session_state.get(upload_key, None)
                    # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'name' í‚¤ì—ì„œ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
                    if isinstance(last_upload_info, dict):
                        last_upload_name = last_upload_info.get('name', None)
                    else:
                        last_upload_name = last_upload_info
                    
                    # ìƒˆ íŒŒì¼ì´ê±°ë‚˜ ì´ì „ê³¼ ë‹¤ë¥¸ íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ ì €ì¥
                    if current_upload_name != last_upload_name:
                        try:
                            # ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ë¥¼ ì¦‰ì‹œ ì½ê¸° (rerun ì „ì— ì²˜ë¦¬)
                            video_bytes = uploaded_video.read()
                            current_upload_size = len(video_bytes)
                            
                            if not video_bytes or len(video_bytes) == 0:
                                st.error(L["video_empty_error"])
                            else:
                                # íŒŒì¼ëª… ë° í™•ì¥ì ê²°ì •
                                uploaded_filename = uploaded_video.name if hasattr(uploaded_video, 'name') else f"{gender_key}_{emotion_key}.mp4"
                                file_ext = os.path.splitext(uploaded_filename)[1].lower() if uploaded_filename else ".mp4"
                                if file_ext not in ['.mp4', '.webm', '.ogg', '.mpeg4']:
                                    file_ext = ".mp4"
                                
                                # MIME íƒ€ì… ê²°ì •
                                mime_type = uploaded_video.type if hasattr(uploaded_video, 'type') else f"video/{file_ext.lstrip('.')}"
                                if not mime_type or mime_type == "application/octet-stream":
                                    mime_type = f"video/{file_ext.lstrip('.')}"
                                
                                # â­ 1ì°¨ í•´ê²°ì±…: ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì§ì ‘ ì €ì¥ (ê°€ì¥ ì•ˆì •ì )
                                st.session_state[video_bytes_key] = video_bytes
                                st.session_state[video_key] = video_bytes_key  # ê²½ë¡œ ëŒ€ì‹  ë°”ì´íŠ¸ í‚¤ ì €ì¥
                                st.session_state[upload_key] = {
                                    'name': current_upload_name,
                                    'size': current_upload_size,
                                    'mime': mime_type,
                                    'ext': file_ext
                                }
                                
                                file_size_mb = current_upload_size / (1024 * 1024)
                                st.success(L["video_bytes_saved"].format(name=current_upload_name, size=f"{file_size_mb:.2f}"))
                                
                                # â­ ì¦‰ì‹œ ë¯¸ë¦¬ë³´ê¸° (ë°”ì´íŠ¸ ë°ì´í„° ì§ì ‘ ì‚¬ìš©)
                                try:
                                    st.video(video_bytes, format=mime_type, autoplay=False, loop=False, muted=False)
                                except Exception as video_error:
                                    st.warning(f"âš ï¸ {L.get('video_preview_error', 'ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜')}: {video_error}")
                                    # MIME íƒ€ì…ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¬ì‹œë„
                                    try:
                                        st.video(video_bytes, format=f"video/{file_ext.lstrip('.')}", autoplay=False, loop=False, muted=False)
                                    except:
                                        st.error(L["video_playback_error"])
                                
                                # â­ ì˜µì…˜: íŒŒì¼ ì €ì¥ë„ ì‹œë„ (ë°±ì—…ìš©, ì‹¤íŒ¨í•´ë„ ë°”ì´íŠ¸ëŠ” ì´ë¯¸ ì €ì¥ë¨)
                                try:
                                    video_dir = os.path.join(DATA_DIR, "videos")
                                    os.makedirs(video_dir, exist_ok=True)
                                    video_filename = f"{gender_key}_{emotion_key}{file_ext}"
                                    video_path = os.path.join(video_dir, video_filename)
                                    
                                    # íŒŒì¼ ì €ì¥ ì‹œë„ (ê¶Œí•œ ë¬¸ì œê°€ ìˆì–´ë„ ë°”ì´íŠ¸ëŠ” ì´ë¯¸ ì €ì¥ë¨)
                                    try:
                                        with open(video_path, "wb") as f:
                                            f.write(video_bytes)
                                            f.flush()
                                        st.info(f"ğŸ“‚ íŒŒì¼ë„ ì €ì¥ë¨: {video_path}")
                                    except Exception as save_error:
                                        st.info(f"ğŸ’¡ íŒŒì¼ ì €ì¥ì€ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤ (ë°”ì´íŠ¸ ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥ë¨): {save_error}")
                                except:
                                    pass  # íŒŒì¼ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë°”ì´íŠ¸ëŠ” ì´ë¯¸ ì €ì¥ë¨
                                
                        except Exception as e:
                            st.error(L["video_upload_error"].format(error=str(e)))
                            import traceback
                            st.code(traceback.format_exc())
                
                # ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ê°€ ìˆìœ¼ë©´ í˜„ì¬ ì„ íƒëœ ì¡°í•©ì˜ ë¹„ë””ì˜¤ í‘œì‹œ
                st.markdown("---")
                st.markdown(f"**{L['video_current_selection'].format(gender=video_gender, emotion=video_emotion)}**")
                
                # â­ Gemini ì œì•ˆ: ì„¸ì…˜ ìƒíƒœì—ì„œ ë°”ì´íŠ¸ ë°ì´í„° ì§ì ‘ ì¡°íšŒ
                video_bytes_key = f"video_bytes_{gender_key}_{emotion_key}"
                current_video_bytes = st.session_state.get(video_bytes_key, None)
                
                if current_video_bytes:
                    # ë°”ì´íŠ¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì§ì ‘ ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
                    upload_info = st.session_state.get(upload_key, {})
                    mime_type = upload_info.get('mime', 'video/mp4')
                    file_ext = upload_info.get('ext', '.mp4')
                    
                    st.success(f"âœ… ë¹„ë””ì˜¤ ë°”ì´íŠ¸ ë°ì´í„° ë°œê²¬: {upload_info.get('name', 'ì—…ë¡œë“œëœ ë¹„ë””ì˜¤')}")
                    try:
                        st.video(current_video_bytes, format=mime_type, autoplay=False, loop=False, muted=False)
                        st.caption(L["video_auto_play_info"].format(gender=video_gender, emotion=video_emotion))
                    except Exception as e:
                        st.warning(f"ë¹„ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                        # MIME íƒ€ì…ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì¬ì‹œë„
                        try:
                            st.video(current_video_bytes, format=f"video/{file_ext.lstrip('.')}", autoplay=False, loop=False, muted=False)
                        except:
                            st.error(L["video_playback_error"])
                else:
                    # ë°”ì´íŠ¸ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ ê²½ë¡œë¡œ ì‹œë„ (í•˜ìœ„ í˜¸í™˜ì„±)
                    current_video_path = get_video_path_by_avatar(
                        gender_key,
                        video_emotion,
                        is_speaking=False,
                        gesture="NONE"
                    )
                    
                    if current_video_path and os.path.exists(current_video_path):
                        st.success(f"âœ… ë¹„ë””ì˜¤ íŒŒì¼ ë°œê²¬: {os.path.basename(current_video_path)}")
                        try:
                            with open(current_video_path, "rb") as f:
                                existing_video_bytes = f.read()
                            st.video(existing_video_bytes, format="video/mp4", autoplay=False, loop=False, muted=False)
                            st.caption(L["video_auto_play_info"].format(gender=video_gender, emotion=video_emotion))
                        except Exception as e:
                            st.warning(f"ë¹„ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                    else:
                        st.info(L["video_upload_prompt"].format(filename=f"{gender_key}_{emotion_key}.mp4"))
                    
                    # ë””ë²„ê¹… ì •ë³´: ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ ëª©ë¡ í‘œì‹œ
                    video_dir = os.path.join(DATA_DIR, "videos")
                    st.caption(L["video_save_path"] + f" {video_dir}")
                    
                    if os.path.exists(video_dir):
                        all_videos = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.webm', '.ogg'))]
                        if all_videos:
                            st.caption(f"{L['video_uploaded_files']} ({len(all_videos)}ê°œ):")
                            for vid in all_videos:
                                st.caption(f"  - {vid}")
                            
                            # ë¹„ìŠ·í•œ ë¹„ë””ì˜¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                            similar_videos = [
                                f for f in all_videos
                                if f.startswith(f"{gender_key}_") and f.endswith(('.mp4', '.webm', '.ogg'))
                            ]
                            if similar_videos:
                                st.caption(f"ğŸ“ {L.get('video_similar_gender', 'ê°™ì€ ì„±ë³„ì˜ ë‹¤ë¥¸ ë¹„ë””ì˜¤')}: {', '.join(similar_videos[:3])}")
                                st.caption(L.get("video_rename_hint", "ğŸ’¡ ìœ„ ë¹„ë””ì˜¤ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ íŒŒì¼ëª…ì„ ë³€ê²½í•˜ê±°ë‚˜ ìƒˆë¡œ ì—…ë¡œë“œí•˜ì„¸ìš”."))
                        else:
                            st.caption(L["video_directory_empty"])
                    else:
                        st.caption(L["video_directory_not_exist"].format(path=video_dir))
                
                # ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì…ë ¥ ë° ë³µì‚¬
                video_path_input = st.text_input(
                    L["video_local_path_input"],
                    placeholder=L["video_local_path_placeholder"],
                    key="video_path_input"
                )
                
                if video_path_input:
                    try:
                        # â­ Gemini ì œì•ˆ: ì ˆëŒ€ ê²½ë¡œ ê²€ì¦ ê°•í™”
                        if not os.path.isabs(video_path_input):
                            st.error("âŒ ë¡œì»¬ ê²½ë¡œ ì…ë ¥ ì‹œ ë°˜ë“œì‹œ **ì ˆëŒ€ ê²½ë¡œ**ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš” (ì˜ˆ: C:\\Users\\...\\video.mp4).")
                            st.error("ğŸ’¡ Streamlit ì•±ì´ ì‹¤í–‰ë˜ëŠ” ì„œë²„ í™˜ê²½ê³¼ íŒŒì¼ ì‹œìŠ¤í…œì´ ë‹¤ë¥´ë©´ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            st.stop()
                        
                        source_video_path = video_path_input
                        
                        if not os.path.exists(source_video_path):
                            st.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_video_path}")
                            st.error("ğŸ’¡ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³ , Streamlit ì•±ì´ ì‹¤í–‰ë˜ëŠ” ì„œë²„ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•œ ê²½ë¡œì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                            st.stop()
                        
                        # ì›ë³¸ íŒŒì¼ ì½ê¸°
                        with open(source_video_path, "rb") as f:
                            video_bytes = f.read()
                        
                        if len(video_bytes) == 0:
                            st.error("âŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                            st.stop()
                        
                        # íŒŒì¼ëª… ë° í™•ì¥ì ê²°ì •
                        source_filename = os.path.basename(source_video_path)
                        file_ext = os.path.splitext(source_filename)[1].lower()
                        if file_ext not in ['.mp4', '.webm', '.ogg', '.mpeg4']:
                            file_ext = ".mp4"
                        
                        mime_type = f"video/{file_ext.lstrip('.')}"
                        
                        # â­ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì§ì ‘ ì €ì¥ (íŒŒì¼ ë³µì‚¬ëŠ” ì˜µì…˜)
                        video_bytes_key = f"video_bytes_{gender_key}_{emotion_key}"
                        st.session_state[video_bytes_key] = video_bytes
                        st.session_state[video_key] = video_bytes_key
                        st.session_state[upload_key] = {
                            'name': source_filename,
                            'size': len(video_bytes),
                            'mime': mime_type,
                            'ext': file_ext
                        }
                        
                        file_size_mb = len(video_bytes) / (1024 * 1024)
                        st.success(f"âœ… ë¹„ë””ì˜¤ ë°”ì´íŠ¸ ë¡œë“œ ì™„ë£Œ: {source_filename} ({file_size_mb:.2f} MB)")
                        
                        # ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸° (ë°”ì´íŠ¸ ë°ì´í„° ì§ì ‘ ì‚¬ìš©)
                        try:
                            st.video(video_bytes, format=mime_type, autoplay=False, loop=False, muted=False)
                        except Exception as video_error:
                            st.warning(f"âš ï¸ ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: {video_error}")
                        
                        # â­ ì˜µì…˜: íŒŒì¼ ë³µì‚¬ë„ ì‹œë„ (ë°±ì—…ìš©)
                        try:
                            video_dir = os.path.join(DATA_DIR, "videos")
                            os.makedirs(video_dir, exist_ok=True)
                            video_filename = f"{gender_key}_{emotion_key}{file_ext}"
                            target_video_path = os.path.join(video_dir, video_filename)
                            
                            with open(target_video_path, "wb") as f:
                                f.write(video_bytes)
                                f.flush()
                            st.info(f"ğŸ“‚ íŒŒì¼ë„ ë³µì‚¬ë¨: {target_video_path}")
                        except Exception as copy_error:
                            st.info(f"ğŸ’¡ íŒŒì¼ ë³µì‚¬ëŠ” ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤ (ë°”ì´íŠ¸ ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥ë¨): {copy_error}")
                        
                        # ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”
                        st.session_state.video_path_input = ""
                        
                    except Exception as e:
                        st.error(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())
            
            # ìƒíƒœ ì„ íƒ ë° ë¹„ë””ì˜¤ í‘œì‹œ
            st.markdown("---")
            st.markdown(f"**{L['video_current_avatar']}**")
            
            if st.session_state.is_on_hold:
                avatar_state = "HOLD"
            else:
                avatar_state = st.session_state.customer_avatar.get("state", "NEUTRAL")
            
            customer_gender = st.session_state.customer_avatar.get("gender", "male")
            
            # get_video_path_by_avatar í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ê²½ë¡œ ì°¾ê¸°
            video_path = get_video_path_by_avatar(
                customer_gender, 
                avatar_state, 
                is_speaking=False,  # ë¯¸ë¦¬ë³´ê¸°ëŠ” ìë™ ì¬ìƒí•˜ì§€ ì•ŠìŒ
                gesture="NONE"
            )
            
            # ë¹„ë””ì˜¤ í‘œì‹œ
            if video_path and os.path.exists(video_path):
                try:
                    with open(video_path, "rb") as f:
                        video_bytes = f.read()
                    
                    # ë¹„ë””ì˜¤ ì •ë³´ í‘œì‹œ
                    avatar_emoji = {
                        "NEUTRAL": "ğŸ˜",
                        "HAPPY": "ğŸ˜Š",
                        "ANGRY": "ğŸ˜ ",
                        "ASKING": "ğŸ¤”",
                        "SAD": "ğŸ˜¢",
                        "HOLD": "â¸ï¸"
                    }.get(avatar_state, "ğŸ˜")
                    
                    st.markdown(f"### {avatar_emoji} {customer_gender.upper()} - {avatar_state}")
                    st.caption(f"ë¹„ë””ì˜¤: {os.path.basename(video_path)}")
                    
                    # í˜„ì¬ ë§í•˜ëŠ” ì¤‘ì´ë©´ ìë™ ì¬ìƒ, ì•„ë‹ˆë©´ ìˆ˜ë™ ì¬ìƒ
                    is_speaking = bool(
                        st.session_state.get("customer_initial_audio_bytes") or 
                        st.session_state.get("current_customer_audio_text")
                    )
                    
                    autoplay_video = st.session_state.is_video_sync_enabled and is_speaking
                    st.video(video_bytes, format="video/mp4", autoplay=autoplay_video, loop=False, muted=False)
                    
                except Exception as e:
                    st.warning(f"ë¹„ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                    avatar_emoji = {
                        "NEUTRAL": "ğŸ˜",
                        "HAPPY": "ğŸ˜Š",
                        "ANGRY": "ğŸ˜ ",
                        "ASKING": "ğŸ¤”",
                        "SAD": "ğŸ˜¢",
                        "HOLD": "â¸ï¸"
                    }.get(avatar_state, "ğŸ˜")
                    st.markdown(f"### {avatar_emoji} {L['customer_avatar']}")
                    st.info(L.get("avatar_status_info", "ìƒíƒœ: {state} | ì„±ë³„: {gender}").format(state=avatar_state, gender=customer_gender))
            else:
                # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì´ëª¨ì§€ë¡œ í‘œì‹œ
                avatar_emoji = {
                    "NEUTRAL": "ğŸ˜",
                    "HAPPY": "ğŸ˜Š",
                    "ANGRY": "ğŸ˜ ",
                    "ASKING": "ğŸ¤”",
                    "SAD": "ğŸ˜¢",
                    "HOLD": "â¸ï¸"
                }.get(avatar_state, "ğŸ˜")
                
                st.markdown(f"### {avatar_emoji} ê³ ê° ì•„ë°”íƒ€")
                st.info(L.get("avatar_status_info", "ìƒíƒœ: {state} | ì„±ë³„: {gender}").format(state=avatar_state, gender=customer_gender))
                st.warning(L["video_avatar_upload_prompt"].format(filename=f"{customer_gender}_{avatar_state.lower()}.mp4"))
                
                # ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ ëª©ë¡ í‘œì‹œ
                video_dir = os.path.join(DATA_DIR, "videos")
                if os.path.exists(video_dir):
                    uploaded_videos = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.webm', '.ogg'))]
                    if uploaded_videos:
                        st.caption(f"{L['video_uploaded_files']}: {', '.join(uploaded_videos[:5])}")
                        if len(uploaded_videos) > 5:
                            st.caption(L.get("video_more_files", f"... ì™¸ {len(uploaded_videos) - 5}ê°œ").format(count=len(uploaded_videos) - 5))

    with col_cc:
        # â­ ìˆ˜ì •: "ì „í™” ìˆ˜ì‹  ì¤‘" ë©”ì‹œì§€ëŠ” í†µí™” ì¤‘ì¼ ë•Œë§Œ í‘œì‹œ
        if st.session_state.call_sim_stage == "IN_CALL":
            if st.session_state.call_sim_mode == "INBOUND":
                st.markdown(
                    f"## {L['call_status_ringing'].format(number=st.session_state.incoming_phone_number)}"
                )
            else:
                st.markdown(
                    f"## {L['button_call_outbound']} ({st.session_state.incoming_phone_number})"
                )
        st.markdown("---")

    # ========================================
    # WAITING / RINGING ìƒíƒœ
    # ========================================
    if st.session_state.call_sim_stage in ["WAITING_CALL", "RINGING"]:

        if "call_sim_mode" not in st.session_state:
            st.session_state.call_sim_mode = "INBOUND"  # INBOUND or OUTBOUND

        if st.session_state.call_sim_mode == "INBOUND":
            st.subheader(L["call_status_waiting"])
        else:
            st.subheader(L["button_call_outbound"])

        # í™ˆí˜ì´ì§€ ì›¹ ì£¼ì†Œ ì…ë ¥ (ì„ íƒì‚¬í•­)
        st.session_state.call_website_url = st.text_input(
            L.get("website_url_label", "í™ˆí˜ì´ì§€ ì›¹ ì£¼ì†Œ (ì„ íƒì‚¬í•­)"),
            key="call_website_url_input",
            value=st.session_state.call_website_url,
            placeholder=L.get("website_url_placeholder", "https://example.com (í™ˆí˜ì´ì§€ ì£¼ì†Œê°€ ìˆìœ¼ë©´ ì…ë ¥í•˜ì„¸ìš”)"),
        )

        # ì´ˆê¸° ë¬¸ì˜ ì…ë ¥ (ê³ ê°ì´ ì „í™”ë¡œ ë§í•  ë‚´ìš©)
        st.session_state.call_initial_query = st.text_area(
            L["customer_query_label"],
            key="call_initial_query_text_area",
            height=100,
            placeholder=L["call_query_placeholder"],
        )

        # ê°€ìƒ ì „í™”ë²ˆí˜¸ í‘œì‹œ
        st.session_state.incoming_phone_number = st.text_input(
            "Incoming/Outgoing Phone Number",
            key="incoming_phone_number_input",
            value=st.session_state.incoming_phone_number,
            placeholder=L["call_number_placeholder"],
        )

        # ê³ ê° ìœ í˜• ì„ íƒ
        customer_type_options = L["customer_type_options"]
        default_idx = customer_type_options.index(
            st.session_state.customer_type_sim_select) if st.session_state.customer_type_sim_select in customer_type_options else 0

        st.session_state.customer_type_sim_select = st.selectbox(
            L["customer_type_label"],
            customer_type_options,
            index=default_idx,
            key="call_customer_type_sim_select_widget",
        )

        # â­ ì¶”ê°€: ê³ ê° ì„±ë³„ ë° ê°ì • ìƒíƒœ ì„¤ì •
        col_gender, col_emotion = st.columns(2)
        
        with col_gender:
            # ê³ ê° ì„±ë³„ ì„ íƒ
            if "customer_gender" not in st.session_state:
                st.session_state.customer_gender = "male"
            
            # â­ ìˆ˜ì •: ë²ˆì—­ í‚¤ ì‚¬ìš©
            gender_options = [L["gender_male"], L["gender_female"]]
            current_gender = st.session_state.customer_avatar.get("gender", "male")
            default_gender_idx = 0 if current_gender == "male" else 1
            
            selected_gender_display = st.radio(
                L["customer_gender_label"],
                gender_options,
                index=default_gender_idx,
                key="call_customer_gender_radio",
                horizontal=True
            )
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ì˜ì–´ë¡œ)
            st.session_state.customer_avatar["gender"] = "male" if selected_gender_display == L["gender_male"] else "female"
            st.session_state.customer_gender = st.session_state.customer_avatar["gender"]
        
        with col_emotion:
            # ê³ ê° ê°ì • ìƒíƒœ ì„ íƒ
            # â­ ìˆ˜ì •: ë²ˆì—­ í‚¤ ì‚¬ìš©
            emotion_options = [
                L["emotion_happy"],
                L["emotion_dissatisfied"],
                L["emotion_angry"],
                L["emotion_sad"],
                L["emotion_neutral"]
            ]
            emotion_mapping = {
                L["emotion_happy"]: "HAPPY",
                L["emotion_dissatisfied"]: "ASKING",
                L["emotion_angry"]: "ANGRY",
                L["emotion_sad"]: "SAD",
                L["emotion_neutral"]: "NEUTRAL"
            }
            
            current_emotion_state = st.session_state.customer_avatar.get("state", "NEUTRAL")
            default_emotion_idx = 4  # ê¸°ë³¸ê°’: ì¤‘ë¦½
            for i, (emotion_display, emotion_state) in enumerate(emotion_mapping.items()):
                if emotion_state == current_emotion_state:
                    default_emotion_idx = i
                    break
            
            selected_emotion = st.selectbox(
                L["customer_emotion_label"],
                emotion_options,
                index=default_emotion_idx,
                key="call_customer_emotion_select",
            )
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            st.session_state.customer_avatar["state"] = emotion_mapping.get(selected_emotion, "NEUTRAL")

        st.markdown("---")

        col_in, col_out = st.columns(2)

        # ì „í™” ì‘ë‹µ (ìˆ˜ì‹ )
        with col_in:
            if st.button(L["button_answer"], key=f"answer_call_btn_{st.session_state.sim_instance_id}"):
                # ì…ë ¥ ê²€ì¦
                if not st.session_state.call_initial_query.strip():
                    st.warning(L["simulation_warning_query"])
                    # st.stop()

                # â­ ìˆ˜ì •: OpenAI ë˜ëŠ” Gemini API í‚¤ ì²´í¬
                has_openai = st.session_state.openai_client is not None
                has_gemini = bool(get_api_key("gemini"))
                
                if not st.session_state.is_llm_ready or (not has_openai and not has_gemini):
                    st.error(L["simulation_no_key_warning"] + " (OpenAI ë˜ëŠ” Gemini API Key í•„ìš”)")
                    # st.stop()

                # INBOUND ëª¨ë“œ ì„¤ì •
                st.session_state.call_sim_mode = "INBOUND"

                # ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ë° ì‹œì‘
                st.session_state.call_sim_stage = "IN_CALL"
                st.session_state.is_call_ended = False
                st.session_state.is_on_hold = False
                st.session_state.total_hold_duration = timedelta(0)
                st.session_state.hold_start_time = None
                st.session_state.start_time = datetime.now()  # í†µí™” ì‹œì‘ ì‹œê°„ (AHT ì‹œì‘)
                st.session_state.simulator_messages = []
                st.session_state.current_customer_audio_text = ""
                st.session_state.current_agent_audio_text = ""
                st.session_state.agent_response_input_box_widget_call = ""
                st.session_state.sim_instance_id = str(uuid.uuid4())
                st.session_state.call_summary_text = ""  # ìš”ì•½ ì´ˆê¸°í™”
                st.session_state.customer_initial_audio_bytes = None  # ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
                st.session_state.customer_history_summary = ""  # AI ìš”ì•½ ì´ˆê¸°í™” (ì¶”ê°€)
                st.session_state.sim_audio_bytes = None  # ë…¹ìŒ íŒŒì¼ ì´ˆê¸°í™” (ì¶”ê°€)

                # â­ ìˆ˜ì •: ìë™ ì¸ì‚¬ë§ ìƒì„± ì œê±° - ì—ì´ì „íŠ¸ê°€ ì§ì ‘ ë…¹ìŒí•˜ë„ë¡ ë³€ê²½
                st.session_state.just_entered_call = False
                st.session_state.customer_turn_start = False  # ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ ì™„ë£Œ ì „ê¹Œì§€ False

                # ê³ ê°ì˜ ì²« ë²ˆì§¸ ìŒì„± ë©”ì‹œì§€ (ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ë©”ì‹œì§€)
                initial_query_text = st.session_state.call_initial_query.strip()
                st.session_state.current_customer_audio_text = initial_query_text

                # â­ ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€ ë° ì–¸ì–´ ì„¤ì • ì—…ë°ì´íŠ¸
                try:
                    detected_lang = detect_text_language(initial_query_text)
                    if detected_lang in ["ko", "en", "ja"] and detected_lang != st.session_state.language:
                        st.session_state.language = detected_lang
                        st.info(f"ğŸŒ ì…ë ¥ ì–¸ì–´ê°€ ê°ì§€ë˜ì–´ ì–¸ì–´ ì„¤ì •ì´ '{detected_lang}'ë¡œ ìë™ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f"Language detection failed in call: {e}")
                    detected_lang = st.session_state.language

                # â­ ê³ ê°ì˜ ì²« ë¬¸ì˜ TTS ìŒì„± ìƒì„± ë° ì €ì¥ (ê°ì§€ëœ ì–¸ì–´ ì‚¬ìš©)
                with st.spinner(L["tts_status_generating"] + " (Initial Customer Query)"):
                    audio_bytes, msg = synthesize_tts(initial_query_text, st.session_state.language, role="customer")
                    if audio_bytes:
                        st.session_state.customer_initial_audio_bytes = audio_bytes
                    else:
                        st.error(f"âŒ {msg}")
                        st.session_state.customer_initial_audio_bytes = None

                # âœ… ìƒíƒœ ë³€ê²½ í›„ ì¬ì‹¤í–‰í•˜ì—¬ IN_CALL ìƒíƒœë¡œ ì „í™˜
                # ì—ì´ì „íŠ¸ê°€ ì¸ì‚¬ë§ì„ ë…¹ìŒí•  ìˆ˜ ìˆë„ë¡ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
                st.info(L["call_started_message"])

        # ì „í™” ë°œì‹  (ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘)
        with col_out:
            st.markdown(f"### {L['button_call_outbound']}")
            call_targets = [
                L["call_target_customer"],
                L["call_target_partner"]
            ]

            call_target_selection = st.radio(
                L.get("call_target_select_label", "ë°œì‹  ëŒ€ìƒ ì„ íƒ"),
                call_targets,
                key="outbound_call_target_radio",
                horizontal=True
            )

            # ì„ íƒëœ ëŒ€ìƒì— ë”°ë¼ ë²„íŠ¼ í…ìŠ¤íŠ¸ ë³€ê²½
            if call_target_selection == L["call_target_customer"]:
                button_text = L["button_call_outbound_to_customer"]
            else:
                button_text = L["button_call_outbound_to_provider"]

            if st.button(button_text, key=f"outbound_call_start_btn_{st.session_state.sim_instance_id}", type="secondary", use_container_width=True):
                # ì…ë ¥ ê²€ì¦
                if not st.session_state.call_initial_query.strip():
                    st.warning("ì „í™” ë°œì‹  ëª©í‘œ (ê³ ê° ë¬¸ì˜ ë‚´ìš©)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚")
                    # st.stop()

                # â­ ìˆ˜ì •: OpenAI ë˜ëŠ” Gemini API í‚¤ ì²´í¬
                has_openai = st.session_state.openai_client is not None
                has_gemini = bool(get_api_key("gemini"))
                
                if not st.session_state.is_llm_ready or (not has_openai and not has_gemini):
                    st.error(L["simulation_no_key_warning"] + " (OpenAI ë˜ëŠ” Gemini API Key í•„ìš”)")
                    # st.stop()

                # OUTBOUND ëª¨ë“œ ì„¤ì • ë° ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘
                st.session_state.call_sim_mode = "OUTBOUND"

                # ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ë° ì‹œì‘
                st.session_state.call_sim_stage = "IN_CALL"
                st.session_state.is_call_ended = False
                st.session_state.is_on_hold = False
                st.session_state.total_hold_duration = timedelta(0)
                st.session_state.hold_start_time = None
                st.session_state.start_time = datetime.now()  # í†µí™” ì‹œì‘ ì‹œê°„ (AHT ì‹œì‘)
                st.session_state.simulator_messages = []

                # â­ ìˆ˜ì •: ìë™ ì¸ì‚¬ë§ ìƒì„± ì œê±° - ì—ì´ì „íŠ¸ê°€ ì§ì ‘ ë…¹ìŒí•˜ë„ë¡ ë³€ê²½
                st.session_state.just_entered_call = False
                st.session_state.customer_turn_start = False

                initial_query_text = st.session_state.call_initial_query.strip()

                # ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ì—ì´ì „íŠ¸ê°€ ë¨¼ì € ë§í•´ì•¼ í•˜ë¯€ë¡œ, ê³ ê° CC í…ìŠ¤íŠ¸ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ë¡œ ì„¤ì •
                st.session_state.current_customer_audio_text = f"ğŸ“ {L['button_call_outbound']} ì„±ê³µ! {call_target_selection}ì´(ê°€) ë°›ì•˜ìŠµë‹ˆë‹¤ã€‚ ì ì‹œ í›„ ì‘ë‹µì´ ì‹œì‘ë©ë‹ˆë‹¤ã€‚ (ë¬¸ì˜ ëª©í‘œ: {initial_query_text[:50]}...)"
                st.session_state.current_agent_audio_text = ""  # Agent speaks first
                st.session_state.agent_response_input_box_widget_call = ""
                st.session_state.sim_instance_id = str(uuid.uuid4())
                st.session_state.call_summary_text = ""
                st.session_state.customer_initial_audio_bytes = None
                st.session_state.customer_history_summary = ""
                st.session_state.sim_audio_bytes = None

                st.success(f"'{call_target_selection}'ì—ê²Œ ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì¸ì‚¬ë§ì„ ë…¹ìŒí•˜ì„¸ìš”ã€‚")

        # ------------------
        # IN_CALL ìƒíƒœ (í†µí™” ì¤‘)
        # ------------------
    elif st.session_state.call_sim_stage == "IN_CALL":
        # â­ ìˆ˜ì •: ìë™ ì¸ì‚¬ë§ ìƒì„± ë¡œì§ ì œê±° - ì—ì´ì „íŠ¸ê°€ ì§ì ‘ ë…¹ìŒí•˜ë„ë¡ ë³€ê²½
        
        # ------------------------------
        # ì „í™” í†µí™” ì œëª© (í†µí™” ì¤‘ì¼ ë•Œë§Œ í‘œì‹œ)
        # ------------------------------
        # â­ ìˆ˜ì •: ì œëª©ì€ ì´ë¯¸ ìœ„ì—ì„œ í‘œì‹œë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
        # st.markdown(f"## {title}")
        # st.markdown("---")

        # ------------------------------
        # Hangup / Hold ë²„íŠ¼
        # ------------------------------
        col_hangup, col_hold = st.columns(2)

        with col_hangup:
            if st.button(L["button_hangup"], key="hangup_call_btn"):

                # Hold ì •ì‚°
                if st.session_state.is_on_hold and st.session_state.hold_start_time:
                    st.session_state.total_hold_duration += datetime.now() - st.session_state.hold_start_time

                # ìš”ì•½ ìƒì„±
                with st.spinner("AI ìš”ì•½ ìƒì„± ì¤‘..."):
                    # â­ [ìˆ˜ì • 9] í•¨ìˆ˜ëª… í†µì¼: summarize_history_for_callë¡œ ë³€ê²½ ë° í˜¸ì¶œ
                    summary = summarize_history_for_call(
                        st.session_state.simulator_messages,
                        st.session_state.call_initial_query,
                        st.session_state.language
                    )
                    st.session_state.call_summary_text = summary

                # ì¢…ë£Œ
                st.session_state.call_sim_stage = "CALL_ENDED"
                st.session_state.is_call_ended = True

                # â­ [ìˆ˜ì • 10] Hangup í›„ UI ê°±ì‹ ì„ ìœ„í•´ rerun ì¶”ê°€
                st.rerun()

        # ------------------------------
        # Hold / Resume
        # ------------------------------
        with col_hold:
            if st.session_state.is_on_hold:
                if st.button(L["button_resume"], key="resume_call_btn"):
                    # Hold ìƒíƒœ í•´ì œ ë° ì‹œê°„ ì •ì‚°
                    st.session_state.is_on_hold = False
                    if st.session_state.hold_start_time:
                        st.session_state.total_hold_duration += datetime.now() - st.session_state.hold_start_time
                        st.session_state.hold_start_time = None
            else:
                if st.button(L["button_hold"], key="hold_call_btn"):
                    st.session_state.is_on_hold = True
                    st.session_state.hold_start_time = datetime.now()

        # ------------------------------
        # Hold í‘œì‹œ
        # ------------------------------
        if st.session_state.is_on_hold:
            if st.session_state.hold_start_time:
                current_hold = datetime.now() - st.session_state.hold_start_time
            else:
                current_hold = timedelta(0)

            total_hold = st.session_state.total_hold_duration + current_hold
            hold_str = str(total_hold).split('.')[0]

            st.warning(L["hold_status"].format(duration=hold_str))
            time.sleep(1)

        # ------------------------------
        # (ì¤‘ëµ) - **ì´ê´€, íŒíŠ¸, ìš”ì•½, CC, Whisper ì „ì‚¬, ê³ ê° ë°˜ì‘ ìƒì„±**
        # ------------------------------
        def transfer_session(target_lang: str, current_messages: List[Dict[str, str]]):
            """ì–¸ì–´ ì´ê´€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  ì„¸ì…˜ ì–¸ì–´ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""

            current_lang = st.session_state.language  # í˜„ì¬ ì–¸ì–´ í™•ì¸ (Source language)
            L = LANG[current_lang]

            # API í‚¤ ì²´í¬
            if not st.session_state.is_llm_ready:
                st.error(L["simulation_no_key_warning"].replace('API Key', 'LLM API Key'))
                return

            current_lang_at_start = st.session_state.language  # Source language

            # AHT íƒ€ì´ë¨¸ ì •ì§€ (ì‹¤ì œë¡œ í†µí™”ê°€ ì¢…ë£Œë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ, AHTëŠ” ê³„ì† íë¦„)
            # st.session_state.start_time = None

            # 1. ë¡œë”© ì‹œì‘ (ì‹œê°„ ì–‘í•´ ë©”ì‹œì§€ ì‹œë®¬ë ˆì´ì…˜)
            with st.spinner(L["transfer_loading"]):
                time.sleep(np.random.uniform(5, 10))

                # 2. ëŒ€í™” ê¸°ë¡ì„ ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¡œ ê°€ê³µ
                history_text = ""
                for msg in current_messages:
                    role = "Customer" if msg["role"].startswith("customer") or msg[
                        "role"] == "initial_query" else "Agent"
                    if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                       "customer_closing_response", "phone_exchange"]:  # phone_exchange ì¶”ê°€
                        history_text += f"{role}: {msg['content']}\n"

                # â­ ìˆ˜ì •: ë¨¼ì € í•µì‹¬ í¬ì¸íŠ¸ë§Œ ìš”ì•½í•œ í›„ ë²ˆì—­
                # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
                lang_name_source = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(current_lang_at_start, "Korean")
                summary_prompt = f"""
You are an AI assistant that summarizes customer service conversations. 
Extract ONLY the key points from the conversation below. Keep it concise and focused on:
1. Customer's main inquiry/question
2. Key information provided by the agent
3. Important decisions or outcomes
4. Any unresolved issues

Write the summary in {lang_name_source}. Maximum 200 words. Be brief and to the point.

--- Conversation ---
{history_text}
---

Key Points Summary:
"""
                
                # ìš”ì•½ ìƒì„±
                summarized_text = ""
                if st.session_state.is_llm_ready:
                    try:
                        summarized_text = run_llm(summary_prompt).strip()
                    except Exception as e:
                        print(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨, ì „ì²´ ëŒ€í™” ì‚¬ìš©: {e}")
                        summarized_text = history_text  # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì „ì²´ ëŒ€í™” ì‚¬ìš©
                else:
                    summarized_text = history_text  # LLMì´ ì—†ìœ¼ë©´ ì „ì²´ ëŒ€í™” ì‚¬ìš©

                # 3. LLM ë²ˆì—­ ì‹¤í–‰ (ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­)
                translated_summary, is_success = translate_text_with_llm(summarized_text, target_lang,
                                                             current_lang_at_start)  # Use current_lang_at_start as source

                # 4. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.transfer_summary_text = translated_summary
                st.session_state.translation_success = is_success
                st.session_state.language_at_transfer = target_lang  # Save destination language
                st.session_state.language_at_transfer_start = current_lang_at_start  # Save source language for retry
                st.session_state.language = target_lang  # Language switch

                # --- ì‹œìŠ¤í…œ ì´ê´€ ë©”ì‹œì§€ ì¶”ê°€ ---
                # ì „í™”ì—ì„œëŠ” ë³„ë„ì˜ Supervisor ë©”ì‹œì§€ ì—†ì´ ë¡œê·¸ì—ë§Œ ë‚¨ê¹€
                st.session_state.simulator_messages.append(
                    {"role": "system_transfer",
                     "content": LANG[target_lang]['transfer_system_msg'].format(target_lang=target_lang)})

                st.session_state.is_solution_provided = False
                st.session_state.language_transfer_requested = False

                # ì´ê´€ í›„ ìƒíƒœ ì „í™˜: í†µí™” ì¤‘ì¸ ìƒíƒœëŠ” ìœ ì§€
                st.session_state.call_sim_stage = "IN_CALL"

                # 5. ì´ë ¥ ì €ì¥
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                save_simulation_history_local(
                    st.session_state.call_initial_query,
                    customer_type_display + f" (Transferred from {current_lang_at_start} to {target_lang})",
                    st.session_state.simulator_messages,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                    is_chat_ended=False,
                    is_call=(st.session_state.call_sim_stage == "IN_CALL")  # ì „í™” ì´ë ¥ì„ì„ í‘œì‹œ
                )

            # 6. UI ì¬ì‹¤í–‰ (ì–¸ì–´ ë³€ê²½ ì ìš©)
            st.success(f"âœ… {LANG[target_lang]['transfer_summary_header']}ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")


        st.markdown("---")
        st.markdown(f"**{L['transfer_header']}**")
        transfer_cols = st.columns(len(LANG) - 1)

        languages = list(LANG.keys())
        languages.remove(current_lang)

        # transfer_session í•¨ìˆ˜ë¥¼ ì¬ì •ì˜í•˜ì§€ ì•Šê³ , ê¸°ì¡´ì˜ transfer_session í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        for i, target_lang in enumerate(languages):
            button_label_key = f"transfer_to_{target_lang}"
            button_label = L.get(button_label_key, f"Transfer to {target_lang.capitalize()} Team")

            # â­ [ìˆ˜ì • FIX] í‚¤ ì¤‘ë³µ ì˜¤ë¥˜ í•´ê²°: ì„¸ì…˜ IDì™€ ëŒ€ìƒ ì–¸ì–´ë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
            if transfer_cols[i].button(button_label, key=f"btn_transfer_phone_{target_lang}_{st.session_state.sim_instance_id}"):
                # transfer_session í˜¸ì¶œ ì‹œ, í˜„ì¬ í†µí™” ë©”ì‹œì§€(simulator_messages)ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.
                transfer_session(target_lang, st.session_state.simulator_messages)

        # =========================
        # AI ìš”ì•½ ë²„íŠ¼ ë° í‘œì‹œ ë¡œì§ (ì¶”ê°€ëœ ê¸°ëŠ¥)
        # =========================
        st.markdown("---")
        # â­ history_expander_titleì—ì„œ ê´„í˜¸ ì•ˆ ë‚´ìš©ë§Œ ì œê±° (ì˜ˆ: (ìµœê·¼ 10ê±´))
        summary_title = L['history_expander_title'].split('(')[0].strip()
        st.markdown(f"### ğŸ“‘ {summary_title} ìš”ì•½")

        # 1. ìš”ì•½/ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ ì˜ì—­
        col_sum_btn, col_trans_btn = st.columns(2)

        with col_sum_btn:
            # â­ [ìˆ˜ì • FIX] í‚¤ ì¤‘ë³µ ì˜¤ë¥˜ í•´ê²°: ì„¸ì…˜ IDë¥¼ í‚¤ì— ì¶”ê°€
            if st.button(L["btn_request_phone_summary"], key=f"btn_request_phone_summary_{st.session_state.sim_instance_id}"):
                # ìš”ì•½ í•¨ìˆ˜ í˜¸ì¶œ
                st.session_state.customer_history_summary = summarize_history_with_ai(st.session_state.language)

        # 2. ì´ê´€ ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ (ì´ê´€ í›„ ë²ˆì—­ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš°)
        if st.session_state.language != st.session_state.language_at_transfer_start and not st.session_state.transfer_summary_text:
            with col_trans_btn:
                # â­ [ìˆ˜ì • FIX] í‚¤ ì¤‘ë³µ ì˜¤ë¥˜ í•´ê²°: ì„¸ì…˜ IDì™€ ì–¸ì–´ ì½”ë“œë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
                retry_key = f"btn_retry_translation_{st.session_state.language_at_transfer_start}_{st.session_state.language}_{st.session_state.sim_instance_id}"
                if st.button(L["button_retry_translation"], key=retry_key):
                    with st.spinner(L["transfer_loading"]):
                        # ì´ê´€ ë²ˆì—­ ë¡œì§ ì¬ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                        translated_summary, is_success = translate_text_with_llm(
                            get_chat_history_for_prompt(include_attachment=False),
                            st.session_state.language,
                            st.session_state.language_at_transfer_start
                        )
                        st.session_state.transfer_summary_text = translated_summary
                        st.session_state.translation_success = is_success

        # 3. ìš”ì•½ ë‚´ìš© í‘œì‹œ
        if st.session_state.transfer_summary_text:
            st.subheader(f"ğŸ” {L['transfer_summary_header']}")
            st.info(st.session_state.transfer_summary_text)
            # â­ ì´ê´€ ìš”ì•½ì— TTS ë²„íŠ¼ ì¶”ê°€
            render_tts_button(
                st.session_state.transfer_summary_text,
                st.session_state.language,
                role="agent",
                prefix="trans_summary_tts_call",
                index=-1  # ê³ ìœ  ì„¸ì…˜ ID ê¸°ë°˜ì˜ í‚¤ë¥¼ ìƒì„±í•˜ë„ë¡ ì§€ì‹œ
            )
        elif st.session_state.customer_history_summary:
            st.subheader("ğŸ’¡ AI ìš”ì•½")
            st.info(st.session_state.customer_history_summary)

        st.markdown("---")

        # --- ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸ ì˜ì—­ ---
        hint_cols = st.columns([4, 1])
        with hint_cols[0]:
            st.info(L["hint_placeholder"] + st.session_state.realtime_hint_text)

        with hint_cols[1]:
            # íŒíŠ¸ ìš”ì²­ ë²„íŠ¼
            if st.button(L["button_request_hint"], key=f"btn_request_hint_call_{st.session_state.sim_instance_id}"):
                with st.spinner(L["response_generating"]):
                    # ì „í™” íƒ­ì´ë¯€ë¡œ is_call=True
                    hint = generate_realtime_hint(current_lang, is_call=True)
                    st.session_state.realtime_hint_text = hint

        # =========================
        # CC ìë§‰ / ìŒì„± ì…ë ¥ ë° ì œì–´ ë¡œì§ (ê¸°ì¡´ ë¡œì§)
        # =========================================

        # --- ì‹¤ì‹œê°„ CC ìë§‰ / ì „ì‚¬ ì˜ì—­ ---
        st.subheader(L["cc_live_transcript"])

        if st.session_state.is_on_hold:
            st.text_area("Customer", value=L["customer_waiting_hold"], height=50, disabled=True, key="customer_live_cc_area")
            st.text_area("Agent", value=L["agent_hold_message"], height=50, disabled=True,
                         key="agent_live_cc_area")
        else:
            # ê³ ê° CC (LLM ìƒì„± í…ìŠ¤íŠ¸ ë˜ëŠ” ì´ˆê¸° ë¬¸ì˜)
            # â­ ìˆ˜ì •: ê³ ê° ë¬¸ì˜ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì´ˆê¸° ë¬¸ì˜ë¥¼ í‘œì‹œ
            customer_cc_text = st.session_state.current_customer_audio_text
            if not customer_cc_text and st.session_state.call_initial_query:
                customer_cc_text = st.session_state.call_initial_query
            st.text_area(
                "Customer",
                value=customer_cc_text,
                height=50,
                disabled=True,
                key="customer_live_cc_area",
            )

            # ì—ì´ì „íŠ¸ CC (ë§ˆì´í¬ ì „ì‚¬)
            st.text_area(
                "Agent",
                value=st.session_state.current_agent_audio_text,
                height=50,
                disabled=True,
                key="agent_live_cc_area",
            )

        st.markdown("---")

        # --- ì—ì´ì „íŠ¸ ìŒì„± ì…ë ¥ / ë…¹ìŒ ---
        st.subheader(L["mic_input_status"])

        # ìŒì„± ì…ë ¥: ì§§ì€ ì²­í¬ë¡œ ëŠì–´ì„œ ì „ì‚¬í•´ì•¼ ì‹¤ì‹œê°„ CC ëª¨ë°© ê°€ëŠ¥
        if st.session_state.is_on_hold:
            st.info(L["call_on_hold_message"])
            mic_audio = None
        else:
            # âœ… ë§ˆì´í¬ ìœ„ì ¯ì„ í•­ìƒ ë Œë”ë§í•˜ì—¬ í™œì„±í™” ìƒíƒœë¥¼ ìœ ì§€
            mic_audio = mic_recorder(
                start_prompt=L["agent_response_prompt"],
                stop_prompt=L["agent_response_stop_and_send"],
                just_once=True,
                format="wav",
                use_container_width=True,
                key="call_sim_mic_recorder",
            )

            # ë…¹ìŒ ì™„ë£Œ (mic_audio.get("bytes")ê°€ ì±„ì›Œì§) ì‹œ, ë°”ì´íŠ¸ë¥¼ ì €ì¥í•˜ê³  ì¬ì‹¤í–‰
            # â­ ìˆ˜ì •: ì±„íŒ…/ì´ë©”ì¼ íƒ­ê³¼ ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ ìˆ˜ì • - ì¡°ê±´ ë‹¨ìˆœí™”
            if mic_audio and mic_audio.get("bytes"):
                # â­ ìˆ˜ì •: ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ê²½ìš° ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
                if "bytes_to_process" not in st.session_state or st.session_state.bytes_to_process is None:
                    st.session_state.bytes_to_process = mic_audio["bytes"]
                    st.session_state.current_agent_audio_text = L["recording_complete_transcribing"]
                    # âœ… ì¬ì‹¤í–‰í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ì „ì‚¬ ë¡œì§ì„ ì²˜ë¦¬
                    st.rerun()

        # â­ ìˆ˜ì •: ì „ì‚¬ ë¡œì§ì„ ë§ˆì´í¬ ìœ„ì ¯ ë Œë”ë§ ë¸”ë¡ ë°–ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì‹¤í–‰ ìˆœì„œ ë³´ì¥
        # ì „ì‚¬ ë¡œì§: bytes_to_processì— ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰
        if "bytes_to_process" in st.session_state and st.session_state.bytes_to_process is not None:
            # â­ ìˆ˜ì •: OpenAI ë˜ëŠ” Gemini API í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
            has_openai = st.session_state.openai_client is not None
            has_gemini = bool(get_api_key("gemini"))
            
            if not has_openai and not has_gemini:
                st.error(L["openai_missing"] + " ë˜ëŠ” Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                st.session_state.bytes_to_process = None
                # â­ ìµœì í™”: ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ í›„ ë¶ˆí•„ìš”í•œ rerun ì œê±° (ì‚¬ìš©ìê°€ API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰ë¨)
            else:
                # â­ ì „ì‚¬ ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”
                agent_response_transcript = None

                # â­ [ìˆ˜ì •]: Whisper ì „ì‚¬ ë¡œì§ (ì±„íŒ…/ì´ë©”ì¼ íƒ­ê³¼ ë™ì¼í•œ íŒ¨í„´)
                # ì „ì‚¬ í›„ ë°”ì´íŠ¸ ë°ì´í„° ë°±ì—… (ì „ì‚¬ ì „ì— ë°±ì—…)
                audio_bytes_backup = st.session_state.bytes_to_process
                
                # ì „ì‚¬ í›„ ë°”ì´íŠ¸ ë°ì´í„° ì¦‰ì‹œ ì‚­ì œ (ì¡°ê±´ë¬¸ ì¬í‰ê°€ ë°©ì§€)
                st.session_state.bytes_to_process = None
                
                with st.spinner(L["whisper_processing"]):
                    try:
                        # 1) Whisper ì „ì‚¬ (ìë™ ì–¸ì–´ ê°ì§€ ì‚¬ìš©) - ì±„íŒ…/ì´ë©”ì¼ê³¼ ë™ì¼í•œ ë°©ì‹
                        agent_response_transcript = transcribe_bytes_with_whisper(
                            audio_bytes_backup,
                            "audio/wav",
                            lang_code=None,
                            auto_detect=True
                        )
                    except Exception as e:
                        agent_response_transcript = f"âŒ ì „ì‚¬ ì˜¤ë¥˜: {e}"

                # 2) ì „ì‚¬ ì‹¤íŒ¨ ì²˜ë¦¬ (ì±„íŒ…/ì´ë©”ì¼ê³¼ ë™ì¼í•œ íŒ¨í„´)
                if not agent_response_transcript or agent_response_transcript.startswith("âŒ"):
                    error_msg = agent_response_transcript if agent_response_transcript else L["transcription_no_result"]
                    st.error(error_msg)
                    st.session_state.current_agent_audio_text = L["transcription_error"]
                    # â­ ìµœì í™”: ì „ì‚¬ ì‹¤íŒ¨ ì‹œì—ë„ CCì— ë°˜ì˜ë˜ì§€ë§Œ ë¶ˆí•„ìš”í•œ rerun ì œê±° (Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰)
                elif not agent_response_transcript.strip(): # â­ ìˆ˜ì •: ì „ì‚¬ ê²°ê³¼ê°€ ë¹„ì–´ ìˆê±°ë‚˜ (ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°) ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ í•´ê²°
                    st.warning(L["transcription_empty_warning"])
                    st.session_state.current_agent_audio_text = ""
                    # â­ ìµœì í™”: ë¶ˆí•„ìš”í•œ rerun ì œê±°
                elif agent_response_transcript.strip():
                    # 3) ì „ì‚¬ ì„±ê³µ - CCì— ë°˜ì˜ (ì „ì‚¬ ê²°ê³¼ë¥¼ ë¨¼ì € CC ì˜ì—­ì— í‘œì‹œ)
                    agent_response_transcript = agent_response_transcript.strip()
                    st.session_state.current_agent_audio_text = agent_response_transcript
                    
                    # ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œ (ì±„íŒ…/ì´ë©”ì¼ê³¼ ìœ ì‚¬)
                    snippet = agent_response_transcript[:50].replace("\n", " ")
                    if len(agent_response_transcript) > 50:
                        snippet += "..."
                    st.success(L["whisper_success"] + f" **ì¸ì‹ ë‚´ìš©:** *{snippet}*")

                    # â­ ìˆ˜ì •: ì²« ì¸ì‚¬ë§ì¸ì§€ í™•ì¸ (simulator_messagesì— phone_exchangeê°€ ì—†ìœ¼ë©´ ì²« ì¸ì‚¬ë§)
                    is_first_greeting = not any(
                        msg.get("role") == "phone_exchange" 
                        for msg in st.session_state.simulator_messages
                    )
                    
                    # â­ ìˆ˜ì •: ì „í™” ë°œì‹  ëª¨ë“œ í™•ì¸
                    is_outbound_call = st.session_state.get("call_sim_mode", "INBOUND") == "OUTBOUND"

                    if is_first_greeting:
                        # ì²« ì¸ì‚¬ë§ì¸ ê²½ìš°: ë¡œê·¸ì— ê¸°ë¡í•˜ê³  ê³ ê° ë¬¸ì˜ ì¬ìƒ ì¤€ë¹„
                        st.session_state.simulator_messages.append(
                            {"role": "agent", "content": agent_response_transcript}
                        )
                        # ì•„ë°”íƒ€ í‘œì • ì´ˆê¸°í™”
                        st.session_state.customer_avatar["state"] = "NEUTRAL"
                        
                        # â­ ìˆ˜ì •: ì „í™” ë°œì‹  ëª¨ë“œì—ì„œ customer_initial_audio_bytesê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ê³ ê° ì‘ë‹µ ìƒì„±
                        if is_outbound_call and not st.session_state.get("customer_initial_audio_bytes"):
                            # ì „í™” ë°œì‹  ëª¨ë“œì´ê³  ê³ ê° ë¬¸ì˜ ì˜¤ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ê³ ê° ì‘ë‹µ ìƒì„±
                            st.session_state.current_agent_audio_text = agent_response_transcript
                            st.session_state.process_customer_reaction = True
                            st.session_state.pending_agent_transcript = agent_response_transcript
                            st.rerun()
                        else:
                            # â­ ìˆ˜ì •: ê³ ê° ë¬¸ì˜ë¥¼ CC ìë§‰ì— ë¯¸ë¦¬ ë°˜ì˜ (ì¬ìƒ ì „ì— ë°˜ì˜)
                            if st.session_state.call_initial_query:
                                st.session_state.current_customer_audio_text = st.session_state.call_initial_query
                            # â­ ìˆ˜ì •: ê³ ê° ë¬¸ì˜ ì¬ìƒì„ ë°”ë¡œ ì‹¤í–‰ (ê°™ì€ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ì²˜ë¦¬)
                            # ê³ ê° ë¬¸ì˜ ì¬ìƒ ë¡œì§ì´ ì•„ë˜ì— ìˆìœ¼ë¯€ë¡œ í”Œë˜ê·¸ë§Œ ì„¤ì •
                            st.session_state.customer_turn_start = True
                            # â­ ìµœì í™”: í”Œë˜ê·¸ ì„¤ì • í›„ ì¬ì‹¤í–‰í•˜ì—¬ ê³ ê° ë¬¸ì˜ ì¬ìƒ ë¡œì§ ì‹¤í–‰
                            st.rerun()
                    else:
                        # ì´í›„ ì‘ë‹µì¸ ê²½ìš°: ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ ê³ ê° ë°˜ì‘ ìƒì„±
                        # â­ ìˆ˜ì •: ì „í™” ë°œì‹  ëª¨ë“œì—ì„œë„ ê³ ê° ë°˜ì‘ì´ ìƒì„±ë˜ë„ë¡ ë³´ì¥
                        # â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­
                        # ğŸ¯ ì•„ë°”íƒ€ í‘œì • ì—…ë°ì´íŠ¸ (LLM ê¸°ë°˜ ì˜ìƒ RAG)
                        # LLMì´ ì—ì´ì „íŠ¸ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ê³ ê°ì˜ ì˜ˆìƒ ë°˜ì‘(ê°ì •)ì„ íŒë‹¨
                        # ì´ëŠ” ê³ ê°ì´ ë‹¤ìŒì— ë§í•  ë•Œ ì–´ë–¤ ë¹„ë””ì˜¤ë¥¼ ë³´ì—¬ì¤„ì§€ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
                        try:
                            # LLM ê¸°ë°˜ ë¶„ì„ (ì—ì´ì „íŠ¸ ì‘ë‹µì— ëŒ€í•œ ê³ ê°ì˜ ì˜ˆìƒ ë°˜ì‘)
                            # ì—ì´ì „íŠ¸ê°€ "í™˜ë¶ˆ"ì„ ì–¸ê¸‰í•˜ë©´ ê³ ê°ì€ ê¸°ì  ê²ƒì´ê³ ,
                            # "ê¸°ë‹¤ë ¤"ë¥¼ ìš”ì²­í•˜ë©´ ê³ ê°ì€ ì§ˆë¬¸í•  ê²ƒì´ê³ ,
                            # "ë¶ˆê°€"ë¥¼ ë§í•˜ë©´ ê³ ê°ì€ í™”ë‚  ê²ƒì…ë‹ˆë‹¤.
                            # â­ Gemini ì œì•ˆ: ì—ì´ì „íŠ¸ ë‹µë³€ê³¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ
                            analysis_result = analyze_text_for_video_selection(
                                agent_response_transcript,
                                st.session_state.language,
                                agent_last_response=agent_response_transcript,
                                conversation_context=st.session_state.simulator_messages[-5:] if st.session_state.simulator_messages else None
                            )
                            # ê³ ê°ì˜ ì˜ˆìƒ ê°ì • ìƒíƒœ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ê³ ê° ë°˜ì‘ì— ì‚¬ìš©)
                            predicted_emotion = analysis_result.get("emotion", "NEUTRAL")
                            st.session_state.customer_avatar["state"] = predicted_emotion
                        except Exception as e:
                            # LLM ë¶„ì„ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±
                            print(f"LLM ë¶„ì„ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ í´ë°±: {e}")
                            response_text = agent_response_transcript.lower()
                            if "refund" in response_text or "í™˜ë¶ˆ" in response_text:
                                st.session_state.customer_avatar["state"] = "HAPPY"
                            elif ("wait" in response_text or "ê¸°ë‹¤ë ¤" in response_text or "ì ì‹œë§Œ" in response_text):
                                st.session_state.customer_avatar["state"] = "ASKING"
                            elif ("no" in response_text or "ë¶ˆê°€" in response_text or "ì•ˆ ë©ë‹ˆë‹¤" in response_text or "cannot" in response_text):
                                st.session_state.customer_avatar["state"] = "ANGRY"
                            else:
                                st.session_state.customer_avatar["state"] = "NEUTRAL"
                        # â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­

                        # â­ ìˆ˜ì •: ì „ì‚¬ ê²°ê³¼ë¥¼ CCì— ë¨¼ì € ë°˜ì˜
                        st.session_state.current_agent_audio_text = agent_response_transcript

                        # â­ ìˆ˜ì •: ì „ì‚¬ ê²°ê³¼ê°€ CCì— ë°˜ì˜ë˜ë„ë¡ ë¨¼ì € ì¬ì‹¤í–‰
                        # ì±„íŒ…ê³¼ ë™ì¼í•˜ê²Œ ì „ì‚¬ ê²°ê³¼ë¥¼ ë¨¼ì € í™”ë©´ì— í‘œì‹œí•œ í›„ ê³ ê° ë°˜ì‘ ìƒì„±
                        # ë‹¤ìŒ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ê³ ê° ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
                        st.session_state.process_customer_reaction = True
                        st.session_state.pending_agent_transcript = agent_response_transcript
                        # â­ ìˆ˜ì •: ì „ì‚¬ ì™„ë£Œ í›„ ì¦‰ì‹œ ì¬ì‹¤í–‰í•˜ì—¬ ê³ ê° ë°˜ì‘ ìƒì„± ë‹¨ê³„ë¡œ ì§„í–‰
                        st.rerun()
                # â­ ìˆ˜ì •: else ë¸”ë¡ ì œê±° (ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨)

        # â­ ìˆ˜ì •: ì²« ì¸ì‚¬ë§ í›„ ê³ ê° ë¬¸ì˜ ì¬ìƒ ì²˜ë¦¬
        # customer_turn_start í”Œë˜ê·¸ê°€ Trueì¼ ë•Œ ê³ ê° ë¬¸ì˜ë¥¼ ì¬ìƒ
        if st.session_state.get("customer_turn_start", False) and st.session_state.customer_initial_audio_bytes:
            # â­ ìˆ˜ì •: ê³ ê° ë¬¸ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¦‰ì‹œ CC ì˜ì—­ì— ë°˜ì˜ (ì¬ìƒ ì‹œì‘ ì „, í™•ì‹¤íˆ ë°˜ì˜)
            st.session_state.current_customer_audio_text = st.session_state.call_initial_query
            
            # ê³ ê° ë¬¸ì˜ ì¬ìƒ (ë¹„ë””ì˜¤ì™€ ë™ê¸°í™”) - LLM ê¸°ë°˜ ì˜ìƒ RAG
            try:
                # ë¹„ë””ì˜¤ ë™ê¸°í™”ê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ë¹„ë””ì˜¤ì™€ í•¨ê»˜ ì¬ìƒ
                if st.session_state.is_video_sync_enabled:
                    customer_gender = st.session_state.customer_avatar.get("gender", "male")
                    # â­ LLM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ê°ì •/ì œìŠ¤ì²˜ íŒë‹¨
                    # â­ Gemini ì œì•ˆ: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
                    agent_last_msg = None
                    if st.session_state.simulator_messages:
                        for msg in reversed(st.session_state.simulator_messages):
                            if msg.get("role") == "phone_exchange" and "Agent:" in msg.get("content", ""):
                                agent_last_msg = msg.get("content", "").split("Agent:")[-1].strip()
                                break
                    
                    analysis_result = analyze_text_for_video_selection(
                        st.session_state.call_initial_query,
                        st.session_state.language,
                        agent_last_response=agent_last_msg,
                        conversation_context=st.session_state.simulator_messages[-5:] if st.session_state.simulator_messages else None
                    )
                    avatar_state = analysis_result.get("emotion", st.session_state.customer_avatar.get("state", "NEUTRAL"))
                    gesture = analysis_result.get("gesture", "NONE")
                    context_keywords = analysis_result.get("context_keywords", [])  # â­ Gemini ì œì•ˆ
                    
                    # ë¶„ì„ ê²°ê³¼ë¥¼ ì•„ë°”íƒ€ ìƒíƒœì— ë°˜ì˜
                    st.session_state.customer_avatar["state"] = avatar_state
                    
                    # â­ Gemini ì œì•ˆ: ìƒí™©ë³„ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•œ ë¹„ë””ì˜¤ ì„ íƒ
                    video_path = get_video_path_by_avatar(
                        customer_gender, 
                        avatar_state, 
                        is_speaking=True,
                        gesture=gesture,
                        context_keywords=context_keywords
                    )
                    
                    if video_path and os.path.exists(video_path):
                        with open(video_path, "rb") as f:
                            video_bytes = f.read()
                        # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ë¥¼ í•¨ê»˜ ì¬ìƒ
                        st.video(video_bytes, format="video/mp4", autoplay=True, loop=False, muted=False)
                        st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                    else:
                        # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                        st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                else:
                    # ë¹„ë””ì˜¤ ë™ê¸°í™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                    st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                
                st.success(L["customer_query_playing"])
                st.info(f"{L['query_content_label']} {st.session_state.call_initial_query}")
                
                # â­ ìˆ˜ì •: ì¬ìƒ ì™„ë£Œ ëŒ€ê¸° ë¡œì§ ì™„ì „ ì œê±°
                # ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì¬ìƒë˜ë¯€ë¡œ ì„œë²„ì—ì„œ ê¸°ë‹¤ë¦´ í•„ìš” ì—†ìŒ
                # ì¬ìƒì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì§„í–‰ë˜ë©°, CC ìë§‰ì€ ì´ë¯¸ ë°˜ì˜ë¨
                
            except Exception as e:
                st.warning(L["auto_play_failed"].format(error=str(e)))
                st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=False)
                st.info(f"{L['query_content_label']} {st.session_state.call_initial_query}")
            
            # í”Œë˜ê·¸ ì´ˆê¸°í™”
            st.session_state.customer_turn_start = False
            
            # â­ ìˆ˜ì •: ë§ì¶¤í˜• ë°˜ì‘ ìƒì„±ì„ ê°™ì€ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ì²˜ë¦¬í•˜ë˜, ì¬ìƒì€ ê³„ì† ì§„í–‰ë˜ë„ë¡ í•¨
            # ì—ì´ì „íŠ¸ì˜ ì²« ì¸ì‚¬ë§ ê°€ì ¸ì˜¤ê¸°
            agent_greeting = ""
            for msg in reversed(st.session_state.simulator_messages):
                if msg.get("role") == "agent":
                    agent_greeting = msg.get("content", "")
                    break
            
            if agent_greeting:
                # ë§ì¶¤í˜• ê³ ê° ë°˜ì‘ ìƒì„± (ì¬ìƒê³¼ ë™ì‹œì— ì§„í–‰)
                with st.spinner(L["generating_customized_response"]):
                    customer_reaction = generate_customer_reaction_for_first_greeting(
                        st.session_state.language,
                        agent_greeting,
                        st.session_state.call_initial_query
                    )
                    
                    # ê³ ê° ë°˜ì‘ì„ TTSë¡œ ì¬ìƒ ë° CCì— ë°˜ì˜ (ë¹„ë””ì˜¤ì™€ ë™ê¸°í™”) - LLM ê¸°ë°˜ ì˜ìƒ RAG
                    if not customer_reaction.startswith("âŒ"):
                        audio_bytes, msg = synthesize_tts(customer_reaction, st.session_state.language, role="customer")
                        if audio_bytes:
                            try:
                                # ë¹„ë””ì˜¤ ë™ê¸°í™”ê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ë¹„ë””ì˜¤ì™€ í•¨ê»˜ ì¬ìƒ
                                if st.session_state.is_video_sync_enabled:
                                    customer_gender = st.session_state.customer_avatar.get("gender", "male")
                                    # â­ LLM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ê°ì •/ì œìŠ¤ì²˜ íŒë‹¨
                                    # â­ Gemini ì œì•ˆ: ì—ì´ì „íŠ¸ ë‹µë³€ê³¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
                                    agent_last_msg = st.session_state.current_agent_audio_text if hasattr(st.session_state, 'current_agent_audio_text') else None
                                    analysis_result = analyze_text_for_video_selection(
                                        customer_reaction,
                                        st.session_state.language,
                                        agent_last_response=agent_last_msg,
                                        conversation_context=st.session_state.simulator_messages[-5:] if st.session_state.simulator_messages else None
                                    )
                                    avatar_state = analysis_result.get("emotion", st.session_state.customer_avatar.get("state", "NEUTRAL"))
                                    gesture = analysis_result.get("gesture", "NONE")
                                    context_keywords = analysis_result.get("context_keywords", [])  # â­ Gemini ì œì•ˆ
                                    
                                    # ë¶„ì„ ê²°ê³¼ë¥¼ ì•„ë°”íƒ€ ìƒíƒœì— ë°˜ì˜
                                    st.session_state.customer_avatar["state"] = avatar_state
                                    
                                    # â­ Gemini ì œì•ˆ: ìƒí™©ë³„ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•œ ë¹„ë””ì˜¤ ì„ íƒ
                                    video_path = get_video_path_by_avatar(
                                        customer_gender, 
                                        avatar_state, 
                                        is_speaking=True,
                                        gesture=gesture,
                                        context_keywords=context_keywords
                                    )
                                    
                                    if video_path and os.path.exists(video_path):
                                        with open(video_path, "rb") as f:
                                            video_bytes = f.read()
                                        # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ë¥¼ í•¨ê»˜ ì¬ìƒ
                                        st.video(video_bytes, format="video/mp4", autoplay=True, loop=False, muted=False)
                                        st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                                        
                                        # â­ Gemini ì œì•ˆ: ì‚¬ìš©ì í”¼ë“œë°± í‰ê°€ UI ì¶”ê°€
                                        st.markdown("---")
                                        st.markdown("**ğŸ’¬ ë¹„ë””ì˜¤ ë§¤ì¹­ í‰ê°€**")
                                        st.caption("ì´ ë¹„ë””ì˜¤ê°€ ê³ ê°ì˜ í…ìŠ¤íŠ¸ì™€ ê°ì •ì— ìì—°ìŠ¤ëŸ½ê²Œ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆê¹Œ?")
                                        
                                        feedback_key = f"video_feedback_call_{st.session_state.sim_instance_id}_{len(st.session_state.simulator_messages)}"
                                        
                                        col_rating, col_comment = st.columns([2, 3])
                                        with col_rating:
                                            rating = st.slider(
                                                "í‰ê°€ ì ìˆ˜ (1-5ì )",
                                                min_value=1,
                                                max_value=5,
                                                value=3,
                                                key=f"{feedback_key}_rating",
                                                help="1ì : ë§¤ìš° ë¶€ìì—°ìŠ¤ëŸ¬ì›€, 5ì : ë§¤ìš° ìì—°ìŠ¤ëŸ¬ì›€"
                                            )
                                        
                                        with col_comment:
                                            comment = st.text_input(
                                                "ì˜ê²¬ (ì„ íƒì‚¬í•­)",
                                                key=f"{feedback_key}_comment",
                                                placeholder="ì˜ˆ: ë¹„ë””ì˜¤ê°€ í…ìŠ¤íŠ¸ì™€ ì˜ ë§ì•˜ìŠµë‹ˆë‹¤"
                                            )
                                        
                                        if st.button("í”¼ë“œë°± ì œì¶œ", key=f"{feedback_key}_submit"):
                                            # í”¼ë“œë°±ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                                            add_video_mapping_feedback(
                                                customer_text=customer_reaction,
                                                selected_video_path=video_path,
                                                emotion=avatar_state,
                                                gesture=gesture,
                                                context_keywords=context_keywords,
                                                user_rating=rating,
                                                user_comment=comment
                                            )
                                            st.success(f"âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ì ìˆ˜: {rating}/5)")
                                            st.info("ğŸ’¡ ì´ í”¼ë“œë°±ì€ í–¥í›„ ë¹„ë””ì˜¤ ì„ íƒ ì •í™•ë„ë¥¼ ê°œì„ í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.")
                                    else:
                                        # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                                        st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                                else:
                                    # ë¹„ë””ì˜¤ ë™ê¸°í™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                                    st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                                
                                st.success(L["customer_responded"].format(reaction=customer_reaction.strip()[:50] + "..."))
                            except Exception as e:
                                st.warning(L["auto_play_failed"].format(error=str(e)))
                                st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                                st.success(L["customer_responded"].format(reaction=customer_reaction.strip()[:50] + "..."))
                        else:
                            st.error(L["customer_voice_generation_error"].format(error=msg))
                        
                        # â­ ìˆ˜ì •: ê³ ê° ë°˜ì‘ì„ CC ì˜ì—­ì— ì¶”ê°€ (ê³ ê° ë¬¸ì˜ëŠ” ìœ ì§€)
                        # ê³ ê° ë¬¸ì˜ì™€ ë°˜ì‘ì„ ëª¨ë‘ í‘œì‹œ
                        if st.session_state.current_customer_audio_text == st.session_state.call_initial_query:
                            # ê³ ê° ë¬¸ì˜ë§Œ ìˆëŠ” ê²½ìš° ë°˜ì‘ ì¶”ê°€
                            st.session_state.current_customer_audio_text = f"{st.session_state.call_initial_query}\n\nâ†’ {customer_reaction.strip()}"
                        else:
                            # ì´ë¯¸ ë°˜ì‘ì´ ìˆëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
                            st.session_state.current_customer_audio_text = customer_reaction.strip()
                        
                        # ì´ë ¥ ì €ì¥
                        log_entry = f"Agent: {agent_greeting} | Customer: {customer_reaction.strip()}"
                        st.session_state.simulator_messages.append(
                            {"role": "phone_exchange", "content": log_entry})
                    else:
                        st.error(customer_reaction)
            
            # â­ ìˆ˜ì •: rerun ì™„ì „ ì œê±° - ì¬ìƒì€ ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì§„í–‰ë˜ë¯€ë¡œ ì„œë²„ì—ì„œ ê¸°ë‹¤ë¦´ í•„ìš” ì—†ìŒ

        # â­ ìˆ˜ì •: ì „ì‚¬ í›„ ê³ ê° ë°˜ì‘ ìƒì„± ì²˜ë¦¬ (ë§ˆì´í¬ ìœ„ì ¯ ë Œë”ë§ ì´í›„ì— ìœ„ì¹˜)
        # ì „ì‚¬ ê²°ê³¼ê°€ CCì— ë¨¼ì € í‘œì‹œëœ í›„ ê³ ê° ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ ë¶„ë¦¬
        if st.session_state.get("process_customer_reaction") and st.session_state.get("pending_agent_transcript"):
            pending_transcript = st.session_state.pending_agent_transcript
            # í”Œë˜ê·¸ ì´ˆê¸°í™”
            st.session_state.process_customer_reaction = False
            del st.session_state.pending_agent_transcript

            # â­ ìˆ˜ì •: ì—ì´ì „íŠ¸ ì‘ë‹µì„ ë¨¼ì € CCì— ë°˜ì˜
            if hasattr(st.session_state, 'current_agent_audio_text'):
                st.session_state.current_agent_audio_text = pending_transcript
            else:
                st.session_state.current_agent_audio_text = pending_transcript

            # ê³ ê° ë°˜ì‘ ìƒì„±
            with st.spinner(L["generating_customer_response"]):
                customer_reaction = generate_customer_reaction_for_call(
                    st.session_state.language,
                    pending_transcript
                )

                # ê³ ê° ë°˜ì‘ì„ TTSë¡œ ì¬ìƒ ë° CCì— ë°˜ì˜ (ë¹„ë””ì˜¤ì™€ ë™ê¸°í™”) - LLM ê¸°ë°˜ ì˜ìƒ RAG
                if not customer_reaction.startswith("âŒ"):
                    audio_bytes, msg = synthesize_tts(customer_reaction, st.session_state.language, role="customer")
                    if audio_bytes:
                        # Streamlit ë¬¸ì„œ: autoplayëŠ” ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì œí•œë  ìˆ˜ ìˆìŒ
                        try:
                            # ë¹„ë””ì˜¤ ë™ê¸°í™”ê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ë¹„ë””ì˜¤ì™€ í•¨ê»˜ ì¬ìƒ
                            if st.session_state.is_video_sync_enabled:
                                customer_gender = st.session_state.customer_avatar.get("gender", "male")
                                # â­ LLM ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ê°ì •/ì œìŠ¤ì²˜ íŒë‹¨
                                # â­ Gemini ì œì•ˆ: ì—ì´ì „íŠ¸ ë‹µë³€ê³¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
                                agent_last_msg = st.session_state.current_agent_audio_text if hasattr(st.session_state, 'current_agent_audio_text') else None
                                analysis_result = analyze_text_for_video_selection(
                                    customer_reaction,
                                    st.session_state.language,
                                    agent_last_response=agent_last_msg,
                                    conversation_context=st.session_state.simulator_messages[-5:] if st.session_state.simulator_messages else None
                                )
                                avatar_state = analysis_result.get("emotion", st.session_state.customer_avatar.get("state", "NEUTRAL"))
                                gesture = analysis_result.get("gesture", "NONE")
                                context_keywords = analysis_result.get("context_keywords", [])  # â­ Gemini ì œì•ˆ
                                
                                # ë¶„ì„ ê²°ê³¼ë¥¼ ì•„ë°”íƒ€ ìƒíƒœì— ë°˜ì˜
                                st.session_state.customer_avatar["state"] = avatar_state
                                
                                # â­ Gemini ì œì•ˆ: ìƒí™©ë³„ í‚¤ì›Œë“œë¥¼ ê³ ë ¤í•œ ë¹„ë””ì˜¤ ì„ íƒ
                                video_path = get_video_path_by_avatar(
                                    customer_gender, 
                                    avatar_state, 
                                    is_speaking=True,
                                    gesture=gesture,
                                    context_keywords=context_keywords
                                )
                                
                                if video_path and os.path.exists(video_path):
                                    with open(video_path, "rb") as f:
                                        video_bytes = f.read()
                                    # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ë¥¼ í•¨ê»˜ ì¬ìƒ
                                    st.video(video_bytes, format="video/mp4", autoplay=True, loop=False, muted=False)
                                    st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                                    
                                    # â­ Gemini ì œì•ˆ: ì‚¬ìš©ì í”¼ë“œë°± í‰ê°€ UI ì¶”ê°€
                                    st.markdown("---")
                                    st.markdown("**ğŸ’¬ ë¹„ë””ì˜¤ ë§¤ì¹­ í‰ê°€**")
                                    st.caption("ì´ ë¹„ë””ì˜¤ê°€ ê³ ê°ì˜ í…ìŠ¤íŠ¸ì™€ ê°ì •ì— ìì—°ìŠ¤ëŸ½ê²Œ ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆê¹Œ?")
                                    
                                    feedback_key = f"video_feedback_{st.session_state.sim_instance_id}_{len(st.session_state.simulator_messages)}"
                                    
                                    col_rating, col_comment = st.columns([2, 3])
                                    with col_rating:
                                        rating = st.slider(
                                            "í‰ê°€ ì ìˆ˜ (1-5ì )",
                                            min_value=1,
                                            max_value=5,
                                            value=3,
                                            key=f"{feedback_key}_rating",
                                            help="1ì : ë§¤ìš° ë¶€ìì—°ìŠ¤ëŸ¬ì›€, 5ì : ë§¤ìš° ìì—°ìŠ¤ëŸ¬ì›€"
                                        )
                                    
                                    with col_comment:
                                        comment = st.text_input(
                                            "ì˜ê²¬ (ì„ íƒì‚¬í•­)",
                                            key=f"{feedback_key}_comment",
                                            placeholder="ì˜ˆ: ë¹„ë””ì˜¤ê°€ í…ìŠ¤íŠ¸ì™€ ì˜ ë§ì•˜ìŠµë‹ˆë‹¤"
                                        )
                                    
                                    if st.button("í”¼ë“œë°± ì œì¶œ", key=f"{feedback_key}_submit"):
                                        # í”¼ë“œë°±ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                                        add_video_mapping_feedback(
                                            customer_text=customer_reaction,
                                            selected_video_path=video_path,
                                            emotion=avatar_state,
                                            gesture=gesture,
                                            context_keywords=context_keywords,
                                            user_rating=rating,
                                            user_comment=comment
                                        )
                                        st.success(f"âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ì ìˆ˜: {rating}/5)")
                                        st.info("ğŸ’¡ ì´ í”¼ë“œë°±ì€ í–¥í›„ ë¹„ë””ì˜¤ ì„ íƒ ì •í™•ë„ë¥¼ ê°œì„ í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.")
                                else:
                                    # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                                    st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                            else:
                                # ë¹„ë””ì˜¤ ë™ê¸°í™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì˜¤ë””ì˜¤ë§Œ ì¬ìƒ
                                st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                            
                            st.success(L["customer_responded"].format(reaction=customer_reaction.strip()[:50] + "..."))
                            # â­ ìˆ˜ì •: ê³ ê° ë°˜ì‘ ì¬ìƒ ì‹œê°„ í™•ë³´ë¥¼ ìœ„í•´ ì§§ì€ ëŒ€ê¸°
                            time.sleep(0.5)
                        except Exception as e:
                            st.warning(L["auto_play_failed"].format(error=str(e)))
                            st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                            st.success(L["customer_responded"].format(reaction=customer_reaction.strip()[:50] + "..."))
                    else:
                        st.error(L["customer_voice_generation_error"].format(error=msg))

                    # ê³ ê° ë°˜ì‘ í…ìŠ¤íŠ¸ë¥¼ CC ì˜ì—­ì— ë°˜ì˜
                    st.session_state.current_customer_audio_text = customer_reaction.strip()
                    
                    # â­ ìˆ˜ì •: ê³ ê° ë°˜ì‘ì„ ì´ë ¥ì— ì €ì¥ (ì „í™” ë°œì‹  ëª¨ë“œì—ì„œë„ ì‘ë™)
                    agent_response_text = st.session_state.get("current_agent_audio_text", pending_transcript)
                    log_entry = f"Agent: {agent_response_text} | Customer: {customer_reaction.strip()}"
                    st.session_state.simulator_messages.append(
                        {"role": "phone_exchange", "content": log_entry}
                    )

                    # â­ ìˆ˜ì •: "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ì‘ë‹µ ì²˜ë¦¬ - ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ í›„ ì¢…ë£Œ
                    if L['customer_no_more_inquiries'] in customer_reaction:
                        # â­ ìˆ˜ì •: ì´ë ¥ ì €ì¥ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì €ì¥ ë°©ì§€
                        
                        # â­ ì¶”ê°€: ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ ë©”ì‹œì§€ ì „ì†¡
                        agent_name = st.session_state.get("agent_name", "000")
                        current_lang_call = st.session_state.get("language", "ko")
                        if current_lang_call == "ko":
                            agent_closing_msg = f"ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› {agent_name}ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì„¸ìš”."
                        elif current_lang_call == "en":
                            agent_closing_msg = f"Thank you for contacting us. This was {agent_name}. Have a great day!"
                        else:  # ja
                            agent_closing_msg = f"ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚æ‹…å½“ã¯{agent_name}ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚"
                        
                        st.session_state.simulator_messages.append(
                            {"role": "phone_exchange", "content": f"Agent: {agent_closing_msg}"}
                        )
                        
                        # í†µí™” ìš”ì•½ ìƒì„±
                        with st.spinner("AI ìš”ì•½ ìƒì„± ì¤‘..."):
                            summary = summarize_history_for_call(
                                st.session_state.simulator_messages,
                                st.session_state.call_initial_query,
                                st.session_state.language
                            )
                            st.session_state.call_summary_text = summary
                        
                        # í†µí™” ì¢…ë£Œ
                        st.session_state.call_sim_stage = "CALL_ENDED"
                        st.session_state.is_call_ended = True
                        
                        # ì—ì´ì „íŠ¸ ì…ë ¥ ì˜ì—­ ì´ˆê¸°í™”
                        st.session_state.current_agent_audio_text = ""
                        st.session_state.realtime_hint_text = ""
                        if "bytes_to_process" in st.session_state:
                            st.session_state.bytes_to_process = None
                        
                        st.success("âœ… ê³ ê°ì´ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ë‹¤ê³  í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ê°€ ê°ì‚¬ ì¸ì‚¬ë¥¼ ì „ì†¡í•œ í›„ í†µí™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
                    # â­ ì¶”ê°€: "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤" ì‘ë‹µ ì²˜ë¦¬ (í†µí™” ê³„ì†)
                    elif L['customer_has_additional_inquiries'] in customer_reaction:
                        # â­ ìˆ˜ì •: ì´ë ¥ ì €ì¥ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì €ì¥ ë°©ì§€
                        
                        # ì—ì´ì „íŠ¸ ì…ë ¥ ì˜ì—­ ì´ˆê¸°í™” (ë‹¤ìŒ ë…¹ìŒì„ ìœ„í•´)
                        st.session_state.current_agent_audio_text = ""
                        st.session_state.realtime_hint_text = ""
                        if "bytes_to_process" in st.session_state:
                            st.session_state.bytes_to_process = None
                        
                        st.info("ğŸ’¡ ê³ ê°ì´ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆë‹¤ê³  í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‘ë‹µì„ ë…¹ìŒí•˜ì„¸ìš”.")
                    else:
                        # ì¼ë°˜ ê³ ê° ë°˜ì‘ ì²˜ë¦¬
                        # â­ ìˆ˜ì •: ì´ë ¥ ì €ì¥ì€ ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì €ì¥ ë°©ì§€

                        # ì—ì´ì „íŠ¸ ì…ë ¥ ì˜ì—­ ì´ˆê¸°í™” (ë‹¤ìŒ ë…¹ìŒì„ ìœ„í•´)
                        st.session_state.current_agent_audio_text = ""
                        st.session_state.realtime_hint_text = ""
                        # â­ ìµœì í™”: bytes_to_processë„ ì´ˆê¸°í™”í•˜ì—¬ ë‹¤ìŒ ë…¹ìŒì„ ì¤€ë¹„
                        if "bytes_to_process" in st.session_state:
                            st.session_state.bytes_to_process = None

                    # â­ ìˆ˜ì •: rerun ì œê±° - ì¬ìƒì€ ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì§„í–‰ë˜ë¯€ë¡œ ì„œë²„ì—ì„œ ê¸°ë‹¤ë¦´ í•„ìš” ì—†ìŒ
                    # ì²« ë¬¸ì˜ì™€ ë™ì¼í•˜ê²Œ rerunì„ ì œê±°í•˜ì—¬ ì¬ìƒì´ ëê¹Œì§€ ì§„í–‰ë˜ë„ë¡ í•¨


    # ========================================
    # CALL_ENDED ìƒíƒœ
    # ========================================
    elif st.session_state.call_sim_stage == "CALL_ENDED":
        st.success(L["call_end_message"])

        # AHT
        if st.session_state.start_time is not None:
            final_aht_seconds = max(0, (datetime.now() - st.session_state.start_time).total_seconds())
            final_aht_str = str(timedelta(seconds=final_aht_seconds)).split('.')[0]
            st.metric("Final AHT", final_aht_str)

            hold_str = str(st.session_state.total_hold_duration).split('.')[0]
            st.metric("Total Hold Time", hold_str)
        else:
            st.warning(L["aht_not_recorded"])

        st.markdown("---")

        # â­ ì¶”ê°€: í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ (ì±„íŒ…/ì´ë©”ì¼ê³¼ ë™ì¼)
        st.markdown("**ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ**")
        download_col1, download_col2, download_col3 = st.columns(3)
        
        # í˜„ì¬ ì„¸ì…˜ì˜ ì´ë ¥ì„ ìƒì„±
        current_session_history = None
        if st.session_state.simulator_messages:
            try:
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                # ì „í™” ìš”ì•½ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒì„±
                if st.session_state.call_summary_text:
                    # call_summary_textë¥¼ summary í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    summary_data = {
                        "main_inquiry": st.session_state.call_initial_query,
                        "key_responses": [],
                        "customer_sentiment_score": 50,  # ê¸°ë³¸ê°’
                        "customer_satisfaction_score": 50,  # ê¸°ë³¸ê°’
                        "customer_characteristics": {},
                        "privacy_info": {},
                        "summary": st.session_state.call_summary_text
                    }
                else:
                    # ìš”ì•½ ìƒì„±
                    summary_data = generate_chat_summary(
                        st.session_state.simulator_messages,
                        st.session_state.call_initial_query,
                        customer_type_display,
                        st.session_state.language
                    )
                
                current_session_history = [{
                    "id": f"call_session_{st.session_state.sim_instance_id}",
                    "timestamp": datetime.now().isoformat(),
                    "initial_query": st.session_state.call_initial_query,
                    "customer_type": customer_type_display,
                    "language_key": st.session_state.language,
                    "messages": st.session_state.simulator_messages,
                    "summary": summary_data,
                    "is_chat_ended": True,
                    "attachment_context": st.session_state.get("sim_attachment_context_for_llm", ""),
                    "is_call": True
                }]
            except Exception as e:
                st.warning(f"ì´ë ¥ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤ì„ ì§ì ‘ í‘œì‹œ
        if current_session_history:
            # í˜„ì¬ ì–¸ì–´ ê°€ì ¸ì˜¤ê¸°
            current_lang = st.session_state.get("language", "ko")
            if current_lang not in ["ko", "en", "ja"]:
                current_lang = "ko"
            
            with download_col1:
                try:
                    filepath_word = export_history_to_word(current_session_history, lang=current_lang)
                    with open(filepath_word, "rb") as f:
                        st.download_button(
                            label=L.get("download_history_word", "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (Word)"),
                            data=f.read(),
                            file_name=os.path.basename(filepath_word),
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            key="download_call_word_file"
                        )
                except Exception as e:
                    st.error(f"Word ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col2:
                try:
                    filepath_pptx = export_history_to_pptx(current_session_history, lang=current_lang)
                    with open(filepath_pptx, "rb") as f:
                        st.download_button(
                            label=L.get("download_history_pptx", "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PPTX)"),
                            data=f.read(),
                            file_name=os.path.basename(filepath_pptx),
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                            key="download_call_pptx_file"
                        )
                except Exception as e:
                    st.error(f"PPTX ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col3:
                try:
                    filepath_pdf = export_history_to_pdf(current_session_history, lang=current_lang)
                    with open(filepath_pdf, "rb") as f:
                        st.download_button(
                            label=L.get("download_history_pdf", "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PDF)"),
                            data=f.read(),
                            file_name=os.path.basename(filepath_pdf),
                            mime="application/pdf",
                            key="download_call_pdf_file"
                        )
                except Exception as e:
                    st.error(f"PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        else:
            st.warning("ë‹¤ìš´ë¡œë“œí•  ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")

        st.markdown("---")

        with st.expander("í†µí™” ê¸°ë¡ ìš”ì•½"):
            st.subheader("AI í†µí™” ìš”ì•½")

            if st.session_state.call_summary_text:
                st.info(st.session_state.call_summary_text)
            else:
                st.error("âŒ í†µí™” ìš”ì•½ ìƒì„± ì‹¤íŒ¨")

            st.markdown("---")

            st.subheader("ê³ ê° ìµœì´ˆ ë¬¸ì˜ (ìŒì„±)")
            if st.session_state.customer_initial_audio_bytes:
                # Streamlit ë¬¸ì„œ: bytes ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
                try:
                    st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=False)
                except Exception as e:
                    st.error(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                st.caption(f"ì „ì‚¬: {st.session_state.call_initial_query}")
            else:
                st.info("ê³ ê° ìµœì´ˆ ìŒì„± ì—†ìŒ")

            st.markdown("---")
            st.subheader("ì „ì²´ êµí™˜ ë¡œê·¸")
            for log in st.session_state.simulator_messages:
                st.write(log["content"])

        # ìƒˆ ì‹œë®¬ë ˆì´ì…˜
        if st.button(L["new_simulation_button"]):
            st.session_state.call_sim_stage = "WAITING_CALL"
            st.session_state.call_sim_mode = "INBOUND"
            st.session_state.is_on_hold = False
            st.session_state.total_hold_duration = timedelta(0)
            st.session_state.hold_start_time = None
            st.session_state.start_time = None
            st.session_state.current_customer_audio_text = ""
            st.session_state.current_agent_audio_text = ""
            st.session_state.agent_response_input_box_widget_call = ""
            st.session_state.call_initial_query = ""
            st.session_state.call_website_url = ""  # í™ˆí˜ì´ì§€ ì£¼ì†Œ ì´ˆê¸°í™”
            st.session_state.simulator_messages = []
            st.session_state.call_summary_text = ""
            st.session_state.customer_initial_audio_bytes = None
            st.session_state.customer_history_summary = ""
            st.session_state.sim_audio_bytes = None


# -------------------- RAG Tab --------------------
elif feature_selection == L["rag_tab"]:
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    st.markdown("---")

    # â­ RAG ë°ì´í„° í•™ìŠµ ê¸°ëŠ¥ ì¶”ê°€ - AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° ë°ì´í„°ë¥¼ ì¼ì¼ íŒŒì¼ë¡œ í•™ìŠµ
    st.subheader("ğŸ“š ê³ ê° ê°€ì´ë“œ ìë™ ìƒì„± (ì¼ì¼ í•™ìŠµ)")
    
    if st.button("ì˜¤ëŠ˜ ë‚ ì§œ ê³ ê° ê°€ì´ë“œ ìƒì„±", key="generate_daily_guide"):
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: 251130_ê³ ê°ê°€ì´ë“œ.TXT)
        today_str = datetime.now().strftime("%y%m%d")
        guide_filename = f"{today_str}_ê³ ê°ê°€ì´ë“œ.TXT"
        guide_filepath = os.path.join(DATA_DIR, guide_filename)
        
        # ìµœê·¼ ì´ë ¥ ë¡œë“œ
        all_histories = load_simulation_histories_local(st.session_state.language)
        recent_histories = all_histories[:50]  # ìµœê·¼ 50ê°œ ì´ë ¥ ì‚¬ìš©
        
        if recent_histories:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ ê³ ê° ê°€ì´ë“œ ìƒì„±
            guide_prompt = f"""
ë‹¹ì‹ ì€ CS ì„¼í„° êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê³ ê° ì‘ëŒ€ ì´ë ¥ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ê³ ê° ì‘ëŒ€ ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•˜ì„¸ìš”.

ë¶„ì„í•  ì´ë ¥ ë°ì´í„°:
{json.dumps([h.get('summary', {}) for h in recent_histories if h.get('summary')], ensure_ascii=False, indent=2)}

ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•˜ì„¸ìš”:
1. ê³ ê° ìœ í˜•ë³„ ì‘ëŒ€ ì „ëµ (ì¼ë°˜/ê¹Œë‹¤ë¡œìš´/ë§¤ìš° ë¶ˆë§Œì¡±)
2. ë¬¸í™”ê¶Œë³„ ì‘ëŒ€ ê°€ì´ë“œ (ì–¸ì–´, ë¬¸í™”ì  ë°°ê²½ ê³ ë ¤)
3. ì£¼ìš” ë¬¸ì˜ ìœ í˜•ë³„ í•´ê²° ë°©ë²•
4. ê³ ê° ê°ì • ì ìˆ˜ì— ë”°ë¥¸ ì‘ëŒ€ ì „ëµ
5. ê°œì¸ì •ë³´ ì²˜ë¦¬ ê°€ì´ë“œ
6. íš¨ê³¼ì ì¸ ì†Œí†µ ìŠ¤íƒ€ì¼ ê¶Œì¥ì‚¬í•­

ê°€ì´ë“œë¼ì¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
            
            if st.session_state.is_llm_ready:
                with st.spinner("ê³ ê° ê°€ì´ë“œ ìƒì„± ì¤‘..."):
                    guide_content = run_llm(guide_prompt)
                    
                    # íŒŒì¼ ì €ì¥
                    with open(guide_filepath, "w", encoding="utf-8") as f:
                        f.write(f"ê³ ê° ì‘ëŒ€ ê°€ì´ë“œë¼ì¸\n")
                        f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ë¶„ì„ ì´ë ¥ ìˆ˜: {len(recent_histories)}\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(guide_content)
                    
                    st.success(f"âœ… ê³ ê° ê°€ì´ë“œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {guide_filename}")
                    st.info(f"íŒŒì¼ ìœ„ì¹˜: {guide_filepath}")
                    
                    # ìƒì„±ëœ íŒŒì¼ì„ ìë™ìœ¼ë¡œ RAGì— ì¶”ê°€í• ì§€ ì„ íƒ
                    if st.button("ìƒì„±ëœ ê°€ì´ë“œë¥¼ RAGì— ì¶”ê°€", key="add_guide_to_rag"):
                        # íŒŒì¼ì„ ì—…ë¡œë“œëœ íŒŒì¼ì²˜ëŸ¼ ì²˜ë¦¬í•˜ì—¬ RAGì— ì¶”ê°€
                        st.info("RAG ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
                        # ì‹¤ì œë¡œëŠ” íŒŒì¼ì„ ì½ì–´ì„œ RAG ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ëŠ” ë¡œì§ í•„ìš”
            else:
                st.error("LLMì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API Keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        else:
            st.warning("ë¶„ì„í•  ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    st.markdown("---")

    # --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
    # â­ ìˆ˜ì •ëœ ë¶€ë¶„: RAG íƒ­ ì „ìš© í‚¤ ì‚¬ìš©
    uploaded_files = st.file_uploader(
        L["file_uploader"],
        type=["pdf", "txt", "html"],
        key="rag_file_uploader", # RAG ì „ìš© í‚¤
        accept_multiple_files=True
    )

    if uploaded_files:
        if uploaded_files != st.session_state.uploaded_files_state:
            # íŒŒì¼ì´ ë³€ê²½ë˜ë©´ RAG ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.is_rag_ready = False
            st.session_state.rag_vectorstore = None
            st.session_state.uploaded_files_state = uploaded_files

        if not st.session_state.is_rag_ready:
            if st.button(L["button_start_analysis"]):
                if not st.session_state.is_llm_ready:
                    st.error(L["simulation_no_key_warning"])
                else:
                    with st.spinner(L["data_analysis_progress"]):
                        vectorstore, count = build_rag_index(uploaded_files)

                    if vectorstore:
                        st.session_state.rag_vectorstore = vectorstore
                        st.session_state.is_rag_ready = True
                        st.success(L["embed_success"].format(count=count))
                        st.session_state.rag_messages = [
                            {"role": "assistant", "content": f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}
                        ]
                    else:
                        st.error(L["embed_fail"])
                        st.session_state.is_rag_ready = False
    else:
        st.info(L["warning_no_files"])
        st.session_state.is_rag_ready = False
        st.session_state.rag_vectorstore = None
        st.session_state.rag_messages = []

    st.markdown("---")

    # --- ì±—ë´‡ ì„¹ì…˜ ---
    if st.session_state.is_rag_ready and st.session_state.rag_vectorstore:
        if "rag_messages" not in st.session_state:
            st.session_state.rag_messages = [{"role": "assistant", "content": "ë¶„ì„ëœ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}]

        for message in st.session_state.rag_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.rag_messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner(L["response_generating"]):
                    response = rag_answer(
                        prompt,
                        st.session_state.rag_vectorstore,
                        st.session_state.language
                    )
                    st.markdown(response)

            st.session_state.rag_messages.append({"role": "assistant", "content": response})
    else:
        st.warning(L["warning_rag_not_ready"])

# -------------------- Content Tab --------------------
elif feature_selection == L["content_tab"]:
    st.header(L["content_header"])
    st.markdown(L["content_desc"])
    st.markdown("---")

    if not st.session_state.is_llm_ready:
        st.warning(L["simulation_no_key_warning"])
        st.info("ğŸ’¡ API Keyë¥¼ ì„¤ì •í•˜ë©´ ì½˜í…ì¸  ìƒì„± ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # st.stop() ì œê±°: UIëŠ” í‘œì‹œí•˜ë˜ ê¸°ëŠ¥ë§Œ ë¹„í™œì„±í™”

    # ë‹¤êµ­ì–´ ë§µí•‘ ë³€ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
    level_map = {
        "ì´ˆê¸‰": "Beginner",
        "ì¤‘ê¸‰": "Intermediate",
        "ê³ ê¸‰": "Advanced",
        "Beginner": "Beginner",
        "Intermediate": "Intermediate",
        "Advanced": "Advanced",
        "åˆç´š": "Beginner",
        "ä¸­ç´š": "Intermediate",
        "ä¸Šç´š": "Advanced",
    }
    content_map = {
        "í•µì‹¬ ìš”ì•½ ë…¸íŠ¸": "summary",
        "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­": "quiz",
        "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´": "example",
        "Key Summary Note": "summary",
        "10 MCQ Questions": "quiz",
        "Practical Example Idea": "example",
        "æ ¸å¿ƒè¦ç´„ãƒãƒ¼ãƒˆ": "summary",
        "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•": "quiz",
        "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢": "example",
    }

    topic = st.text_input(L["topic_label"])
    level_display = st.selectbox(L["level_label"], L["level_options"])
    content_display = st.selectbox(L["content_type_label"], L["content_options"])

    level = level_map.get(level_display, "Beginner")
    content_type = content_map.get(content_display, "summary")

    if st.button(L["button_generate"]):
        if not topic.strip():
            st.warning(L["warning_topic"])
            # st.stop() ì œê±°: ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ê³„ì† ì§„í–‰
        elif not st.session_state.is_llm_ready:
            st.error("âŒ LLMì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API Keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            # st.stop() ì œê±°: ì—ëŸ¬ë§Œ í‘œì‹œí•˜ê³  ê³„ì† ì§„í–‰
        else:
            target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]

            # ê³µí†µ í”„ë¡¬í”„íŠ¸ ì„¤ì • (í€´ì¦ˆ í˜•ì‹ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ê¸°ë³¸ í…œí”Œë¦¿)
            system_prompt = f"""
            You are a professional AI coach. Generate learning content in {target_lang} for the topic '{topic}' at the '{level}' difficulty.
            The content format requested is: {content_display}.
            Output ONLY the raw content.
            """

            if content_type == "quiz":
                # í€´ì¦ˆ ì „ìš© í”„ë¡¬í”„íŠ¸ ë° JSON êµ¬ì¡° ê°•ì œ (ë¡œì§ ìœ ì§€)
                lang_instruction = {"ko": "í•œêµ­ì–´ë¡œ", "en": "in English", "ja": "æ—¥æœ¬èªã§"}.get(st.session_state.language, "in Korean")
                quiz_prompt = f"""
                You are an expert quiz generator. Based on the topic '{topic}' and difficulty '{level}', generate 10 multiple-choice questions.
                IMPORTANT: All questions, options, and explanations must be written {lang_instruction}.
                Your output MUST be a **raw JSON object** containing a single key "quiz_questions" which holds an array of 10 questions.
                Each object in the array must strictly follow the required keys: 
                - "question" (string): The question text in {lang_instruction}
                - "options" (array of 4 strings): Four answer choices in {lang_instruction}
                - "answer" (integer): The correct answer index starting from 1 (1-4)
                - "explanation" (string): A DETAILED and COMPREHENSIVE explanation (at least 2-3 sentences, preferably 50-100 words) explaining:
                  * Why the correct answer is right
                  * Why other options are incorrect (briefly mention key differences)
                  * Additional context or background information that helps understanding
                  * Real-world examples or applications if relevant
                  Write the explanation in {lang_instruction} with clear, educational content.
                DO NOT include any explanation, introductory text, or markdown code blocks (e.g., ```json).
                Output ONLY the raw JSON object, starting with '{{' and ending with '}}'.
                Example structure:
                {{
                  "quiz_questions": [
                    {{
                      "question": "ì§ˆë¬¸ ë‚´ìš©",
                      "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
                      "answer": 1,
                      "explanation": "ì •ë‹µì¸ ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•˜ê³ , ë‹¤ë¥¸ ì„ íƒì§€ê°€ ì™œ í‹€ë ¸ëŠ”ì§€ ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ë©°, ê´€ë ¨ ë°°ê²½ ì§€ì‹ì´ë‚˜ ì‹¤ì œ ì‚¬ë¡€ë¥¼ í¬í•¨í•œ ì¶©ë¶„íˆ ê¸´ í•´ì„¤ ë‚´ìš© (ìµœì†Œ 2-3ë¬¸ì¥, 50-100ë‹¨ì–´ ì •ë„)"
                    }}
                  ]
                }}
                """

            # JSON ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜
            def extract_json_from_text(text):
                """í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
                if not text:
                    return None
                
                text = text.strip()
                
                # 1. Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
                if "```json" in text:
                    start = text.find("```json") + 7
                    end = text.find("```", start)
                    if end != -1:
                        text = text[start:end].strip()
                elif "```" in text:
                    start = text.find("```") + 3
                    end = text.find("```", start)
                    if end != -1:
                        text = text[start:end].strip()
                
                # 2. ì²« ë²ˆì§¸ '{' ë¶€í„° ë§ˆì§€ë§‰ '}' ê¹Œì§€ ì¶”ì¶œ
                first_brace = text.find('{')
                if first_brace == -1:
                    return None
                
                # ì¤‘ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ JSON ê°ì²´ ë ì°¾ê¸°
                brace_count = 0
                last_brace = -1
                for i in range(first_brace, len(text)):
                    if text[i] == '{':
                        brace_count += 1
                    elif text[i] == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_brace = i
                            break
                
                if last_brace != -1:
                    json_str = text[first_brace:last_brace + 1]
                    return json_str.strip()
                
                return None

            generated_json_text = None
            raw_response_text = None
            llm_attempts = []

            # 1ìˆœìœ„: OpenAI (JSON modeê°€ ê°€ì¥ ì•ˆì •ì )
            if get_api_key("openai"):
                llm_attempts.append(("openai", get_api_key("openai"), "gpt-4o"))
            # 2ìˆœìœ„: Gemini (Fallback)
            if get_api_key("gemini"):
                llm_attempts.append(("gemini", get_api_key("gemini"), "gemini-2.5-flash"))

            with st.spinner(L["response_generating"]):
                for provider, api_key, model_name in llm_attempts:
                    try:
                        if provider == "openai":
                            client = OpenAI(api_key=api_key)
                            response = client.chat.completions.create(
                                model=model_name,
                                messages=[{"role": "user", "content": quiz_prompt}],
                                # JSON Mode ê°•ì œ
                                response_format={"type": "json_object"},
                            )
                            raw_response_text = response.choices[0].message.content.strip()
                            # OpenAIëŠ” JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ì§ì ‘ ì‚¬ìš© ì‹œë„
                            generated_json_text = extract_json_from_text(raw_response_text) or raw_response_text
                            break

                        elif provider == "gemini":
                            # GeminiëŠ” response_formatì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, run_llmì„ í†µí•´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ í˜¸ì¶œ
                            raw_response_text = run_llm(quiz_prompt)
                            generated_json_text = extract_json_from_text(raw_response_text)
                            
                            # JSON ì¶”ì¶œ ì„±ê³µ ì‹œ ì‹œë„ ì¢…ë£Œ
                            if generated_json_text:
                                break

                    except Exception as e:
                        print(f"JSON generation failed with {provider}: {e}")
                        continue

            # --- START: JSON Parsing and Error Handling Logic ---
            parsed_obj = None
            quiz_data = None
            
            if generated_json_text:
                try:
                    # JSON ê°ì²´ íŒŒì‹± ì‹œë„
                    parsed_obj = json.loads(generated_json_text)

                    # 'quiz_questions' í‚¤ì—ì„œ ë°°ì—´ ì¶”ì¶œ
                    quiz_data = parsed_obj.get("quiz_questions")

                    if not isinstance(quiz_data, list) or len(quiz_data) < 1:
                        raise ValueError("Missing 'quiz_questions' key or empty array.")

                    # ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬: ê° ë¬¸ì œì— í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    for i, q in enumerate(quiz_data):
                        if not isinstance(q, dict):
                            raise ValueError(f"Question {i+1} is not a valid object.")
                        if "question" not in q or "options" not in q or "answer" not in q:
                            raise ValueError(f"Question {i+1} is missing required fields (question, options, or answer).")
                        if not isinstance(q["options"], list) or len(q["options"]) != 4:
                            raise ValueError(f"Question {i+1} must have exactly 4 options.")
                        if not isinstance(q["answer"], int) or q["answer"] < 1 or q["answer"] > 4:
                            raise ValueError(f"Question {i+1} answer must be an integer between 1 and 4.")

                    # íŒŒì‹± ì„±ê³µ ë° ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ í›„ ìƒíƒœ ì €ì¥
                    st.session_state.quiz_data = quiz_data
                    st.session_state.current_question_index = 0
                    st.session_state.quiz_score = 0
                    st.session_state.quiz_answers = [1] * len(quiz_data)
                    st.session_state.show_explanation = False
                    st.session_state.is_quiz_active = True
                    st.session_state.quiz_type_key = str(uuid.uuid4())

                    st.success(f"**{topic}** - {content_display} ìƒì„± ì™„ë£Œ")

                except json.JSONDecodeError as e:
                    # JSON íŒŒì‹± ì˜¤ë¥˜
                    st.error(L["quiz_error_llm"])
                    st.caption(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    st.subheader(L["quiz_original_response"])
                    st.code(raw_response_text or generated_json_text, language="text")
                    if generated_json_text:
                        st.caption("ì¶”ì¶œëœ JSON í…ìŠ¤íŠ¸:")
                        st.code(generated_json_text, language="text")
                    
                except ValueError as e:
                    # ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜
                    st.error(L["quiz_error_llm"])
                    st.caption(f"ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: {str(e)}")
                    st.subheader(L["quiz_original_response"])
                    st.code(raw_response_text or generated_json_text, language="text")
                    if parsed_obj:
                        st.caption("íŒŒì‹±ëœ ê°ì²´:")
                        st.json(parsed_obj)
                        
            else:
                # JSON ì¶”ì¶œ ì‹¤íŒ¨
                st.error(L["quiz_error_llm"])
                st.caption("LLM ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                if raw_response_text:
                    st.subheader(L["quiz_original_response"])
                    st.text_area("", raw_response_text, height=300)
                elif generated_json_text:
                    st.subheader(L["quiz_original_response"])
                    st.text_area("", generated_json_text, height=300)
                # --- END: JSON Parsing and Error Handling Logic ---

                else:  # ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
                    st.session_state.is_quiz_active = False
                with st.spinner(L["response_generating"]):
                    content = run_llm(system_prompt)
                st.session_state.generated_content = content

                st.markdown("---")
                st.markdown(f"### {content_display}")
                st.markdown(st.session_state.generated_content)

    # --- í€´ì¦ˆ/ì¼ë°˜ ì½˜í…ì¸  ì¶œë ¥ ë¡œì§ ---
    if st.session_state.get("is_quiz_active", False) and st.session_state.get("quiz_data"):
        # í€´ì¦ˆ ì§„í–‰ ë¡œì§ (ìƒëµ - ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        quiz_data = st.session_state.quiz_data
        idx = st.session_state.current_question_index

        # â­ í€´ì¦ˆ ì™„ë£Œ ì‹œ IndexError ë°©ì§€ ë¡œì§ (idx >= len(quiz_data))
        if idx >= len(quiz_data):
            # í€´ì¦ˆ ì™„ë£Œ ì‹œ ìµœì¢… ì ìˆ˜ í‘œì‹œ
            st.success(L["quiz_complete"])
            total_questions = len(quiz_data)
            score = st.session_state.quiz_score
            incorrect_count = total_questions - score
            st.subheader(f"{L['score']}: {score} / {total_questions} ({(score / total_questions) * 100:.1f}%)")

            # ì›í˜• ì°¨íŠ¸ë¡œ ë§ì€ ë¬¸ì œ/í‹€ë¦° ë¬¸ì œ í‘œì‹œ
            if IS_PLOTLY_AVAILABLE:
                col1, col2 = st.columns([1, 2])
                with col1:
                    # ì›í˜• ì°¨íŠ¸ ìƒì„±
                    fig = go.Figure(data=[go.Pie(
                        labels=[L["correct_questions"], L["incorrect_questions"]],
                        values=[score, incorrect_count],
                        hole=0.4,
                        marker_colors=['#28a745', '#dc3545'],
                        textinfo='label+percent',
                        textposition='outside'
                    )])
                    fig.update_layout(
                        title=L["question_result"],
                        showlegend=True,
                        height=300,
                        margin=dict(l=20, r=20, t=50, b=20)
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    st.markdown("### " + L["question_result"])
                    # ë¬¸ì œë³„ ì •ì˜¤ ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
                    for i, question_item in enumerate(quiz_data):
                        user_answer = st.session_state.quiz_answers[i] if i < len(st.session_state.quiz_answers) else None
                        is_correct = user_answer == 'Correctly Scored'
                        correct_answer_idx = question_item.get('answer', 1)
                        correct_answer_text = question_item['options'][correct_answer_idx - 1] if 0 < correct_answer_idx <= len(question_item['options']) else "N/A"
                        
                        # ì‚¬ìš©ì ë‹µì•ˆ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                        if is_correct:
                            user_answer_text = correct_answer_text
                            status_icon = "âœ…"
                            status_color = "green"
                        else:
                            if isinstance(user_answer, int) and 0 < user_answer <= len(question_item['options']):
                                user_answer_text = question_item['options'][user_answer - 1]
                            else:
                                user_answer_text = "ë¯¸ì‘ë‹µ"
                            status_icon = "âŒ"
                            status_color = "red"
                        
                        # ë¬¸ì œë³„ ê²°ê³¼ í‘œì‹œ
                        with st.container():
                            st.markdown(f"""
                            <div style="border-left: 4px solid {status_color}; padding-left: 10px; margin-bottom: 15px;">
                                <strong>{status_icon} ë¬¸í•­ {i+1}:</strong> {question_item['question']}<br>
                                <span style="color: {status_color};">{L['your_answer']}: {user_answer_text}</span><br>
                                <span style="color: green;">{L['correct_answer_label']}: {correct_answer_text}</span>
                            </div>
                            """, unsafe_allow_html=True)
            else:
                # Plotlyê°€ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ë¡œë§Œ í‘œì‹œ
                st.markdown(f"**{L['correct_questions']}:** {score}ê°œ")
                st.markdown(f"**{L['incorrect_questions']}:** {incorrect_count}ê°œ")
                st.markdown("### " + L["question_result"])
                for i, question_item in enumerate(quiz_data):
                    user_answer = st.session_state.quiz_answers[i] if i < len(st.session_state.quiz_answers) else None
                    is_correct = user_answer == 'Correctly Scored'
                    correct_answer_idx = question_item.get('answer', 1)
                    correct_answer_text = question_item['options'][correct_answer_idx - 1] if 0 < correct_answer_idx <= len(question_item['options']) else "N/A"
                    
                    if is_correct:
                        user_answer_text = correct_answer_text
                        status_icon = "âœ…"
                    else:
                        if isinstance(user_answer, int) and 0 < user_answer <= len(question_item['options']):
                            user_answer_text = question_item['options'][user_answer - 1]
                        else:
                            user_answer_text = "ë¯¸ì‘ë‹µ"
                        status_icon = "âŒ"
                    
                    st.markdown(f"**{status_icon} ë¬¸í•­ {i+1}:** {question_item['question']}")
                    st.markdown(f"- {L['your_answer']}: {user_answer_text}")
                    st.markdown(f"- {L['correct_answer_label']}: {correct_answer_text}")
                    st.markdown("---")

            if st.button(L["retake_quiz"], key="retake_quiz_btn"):
                # í€´ì¦ˆ ìƒíƒœë§Œ ì´ˆê¸°í™” (í€´ì¦ˆ ë°ì´í„°ëŠ” ìœ ì§€í•˜ì—¬ ê°™ì€ í€´ì¦ˆë¥¼ ë‹¤ì‹œ í’€ ìˆ˜ ìˆë„ë¡)
                st.session_state.current_question_index = 0
                st.session_state.quiz_score = 0
                st.session_state.quiz_answers = [1] * len(quiz_data)  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
                st.session_state.show_explanation = False
                st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì²« ë²ˆì§¸ ë¬¸ì œë¡œ ì´ë™
            # st.stop() ì œê±°: í€´ì¦ˆ ì™„ë£Œ í›„ì—ë„ UIëŠ” ê³„ì† í‘œì‹œ
        else:
            # í€´ì¦ˆ ì§„í–‰ (í˜„ì¬ ë¬¸í•­)
            question_data = quiz_data[idx]
            st.subheader(f"{L.get('question_label', 'ë¬¸í•­')} {idx + 1}/{len(quiz_data)}")
            st.markdown(f"**{question_data['question']}**")

            # ê¸°ì¡´ í€´ì¦ˆ ì§„í–‰ ë° ì±„ì  ë¡œì§ (ë³€í™” ì—†ìŒ)
            current_selection_index = st.session_state.quiz_answers[idx]

            options = question_data['options']
            current_answer = st.session_state.quiz_answers[idx]

            if current_answer is None or not isinstance(current_answer, int) or current_answer <= 0:
                radio_index = 0
            else:
                radio_index = min(current_answer - 1, len(options) - 1)

            selected_option = st.radio(
                L["select_answer"],
                options,
                index=radio_index,
                key=f"quiz_radio_{st.session_state.quiz_type_key}_{idx}"
            )

            selected_option_index = options.index(selected_option) + 1 if selected_option in options else None

            check_col, next_col = st.columns([1, 1])

            if check_col.button(L["check_answer"], key=f"check_answer_btn_{idx}"):
                if selected_option_index is None:
                    st.warning("ì„ íƒì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
                else:
                    # ì ìˆ˜ ê³„ì‚° ë¡œì§
                    if st.session_state.quiz_answers[idx] != 'Correctly Scored':
                        correct_answer = question_data.get('answer')  # answer í‚¤ê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„
                        if selected_option_index == correct_answer:
                            st.session_state.quiz_score += 1
                            st.session_state.quiz_answers[idx] = 'Correctly Scored'
                            st.success(L["correct_answer"])
                        else:
                            st.session_state.quiz_answers[idx] = selected_option_index  # ì˜¤ë‹µì€ ì„ íƒì§€ ì¸ë±ìŠ¤ ì €ì¥
                            st.error(L["incorrect_answer"])

                    st.session_state.show_explanation = True

            # ì •ë‹µ ë° í•´ì„¤ í‘œì‹œ
            if st.session_state.show_explanation:
                correct_index = question_data.get('answer', 1)
                correct_answer_text = question_data['options'][correct_index - 1] if 0 < correct_index <= len(
                    question_data['options']) else "N/A"

                st.markdown("---")
                st.markdown(f"**{L['correct_is']}:** {correct_answer_text}")
                with st.expander(f"**{L['explanation']}**", expanded=True):
                    st.info(question_data.get('explanation', 'í•´ì„¤ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'))

                # ë‹¤ìŒ ë¬¸í•­ ë²„íŠ¼
                if next_col.button(L["next_question"], key=f"next_question_btn_{idx}"):
                    st.session_state.current_question_index += 1
                    st.session_state.show_explanation = False

            else:
                # ì‚¬ìš©ìê°€ ì´ë¯¸ ì •ë‹µì„ ì²´í¬í–ˆê³  (ë‹¤ì‹œ ë¡œë“œëœ ê²½ìš°), ë‹¤ìŒ ë²„íŠ¼ì„ ë°”ë¡œ í‘œì‹œ
                if st.session_state.quiz_answers[idx] == 'Correctly Scored' or (
                        isinstance(st.session_state.quiz_answers[idx], int) and st.session_state.quiz_answers[idx] > 0):
                    if next_col.button(L["next_question"], key=f"next_question_btn_after_check_{idx}"):
                        st.session_state.current_question_index += 1
                        st.session_state.show_explanation = False

    else:
        # ì¼ë°˜ ì½˜í…ì¸  (í•µì‹¬ ìš”ì•½ ë…¸íŠ¸, ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´) ì¶œë ¥
        if st.session_state.get("generated_content"):
            content = st.session_state.generated_content  # Contentë¥¼ ë‹¤ì‹œ ê°€ì ¸ì˜´
            content_lines = content.split('\n')

            st.markdown("---")
            st.markdown(f"### {content_display}")

            # --- START: íš¨ìœ¨ì„± ê°œì„  (ìƒë‹¨ ë¶„ì„/í•˜ë‹¨ ë³¸ë¬¸) ---

            st.subheader("ğŸ’¡ ì½˜í…ì¸  ë¶„ì„ (Plotly ì‹œê°í™”)")

            if IS_PLOTLY_AVAILABLE:
                # 1. í‚¤ì›Œë“œ ë¹ˆë„ ì‹œê°í™” (ëª¨ì˜ ë°ì´í„°)

                # ì½˜í…ì¸ ë¥¼ í…ìŠ¤íŠ¸ ì¤„ë¡œ ë¶„í• í•˜ì—¬ ëª¨ì˜ í‚¤ì›Œë“œ ë° ì£¼ìš” ë¬¸ì¥ ìƒì„±
                content = st.session_state.generated_content
                content_lines = content.split('\n')
                all_words = ' '.join(content_lines).replace('.', '').replace(',', '').split()

                # ëª¨ì˜ í‚¤ì›Œë“œ ë¹ˆë„ ë°ì´í„° ìƒì„±
                words = ['AI', 'ê¸°ìˆ í˜ì‹ ', 'ê³ ê°ê²½í—˜', 'ë°ì´í„°ë¶„ì„', 'íš¨ìœ¨ì„±', 'ì—¬í–‰ì‚°ì—…']
                np.random.seed(42)
                counts = np.random.randint(5, 30, size=len(words))

                # ë‚œì´ë„ì— ë”°ë¼ ì ìˆ˜ ê°€ì¤‘ì¹˜ (ëª¨ì˜ ê°ì„± ì ìˆ˜ ë³€í™”)
                difficulty_score = {'Beginner': 60, 'Intermediate': 75, 'Advanced': 90}.get(level, 70)

                # --- ì°¨íŠ¸ 1: í‚¤ì›Œë“œ ë¹ˆë„ (Plotly Bar Chart) ---
                fig_bar = go.Figure(data=[
                    go.Bar(
                        x=words,
                        y=counts,
                        marker_color=px.colors.sequential.Plotly3,
                        name="í‚¤ì›Œë“œ ë¹ˆë„"
                    )
                ])
                fig_bar.update_layout(
                    title_text=f"ì£¼ìš” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„",
                    height=300,
                    margin=dict(l=20, r=20, t=50, b=20)
                )
                st.plotly_chart(fig_bar, use_container_width=True)

                # --- ì°¨íŠ¸ 2: ì½˜í…ì¸  ê°ì„± ë° ë³µì¡ë„ ì¶”ì´ (Plotly Line Chart) ---
                # ëª¨ì˜ ê°ì„±/ë³µì¡ë„ ì ìˆ˜ ì¶”ì´ (5ê°œ ë¬¸ë‹¨ ëª¨ì˜)
                sections = ['ë„ì…ë¶€', 'í•µì‹¬1', 'í•µì‹¬2', 'í•´ê²°ì±…', 'ê²°ë¡ ']
                sentiment_scores = [difficulty_score - 10, difficulty_score + 5, difficulty_score,
                                    difficulty_score + 10, difficulty_score + 2]

                fig_line = go.Figure()
                fig_line.add_trace(go.Scatter(
                    x=sections,
                    y=sentiment_scores,
                    mode='lines+markers',
                    name='ê°ì„±/ë³µì¡ë„ ì ìˆ˜',
                    line=dict(color='orange', width=2),
                    marker=dict(size=8)
                ))
                fig_line.update_layout(
                    title_text="ì½˜í…ì¸  ì„¹ì…˜ë³„ ê°ì„± ë° ë³µì¡ë„ ì¶”ì´ (ëª¨ì˜)",
                    yaxis_range=[50, 100],
                    height=300,
                    margin=dict(l=20, r=20, t=50, b=20)
                )
                st.plotly_chart(fig_line, use_container_width=True)

            else:  # Plotlyê°€ ì—†ì„ ê²½ìš° ê¸°ì¡´ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ì˜ ìœ ì§€
                st.info("Plotly ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ì˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                all_words = ' '.join(content_lines).replace('.', '').replace(',', '').split()
                unique_words = sorted(set(all_words), key=len, reverse=True)[:5] if all_words else ["N/A"]
                key_sentences = [
                    content_lines[0].strip() if content_lines else "N/A",
                    content_lines[len(content_lines) // 2].strip() if len(content_lines) > 1 else "",
                    content_lines[-1].strip() if len(content_lines) > 1 else ""
                ]
                key_sentences = [s for s in key_sentences if s and s != "N/A"]

                col_keyword, col_sentences = st.columns([1, 1])

                with col_keyword:
                    st.markdown("**í•µì‹¬ í‚¤ì›Œë“œ/ê°œë… (ëª¨ì˜)**")
                    st.info(f"[{', '.join(unique_words)}...]")

                with col_sentences:
                    st.markdown("**ì£¼ìš” ë¬¸ì¥ ìš”ì•½ (ëª¨ì˜)**")
                    for sentence in key_sentences[:2]:
                        st.write(f"â€¢ {sentence[:50]}...")

            st.markdown("---")

            # 2. í•˜ë‹¨ ë³¸ë¬¸ ì¶œë ¥
            st.markdown(f"### ğŸ“ ì›ë³¸ ì½˜í…ì¸ ")
            st.markdown(content)

            # --- END: íš¨ìœ¨ì„± ê°œì„  ---

            # --- START: ì•„ì´ì½˜ ë²„íŠ¼ í™œì„±í™” ---
            st.markdown("---")

            # 1. ë³µì‚¬í•  ë‚´ìš© ì •ë¦¬ ë° ì´ìŠ¤ì¼€ì´í”„
            content_for_js = json.dumps(content)

            # JavaScript ì½”ë“œëŠ” ì´ìŠ¤ì¼€ì´í”„ëœ ì¤‘ê´„í˜¸ {{}}ë¥¼ ì‚¬ìš©
            js_copy_script = """
               function copyToClipboard(text) {{
                   navigator.clipboard.writeText(text).then(function() {{
                       // Streamlit toast í˜¸ì¶œ (ëª¨ì˜)
                       const elements = window.parent.document.querySelectorAll('[data-testid="stToast"]');
                       if (elements.length === 0) {{
                           // Fallback UI update (use Streamlit's native mechanism if possible, or simple alert)
                           console.log("ë³µì‚¬ ì™„ë£Œ: " + text.substring(0, 50) + "...");
                           }}
                       }}, function(err) {{
                           // Fallback: Copy via execCommand (deprecated but often works in Streamlit's iframe)
                           const textarea = document.createElement('textarea');
                           textarea.value = text;
                           document.body.appendChild(textarea);
                           textarea.select();
                           document.execCommand('copy');
                           document.body.removeChild(textarea);
                           alert("ë³µì‚¬ ì™„ë£Œ!"); 
                       }});
                   }}
                   // f-string ëŒ€ì‹  .formatì„ ì‚¬ìš©í•˜ì—¬ JavaScript ì½”ë“œì— ì£¼ì…
                   // content_for_jsëŠ” ì´ë¯¸ Pythonì—ì„œ JSON ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ì´ìŠ¤ì¼€ì´í”„ë¨
                   copyToClipboard(JSON.parse('{content_json_safe}'));
               """.format(content_json_safe=content_for_js)

            # --- JavaScript for SHARE Menu (Messenger Mock) ---
            # Streamlitì€ í˜„ì¬ ì†Œì…œ ë¯¸ë””ì–´ APIë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, URL ë³µì‚¬ë¥¼ ì‚¬ìš©í•˜ê³  UIì— ë©”ì‹œì§€ ì˜µì…˜ì„ ëª¨ì˜í•©ë‹ˆë‹¤.
            js_share_url_copy = """
               function copyShareUrl() {{
                   const url = window.location.href;
                   navigator.clipboard.writeText(url).then(function() {{
                       console.log('App URL copied');
                   }}, function(err) {{
                       // Fallback
                       const textarea = document.createElement('textarea');
                       textarea.value = url;
                       document.body.appendChild(textarea);
                       textarea.select();
                       document.execCommand('copy');
                       document.body.removeChild(textarea);
                   }});
               }}
            """

            # --- JavaScript for SHARE Menu (Messenger Mock) ---
            # Streamlitì€ í˜„ì¬ ì†Œì…œ ë¯¸ë””ì–´ APIë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, URL ë³µì‚¬ë¥¼ ì‚¬ìš©í•˜ê³  UIì— ë©”ì‹œì§€ ì˜µì…˜ì„ ëª¨ì˜í•©ë‹ˆë‹¤.
            js_native_share = """
               function triggerNativeShare(title, text, url) {{
                   if (navigator.share) {{
                       // 1. ë„¤ì´í‹°ë¸Œ ê³µìœ  API ì§€ì› ì‹œ ì‚¬ìš©
                       navigator.share({{
                           title: title,
                           text: text,
                           url: url,
                       }}).then(() => {{
                           console.log('Successful share');
                       }}).catch((error) => {{
                           console.log('Error sharing', error);
                       }});
                       return true;
                   }} else {{
                      // 2. ë„¤ì´í‹°ë¸Œ ê³µìœ  API ë¯¸ì§€ì› ì‹œ (PC í™˜ê²½ ë“±)
                      return false;
                   }}
               }}
            """


            # --- ë” ë³´ê¸° ë©”ë‰´ (íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì—´ê¸° ëª¨ì˜) ---

            def mock_download(file_type: str, file_name: str):
                """ëª¨ì˜ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥: íŒŒì¼ëª…ê³¼ í•¨ê»˜ ì„±ê³µ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
                st.toast(f"ğŸ“¥ {file_type} íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: {file_name}")
                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë¡œì§ì€ Streamlit ì»´í¬ë„ŒíŠ¸ í™˜ê²½ì—ì„œëŠ” ë³µì¡í•˜ì—¬ ìƒëµí•©ë‹ˆë‹¤.


            col_like, col_dislike, col_share, col_copy, col_more = st.columns([1, 1, 1, 1, 6])
            current_content_id = str(uuid.uuid4())  # ë™ì  ID ìƒì„±

            # 1. ì¢‹ì•„ìš” ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™”)
            if col_like.button("ğŸ‘", key=f"content_like_{current_content_id}"):
                st.toast(L["toast_like"])

            # 2. ì‹«ì–´ìš” ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™”)
            if col_dislike.button("ğŸ‘", key=f"content_dislike_{current_content_id}"):
                st.toast(L["toast_dislike"])

            # 3. ê³µìœ  ë²„íŠ¼ (Web Share API í˜¸ì¶œ í†µí•©)
            with col_share:
                share_clicked = st.button("ğŸ”—", key=f"content_share_{current_content_id}")

            if share_clicked:
                # 1ë‹¨ê³„: ë„¤ì´í‹°ë¸Œ ê³µìœ  API í˜¸ì¶œ ì‹œë„ (ëª¨ë°”ì¼ í™˜ê²½ ëŒ€ìƒ)
                share_title = f"{content_display} ({topic})"
                share_text = content[:150] + "..."
                share_url = "https://utility-convenience-salmonyeonwoo.streamlit.app/"  # ì‹¤ì œ ë°°í¬ URLë¡œ ê°€ì •

                # JavaScript ì‹¤í–‰: ë„¤ì´í‹°ë¸Œ ê³µìœ  í˜¸ì¶œ
                st.components.v1.html(
                    f"""
                    <script>{js_native_share}
                        const shared = triggerNativeShare('{share_title}', '{share_text}', '{share_url}');
                        if (shared) {{
                           // ë„¤ì´í‹°ë¸Œ ê³µìœ  ì„±ê³µ ì‹œ (í† ìŠ¤íŠ¸ ë©”ì‹œì§€ëŠ” ë¸Œë¼ìš°ì €ê°€ ê´€ë¦¬)
                            console.log("Native Share Attempted.");
                        }} else {{
                           // ë„¤ì´í‹°ë¸Œ ê³µìœ  ë¯¸ì§€ì› ì‹œ, ëŒ€ì‹  URL ë³µì‚¬
                           const url = window.location.href;
                           const textarea = document.createElement('textarea');
                           textarea.value = url;
                           document.body.appendChild(textarea);
                           textarea.select();
                           document.execCommand('copy');
                           document.body.removeChild(textarea);
                           // PC í™˜ê²½ì—ì„œ URL ë³µì‚¬ ì™„ë£Œ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶œë ¥
                           const toastElement = window.parent.document.querySelector('[data-testid="stToast"]');
                           if (toastElement) {{
                               // ì´ë¯¸ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ê°€ ì—´ë ¤ ìˆë‹¤ë©´ ê°±ì‹  (Streamlitì˜ toast ê¸°ëŠ¥ì„ ê°€ì •)
                           }} else {{
                              alert('URLì´ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.');
                           }}
                        }}
                    </script>
                    """,
                    height=0,
                )

                # Streamlitì˜ toast ë©”ì‹œì§€ëŠ” ë„¤ì´í‹°ë¸Œ ê³µìœ  ì„±ê³µ ì—¬ë¶€ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ URL ë³µì‚¬ ì™„ë£Œë¥¼ ì•Œë¦¼
                st.toast(L["toast_share"])


            # 4. ë³µì‚¬ ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™” - ì½˜í…ì¸  í…ìŠ¤íŠ¸ ë³µì‚¬)
            if col_copy.button("ğŸ“‹", key=f"content_copy_{current_content_id}"):
                # JavaScriptë¥¼ ì‹¤í–‰í•˜ì—¬ ë³µì‚¬ (execCommand ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •í™”)
                st.components.v1.html(
                    f"""<script>{js_copy_script}</script>""",
                    height=0,
                )
                st.toast(L["toast_copy"])

            # 5. ë”ë³´ê¸° ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™” - íŒŒì¼ ì˜µì…˜ ëª¨ì˜)
            with col_more:
                more_clicked = st.button("â€¢â€¢â€¢", key=f"content_more_{current_content_id}")

            if more_clicked:
                st.toast(L["toast_more"])

                # íŒŒì¼ ì˜µì…˜ ëª¨ì˜ ì¶œë ¥ (ë²„íŠ¼ ë°°ì¹˜)
                st.markdown("**ë¬¸ì„œ ì˜µì…˜ (ëª¨ì˜):**")
                col_doc1, col_doc2, col_doc3 = st.columns(3)

                # ë‹¤êµ­ì–´ ë ˆì´ë¸” ì ìš©
                if col_doc1.button(L["mock_pdf_save"], key=f"mock_pdf_save_{current_content_id}"):  # ë™ì  ID ì ìš©
                    mock_download("PDF", f"{topic}_summary.pdf")
                if col_doc2.button(L["mock_word_open"], key=f"mock_word_open_{current_content_id}"):  # ë™ì  ID ì ìš©
                    mock_download("Word", f"{topic}_summary.docx")
                if col_doc3.button(L["mock_print"], key=f"mock_print_{current_content_id}"):  # ë™ì  ID ì ìš©
                    st.toast("ğŸ–¨ ë¸Œë¼ìš°ì € ì¸ì‡„ ì°½ì´ ì—´ë¦½ë‹ˆë‹¤.")

            # --- END: íš¨ìœ¨ì„± ê°œì„  ---

            # --- END: ì•„ì´ì½˜ ë²„íŠ¼ ì¶”ê°€ ---

# -------------------- LSTM Tab --------------------
elif feature_selection == L["lstm_tab"]:
    # ... (ê¸°ì¡´ LSTM íƒ­ ë¡œì§ ìœ ì§€)
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])

    # â­ ìµœì í™”: ë²„íŠ¼ ìì²´ê°€ rerunì„ ìœ ë„í•˜ë¯€ë¡œ ëª…ì‹œì  rerun ì œê±° (ë²„íŠ¼ í´ë¦­ ì‹œ ìë™ ì¬ì‹¤í–‰)
    if st.button(L["lstm_rerun_button"]):
        # ë²„íŠ¼ í´ë¦­ ì‹œ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰
        pass

    try:
        data = load_or_train_lstm()
        predicted_score = float(np.clip(data[-1] + np.random.uniform(-3, 5), 50, 100))

        st.markdown("---")
        st.subheader(L["lstm_result_header"])

        col_score, col_chart = st.columns([1, 2])

        with col_score:
            suffix = "ì " if st.session_state.language == "ko" else ""
            st.metric(L["lstm_score_metric"], f"{predicted_score:.1f}{suffix}")
            st.info(L["lstm_score_info"].format(predicted_score=predicted_score))
        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label="Past Scores", marker="o")
            ax.plot(len(data), predicted_score, marker="*", markersize=10)
            ax.set_title(L["lstm_header"])
            ax.set_xlabel("Time (attempts)")
            ax.set_ylabel("Score (0-100)")
            ax.legend()
            st.pyplot(fig)
    except Exception as e:
        st.info(f"LSTM ê¸°ëŠ¥ ì—ëŸ¬: {e}")
