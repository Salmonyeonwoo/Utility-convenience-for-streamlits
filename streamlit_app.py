# ========================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ (ìµœì¢… Firebase ì˜êµ¬ ì €ì¥ì†Œ í†µí•© ë° ì‹œë®¬ë ˆì´í„° í™•ì¥)
# ========================================
import streamlit as st
import os
# ... (ì¤‘ëµ: í•„ìš”í•œ import êµ¬ë¬¸)
from firebase_admin import credentials, firestore, initialize_app, get_app
# Admin SDKì˜ firestoreì™€ Google Cloud SDKì˜ firestoreë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ alias ì‚¬ìš©
from google.cloud import firestore as gcp_firestore
from google.cloud.firestore import Query # Firestore ì¿¼ë¦¬ìš© import ì¶”ê°€

# ... (ì¤‘ëµ: ë‚˜ë¨¸ì§€ import êµ¬ë¬¸)


# ================================
# 1. Firebase Admin SDK ì´ˆê¸°í™” ë° Secrets ì²˜ë¦¬ í•¨ìˆ˜
# ================================

def _get_admin_credentials():
    """Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if "FIREBASE_SERVICE_ACCOUNT_JSON" not in st.secrets:
        return None, "FIREBASE_SERVICE_ACCOUNT_JSON Secretì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    service_account_data = st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"]
    sa_info = None

    if isinstance(service_account_data, str):
        try:
            sa_info = json.loads(service_account_data.strip())
        except json.JSONDecodeError as e:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ JSON êµ¬ë¬¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ê°’ì„ í™•ì¸í•˜ì„¸ìš”. ìƒì„¸ ì˜¤ë¥˜: {e}"
    elif hasattr(service_account_data, 'get'):
        try:
            sa_info = dict(service_account_data) # AttrDictë¥¼ í‘œì¤€ dictë¡œ ë³€í™˜
        except Exception:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì‹¤íŒ¨. íƒ€ì…: {type(service_account_data)}"
    else:
        return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (Type: {type(service_account_data)})"
    
    if not sa_info.get("project_id") or not sa_info.get("private_key"):
        return None, "JSON ë‚´ 'project_id' ë˜ëŠ” 'private_key' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."

    return sa_info, None

@st.cache_resource(ttl=None)
def initialize_firestore_admin():
    """Secretsì—ì„œ ë¡œë“œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    sa_info, error_message = _get_admin_credentials()

    if error_message:
        st.error(f"âŒ Firebase Secret ì˜¤ë¥˜: {error_message}")
        return None

    try:
        get_app()
    except ValueError:
        pass 
    else:
        try:
            return firestore.client()
        except Exception as e:
            st.error(f"ğŸ”¥ Firebase í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    try:
        cred = credentials.Certificate(sa_info) 
        initialize_app(cred)
        
        db_client = firestore.client()
        st.session_state["db"] = db_client
        st.success("âœ… Firebase Admin SDK ì´ˆê¸°í™” ì™„ë£Œ! (Secrets ê¸°ë°˜)")
        return db_client
    except Exception as e:
        st.error(f"ğŸ”¥ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¬¸ì œ. ì˜¤ë¥˜: {e}")
        return None


def save_index_to_firestore(db, vector_store, index_id="user_portfolio_rag"):
Â  Â  """FAISS ì¸ë±ìŠ¤ë¥¼ Firestoreì— Base64 í˜•íƒœë¡œ ì§ë ¬í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
Â  Â  if not db: return False
Â  Â  temp_dir = tempfile.mkdtemp()
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  vector_store.save_local(folder_path=temp_dir, index_name="index")
Â  Â  Â  Â Â 
Â  Â  Â  Â  with open(f"{temp_dir}/index.faiss", "rb") as f: faiss_bytes = f.read()
Â  Â  Â  Â  with open(f"{temp_dir}/index.pkl", "rb") as f: metadata_bytes = f.read()
Â  Â  Â  Â Â 
Â  Â  Â  Â  encoded_data = {
Â  Â  Â  Â  Â  Â  "faiss_data": base64.b64encode(faiss_bytes).decode('utf-8'),
Â  Â  Â  Â  Â  Â  "metadata_data": base64.b64encode(metadata_bytes).decode('utf-8'),
Â  Â  Â  Â  Â  Â  "timestamp": gcp_firestore.SERVER_TIMESTAMPÂ 
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  db.collection("rag_indices").document(index_id).set(encoded_data)
Â  Â  Â  Â  return True
Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"DB ì €ì¥ ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
Â  Â  Â  Â  print(f"Error saving index to Firestore: {e}")
Â  Â  Â  Â  return False

def load_index_from_firestore(db, embeddings, index_id="user_portfolio_rag"):
Â  Â  """Firestoreì—ì„œ Base64 ë¬¸ìì—´ì„ ë¡œë“œí•˜ì—¬ FAISS ì¸ë±ìŠ¤ë¡œ ì—­ì§ë ¬í™”í•©ë‹ˆë‹¤."""
Â  Â  if not db: return False

Â  Â  try:
Â  Â  Â  Â  doc = db.collection("rag_indices").document(index_id).get()
Â  Â  Â  Â  if not doc.exists:
Â  Â  Â  Â  Â  Â  return NoneÂ 

Â  Â  Â  Â  encoded_data = doc.to_dict()
Â  Â  Â  Â Â 
Â  Â  Â  Â  faiss_bytes = base64.b64decode(encoded_data["faiss_data"])
Â  Â  Â  Â  metadata_bytes = base64.b64decode(encoded_data["metadata_data"])
Â  Â  Â  Â Â 
Â  Â  Â  Â  temp_dir = tempfile.mkdtemp()
Â  Â  Â  Â  with open(f"{temp_dir}/index.faiss", "wb") as f: f.write(faiss_bytes)
Â  Â  Â  Â  with open(f"{temp_dir}/index.pkl", "wb") as f: f.write(metadata_bytes)
Â  Â  Â  Â Â 
Â  Â  Â  Â  vector_store = FAISS.load_local(folder_path=temp_dir, embeddings=embeddings, index_name="index")
Â  Â  Â  Â  return vector_store
Â  Â  Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"Error loading index from Firestore: {e}")
Â  Â  Â  Â  return None

# â­ ìƒë‹´ ì´ë ¥ ì €ì¥ í•¨ìˆ˜ ì¶”ê°€
def save_simulation_history(db, initial_query, customer_type, messages):
Â  Â  """Firestoreì— ìƒë‹´ ì´ë ¥ì„ ì €ì¥í•©ë‹ˆë‹¤."""
Â  Â  if not db:Â 
Â  Â  Â  Â  st.sidebar.warning("âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€")
Â  Â  Â  Â  return False
Â  Â Â 
Â  Â  # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
Â  Â  history_data = [{k: v for k, v in msg.items()} for msg in messages]

Â  Â  data = {
Â  Â  Â  Â  "initial_query": initial_query,
Â  Â  Â  Â  "customer_type": customer_type,
Â  Â  Â  Â  "messages": history_data,
Â  Â  Â  Â  "timestamp": firestore.SERVER_TIMESTAMP
Â  Â  }
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  db.collection("simulation_histories").add(data)
Â  Â  Â  Â  st.sidebar.success("âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  return True
Â  Â  except Exception as e:
Â  Â  Â  Â  st.sidebar.error(f"âŒ ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")
Â  Â  Â  Â  return False

# â­ ìƒë‹´ ì´ë ¥ ë¡œë“œ í•¨ìˆ˜ ì¶”ê°€
def load_simulation_histories(db):
Â  Â  """Firestoreì—ì„œ ìµœê·¼ ìƒë‹´ ì´ë ¥ì„ ë¡œë“œí•©ë‹ˆë‹¤ (ìµœëŒ€ 10ê°œ)."""
Â  Â  if not db: return []
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  # ìµœê·¼ 10ê°œ ì´ë ¥ì„ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì ¸ì˜´
Â  Â  Â  Â  histories = (
Â  Â  Â  Â  Â  Â  db.collection("simulation_histories")
Â  Â  Â  Â  Â  Â  .order_by("timestamp", direction=Query.DESCENDING)
Â  Â  Â  Â  Â  Â  .limit(10)
Â  Â  Â  Â  Â  Â  .stream()
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  results = []
Â  Â  Â  Â  for doc in histories:
Â  Â  Â  Â  Â  Â  data = doc.to_dict()
Â  Â  Â  Â  Â  Â  data['id'] = doc.id
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # ë©”ì‹œì§€ ë°ì´í„°ê°€ ì§ë ¬í™”ëœ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
Â  Â  Â  Â  Â  Â  if 'messages' in data and isinstance(data['messages'], list) and data['messages']:
Â  Â  Â  Â  Â  Â  Â  Â  results.append(data)

Â  Â  Â  Â  return results
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"âŒ ì´ë ¥ ë¡œë“œ ì‹¤íŒ¨: {e}")
Â  Â  Â  Â  return []

# ================================
# 2. JSON/RAG/LSTM/TTS í•¨ìˆ˜ ì •ì˜
# ================================
def clean_and_load_json(text):
Â  Â  """LLM ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë§Œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ë¡œë“œ"""
Â  Â  match = re.search(r'\{.*\}', text, re.DOTALL)
Â  Â Â 
Â  Â  if match:
Â  Â  Â  Â  json_str = match.group(0)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  return json.loads(json_str)
Â  Â  Â  Â  except json.JSONDecodeError:
Â  Â  Â  Â  Â  Â  return None
Â  Â  return None

def synthesize_and_play_audio(current_lang_key):
Â  Â  """TTS API ëŒ€ì‹  Web Speech APIë¥¼ ìœ„í•œ JS ìœ í‹¸ë¦¬í‹°ë¥¼ Streamlitì— ì‚½ì…í•©ë‹ˆë‹¤."""
Â  Â Â 
Â  Â  # í…œí”Œë¦¿ ë¦¬í„°ëŸ´ ë‚´ë¶€ì—ì„œ L ë”•ì…”ë„ˆë¦¬ë¥¼ ì§ì ‘ ì°¸ì¡°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš©
Â  Â  ko_ready = "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨"
Â  Â  en_ready = "Ready to listen"
Â  Â  ja_ready = "éŸ³å£°å†ç”Ÿã®æº–å‚™ãŒã§ãã¾ã—ãŸ"

Â  Â  # LANG ë”•ì…”ë„ˆë¦¬ê°€ í•¨ìˆ˜ ë°–ì˜ ì „ì—­ ë³€ìˆ˜ì„ì„ ê°€ì •í•˜ê³ , ì§ì ‘ ì ‘ê·¼í•˜ì§€ ì•Šê¸° ìœ„í•´ ê°’ì„ í•˜ë“œì½”ë”© ë˜ëŠ” ì¸ìˆ˜ë¡œ ì „ë‹¬
Â  Â  # ì´ í•¨ìˆ˜ëŠ” ì „ì—­ `LANG` ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ, í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë¶€ë¶„ì—ì„œ ì „ì—­ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ë³´ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
Â  Â  # Streamlit ì•±ì—ì„œ LANG ë³€ìˆ˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì ì— í•œ ë²ˆ ì •ì˜ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ í”¼í•´ì•¼ í•©ë‹ˆë‹¤.
Â  Â  # ì—¬ê¸°ì„œëŠ” L ë”•ì…”ë„ˆë¦¬ë¥¼ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²½ê³ ë¥¼ í”¼í•˜ê¸° ìœ„í•´, ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì¸ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.
Â  Â  ko_generating = "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘..."
Â  Â  ko_success = "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!"
Â  Â  ko_error = "âŒ TTS ì˜¤ë¥˜ ë°œìƒ"

Â  Â  tts_js_code = f"""
Â  Â  <script>
Â  Â  if (!window.speechSynthesis) {{
Â  Â  Â  Â  document.getElementById('tts_status').innerText = 'âŒ TTS Not Supported';
Â  Â  }}

Â  Â  window.speakText = function(text, langKey) {{
Â  Â  Â  Â  if (!window.speechSynthesis || !text) return;

Â  Â  Â  Â  const statusElement = document.getElementById('tts_status');
Â  Â  Â  Â  const utterance = new SpeechSynthesisUtterance(text);
Â  Â  Â  Â Â 
Â  Â  Â  Â  // ë™ì ìœ¼ë¡œ ì–¸ì–´ ì½”ë“œ ì„¤ì •
Â  Â  Â  Â  const langCode = {{ "ko": "ko-KR", "en": "en-US", "ja": "ja-JP" }}[langKey] || "en-US";
Â  Â  Â  Â  utterance.lang = langCode;Â 

Â  Â  Â  Â  // ë™ì ìœ¼ë¡œ ì¤€ë¹„ ìƒíƒœ ë©”ì‹œì§€ ì„¤ì • (L ë”•ì…”ë„ˆë¦¬ ê°’ì„ ì§ì ‘ ì‚¬ìš©)
Â  Â  Â  Â  const getReadyText = (key) => {{
Â  Â  Â  Â  Â  Â  if (key === 'ko') return '{ko_ready}';
Â  Â  Â  Â  Â  Â  if (key === 'en') return '{en_ready}';
Â  Â  Â  Â  Â  Â  if (key === 'ja') return '{ja_ready}';
Â  Â  Â  Â  Â  Â  return '{en_ready}';
Â  Â  Â  Â  }};

Â  Â  Â  Â  let voicesLoaded = false;
Â  Â  Â  Â  const setVoiceAndSpeak = () => {{
Â  Â  Â  Â  Â  Â  const voices = window.speechSynthesis.getVoices();
Â  Â  Â  Â  Â  Â  if (voices.length > 0) {{
Â  Â  Â  Â  Â  Â  Â  Â  // í˜„ì¬ ì–¸ì–´ ì½”ë“œì™€ ì¼ì¹˜í•˜ëŠ” ìŒì„±ì„ ì°¾ê±°ë‚˜, ì²« ë²ˆì§¸ ìŒì„±ì„ ì‚¬ìš©
Â  Â  Â  Â  Â  Â  Â  Â  utterance.voice = voices.find(v => v.lang.startsWith(langCode.substring(0, 2))) || voices[0];
Â  Â  Â  Â  Â  Â  Â  Â  voicesLoaded = true;
Â  Â  Â  Â  Â  Â  Â  Â  window.speechSynthesis.speak(utterance);
Â  Â  Â  Â  Â  Â  }} else if (!voicesLoaded) {{
Â  Â  Â  Â  Â  Â  Â  Â  // ìŒì„±ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°, ì ì‹œ í›„ ì¬ì‹œë„ (ë¹„ë™ê¸° ë¡œë“œ ë¬¸ì œ í•´ê²°)
Â  Â  Â  Â  Â  Â  Â  Â  setTimeout(setVoiceAndSpeak, 100);
Â  Â  Â  Â  Â  Â  }}
Â  Â  Â  Â  }};
Â  Â  Â  Â Â 
Â  Â  Â  Â  // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
Â  Â  Â  Â  utterance.onstart = () => {{
Â  Â  Â  Â  Â  Â  statusElement.innerText = '{ko_generating}'; // í…œí”Œë¦¿ ë¬¸ìì—´ì„ í•˜ë“œì½”ë”©ëœ ê°’ìœ¼ë¡œ ëŒ€ì²´
Â  Â  Â  Â  Â  Â  statusElement.style.backgroundColor = '#fff3e0';
Â  Â  Â  Â  }};
Â  Â  Â  Â Â 
Â  Â  Â  Â  utterance.onend = () => {{
Â  Â  Â  Â  Â  Â  statusElement.innerText = '{ko_success}'; // í…œí”Œë¦¿ ë¬¸ìì—´ì„ í•˜ë“œì½”ë”©ëœ ê°’ìœ¼ë¡œ ëŒ€ì²´
Â  Â  Â  Â  Â  Â  statusElement.style.backgroundColor = '#e8f5e9';
Â  Â  Â  Â  Â  Â  Â setTimeout(() => {{Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â statusElement.innerText = getReadyText(langKey);
Â  Â  Â  Â  Â  Â  Â  Â  Â statusElement.style.backgroundColor = '#f0f0f0';
Â  Â  Â  Â  Â  Â  Â }}, 3000);
Â  Â  Â  Â  }};
Â  Â  Â  Â Â 
Â  Â  Â  Â  utterance.onerror = (event) => {{
Â  Â  Â  Â  Â  Â  statusElement.innerText = '{ko_error}'; // í…œí”Œë¦¿ ë¬¸ìì—´ì„ í•˜ë“œì½”ë”©ëœ ê°’ìœ¼ë¡œ ëŒ€ì²´
Â  Â  Â  Â  Â  Â  statusElement.style.backgroundColor = '#ffebee';
Â  Â  Â  Â  Â  Â  console.error("SpeechSynthesis Error:", event);
Â  Â  Â  Â  Â  Â  Â setTimeout(() => {{Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â statusElement.innerText = getReadyText(langKey);
Â  Â  Â  Â  Â  Â  Â  Â  Â statusElement.style.backgroundColor = '#f0f0f0';
Â  Â  Â  Â  Â  Â  Â }}, 3999);
Â  Â  Â  Â  }};

Â  Â  Â  Â  window.speechSynthesis.cancel(); // Stop any current speech
Â  Â  Â  Â  setVoiceAndSpeak(); // ì¬ìƒ ì‹œì‘

Â  Â  }};
Â  Â  </script>
Â  Â  """
Â  Â  # JS ìœ í‹¸ë¦¬í‹°ë¥¼ Streamlit ì•±ì— ì»´í¬ë„ŒíŠ¸ë¡œ ì‚½ì… (ë†’ì´ ì¡°ì •í•˜ì—¬ ìƒíƒœì°½ë§Œ ë³´ì´ë„ë¡)
Â  Â  st.components.v1.html(tts_js_code, height=5, width=0)

def render_tts_button(text_to_speak, current_lang_key):
Â  Â  """TTS ë²„íŠ¼ UIë¥¼ ë Œë”ë§í•˜ê³  í´ë¦­ ì‹œ JS í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
Â  Â Â 
Â  Â  # ì¤„ ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
Â  Â  safe_text = text_to_speak.replace('\n', ' ').replace('"', '\\"').replace("'", "\\'")
Â  Â Â 
Â  Â  # â­ JS í•¨ìˆ˜ì— ì–¸ì–´ í‚¤ë„ í•¨ê»˜ ì „ë‹¬
Â  Â  js_call = f"window.speakText('{safe_text}', '{current_lang_key}')"

Â  Â  st.markdown(f"""
Â  Â  Â  Â  <button onclick="{js_call}"
Â  Â  Â  Â  Â  Â  Â  Â  style="background-color: #4338CA; color: white; padding: 10px 20px; border-radius: 5px; cursor: pointer; border: none; width: 100%; font-weight: bold; margin-bottom: 10px;">
Â  Â  Â  Â  Â  Â  {LANG[current_lang_key].get("button_listen_audio", "ìŒì„±ìœ¼ë¡œ ë“£ê¸°")} ğŸ§
Â  Â  Â  Â  </button>
Â  Â  """, unsafe_allow_html=True)


def get_mock_response_data(lang_key, customer_type):
Â  Â  """API Keyê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê°€ìƒ ì‘ëŒ€ ë°ì´í„° (ë‹¤êµ­ì–´ ì§€ì›)"""
Â  Â Â 
Â  Â  if lang_key == 'ko':
Â  Â  Â  Â  initial_check = "ê³ ê°ë‹˜ì˜ ì„±í•¨, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼ ë“± ì •í™•í•œ ì—°ë½ì²˜ ì •ë³´ë¥¼ í™•ì¸í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤."
Â  Â  Â  Â  tone = "ê³µê° ë° ì§„ì •"
Â  Â  Â  Â  advice = "ì´ ê³ ê°ì€ ë§¤ìš° ê¹Œë‹¤ë¡œìš´ ì„±í–¥ì´ë¯€ë¡œ, ê°ì •ì— ê³µê°í•˜ë©´ì„œë„ ì •í•´ì§„ ì •ì±… ë‚´ì—ì„œ í•´ê²°ì±…ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ì„±ê¸‰í•œ í™•ë‹µì€ í”¼í•˜ì„¸ìš”."
Â  Â  Â  Â  draft = f"""
{initial_check}

> ê³ ê°ë‹˜, ë¨¼ì € ì£¼ë¬¸í•˜ì‹  ìƒí’ˆ ë°°ì†¡ì´ ëŠ¦ì–´ì ¸ ë§ì´ ë¶ˆí¸í•˜ì…¨ì„ ì  ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.
> í˜„ì¬ ì‹œìŠ¤í…œ ìƒ í™•ì¸ëœ ë°”ë¡œëŠ” [ë°°ì†¡ ì§€ì—° ì‚¬ìœ  ì„¤ëª…].Â 
> ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €í¬ê°€ [êµ¬ì²´ì ì¸ í•´ê²°ì±… 1: ì˜ˆ: ë‹´ë‹¹ íŒ€ì— ì§ì ‘ ì—°ë½] ë° [êµ¬ì²´ì ì¸ í•´ê²°ì±… 2: ì˜ˆ: ì˜¤ëŠ˜ ì¤‘ìœ¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¬í™•ì¸]ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
> ì²˜ë¦¬ë˜ëŠ” ëŒ€ë¡œ ì˜¤ëŠ˜ ì˜¤í›„ [ì‹œê°„]ê¹Œì§€ ê³ ê°ë‹˜ê»˜ ê°œë³„ì ìœ¼ë¡œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
"""
Â  Â  elif lang_key == 'en':
Â  Â  Â  Â  initial_check = "Could you please confirm your accurate contact details, such as your full name, phone number, and email address?"
Â  Â  Â  Â  tone = "Empathy and Calming Tone"
Â  Â  Â  Â  advice = "This customer is highly dissatisfied. You must apologize sincerely, explain the status transparently, and provide concrete next steps to solve the problem within policy boundaries. Avoid making hasty promises."
Â  Â  Â  Â  draft = f"""
{initial_check}

> Dear Customer, I sincerely apologize for the inconvenience caused by the delay in delivering your order. I completely understand your frustration.
> Our system indicates [Reason for delay].Â 
> To resolve this, we will proceed with [Specific Solution 1: e.g., contacting the dedicated team immediately] and [Specific Solution 2: e.g., re-confirming the status update by end of day].
> We will contact you personally by [Time] this afternoon with an update.
"""
Â  Â  elif lang_key == 'ja':
Â  Â  Â  Â  initial_check = "ãŠå®¢æ§˜ã®æ°åã€ãŠé›»è©±ç•ªå·ã€Eãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ã€æ­£ç¢ºãªé€£çµ¡å…ˆæƒ…å ±ã‚’ç¢ºèªã•ã›ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚"
Â  Â  Â  Â  tone = "å…±æ„Ÿã¨é®é™ãƒˆãƒ¼ãƒ³"
Â  Â  Â  Â  advice = "ã“ã®ãŠå®¢æ§˜ã¯éå¸¸ã«é›£ã—ã„å‚¾å‘ã«ã‚ã‚‹ãŸã‚ã€æ„Ÿæƒ…ã«å…±æ„Ÿã—ã¤ã¤ã‚‚ã€å®šã‚ã‚‰ã‚ŒãŸãƒãƒªã‚·ãƒ¼å†…ã§è§£æ±ºç­–ã‚’æ®µéšçš„ã«æç¤ºã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å®‰æ˜“ãªç¢ºç´„ã¯é¿ã‘ã¦ãã ã•ã„ã€‚"
Â  Â  Â  Â  draft = f"""
{initial_check}

> ãŠå®¢æ§˜ã€ã”æ³¨æ–‡å•†å“ã®é…é€ãŒé…ã‚Œã¦ã—ã¾ã„ã€å¤§å¤‰ã”è¿·æƒ‘ã‚’ãŠã‹ã‘ã—ã¦ãŠã‚Šã¾ã™ã“ã¨ã‚’å¿ƒã‚ˆã‚ŠãŠè©«ã³ç”³ã—ä¸Šã’ã¾ã™ã€‚ãŠå®¢æ§˜ã®ãŠæ°—æŒã¡ã€ååˆ†ç†è§£ã—ã¦ãŠã‚Šã¾ã™ã€‚
> ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ã§ç¢ºèªã—ãŸã¨ã“ã‚ã€[é…å»¶ã®ç†ç”±ã‚’èª¬æ˜]ã€‚
> ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€å¼Šç¤¾ã«ã¦[å…·ä½“çš„ãªè§£æ±ºç­–1ï¼šä¾‹ï¼šæ‹…å½“ãƒãƒ¼ãƒ ã«ç›´æ¥é€£çµ¡]ãŠã‚ˆã³[å…·ä½“çš„ãªè§£æ±ºç­–2ï¼šä¾‹ï¼šæœ¬æ—¥ä¸­ã«å†åº¦çŠ¶æ³ã‚’ç¢ºèª]ã‚’ã„ãŸã—ã¾ã™ã€‚
> é€²æ—ãŒã‚ã‚Šæ¬¡ç¬¬ã€æœ¬æ—¥åˆå¾Œ[æ™‚é–“]ã¾ã§ã«å€‹åˆ¥ã«ã”é€£çµ¡å·®ã—ä¸Šã’ã¾ã™ã€‚
"""
Â  Â Â 
Â  Â  return {
Â  Â  Â  Â  "advice_header": f"{LANG[lang_key]['simulation_advice_header']}",
Â  Â  Â  Â  "advice": advice,
Â  Â  Â  Â  "draft_header": f"{LANG[lang_key]['simulation_draft_header']} ({tone})",
Â  Â  Â  Â  "draft": draft
Â  Â  }

def get_closing_messages(lang_key):
Â  Â  """ê³ ê° ì‘ëŒ€ ì¢…ë£Œ ì‹œ ì‚¬ìš©í•˜ëŠ” ë‹¤êµ­ì–´ ë©”ì‹œì§€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
Â  Â Â 
Â  Â  if lang_key == 'ko':
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "additional_query": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?",
Â  Â  Â  Â  Â  Â  "chat_closing": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ ì±„íŒ…ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤. ê³ ê° ë¬¸ì˜ ì„¼í„°ì— ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©°, ì¶”ê°€ë¡œ ì €í¬ ì‘ëŒ€ ì†”ë£¨ì…˜ì— ëŒ€í•œ ì„¤ë¬¸ ì¡°ì‚¬ì— ì‘í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹­ì‹œì˜¤."
Â  Â  Â  Â  }
Â  Â  elif lang_key == 'en':
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "additional_query": "Is there anything else we can assist you with today?",
Â  Â  Â  Â  Â  Â  "chat_closing": "As there are no further inquiries, we will now end this chat session. Thank you for contacting our Customer Support Center. We would be grateful if you could participate in a short survey about our service solution. Please feel free to contact us anytime if you have any additional questions."
Â  Â  Â  Â  }
Â  Â  elif lang_key == 'ja':
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "additional_query": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
Â  Â  Â  Â  Â  Â  "chat_closing": "ãŠå®¢æ§˜ã‹ã‚‰ã®è¿½åŠ ã®ãŠå•ã„åˆã‚ã›ãŒãªã„ãŸã‚ã€æœ¬ãƒãƒ£ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆã‚’çµ‚äº†ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚å¼Šç¤¾ã®å¯¾å¿œã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹ç°¡å˜ãªã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«ã”å”åŠ›ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚è¿½åŠ ã®ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã„ã¤ã§ã‚‚ã”é€£çµ¡ãã ã•ã„ã€‚"
Â  Â  Â  Â  }
Â  Â  return get_closing_messages('ko') # ê¸°ë³¸ê°’


def get_document_chunks(files):
Â  Â  """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
Â  Â  documents = []
Â  Â  temp_dir = tempfile.mkdtemp()
Â  Â  for uploaded_file in files:
Â  Â  Â  Â  temp_filepath = os.path.join(temp_dir, uploaded_file.name)
Â  Â  Â  Â  file_extension = uploaded_file.name.split('.')[-1].lower()
Â  Â  Â  Â  if file_extension == "pdf":
Â  Â  Â  Â  Â  Â  with open(temp_filepath, "wb") as f: f.write(uploaded_file.getvalue())
Â  Â  Â  Â  Â  Â  loader = PyPDFLoader(temp_filepath)
Â  Â  Â  Â  Â  Â  documents.extend(loader.load())
Â  Â  Â  Â  elif file_extension == "html":
Â  Â  Â  Â  Â  Â  raw_html = uploaded_file.getvalue().decode('utf-8')
Â  Â  Â  Â  Â  Â  soup = BeautifulSoup(raw_html, 'html.parser')
Â  Â  Â  Â  Â  Â  text_content = soup.get_text(separator=' ', strip=True)
Â  Â  Â  Â  Â  Â  documents.append(Document(page_content=text_content, metadata={"source": uploaded_file.name}))
Â  Â  Â  Â  elif file_extension == "txt":
Â  Â  Â  Â  Â  Â  with open(temp_filepath, "wb") as f: f.write(uploaded_file.getvalue())
Â  Â  Â  Â  Â  Â  loader = TextLoader(temp_filepath, encoding="utf-8")
Â  Â  Â  Â  Â  Â  documents.extend(loader.load())
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print(f"File '{uploaded_file.name}' not supported.")
Â  Â  Â  Â  Â  Â  continue
Â  Â  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
Â  Â  return text_splitter.split_documents(documents)

def get_vector_store(text_chunks):
Â  Â  """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  Vector Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
Â  Â  cache_key = tuple(doc.page_content for doc in text_chunks)
Â  Â  if cache_key in st.session_state.embedding_cache: return st.session_state.embedding_cache[cache_key]
Â  Â  if not st.session_state.is_llm_ready: return None
Â  Â  try:
Â  Â  Â  Â  vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
Â  Â  Â  Â  st.session_state.embedding_cache[cache_key] = vector_store
Â  Â  Â  Â  return vector_store
Â  Â  except Exception as e:
Â  Â  Â  Â  if "429" in str(e): return None
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  print(f"Vector Store creation failed: {e}")Â 
Â  Â  Â  Â  Â  Â  return None

def get_rag_chain(vector_store):
Â  Â  """ê²€ìƒ‰ ì²´ì¸(ConversationalRetrievalChain)ì„ ìƒì„±í•©ë‹ˆë‹¤."""
Â  Â  if vector_store is None: return None
Â  Â  # â­ RAG ì²´ì¸ì— memory_keyë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
Â  Â  return ConversationalRetrievalChain.from_llm(
Â  Â  Â  Â  llm=st.session_state.llm,
Â  Â  Â  Â  retriever=vector_store.as_retriever(),
Â  Â  Â  Â  memory=st.session_state.memory
Â  Â  )

@st.cache_resource
def load_or_train_lstm():
Â  Â  """ê°€ìƒì˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ì„ ìœ„í•œ LSTM ëª¨ë¸ì„ ìƒì„±í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤."""
Â  Â  np.random.seed(42)
Â  Â  data = np.cumsum(np.random.normal(loc=5, scale=5, size=50)) + 60
Â  Â  data = np.clip(data, 50, 95)
Â  Â  def create_dataset(dataset, look_back=3):
Â  Â  Â  Â  X, Y = [], []
Â  Â  Â  Â  for i in range(len(dataset) - look_back):
Â  Â  Â  Â  Â  Â  X.append(dataset[i:(i + look_back)])
Â  Â  Â  Â  Â  Â  Y.append(dataset[i + look_back])
Â  Â  Â  Â  return np.array(X), np.array(Y)
Â  Â  look_back = 5
Â  Â  X, Y = create_dataset(data, look_back)
Â  Â  X = np.reshape(X, (X.shape[0], X.shape[1], 1))
Â  Â  model = Sequential([
Â  Â  Â  Â  LSTM(50, activation='relu', input_shape=(look_back, 1)),
Â  Â  Â  Â  Dense(1)
Â  Â  ])
Â  Â  model.compile(optimizer='adam', loss='mse')
Â  Â  model.fit(X, Y, epochs=10, batch_size=1, verbose=0)
Â  Â  return model, data


def render_interactive_quiz(quiz_data, current_lang):
Â  Â  """ìƒì„±ëœ í€´ì¦ˆ ë°ì´í„°ë¥¼ Streamlit UIë¡œ ë Œë”ë§í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤."""
Â  Â  L = LANG[current_lang]
Â  Â  if not quiz_data or 'quiz_questions' not in quiz_data: return

Â  Â  questions = quiz_data['quiz_questions']
Â  Â  num_questions = len(questions)

Â  Â  if "current_question" not in st.session_state or st.session_state.current_question >= num_questions:
Â  Â  Â  Â  st.session_state.current_question = 0
Â  Â  Â  Â  st.session_state.quiz_results = [None] * num_questions
Â  Â  Â  Â  st.session_state.quiz_submitted = False
Â  Â  Â  Â Â 
Â  Â  q_index = st.session_state.current_question
Â  Â  q_data = questions[q_index]
Â  Â Â 
Â  Â  st.subheader(f"{q_index + 1}. {q_data['question']}")
Â  Â Â 
Â  Â  options_dict = {}
Â  Â  try:
Â  Â  Â  Â  options_dict = {f"{opt['option']}": f"{opt['option']}) {opt['text']}" for opt in q_data['options']}
Â  Â  except KeyError:
Â  Â  Â  Â  st.error(L["quiz_fail_structure"])
Â  Â  Â  Â  if 'quiz_data_raw' in st.session_state: st.code(st.session_state.quiz_data_raw, language="json")
Â  Â  Â  Â  return

Â  Â  options_list = list(options_dict.values())
Â  Â Â 
Â  Â  selected_answer = st.radio(
Â  Â  Â  Â  L.get("select_answer", "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”"),
Â  Â  Â  Â  options=options_list,
Â  Â  Â  Â  key=f"q_radio_{q_index}"
Â  Â  )

Â  Â  col1, col2 = st.columns(2)

Â  Â  if col1.button(L.get("check_answer", "ì •ë‹µ í™•ì¸"), key=f"check_btn_{q_index}", disabled=st.session_state.quiz_submitted):
Â  Â  Â  Â  user_choice_letter = selected_answer.split(')')[0] if selected_answer else None
Â  Â  Â  Â  correct_answer_letter = q_data['correct_answer']

Â  Â  Â  Â  is_correct = (user_choice_letter == correct_answer_letter)
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.session_state.quiz_results[q_index] = is_correct
Â  Â  Â  Â  st.session_state.quiz_submitted = True
Â  Â  Â  Â Â 
Â  Â  Â  Â  if is_correct:
Â  Â  Â  Â  Â  Â  st.success(L.get("correct_answer", "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰"))
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.error(L.get("incorrect_answer", "ì˜¤ë‹µì…ë‹ˆë‹¤.ğŸ˜"))
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.markdown(f"**{L.get('correct_is', 'ì •ë‹µ')}: {correct_answer_letter}**")
Â  Â  Â  Â  st.info(f"**{L.get('explanation', 'í•´ì„¤')}:** {q_data['explanation']}")

Â  Â  if st.session_state.quiz_submitted:
Â  Â  Â  Â  if q_index < num_questions - 1:
Â  Â  Â  Â  Â  Â  if col2.button(L.get("next_question", "ë‹¤ìŒ ë¬¸í•­"), key=f"next_btn_{q_index}"):
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_question += 1
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.quiz_submitted = False
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  total_correct = st.session_state.quiz_results.count(True)
Â  Â  Â  Â  Â  Â  total_questions = len(st.session_state.quiz_results)
Â  Â  Â  Â  Â  Â  st.success(f"**{L.get('quiz_complete', 'í€´ì¦ˆ ì™„ë£Œ!')}** {L.get('score', 'ì ìˆ˜')}: {total_correct}/{total_questions}")
Â  Â  Â  Â  Â  Â  if st.button(L.get("retake_quiz", "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°"), key="retake"):
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_question = 0
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.quiz_results = [None] * num_questions
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.quiz_submitted = False
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

# ================================
# 3. ë‹¤êµ­ì–´ ì§€ì› ë”•ì…”ë„ˆë¦¬ (Language Dictionary)
# ================================
LANG = {
Â  Â  "ko": {
Â  Â  Â  Â  "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
Â  Â  Â  Â  "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
Â  Â  Â  Â  "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
Â  Â  Â  Â  "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
Â  Â  Â  Â  "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
Â  Â  Â  Â  "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",Â 
Â  Â  Â  Â  "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
Â  Â  Â  Â  "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
Â  Â  Â  Â  "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "topic_label": "í•™ìŠµ ì£¼ì œ",
Â  Â  Â  Â  "level_label": "ë‚œì´ë„",
Â  Â  Â  Â  "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
Â  Â  Â  Â  "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
Â  Â  Â  Â  "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
Â  Â  Â  Â  "button_generate": "ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
Â  Â  Â  Â  "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "lstm_disabled_error": "The LSTM feature is temporarily disabled due to build environment issues. Please use the 'Custom Content Generation' feature first.",
Â  Â  Â  Â  "lang_select": "ì–¸ì–´ ì„ íƒ",
Â  Â  Â  Â  "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
Â  Â  Â  Â  "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
Â  Â  Â  Â  "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
Â  Â  Â  Â  "check_answer": "ì •ë‹µ í™•ì¸",
Â  Â  Â  Â  "next_question": "ë‹¤ìŒ ë¬¸í•­",
Â  Â  Â  Â  "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
Â  Â  Â  Â  "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤. ğŸ˜",
Â  Â  Â  Â  "correct_is": "ì •ë‹µ",
Â  Â  Â  Â  "explanation": "í•´ì„¤",
Â  Â  Â  Â  "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
Â  Â  Â  Â  "score": "ì ìˆ˜",
Â  Â  Â  Â  "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
Â  Â  Â  Â  "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ ì›ë³¸ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
Â  Â  Â  Â  "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
Â  Â  Â  Â Â 
Â  Â  Â  Â  # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
Â  Â  Â  Â  "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
Â  Â  Â  Â  "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— ëŒ€í•´ AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
Â  Â  Â  Â  "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
Â  Â  Â  Â  "customer_type_label": "ê³ ê° ì„±í–¥",
Â  Â  Â  Â  "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
Â  Â  Â  Â  "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
Â  Â  Â  Â  "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ëŠ” ê²½ìš°, ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (UI êµ¬ì„±ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.)",
Â  Â  Â  Â  "simulation_advice_ready": "AIì˜ ì‘ëŒ€ ì¡°ì–¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!",
Â  Â  Â  Â  "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
Â  Â  Â  Â  "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
Â  Â  Â  Â  "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
Â  Â  Â  Â  "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
Â  Â  Â  Â  "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
Â  Â  Â  Â  "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!",
Â  Â  Â  Â  "tts_status_fail": "âŒ TTS ìƒì„± ì‹¤íŒ¨ (ë°ì´í„° ì—†ìŒ)",
Â  Â  Â  Â  "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
Â  Â  Â  Â Â 
Â  Â  Â  Â  # â­ ëŒ€í™”í˜•/ì¢…ë£Œ ë©”ì‹œì§€
Â  Â  Â  Â  "button_mic_input": "ìŒì„± ì…ë ¥",
Â  Â  Â  Â  "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ ì±„íŒ…ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤.",
Â  Â  Â  Â  "prompt_survey": "ê³ ê° ë¬¸ì˜ ì„¼í„°ì— ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©°, ì¶”ê°€ë¡œ ì €í¬ ì‘ëŒ€ ì†”ë£¨ì…˜ì— ëŒ€í•œ ì„¤ë¬¸ ì¡°ì‚¬ì— ì‘í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹­ì‹œì˜¤.",
Â  Â  Â  Â  "customer_closing_confirm": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?",
Â  Â  Â  Â  "customer_positive_response": "ì¢‹ì€ ë§ì”€/ì¹œì ˆí•œ ìƒë‹´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.",
Â  Â  Â  Â  "button_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ì¡°ì‚¬ ìš”ì²­)"
Â  Â  },
Â  Â  "en": {
Â  Â  Â  Â  "title": "Personalized AI Study Coach",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AI Study Coach Settings",
Â  Â  Â  Â  "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
Â  Â  Â  Â  "button_start_analysis": "Start Analysis (RAG Indexing)",
Â  Â  Â  Â  "rag_tab": "RAG Knowledge Chatbot",
Â  Â  Â  Â  "content_tab": "Custom Content Generation",
Â  Â  Â  Â  "lstm_tab": "LSTM Achievement Prediction",
Â  Â  Â  Â  "simulator_tab": "AI Customer Response Simulator",Â 
Â  Â  Â  Â  "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
Â  Â  Â  Â  "rag_desc": "Answers questions based on the uploaded documents.",
Â  Â  Â  Â  "rag_input_placeholder": "Ask a question about your study materials",
Â  Â  Â  Â  "llm_error_key": "âš ï¸ Warning: GEMINI API Key is not set. Please set 'GEMINI_API_KEY' in Streamlit Secrets.",
Â  Â  Â  Â  "llm_error_init": "LLM initialization error: Please check your API key.",
Â  Â  Â  Â  "content_header": "Custom Learning Content Generation",
Â  Â  Â  Â  "content_desc": "Generate content tailored to your topic and difficulty.",
Â  Â  Â  Â  "topic_label": "Learning Topic",
Â  Â  Â  Â  "level_label": "Difficulty",
Â  Â  Â  Â  "content_type_label": "Content Type",
Â  Â  Â  Â  "level_options": ["Beginner", "Intermediate", "Advanced"],
Â  Â  Â  Â  "content_options": ["Key Summary Note", "10 Multiple-Choice Questions", "Practical Example Idea"],
Â  Â  Â  Â  "button_generate": "Generate Content",
Â  Â  Â  Â  "warning_topic": "Please enter a learning topic.",
Â  Â  Â  Â  "lstm_header": "LSTM Based Achievement Prediction",
Â  Â  Â  Â  "lstm_desc": "Trains an LSTM model on hypothetical past quiz scores to predict future achievement.",
Â  Â  Â  Â  "lstm_disabled_error": "The LSTM feature is temporarily disabled due to build environment issues. Please use the 'Custom Content Generation' feature first.",
Â  Â  Â  Â  "lang_select": "Select Language",
Â  Â  Â  Â  "embed_success": "Learning DB built with {count} chunks!",
Â  Â  Â  Â  "embed_fail": "Embedding failed: Free tier quota exceeded or network issue.",
Â  Â  Â  Â  "warning_no_files": "Please upload study materials first.",
Â  Â  Â  Â  "warning_rag_not_ready": "RAG is not ready. Upload materials and click Start Analysis.",
Â  Â  Â  Â  "quiz_fail_structure": "Quiz data structure is incorrect.",
Â  Â  Â  Â  "select_answer": "Select answer",
Â  Â  Â  Â  "check_answer": "Confirm answer",
Â  Â  Â  Â  "next_question": "Next Question",
Â  Â  Â  Â  "correct_answer": "Correct! ğŸ‰",
Â  Â  Â  Â  "incorrect_answer": "Incorrect. ğŸ˜",
Â  Â  Â  Â  "correct_is": "Correct answer",
Â  Â  Â  Â  "explanation": "Explanation",
Â  Â  Â  Â  "quiz_complete": "Quiz completed!",
Â  Â  Â  Â  "score": "Score",
Â  Â  Â  Â  "retake_quiz": "Retake Quiz",
Â  Â  Â  Â  "quiz_error_llm": "Quiz generation failed: LLM did not return a valid JSON format. Check the original LLM response.",
Â  Â  Â  Â  "quiz_original_response": "Original LLM Response",
Â  Â  Â  Â  "firestore_loading": "Loading RAG index from database...",
Â  Â  Â  Â Â 
Â  Â  Â  Â  # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
Â  Â  Â  Â  "simulator_header": "AI Customer Response Simulator",
Â  Â  Â  Â  "simulator_desc": "Provides AI-generated response drafts and guidelines for handling challenging customer inquiries.",
Â  Â  Â  Â  "customer_query_label": "Customer Query (Link optional)",
Â  Â  Â  Â  "customer_type_label": "Customer Sentiment",
Â  Â  Â  Â  "customer_type_options": ["General Inquiry", "Challenging Customer", "Highly Dissatisfied Customer"],
Â  Â  Â  Â  "button_simulate": "Request Response Advice",
Â  Â  Â  Â  "simulation_warning_query": "Please enter the customer's query.",
Â  Â  Â  Â  "simulation_no_key_warning": "âš ï¸ API Key is missing. Response generation cannot proceed. (UI configuration is complete.)",
Â  Â  Â  Â  "simulation_advice_ready": "AI's response advice is ready!",
Â  Â  Â  Â  "simulation_advice_header": "AI Response Guidelines",
Â  Â  Â  Â  "simulation_draft_header": "Recommended Response Draft",
Â  Â  Â  Â  "button_listen_audio": "Listen to Audio",
Â  Â  Â  Â  "tts_status_ready": "Ready to listen",
Â  Â  Â  Â  "tts_status_generating": "Generating audio...",
Â  Â  Â  Â  "tts_status_success": "âœ… Audio playback complete!",
Â  Â  Â  Â  "tts_status_fail": "âŒ TTS generation failed (No data)",
Â  Â  Â  Â  "tts_status_error": "âŒ TTS API error occurred",

Â  Â  Â  Â  # â­ ëŒ€í™”í˜•/ì¢…ë£Œ ë©”ì‹œì§€
Â  Â  Â  Â  "button_mic_input": "Voice Input",
Â  Â  Â  Â  "prompt_customer_end": "As there are no further inquiries, we will now end this chat session.",
Â  Â  Â  Â  "prompt_survey": "Thank you for contacting our Customer Support Center. We would be grateful if you could participate in a short survey about our service solution. Please feel free to contact us anytime if you have any additional questions.",
Â  Â  Â  Â  "customer_closing_confirm": "Is there anything else we can assist you with today?",
Â  Â  Â  Â  "customer_positive_response": "Thank you for your kind understanding/friendly advice.",
Â  Â  Â  Â  "button_end_chat": "End Chat (Request Survey)"
Â  Â  },
Â  Â  "ja": {
Â  Â  Â  Â  "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
Â  Â  Â  Â  "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
Â  Â  Â  Â  "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
Â  Â  Â  Â  "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
Â  Â  Â  Â  "content_tab": "ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
Â  Â  Â  Â  "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
Â  Â  Â  Â  "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",Â 
Â  Â  Â  Â  "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
Â  Â  Â  Â  "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
Â  Â  Â  Â  "rag_input_placeholder": "å­¦ç¿’è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
Â  Â  Â  Â  "llm_error_key": "âš ï¸ è­¦å‘Š: GEMINI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secretsã«'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "llm_error_init": "LLMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
Â  Â  Â  Â  "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«åˆã‚ã›ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
Â  Â  Â  Â  "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
Â  Â  Â  Â  "level_label": "é›£æ˜“åº¦",
Â  Â  Â  Â  "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼",
Â  Â  Â  Â  "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
Â  Â  Â  Â  "content_options": ["æ ¸å¿ƒè¦ç´„ãƒãƒ¼ãƒˆ", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢"],
Â  Â  Â  Â  "button_generate": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
Â  Â  Â  Â  "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "lstm_header": "LSTMãƒ™ãƒ¼ã‚¹é”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
Â  Â  Â  Â  "lstm_desc": "ä»®æƒ³ã®éå»ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€LSTMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦å°†æ¥ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—è¡¨ç¤ºã—ã¾ã™ã€‚",
Â  Â  Â  Â  "lstm_disabled_error": "ç¾åœ¨ã€ãƒ“ãƒ«ãƒ‰ç’°å¢ƒã®å•é¡Œã«ã‚ˆã‚ŠLSTMæ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã€Œã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã€æ©Ÿèƒ½ã‚’å…ˆã«ã”åˆ©ç”¨ãã ã•ã„ã€‚ã€",
Â  Â  Â  Â  "lang_select": "è¨€èªé¸æŠ",
Â  Â  Â  Â  "embed_success": "å…¨{count}ãƒãƒ£ãƒ³ã‚¯ã§å­¦ç¿’DBæ§‹ç¯‰å®Œäº†!",
Â  Â  Â  Â  "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—: ãƒ•ãƒªãƒ¼ãƒ†ã‚£ã‚¢ã®ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã€‚",
Â  Â  Â  Â  "warning_no_files": "ã¾ãšå­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "warning_rag_not_ready": "RAGãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€åˆ†æé–‹å§‹ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
Â  Â  Â  Â  "select_answer": "æ­£è§£ã‚’é¸æŠã—ã¦ãã ã•ã„",
Â  Â  Â  Â  "check_answer": "æ­£è§£ã‚’ç¢ºèª",
Â  Â  Â  Â  "next_question": "æ¬¡ã®è³ªå•",
Â  Â  Â  Â  "correct_answer": "æ­£è§£ã§ã™! ğŸ‰",
Â  Â  Â  Â  "incorrect_answer": "ä¸æ­£è§£ã§ã™ã€‚ğŸ˜",
Â  Â  Â  Â  "correct_is": "æ­£è§£",
Â  Â  Â  Â  "explanation": "è§£èª¬",
Â  Â  Â  Â  "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
Â  Â  Â  Â  "score": "ã‚¹ã‚³ã‚¢",
Â  Â  Â  Â  "retake_quiz": "ã‚¯ã‚¤ã‚ºã‚’å†æŒ‘æˆ¦",
Â  Â  Â  Â  "quiz_error_llm": "LLMãŒæ­£ã—ã„JSONã®å½¢å¼ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã®ã§ã€ã‚¯ã‚¤ã‚ºã®ç”ŸæˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚",
Â  Â  Â  Â  "quiz_original_response": "LLM åŸæœ¬å¿œç­”",
Â  Â  Â  Â  "firestore_loading": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...",
Â  Â  Â  Â Â 
Â  Â  Â  Â  # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
Â  Â  Â  Â  "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
Â  Â  Â  Â  "simulator_desc": "é›£ã—ã„é¡§å®¢ã®å•ã„åˆã‚ã›ã«å¯¾ã—ã¦ã€AIã«ã‚ˆã‚‹å¯¾å¿œæ¡ˆã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚",
Â  Â  Â  Â  "customer_query_label": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ï¼ˆãƒªãƒ³ã‚¯ä»»æ„ï¼‰",
Â  Â  Â  Â  "customer_type_label": "é¡§å®¢ã®å‚¾å‘",
Â  Â  Â  Â  "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "æ‰‹ã”ã‚ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"],
Â  Â  Â  Â  "button_simulate": "å¯¾å¿œã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¦æ±‚",
Â  Â  Â  Â  "simulation_warning_query": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "simulation_no_key_warning": "âš ï¸ APIã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å¿œç­”ã®ç”Ÿæˆã¯ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚ï¼ˆUIè¨­å®šã¯å®Œäº†ã—ã¦ã„ã¾ã™ã€‚ï¼‰",
Â  Â  Â  Â  "simulation_advice_ready": "AIã®å¯¾å¿œã‚¢ãƒ‰ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼",
Â  Â  Â  Â  "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
Â  Â  Â  Â  "simulation_draft_header": "æ¨å¥¨ã•ã‚Œã‚‹å¯¾å¿œè‰æ¡ˆ",
Â  Â  Â  Â  "button_listen_audio": "éŸ³å£°ã§èã",
Â  Â  Â  Â  "tts_status_ready": "éŸ³å£°å†ç”Ÿã®æº–å‚™ãŒã§ãã¾ã—ãŸ",
Â  Â  Â  Â  "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
Â  Â  Â  Â  "tts_status_success": "âœ… éŸ³å£°å†ç”Ÿå®Œäº†!",
Â  Â  Â  Â  "tts_status_fail": "âŒ TTSç”Ÿæˆå¤±æ•—ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰",
Â  Â  Â  Â  "tts_status_error": "âŒ TTS APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",

Â  Â  Â  Â  # â­ ëŒ€í™”í˜•/ì¢…ë£Œ ë©”ì‹œì§€
Â  Â  Â  Â  "button_mic_input": "éŸ³å£°å…¥åŠ›",
Â  Â  Â  Â  "prompt_customer_end": "ãŠå®¢æ§˜ã‹ã‚‰ã®è¿½åŠ ã®ãŠå•ã„åˆã‚ã›ãŒãªã„ãŸã‚ã€æœ¬ãƒãƒ£ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆã‚’çµ‚äº†ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
Â  Â  Â  Â  "prompt_survey": "ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚å¼Šç¤¾ã®å¯¾å¿œã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹ç°¡å˜ãªã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«ã”å”åŠ›ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚è¿½åŠ ã®ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã„ã¤ã§ã‚‚ã”é€£çµ¡ãã ã•ã„ã€‚",
Â  Â  Â  Â  "customer_closing_confirm": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
Â  Â  Â  Â  "customer_positive_response": "è¦ªåˆ‡ãªã”å¯¾å¿œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚",
Â  Â  Â  Â  "button_end_chat": "å¯¾å¿œçµ‚äº† (ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã‚’ä¾é ¼)"
Â  Â  }
}


# ================================
# 4. Streamlit í•µì‹¬ Config ì„¤ì • ë° Session State ì´ˆê¸°í™” (CRITICAL ZONE)
# ================================

if 'language' not in st.session_state: st.session_state.language = 'ko'
if 'uploaded_files_state' not in st.session_state: st.session_state.uploaded_files_state = None
if 'is_llm_ready' not in st.session_state: st.session_state.is_llm_ready = False
if 'is_rag_ready' not in st.session_state: st.session_state.is_rag_ready = False
if 'firestore_db' not in st.session_state: st.session_state.firestore_db = None
if 'llm_init_error_msg' not in st.session_state: st.session_state.llm_init_error_msg = None
if 'firestore_load_success' not in st.session_state: st.session_state.firestore_load_success = False

# â­ ì‹œë®¬ë ˆì´í„° ì „ìš© ìƒíƒœ ì´ˆê¸°í™” ì¶”ê°€
if "simulator_memory" not in st.session_state:
Â  Â  # ConversationChainì—ì„œ ì‚¬ìš©í•  ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
Â  Â  st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
if "simulator_messages" not in st.session_state:
Â  Â  st.session_state.simulator_messages = []
if "initial_advice_provided" not in st.session_state:
Â  Â  st.session_state.initial_advice_provided = False
if "simulator_chain" not in st.session_state:
Â  Â  st.session_state.simulator_chain = None
# â­ ì‹œë®¬ë ˆì´í„° ì§„í–‰ ìƒíƒœ ì¶”ê°€
if "is_chat_ended" not in st.session_state:
Â  Â  st.session_state.is_chat_ended = False

# ì–¸ì–´ ì„¤ì • ë¡œë“œ (UI ì¶œë ¥ ì „ í•„ìˆ˜)
L = LANG[st.session_state.language]Â 
API_KEY = os.environ.get("GEMINI_API_KEY")

# =======================================================
# 5. Streamlit UI í˜ì´ì§€ ì„¤ì • (ìŠ¤í¬ë¦½íŠ¸ ë‚´ ì²« ë²ˆì§¸ ST ëª…ë ¹)
# =======================================================
st.set_page_config(page_title=L["title"], layout="wide")

# =======================================================
# 6. ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë° LLM/DB ë¡œì§ (í˜ì´ì§€ ì„¤ì • í›„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰)
# =======================================================

if 'llm' not in st.session_state:Â 
Â  Â  llm_init_error = None # â­ safety initialization
Â  Â  if not API_KEY:
Â  Â  Â  Â  llm_init_error = L["llm_error_key"]
Â  Â  else:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # LLM ë° Embeddings ì´ˆê¸°í™”
Â  Â  Â  Â  Â  Â  st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
Â  Â  Â  Â  Â  Â  st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
Â  Â  Â  Â  Â  Â  st.session_state.is_llm_ready = True
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Admin SDK í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”Â 
Â  Â  Â  Â  Â  Â  sa_info, error_message = _get_admin_credentials()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if error_message:
Â  Â  Â  Â  Â  Â  Â  Â  llm_init_error = f"{L['llm_error_init']} (DB Auth Error: {error_message})"Â 
Â  Â  Â  Â  Â  Â  elif sa_info:
Â  Â  Â  Â  Â  Â  Â  Â  db = initialize_firestore_admin()Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.firestore_db = db
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if not db:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llm_init_error = f"{L['llm_init_error']} (DB Client Error: Firebase Admin Init Failed)"Â 
Â  Â  Â  Â  Â  Â  Â  Â  else: # DBê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ê²½ìš°ì—ë§Œ ë¡œë“œ ì‹œë„
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # DB ë¡œë”© ë¡œì§ (RAG ì±—ë´‡ìš©)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 'conversation_chain' not in st.session_state:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # DB ë¡œë”© ì‹œë„
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  loaded_index = load_index_from_firestore(st.session_state.firestore_db, st.session_state.embeddings)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if loaded_index:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.conversation_chain = get_rag_chain(loaded_index)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_rag_ready = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.firestore_load_success = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.firestore_load_success = False
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # â­ ì‹œë®¬ë ˆì´í„° ì²´ì¸ ì´ˆê¸°í™” (LangChain Prompt Variable Error í•´ê²°)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 1. ì‹œë®¬ë ˆì´í„° ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (PromptTemplate ìƒì„±ì ì‚¬ìš©)
Â  Â  Â  Â  Â  Â  SIMULATOR_PROMPT = PromptTemplate(
Â  Â  Â  Â  Â  Â  Â  Â  template="The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n\n{chat_history}\nHuman: {input}\nAI:",
Â  Â  Â  Â  Â  Â  Â  Â  input_variables=["input", "chat_history"]
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 2. ConversationChain ì´ˆê¸°í™”
Â  Â  Â  Â  Â  Â  st.session_state.simulator_chain = ConversationChain(
Â  Â  Â  Â  Â  Â  Â  Â  llm=st.session_state.llm,
Â  Â  Â  Â  Â  Â  Â  Â  memory=st.session_state.simulator_memory,
Â  Â  Â  Â  Â  Â  Â  Â  prompt=SIMULATOR_PROMPT,
Â  Â  Â  Â  Â  Â  Â  Â  input_key="input",Â 
Â  Â  Â  Â  Â  Â  )


Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  # LLM ì´ˆê¸°í™” ì˜¤ë¥˜ ì²˜ë¦¬Â 
Â  Â  Â  Â  Â  Â  llm_init_error = f"{L['llm_init_error_init']} {e}"Â 
Â  Â  Â  Â  Â  Â  st.session_state.is_llm_ready = False
Â  Â Â 
Â  Â  if llm_init_error:
Â  Â  Â  Â  st.session_state.is_llm_ready = False
Â  Â  Â  Â  st.session_state.llm_init_error_msg = llm_init_errorÂ 

# ë‚˜ë¨¸ì§€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "memory" not in st.session_state:
Â  Â  # RAG ì²´ì¸ìš© ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
Â  Â  st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

if "embedding_cache" not in st.session_state:
Â  Â  st.session_state.embedding_cache = {}

# ================================
# 7. ì´ˆê¸°í™” ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ ë° DB ìƒíƒœ ì•Œë¦¼
# ================================

if st.session_state.llm_init_error_msg:
Â  Â  st.error(st.session_state.llm_init_error_msg)
Â  Â Â 
if st.session_state.get('firestore_db'):
Â  Â  if st.session_state.get('firestore_load_success', False):
Â  Â  Â  Â  st.success("âœ… RAG ì¸ë±ìŠ¤ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
Â  Â  elif not st.session_state.get('is_rag_ready', False) and st.session_state.get('llm_init_error_msg') is None:
Â  Â  Â  Â  st.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”.")


# ================================
# 8. Streamlit UI ì‹œì‘
# ================================

with st.sidebar:
Â  Â  selected_lang_key = st.selectbox(
Â  Â  Â  Â  L["lang_select"],
Â  Â  Â  Â  options=['ko', 'en', 'ja'],
Â  Â  Â  Â  index=['ko', 'en', 'ja'].index(st.session_state.language),
Â  Â  Â  Â  format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
Â  Â  )
Â  Â Â 
Â  Â  if selected_lang_key != st.session_state.language:
Â  Â  Â  Â  st.session_state.language = selected_lang_key
Â  Â  Â  Â  st.rerun()Â 
Â  Â Â 
Â  Â  L = LANG[st.session_state.language]Â 
Â  Â Â 
Â  Â  st.title(L["sidebar_title"])
Â  Â Â 
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  uploaded_files_widget = st.file_uploader(
Â  Â  Â  Â  L["file_uploader"],
Â  Â  Â  Â  type=["pdf","txt","html"],
Â  Â  Â  Â  accept_multiple_files=True
Â  Â  )
Â  Â Â 
Â  Â  if uploaded_files_widget:
Â  Â  Â  Â  st.session_state.uploaded_files_state = uploaded_files_widget
Â  Â  elif 'uploaded_files_state' not in st.session_state:
Â  Â  Â  Â  st.session_state.uploaded_files_state = None
Â  Â Â 
Â  Â  files_to_process = st.session_state.uploaded_files_state if st.session_state.uploaded_files_state else []
Â  Â Â 
Â  Â  if files_to_process and st.session_state.is_llm_ready:
Â  Â  Â  Â  if st.button(L["button_start_analysis"], key="start_analysis"):
Â  Â  Â  Â  Â  Â  with st.spinner(f"ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘..."):
Â  Â  Â  Â  Â  Â  Â  Â  text_chunks = get_document_chunks(files_to_process)
Â  Â  Â  Â  Â  Â  Â  Â  vector_store = get_vector_store(text_chunks)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if vector_store:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # RAG ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ë©´ Firestoreì— ì €ì¥ ì‹œë„
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  db = st.session_state.firestore_db
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_success = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if db:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_success = save_index_to_firestore(db, vector_store)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if save_success:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(L["embed_success"].format(count=len(text_chunks)) + " (DB ì €ì¥ ì™„ë£Œ)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(L["embed_success"].format(count=len(text_chunks)) + " (DB ì €ì¥ ì‹¤íŒ¨)")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.conversation_chain = get_rag_chain(vector_store)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_rag_ready = True
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_rag_ready = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(L["embed_fail"])

Â  Â  else:
Â  Â  Â  Â  st.session_state.is_rag_ready = False
Â  Â  Â  Â  if not st.session_state.is_llm_ready:
Â  Â  Â  Â  Â  Â  st.warning(L.get("llm_error_key"))
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning(L.get("warning_no_files", "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”."))Â 

Â  Â  st.markdown("---")
Â  Â  # â­ ìƒˆë¡œìš´ íƒ­(ì‹œë®¬ë ˆì´í„°)ì„ í¬í•¨í•˜ì—¬ ë¼ë””ì˜¤ ë²„íŠ¼ ì—…ë°ì´íŠ¸
Â  Â  feature_selection = st.radio(
Â  Â  Â  Â  L["content_tab"],Â 
Â  Â  Â  Â  [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["simulator_tab"]]
Â  Â  )

st.title(L["title"])

# ================================
# 9. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================

if feature_selection == L["simulator_tab"]:Â 
Â  Â  st.header(L["simulator_header"])
Â  Â  st.markdown(L["simulator_desc"])
Â  Â Â 
Â  Â  # 1. TTS ìœ í‹¸ë¦¬í‹° (ìƒíƒœ í‘œì‹œê¸° ë° JS í•¨ìˆ˜)ë¥¼ í˜ì´ì§€ ìƒë‹¨ì— ì‚½ì…
Â  Â  st.markdown(f'<div id="tts_status" style="padding: 5px; text-align: center; border-radius: 5px; background-color: #f0f0f0; margin-bottom: 10px;">{L["tts_status_ready"]}</div>', unsafe_allow_html=True)
Â  Â Â 
Â  Â  # TTS JS ìœ í‹¸ë¦¬í‹°ë¥¼ í˜ì´ì§€ ë¡œë“œ ì‹œ ë‹¨ í•œ ë²ˆë§Œ ì‚½ì… (TTS í•¨ìˆ˜ê°€ ê¸€ë¡œë²Œë¡œ ì •ì˜ë˜ë„ë¡)
Â  Â  # â­ TTSëŠ” API Key ì—†ì´ ì‘ë™
Â  Â  if "tts_js_loaded" not in st.session_state:
Â  Â  Â  Â  Â synthesize_and_play_audio(st.session_state.language)Â 
Â  Â  Â  Â  Â st.session_state.tts_js_loaded = True

Â  Â  # â­ Firebase ìƒë‹´ ì´ë ¥ ë¡œë“œ ë° ì„ íƒ ì„¹ì…˜
Â  Â  db = st.session_state.get('firestore_db')
Â  Â  if db:
Â  Â  Â  Â  with st.expander("ğŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê°œ)"):
Â  Â  Â  Â  Â  Â  histories = load_simulation_histories(db)
Â  Â  Â  Â  Â  Â  if histories:
Â  Â  Â  Â  Â  Â  Â  Â  history_options = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"[{h['timestamp'].strftime('%m-%d %H:%M')}] {h['customer_type']} - {h['initial_query'][:30]}...": h
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for h in histories
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  selected_key = st.selectbox(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  options=list(history_options.keys())
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("ì„ íƒëœ ì´ë ¥ ë¡œë“œ"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  selected_history = history_options[selected_key]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ìƒíƒœ ë³µì›
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.customer_query_text_area = selected_history['initial_query']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.initial_advice_provided = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages = selected_history['messages']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_chat_ended = selected_history.get('is_chat_ended', False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë° ë©”ì‹œì§€ ì¬êµ¬ì„± (LangChain í˜¸í™˜ì„±ì„ ìœ„í•´)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.clear()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # LLM ë©”ëª¨ë¦¬ì— ëŒ€í™” ì´ë ¥ ì¬ì£¼ì… (ì‹¤ì œ LLMì´ ì‘ëŒ€í•  ìˆ˜ ìˆë„ë¡)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, msg in enumerate(selected_history['messages']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â if msg['role'] == 'customer':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â elif msg['role'] in ['supervisor', 'customer_rebuttal', 'customer_end', 'system_end']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # supervisorì˜ adviceì™€ customerì˜ ë°˜ë°•ì€ LLM ì‘ë‹µìœ¼ë¡œ ê°„ì£¼
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.simulator_memory.chat_memory.add_ai_message(msg['content'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â elif msg['role'] == 'agent_response':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  # â­ LLM ì´ˆê¸°í™”ê°€ ë˜ì–´ìˆì§€ ì•Šì•„ë„ (API Keyê°€ ì—†ì–´ë„) UIê°€ ì‘ë™í•´ì•¼ í•¨
Â  Â  if st.session_state.is_llm_ready or not API_KEY:
Â  Â  Â  Â  if st.session_state.is_chat_ended:
Â  Â  Â  Â  Â  Â  st.success(L["prompt_customer_end"] + " " + L["prompt_survey"])
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if st.button("ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘", key="new_simulation"):
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.is_chat_ended = False
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.initial_advice_provided = False
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.simulator_messages = []
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.simulator_memory.clear()
Â  Â  Â  Â  Â  Â  Â  Â  Â st.rerun()
Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. ê³ ê° ë¬¸ì˜ ì…ë ¥ í•„ë“œ
Â  Â  Â  Â  if 'customer_query_text_area' not in st.session_state:
Â  Â  Â  Â  Â  Â  st.session_state.customer_query_text_area = ""

Â  Â  Â  Â  customer_query = st.text_area(
Â  Â  Â  Â  Â  Â  L["customer_query_label"],
Â  Â  Â  Â  Â  Â  key="customer_query_text_area",
Â  Â  Â  Â  Â  Â  height=150,
Â  Â  Â  Â  Â  Â  placeholder=L["customer_query_label"] + "...",
Â  Â  Â  Â  Â  Â  disabled=st.session_state.initial_advice_provided
Â  Â  Â  Â  )

Â  Â  Â  Â  # 2. ê³ ê° ì„±í–¥ ì„ íƒ
Â  Â  Â  Â  customer_type_display = st.selectbox(
Â  Â  Â  Â  Â  Â  L["customer_type_label"],
Â  Â  Â  Â  Â  Â  L["customer_type_options"],
Â  Â  Â  Â  Â  Â  disabled=st.session_state.initial_advice_provided
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # ì„ íƒëœ ì–¸ì–´ í‚¤
Â  Â  Â  Â  current_lang_key = st.session_state.languageÂ 

Â  Â  Â  Â  # 4. 'ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­' ë²„íŠ¼: ì´ˆê¸° ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
Â  Â  Â  Â  if st.button(L["button_simulate"], key="start_simulation", disabled=st.session_state.initial_advice_provided):
Â  Â  Â  Â  Â  Â  if not customer_query:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning(L["simulation_warning_query"])
Â  Â  Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # ì´ˆê¸°í™”
Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.clear()
Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages = []
Â  Â  Â  Â  Â  Â  st.session_state.is_chat_ended = False
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "customer", "content": customer_query})
Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_user_message(customer_query)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  initial_prompt = f"""
Â  Â  Â  Â  Â  Â  You are an AI Customer Support Supervisor. Your task is to provide expert guidance to a customer support agent.
Â  Â  Â  Â  Â  Â  The customer sentiment is: {customer_type_display}.
Â  Â  Â  Â  Â  Â  The customer's initial inquiry is: "{customer_query}"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Based on this, provide:
Â  Â  Â  Â  Â  Â  1. Crucial advice on the tone and strategy for dealing with this specific sentiment.Â 
Â  Â  Â  Â  Â  Â  2. A concise and compassionate recommended response draft.
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  The response must be strictly in {LANG[current_lang_key]['lang_select']} and include the required initial contact information check.
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not API_KEY:
Â  Â  Â  Â  Â  Â  Â  Â  # API Keyê°€ ì—†ì„ ê²½ìš° ëª¨ì˜(Mock) ë°ì´í„° ì‚¬ìš©
Â  Â  Â  Â  Â  Â  Â  Â  mock_data = get_mock_response_data(current_lang_key, customer_type_display)
Â  Â  Â  Â  Â  Â  Â  Â  ai_advice_text = f"### {mock_data['advice_header']}\n\n{mock_data['advice']}\n\n### {mock_data['draft_header']}\n\n{mock_data['draft']}"
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(ai_advice_text) # ë©”ëª¨ë¦¬ì—ë„ ì¶”ê°€
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.initial_advice_provided = True
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # â­ Firebase ì´ë ¥ ì €ì¥ (API Key ì—†ì´ë„ UIëŠ” ì‹œì‘ ê°€ëŠ¥)
Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if API_KEY:
Â  Â  Â  Â  Â  Â  Â  Â  # API Keyê°€ ìˆì„ ê²½ìš° LLM í˜¸ì¶œ
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("AI ìŠˆí¼ë°”ì´ì € ì¡°ì–¸ ìƒì„± ì¤‘..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # simulator_chainì´ Noneì´ ì•„ë‹Œì§€ í™•ì¸ (AttributeError ë°©ì§€)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state.simulator_chain is None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(L['llm_error_init'] + " (ì‹œë®¬ë ˆì´í„° ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ConversationChainì˜ predictëŠ” 'input'ë§Œ ë°›ìœ¼ë©°, ê²°ê³¼ëŠ” ë¬¸ìì—´ì…ë‹ˆë‹¤.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_text = st.session_state.simulator_chain.predict(input=initial_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ai_advice_text = response_text
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ConversationChain.predictëŠ” ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ì— ì¶”ê°€í•©ë‹ˆë‹¤.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.initial_advice_provided = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # â­ Firebase ì´ë ¥ ì €ì¥ (API Key ìˆì„ ë•Œ)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 5. ì‹œë®¬ë ˆì´ì…˜ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # ì±„íŒ… ê¸°ë¡ ë Œë”ë§
Â  Â  Â  Â  for message in st.session_state.simulator_messages:
Â  Â  Â  Â  Â  Â  if message["role"] == "customer":
Â  Â  Â  Â  Â  Â  Â  Â  with st.chat_message("user", avatar="ğŸ™‹"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  elif message["role"] == "supervisor":
Â  Â  Â  Â  Â  Â  Â  Â  with st.chat_message("assistant", avatar="ğŸ¤–"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # TTS ë²„íŠ¼ì€ API Key ì—†ì´ ì‘ë™í•˜ë„ë¡ ìˆ˜ì •ë¨
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  render_tts_button(message["content"], st.session_state.language)Â 
Â  Â  Â  Â  Â  Â  elif message["role"] == "agent_response":
Â  Â  Â  Â  Â  Â  Â  Â  Â with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  render_tts_button(message["content"], st.session_state.language) # ì—ì´ì „íŠ¸ ì‘ë‹µë„ TTSë¡œ ë“¤ì„ ìˆ˜ ìˆë„ë¡
Â  Â  Â  Â  Â  Â  elif message["role"] == "customer_rebuttal":
Â  Â  Â  Â  Â  Â  Â  Â  Â with st.chat_message("assistant", avatar="ğŸ˜ "):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  elif message["role"] == "customer_end":
Â  Â  Â  Â  Â  Â  Â  Â  Â with st.chat_message("assistant", avatar="ğŸ˜Š"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  elif message["role"] == "system_end":
Â  Â  Â  Â  Â  Â  Â  Â  Â with st.chat_message("assistant", avatar="âœ¨"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message["content"])

Â  Â  Â  Â  # 6. ëŒ€í™”í˜• ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰ (ì¶”ê°€ ì±„íŒ…)
Â  Â  Â  Â  if st.session_state.initial_advice_provided and not st.session_state.is_chat_ended:
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  last_role = st.session_state.simulator_messages[-1]['role'] if st.session_state.simulator_messages else None
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # ì—ì´ì „íŠ¸(ì‚¬ìš©ì)ê°€ ê³ ê°ì—ê²Œ ì‘ë‹µí•  ì°¨ë¡€ (ì¬ë°˜ë°•, ì¶”ê°€ ì§ˆë¬¸ í›„)
Â  Â  Â  Â  Â  Â  # ê³ ê°ì˜ ë§ˆì§€ë§‰ ë°˜ì‘ì´ 'rebuttal' ë˜ëŠ” 'end'ì˜€ê±°ë‚˜, 'supervisor'(ë§¤ë„ˆ ì§ˆë¬¸)ì¸ ê²½ìš°
Â  Â  Â  Â  Â  Â  if last_role in ["customer_rebuttal", "customer_end", "supervisor"]: 
Â  Â  Â  Â  Â  Â  Â  Â  # --- ìŒì„± ì…ë ¥(ì„ íƒ) + í…ìŠ¤íŠ¸ ì…ë ¥ í¼ (agent ì‘ë‹µ) ---
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("### ğŸ¤ ì—ì´ì „íŠ¸ ì‘ë‹µ (ìŒì„± ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥)")
Â  Â  Â  Â  Â  Â  Â  Â  col_audio, col_text = st.columns([1, 3])

Â  Â  Â  Â  Â  Â  Â  Â  transcript = ""

Â  Â  Â  Â  Â  Â  Â  Â  # 1ï¸âƒ£ ìŒì„± ë…¹ìŒ ìœ„ì ¯
Â  Â  Â  Â  Â  Â  Â  Â  with col_audio:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**ğŸ”Š ìŒì„±ìœ¼ë¡œ ì‘ë‹µ (ì„ íƒ)**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Streamlit Audio Recorder ëŒ€ì‹  st.file_uploaderë¥¼ ì´ìš©í•œ ì„ì‹œ ë…¹ìŒ íŒŒì¼ ì²˜ë¦¬
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_file = st.file_uploader(L["button_mic_input"], type=["wav", "mp3"], key="simulator_audio_input_file")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption("ë…¹ìŒ í›„ ì „ì‚¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³ , í•„ìš”í•˜ë©´ í¸ì§‘í•œ ë’¤ ì „ì†¡í•˜ì„¸ìš”.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if audio_file and st.button("ìŒì„± ì „ì‚¬", key="transcribe_audio"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("ìŒì„± ì „ì‚¬ ì¤‘..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tmp_dir = tempfile.mkdtemp()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tmp_path = f"{tmp_dir}/sim_audio.{audio_file.type.split('/')[-1]}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with open(tmp_path, "wb") as f:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f.write(audio_file.getvalue())

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  r = sr.Recognizer()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_seg = AudioSegment.from_file(tmp_path)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_seg = audio_seg.set_frame_rate(16000).set_channels(1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  converted_path = f"{tmp_dir}/sim_audio_conv.wav"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_seg.export(converted_path, format="wav")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with sr.AudioFile(converted_path) as source:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_data = r.record(source)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lang_code = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ko-KR" if st.session_state.language == "ko" else
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ja-JP" if st.session_state.language == "ja" else "en-US"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  transcript_result = r.recognize_google(audio_data, language=lang_code)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.transcribed_text = transcript_result
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("ğŸ™ï¸ ì „ì‚¬ ì„±ê³µ! í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun() # í…ìŠ¤íŠ¸ ì—ì–´ë¦¬ì–´ì— ê°’ì„ ì±„ìš°ê¸° ìœ„í•´ ë¦¬ëŸ°

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"âš ï¸ ìŒì„± ì „ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.transcribed_text = ""
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # 2ï¸âƒ£ ì „ì‚¬ëœ í…ìŠ¤íŠ¸(ìˆìœ¼ë©´) ë³´ì—¬ì£¼ê³ , ì‚¬ìš©ìê°€ í¸ì§‘í•´ì„œ ë³´ë‚¼ ìˆ˜ ìˆê²Œ ì…ë ¥ë€ ì œê³µ
Â  Â  Â  Â  Â  Â  Â  Â  with col_text:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ (ìŒì„± ì „ì‚¬ ë˜ëŠ” ì§ì ‘ ì…ë ¥)**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  initial_text = st.session_state.get('transcribed_text', "")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  agent_response = st.text_area(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ì—ì´ì „íŠ¸ë¡œì„œ ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš” (ì¬ë°˜ë°• ëŒ€ì‘)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=initial_text,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="agent_response_area",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  height=150,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("ì‘ë‹µ ì „ì†¡", key="send_agent_response"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if agent_response.strip():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "agent_response", "content": agent_response}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_user_message(agent_response)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.transcribed_text = "" # ì „ì‚¬ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("ì‘ë‹µ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

Â  Â  Â  Â  Â  Â  # ì—ì´ì „íŠ¸ê°€ ì‘ë‹µí•œ í›„ì—ëŠ” ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ì„ ê¸°ë‹¤ë¦¼ (ë²„íŠ¼ í‘œì‹œ)
Â  Â  Â  Â  Â  Â  if last_role in ["agent_response", "customer"] and not st.session_state.is_chat_ended:
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # ë§ˆì§€ë§‰ ì‘ë‹µì´ ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì´ì—ˆë‹¤ë©´, ì´ì œ ê³ ê°ì´ ë°˜ì‘í•  ì°¨ë¡€
Â  Â  Â  Â  Â  Â  Â  Â  col_end, col_next = st.columns([1, 2])
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # A) ì‘ëŒ€ ì¢…ë£Œ ë²„íŠ¼ (ë§¤ë„ˆ ì¢…ë£Œ)
Â  Â  Â  Â  Â  Â  Â  Â  if col_end.button(L["button_end_chat"], key="end_chat"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  closing_messages = get_closing_messages(current_lang_key)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ì‹œìŠ¤í…œ ë©”ì„¸ì§€ ì¶”ê°€ëŠ” AIì˜ ì‘ë‹µìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ë©”ëª¨ë¦¬ì— ì¶”ê°€
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": closing_messages["additional_query"]}) # ë§¤ë„ˆ ì§ˆë¬¸
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["additional_query"])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "system_end", "content": closing_messages["chat_closing"]}) # ìµœì¢… ì¢…ë£Œ ì¸ì‚¬
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["chat_closing"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_chat_ended = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # â­ Firebase ì´ë ¥ ì—…ë°ì´íŠ¸: ìµœì¢… ì¢…ë£Œ ìƒíƒœ ì €ì¥
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  Â  Â  Â  Â  # B) ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­ (LLM í˜¸ì¶œ)
Â  Â  Â  Â  Â  Â  Â  Â  if col_next.button("ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­ (LLM í˜¸ì¶œ)", key="request_rebuttal"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not API_KEY:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("API Keyê°€ ì—†ê¸° ë•Œë¬¸ì— LLMì„ í†µí•œ ëŒ€í™”í˜• ì‹œë®¬ë ˆì´ì…˜ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # LLM í˜¸ì¶œ ì‹œ simulator_chainì´ Noneì´ ì•„ë‹Œì§€ ë‹¤ì‹œ í™•ì¸
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state.simulator_chain is None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(L['llm_error_init'] + " (ì‹œë®¬ë ˆì´í„° ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  next_reaction_prompt = f"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Analyze the entire chat history. Roleplay as the customer ({customer_type_display}).Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Based on the agent's last message, generate ONE of the following responses in the customer's voice:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1. A short, challenging rebuttal (still unsatisfied).
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2. A new, follow-up question related to the previous interaction.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  3. A positive closing remark (e.g., "{L['customer_positive_response']}").
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Do not provide any resolution yourself. Just the customer's message.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  The response must be strictly in {LANG[current_lang_key]['lang_select']}.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("ê³ ê°ì˜ ë°˜ì‘ ìƒì„± ì¤‘..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ConversationChainì˜ predictëŠ” 'input' í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  customer_reaction = st.session_state.simulator_chain.predict(input=next_reaction_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ê¸ì •ì  ì¢…ë£Œ í‚¤ì›Œë“œ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  positive_keywords = ["ê°ì‚¬", "thank you", "ã‚ã‚ŠãŒã¨ã†", L['customer_positive_response'].lower().split('/')[-1].strip()]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_positive_close = any(keyword in customer_reaction.lower() for keyword in positive_keywords)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_positive_close:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  role = "customer_end" # ê¸ì •ì  ì¢…ë£Œ
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ConversationChain.predictê°€ ì´ë¯¸ ë©”ëª¨ë¦¬ì— ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ê¸ì • ì¢…ë£Œ í›„ ì—ì´ì „íŠ¸ì—ê²Œ ë§¤ë„ˆ ì§ˆë¬¸ ìš”ì²­
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": L["customer_closing_confirm"]})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(L["customer_closing_confirm"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  role = "customer_rebuttal" # ì¬ë°˜ë°• ë˜ëŠ” ì¶”ê°€ ì§ˆë¬¸
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ConversationChain.predictê°€ ì´ë¯¸ ë©”ëª¨ë¦¬ì— ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

# -----------------------------------------------------------------------------------------
# ë‚˜ë¨¸ì§€ ê¸°ëŠ¥ êµ¬í˜„ (RAG, Content, LSTM)ì€ ë™ì¼í•˜ê²Œ ìœ ì§€
# -----------------------------------------------------------------------------------------

elif feature_selection == L["rag_tab"]:
Â  Â  st.header(L["rag_header"])
Â  Â  st.markdown(L["rag_desc"])
Â  Â  if st.session_state.get('is_rag_ready', False) and st.session_state.get('conversation_chain'):
Â  Â  Â  Â  if "messages" not in st.session_state:
Â  Â  Â  Â  Â  Â  st.session_state.messages = []

Â  Â  Â  Â  for message in st.session_state.messages:
Â  Â  Â  Â  Â  Â  with st.chat_message(message["role"]):
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message["content"])

Â  Â  Â  Â  if prompt := st.chat_input(L["rag_input_placeholder"]):
Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role":"user","content":prompt})
Â  Â  Â  Â  Â  Â  with st.chat_message("user"):
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(prompt)
Â  Â  Â  Â  Â  Â  with st.chat_message("assistant"):
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(f"ë‹µë³€ ìƒì„± ì¤‘..." if st.session_state.language == 'ko' else "Generating response..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = st.session_state.conversation_chain.invoke({"question":prompt})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  answer = response.get('answer','ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' if st.session_state.language == 'ko' else 'Could not generate response.')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(answer)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role":"assistant","content":answer})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ" if st.session_state.language == 'ko' else "An error occurred"})
Â  Â  else:
Â  Â  Â  Â  st.warning(L["warning_rag_not_ready"])

elif feature_selection == L["content_tab"]:
Â  Â  st.header(L["content_header"])
Â  Â  st.markdown(L["content_desc"])

Â  Â  if st.session_state.is_llm_ready:
Â  Â  Â  Â  topic = st.text_input(L["topic_label"])
Â  Â  Â  Â Â 
Â  Â  Â  Â  level_map = dict(zip(L["level_options"], ["Beginner", "Intermediate", "Advanced"]))
Â  Â  Â  Â  content_map = dict(zip(L["content_options"], ["summary", "quiz", "example"]))
Â  Â  Â  Â Â 
Â  Â  Â  Â  level_display = st.selectbox(L["level_label"], L["level_options"])
Â  Â  Â  Â  content_type_display = st.selectbox(L["content_type_label"], L["content_options"])

Â  Â  Â  Â  level = level_map[level_display]
Â  Â  Â  Â  content_type = content_map[content_type_display]

Â  Â  Â  Â  if st.button(L["button_generate"]):
Â  Â  Â  Â  Â  Â  if topic:
Â  Â  Â  Â  Â  Â  Â  Â  target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if content_type == 'quiz':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 10ë¬¸í•­ìœ¼ë¡œ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate exactly 10 multiple-choice questions about the topic in {target_lang}.
Your entire response MUST be a valid JSON object wrapped in ```json tags.
The JSON must have a single key named 'quiz_questions', which is an array of objects.
Each question object must contain: 'question' (string), 'options' (array of objects with 'option' (A,B,C,D) and 'text' (string)), 'correct_answer' (A,B,C, or D), and 'explanation' (string).

Topic: {topic}"""
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_type_text = L["content_options"][L["content_options"].index(content_type_display)]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate clear and educational content in the requested {display_type_text} format based on the topic.
The response MUST be strictly in {target_lang}.

Topic: {topic}
Requested Format: {display_type_text}"""
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(f"Generating {content_type_display} for {topic}..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  quiz_data_raw = None
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = st.session_state.llm.invoke(full_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  quiz_data_raw = response.content
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.quiz_data_raw = quiz_data_raw # ë””ë²„ê¹…ì„ ìœ„í•´ raw data ì €ì¥
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if content_type == 'quiz':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  quiz_data = clean_and_load_json(quiz_data_raw)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if quiz_data and 'quiz_questions' in quiz_data:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.quiz_data = quiz_data
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.current_question = 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.quiz_submitted = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.quiz_results = [None] * len(quiz_data.get('quiz_questions',[]))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"**{topic}** - **{content_type_display}** Result:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(L["quiz_error_llm"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{L['quiz_original_response']}**:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.code(quiz_data_raw, language="json")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: # ì¼ë°˜ ì½˜í…ì¸  (ìš”ì•½, ì˜ˆì œ)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"**{topic}** - **{content_type_display}** Result:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(response.content)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Content Generation Error: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if quiz_data_raw:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{L['quiz_original_response']}**: {quiz_data_raw}")

Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.warning(L["warning_topic"])
Â  Â  else:
Â  Â  Â  Â  st.error(L["llm_error_init"])
Â  Â  Â  Â Â 
Â  Â  # í€´ì¦ˆ í’€ì´ ë Œë”ë§ì„ ë©”ì¸ ë£¨í”„ì—ì„œ ì¡°ê±´ë¶€ë¡œ ë‹¨ í•œ ë²ˆ í˜¸ì¶œ
Â  Â  is_quiz_ready = content_type == 'quiz' and 'quiz_data' in st.session_state and st.session_state.quiz_data
Â  Â  if is_quiz_ready and st.session_state.get('current_question', 0) < len(st.session_state.quiz_data.get('quiz_questions', [])):
Â  Â  Â  Â  render_interactive_quiz(st.session_state.quiz_data, st.session_state.language)

elif feature_selection == L["lstm_tab"]:
Â  Â  st.header(L["lstm_header"])
Â  Â  st.markdown(L["lstm_desc"])
Â  Â  st.warning(L["lstm_disabled_error"])
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  model, data = load_or_train_lstm()
Â  Â  Â  Â  look_back = 5
Â  Â  Â  Â  # ì˜ˆì¸¡
Â  Â  Â  Â  X_input = data[-look_back:]
Â  Â  Â  Â  X_input = np.reshape(X_input, (1, look_back, 1))
Â  Â  Â  Â  predicted_score = model.predict(X_input, verbose=0)[0][0]

Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.subheader("ğŸ“ˆ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼")
Â  Â  Â  Â  col_score, col_chart = st.columns([1, 2])
Â  Â  Â  Â Â 
Â  Â  Â  Â  with col_score:
Â  Â  Â  Â  Â  Â  st.metric("í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„", f"{predicted_score:.1f}ì ")
Â  Â  Â  Â  Â  Â  st.info(f"ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!")

Â  Â  Â  Â  with col_chart:
Â  Â  Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(8, 4))
Â  Â  Â  Â  Â  Â  ax.plot(data, label='Past Scores', marker='o')
Â  Â  Â  Â  Â  Â  ax.plot(len(data), predicted_score, label='Predicted Next Score', marker='*', color='red', markersize=10)
Â  Â  Â  Â  Â  Â  ax.set_title('Learning Achievement Prediction (LSTM)')
Â  Â  Â  Â  Â  Â  ax.set_xlabel('Time (Quiz attempts)')
Â  Â  Â  Â  Â  Â  ax.set_ylabel('Score (0-100)')
Â  Â  Â  Â  Â  Â  ax.legend()
Â  Â  Â  Â  Â  Â  st.pyplot(fig)

Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"LSTM ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
