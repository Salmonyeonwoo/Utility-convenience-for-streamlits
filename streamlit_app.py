# ========================================
# streamlit_app_full_integration_final.py
# ì™„ì„±ë³¸: Streamlit ì•± â€” Whisper ì „ì‚¬, Firestore ë©”íƒ€ë°ì´í„°, GCS ì˜¤ë””ì˜¤ ì €ìž¥, 
# ì´ë ¥ ëª©ë¡/ìž¬ìƒ/ìž¬ì „ì‚¬/ì‚­ì œ, ì‹œë®¬ë ˆì´í„° í†µí•©, ê°œì„ ëœ UI ë° ì™„ë²½í•œ ë‹¤êµ­ì–´ ì§€ì›
# ========================================

import streamlit as st
import os
import tempfile
import time
import json
import re
import base64
import io
import numpy as np
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from datetime import datetime, timedelta, timezone 
from openai import OpenAI

# â­ Firebase / GCS
import firebase_admin
from firebase_admin import credentials, firestore, initialize_app, get_app
from google.cloud import storage
from google.cloud.exceptions import NotFound 
from google.cloud import firestore as gcp_firestore
from google.cloud.firestore import Query 

# LangChain Imports
from langchain.chains import ConversationalRetrievalChain, ConversationChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.schema.document import Document
from langchain.prompts import PromptTemplate 

# -----------------------------
# Config & I18N (ë‹¤êµ­ì–´ ì§€ì›)
# -----------------------------
DEFAULT_LANG = "ko"
if 'language' not in st.session_state:
    st.session_state.language = DEFAULT_LANG

LANG = {
    "ko": {
        "title": "ê°œì¸ ë§žì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜ (ìŒì„± ë° DB í†µí•©)",
        "sidebar_title": "ðŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìžë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìžë£Œ ë¶„ì„ ì‹œìž‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§žì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°", 
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
        "rag_input_placeholder": "í•™ìŠµ ìžë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
        "content_header": "ë§žì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§žì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ìž…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ëž˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
        "embed_fail": "ìž„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "warning_no_files": "ë¨¼ì € í•™ìŠµ ìžë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìžë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
        "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
        "check_answer": "ì •ë‹µ í™•ì¸",
        "next_question": "ë‹¤ìŒ ë¬¸í•­",
        "correct_answer": "ì •ë‹µìž…ë‹ˆë‹¤! ðŸŽ‰",
        "incorrect_answer": "ì˜¤ë‹µìž…ë‹ˆë‹¤. ðŸ˜ž",
        "correct_is": "ì •ë‹µ",
        "explanation": "í•´ì„¤",
        "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
        "score": "ì ìˆ˜",
        "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
        "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
        "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
        "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚", 
        "db_save_complete": "(DB ì €ìž¥ ì™„ë£Œ)", 
        "data_analysis_progress": "ìžë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘...", 
        "response_generating": "ë‹µë³€ ìƒì„± ì¤‘...", 
        "lstm_result_header": "í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼",
        "lstm_score_metric": "í˜„ìž¬ ì˜ˆì¸¡ ì„±ì·¨ë„",
        "lstm_score_info": "ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ìž…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!",
        "lstm_rerun_button": "ìƒˆë¡œìš´ ê°€ìƒ ë°ì´í„°ë¡œ ì˜ˆì¸¡",
        
        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— ëŒ€í•´ AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
        "customer_type_label": "ê³ ê° ì„±í–¥",
        "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
        "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
        "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”ã€‚",
        "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ëŠ” ê²½ìš°, ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
        "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
        "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
        "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
        "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
        "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
        "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ìž¬ìƒ ì™„ë£Œ!",
        "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
        "history_expander_title": "ðŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê°œ)", 
        "initial_query_sample": "í”„ëž‘ìŠ¤ íŒŒë¦¬ì— ë„ì°©í–ˆëŠ”ë°, í´ë£©ì—ì„œ êµ¬ë§¤í•œ eSIMì´ í™œì„±í™”ê°€ ì•ˆ ë©ë‹ˆë‹¤. ì—°ê²°ì´ ì•ˆ ë¼ì„œ ë„ˆë¬´ ê³¤ëž€í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?", 
        "button_mic_input": "ðŸŽ™ ìŒì„± ìž…ë ¥",
        "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ ì±„íŒ…ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤ã€‚",
        "prompt_survey": "ê³ ê° ë¬¸ì˜ ì„¼í„°ì— ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìžˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹­ì‹œì˜¤ã€‚",
        "customer_closing_confirm": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?",
        "customer_positive_response": "ì¢‹ì€ ë§ì”€/ì¹œì ˆí•œ ìƒë‹´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤ã€‚",
        "button_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ì¡°ì‚¬ ìš”ì²­)",
        "agent_response_header": "âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ",
        "agent_response_placeholder": "ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš” (ê³ ê°ì˜ í•„ìˆ˜ ì •ë³´ë¥¼ ìš”ì²­/í™•ì¸í•˜ê±°ë‚˜, ë¬¸ì œ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”)",
        "send_response_button": "ì‘ë‹µ ì „ì†¡",
        "request_rebuttal_button": "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­",
        "new_simulation_button": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œìž‘",
        "history_selectbox_label": "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
        "history_load_button": "ì„ íƒëœ ì´ë ¥ ë¡œë“œ",
        "delete_history_button": "âŒ ëª¨ë“  ì´ë ¥ ì‚­ì œ", 
        "delete_confirm_message": "ì •ë§ë¡œ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ã€‚", 
        "delete_confirm_yes": "ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤", 
        "delete_confirm_no": "ì•„ë‹ˆì˜¤, ìœ ì§€í•©ë‹ˆë‹¤", 
        "delete_success": "âœ… ëª¨ë“  ìƒë‹´ ì´ë ¥ ì‚­ì œ ì™„ë£Œ!",
        "deleting_history_progress": "ì´ë ¥ ì‚­ì œ ì¤‘...", 
        "search_history_label": "ì´ë ¥ í‚¤ì›Œë“œ ê²€ìƒ‰", 
        "date_range_label": "ë‚ ì§œ ë²”ìœ„ í•„í„°", 
        "no_history_found": "ê²€ìƒ‰ ì¡°ê±´ì— ë§žëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        
        # â­ ìŒì„± ê¸°ë¡ í†µí•© ê´€ë ¨ í‚¤ (Voice/GCS)
        "voice_rec_header": 'ìŒì„± ê¸°ë¡ & ê´€ë¦¬',
        "record_help": 'ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.',
        "uploaded_file": 'ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ',
        "rec_list_title": 'ì €ìž¥ëœ ìŒì„± ê¸°ë¡ (Whisper/GCS)',
        "transcribe_btn": 'ì „ì‚¬(Whisper)',
        "save_btn": 'ìŒì„± ê¸°ë¡ ì €ìž¥',
        "transcribing": 'ìŒì„± ì „ì‚¬ ì¤‘...',
        "transcript_result": 'ì „ì‚¬ ê²°ê³¼:',
        "transcript_text": 'ì „ì‚¬ í…ìŠ¤íŠ¸',
        "openai_missing": 'OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.',
        "whisper_client_error": "âŒ ì˜¤ë¥˜: Whisper API Clientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.",
        "whisper_auth_error": "âŒ Whisper API ì¸ì¦ ì‹¤íŒ¨: API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”.",
        "whisper_format_error": "âŒ ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ìž…ë‹ˆë‹¤.",
        "whisper_success": "âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ! í…ìŠ¤íŠ¸ ì°½ì„ í™•ì¸í•˜ì„¸ìš”.",
        "playback": 'ë…¹ìŒ ìž¬ìƒ',
        "retranscribe": 'ìž¬ì „ì‚¬',
        "delete": 'ì‚­ì œ',
        "no_records": 'ì €ìž¥ëœ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.',
        "gcs_missing": 'GCS ë²„í‚·ì´ ì„¤ì •ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤. Secretsì— GCS_BUCKET_NAMEì„ ì¶”ê°€í•˜ì„¸ìš”.',
        "saved_success": 'ì €ìž¥ ì™„ë£Œ!',
        "delete_confirm_rec": 'ì •ë§ë¡œ ì´ ìŒì„± ê¸°ë¡ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? GCS íŒŒì¼ë„ ì‚­ì œë©ë‹ˆë‹¤.',
        "gcs_init_fail": 'GCS ì´ˆê¸°í™” ì‹¤íŒ¨. ê¶Œí•œ ë° ë²„í‚· ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.',
        "firebase_init_fail": 'Firebase Admin ì´ˆê¸°í™” ì‹¤íŒ¨.',
        "upload_fail": 'GCS ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨',
        "gcs_not_conf": 'GCS ë¯¸ì„¤ì • ë˜ëŠ” ì˜¤ë””ì˜¤ ì—†ìŒ',
        "gcs_playback_fail": 'ì˜¤ë””ì˜¤ ìž¬ìƒ ì‹¤íŒ¨',
        "gcs_no_audio": 'ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ (GCS ë¯¸ì„¤ì •)',
        "error": 'ì˜¤ë¥˜:',
    },
    "en": {
        "title": "Personalized AI Study Coach (Voice & DB Integration)",
        "sidebar_title": "ðŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction",
        "simulator_tab": "AI Customer Response Simulator", 
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answers questions based on the uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "llm_error_key": "âš ï¸ Warning: GEMINI API Key is not set. Please set 'GEMINI_API_KEY' in Streamlit Secrets.",
        "llm_error_init": "LLM initialization error: Please check your API key.",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content tailored to your topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "10 Multiple-Choice Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Based Achievement Prediction",
        "lstm_desc": "Trains an LSTM model on hypothetical past quiz scores to predict future achievement.",
        "lang_select": "Select Language",
        "embed_success": "Learning DB built with {count} chunks!",
        "embed_fail": "Embedding failed: Free tier quota exceeded or network issue.",
        "warning_no_files": "Please upload study materials first.",
        "warning_rag_not_ready": "RAG is not ready. Upload materials and click Start Analysis.",
        "quiz_fail_structure": "Quiz data structure is incorrect.",
        "select_answer": "Select answer",
        "check_answer": "Confirm answer",
        "next_question": "Next Question",
        "correct_answer": "Correct! ðŸŽ‰",
        "incorrect_answer": "Incorrect. ðŸ˜ž",
        "correct_is": "Correct answer",
        "explanation": "Explanation",
        "quiz_complete": "Quiz completed!",
        "score": "Score",
        "retake_quiz": "Retake Quiz",
        "quiz_error_llm": "Quiz generation failed: LLM did not return a valid JSON format. Check the original LLM response.",
        "quiz_original_response": "Original LLM Response",
        "firestore_loading": "Loading RAG index from database...",
        "firestore_no_index": "Could not find existing RAG index in database. Please upload files and create a new one.", 
        "db_save_complete": "(DB Save Complete)", 
        "data_analysis_progress": "Analyzing materials and building learning DB...", 
        "response_generating": "Generating response...", 
        "lstm_result_header": "Prediction Results",
        "lstm_score_metric": "Current Predicted Achievement",
        "lstm_score_info": "Your next estimated quiz score is **{predicted_score:.1f}**. Maintain or improve your learning progress!",
        "lstm_rerun_button": "Predict with New Hypothetical Data",

        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AI Customer Response Simulator",
        "simulator_desc": "Provides AI-generated response drafts and guidelines for handling challenging customer inquiries.",
        "customer_query_label": "Customer Query (Link optional)",
        "customer_type_label": "Customer Sentiment",
        "customer_type_options": ["General Inquiry", "Challenging Customer", "Highly Dissatisfied Customer"],
        "button_simulate": "Request Response Advice",
        "simulation_warning_query": "Please enter the customer's query.",
        "simulation_no_key_warning": "âš ï¸ API Key is missing. Response generation cannot proceed.",
        "simulation_advice_header": "AI Response Guidelines",
        "simulation_draft_header": "Recommended Response Draft",
        "button_listen_audio": "Listen to Audio",
        "tts_status_ready": "Ready to listen",
        "tts_status_generating": "Generating audio...",
        "tts_status_success": "âœ… Audio playback complete!",
        "tts_status_error": "âŒ TTS API error occurred",
        "history_expander_title": "ðŸ“ Load Previous Simulation History (Last 10)", 
        "initial_query_sample": "I arrived in Paris, France, but the eSIM I bought from Klook won't activate. I'm really struggling to get connected. What should I do?", 
        "button_mic_input": "ðŸŽ™ Voice Input",
        "prompt_customer_end": "As there are no further inquiries, we will now end this chat session.",
        "prompt_survey": "Thank you for contacting our Customer Support Center. Please feel free to contact us anytime if you have any additional questions.",
        "customer_closing_confirm": "Is there anything else we can assist you with today?",
        "customer_positive_response": "Thank you for your kind understanding/friendly advice.",
        "button_end_chat": "End Chat (Request Survey)",
        "agent_response_header": "âœï¸ Agent Response",
        "agent_response_placeholder": "Respond to the customer (Request/confirm essential information or provide solution steps)",
        "send_response_button": "Send Response",
        "request_rebuttal_button": "Request Customer's Next Reaction",
        "new_simulation_button": "Start New Simulation",
        "history_selectbox_label": "Select history to load:",
        "history_load_button": "Load Selected History",
        "delete_history_button": "âŒ Delete All History", 
        "delete_confirm_message": "Are you sure you want to delete ALL simulation history? This action cannot be undone.", 
        "delete_confirm_yes": "Yes, Delete", 
        "delete_confirm_no": "No, Keep", 
        "delete_success": "âœ… Successfully deleted!", 
        "deleting_history_progress": "Deleting history...", 
        "search_history_label": "Search History by Keyword", 
        "date_range_label": "Date Range Filter", 
        "no_history_found": "No history found matching the criteria.",

        # â­ ìŒì„± ê¸°ë¡ í†µí•© ê´€ë ¨ í‚¤ (Voice/GCS)
        "voice_rec_header": 'Voice Record & Management',
        "record_help": 'Press the microphone button to record or upload a file.',
        "uploaded_file": 'Upload Audio File',
        "rec_list_title": 'Saved Voice Records (Whisper/GCS)',
        "transcribe_btn": 'Transcribe (Whisper)',
        "save_btn": 'Save Voice Record',
        "transcribing": 'Transcribing voice...',
        "transcript_result": 'Transcription Result:',
        "transcript_text": 'Transcribed Text',
        "openai_missing": 'OpenAI API Key is missing. Set OPENAI_API_KEY in Secrets.',
        "whisper_client_error": "âŒ Error: Whisper API Client not initialized. Check OPENAI_API_KEY in Secrets.",
        "whisper_auth_error": "âŒ Whisper API Authentication failed: Check your API Key.",
        "whisper_format_error": "âŒ Error: Unsupported audio format.",
        "whisper_success": "âœ… Voice transcription complete! Check the text box.",
        "playback": 'Playback Recording',
        "retranscribe": 'Re-transcribe',
        "delete": 'Delete',
        "no_records": 'No voice records saved yet.',
        "gcs_missing": 'GCS bucket is not configured. Add GCS_BUCKET_NAME to Secrets.',
        "saved_success": 'Save successful!',
        "delete_confirm_rec": 'Are you sure you want to delete this voice record? The GCS file will also be deleted.',
        "gcs_init_fail": 'GCS initialization failed. Check permissions and bucket name.',
        "firebase_init_fail": 'Firebase Admin initialization failed.',
        "upload_fail": 'GCS audio file upload failed',
        "gcs_not_conf": 'GCS not configured or audio not available',
        "gcs_playback_fail": 'Audio playback failed',
        "gcs_no_audio": 'No audio file (GCS not configured)',
        "error": 'Error:',
    },
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»DBçµ±åˆ)",
        "sidebar_title": "ðŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
        "button_start_analysis": "è³‡æ–™åˆ†æžé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
        "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "content_tab": "ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼", 
        "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
        "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›žç­”ã—ã¾ã™ã€‚",
        "rag_input_placeholder": "å­¦ç¿’è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
        "llm_error_key": "âš ï¸ è­¦å‘Š: GEMINI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secretsã«'GEMINI_API_KEY'ã‚’è¨­ç½®ã—ã¦ãã ã•ã„ã€‚",
        "llm_error_init": "LLMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒžã¨é›£æ˜“åº¦ã«åˆã‚ã›ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒž",
        "level_label": "é›£æ˜“åº¦",
        "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼",
        "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
        "content_options": ["æ ¸å¿ƒè¦ç´„ãƒŽãƒ¼ãƒˆ", "é¸æŠžå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢"],
        "button_generate": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒžã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "lstm_header": "LSTMãƒ™ãƒ¼ã‚¹é”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "lstm_desc": "ä»®æƒ³ã®éŽåŽ»ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€LSTMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦å°†æ¥ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—è¡¨ç¤ºã—ã¾ã™ã€‚",
        "lang_select": "è¨€èªžé¸æŠž",
        "embed_success": "å…¨{count}ãƒãƒ£ãƒ³ã‚¯ã§å­¦ç¿’DBæ§‹ç¯‰å®Œäº†!",
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—: ãƒ•ãƒªãƒ¼ãƒ†ã‚£ã‚¢ã®ã‚¯ã‚©ãƒ¼ã‚¿è¶…éŽã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã€‚",
        "warning_no_files": "ã¾ãšå­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "warning_rag_not_ready": "RAGãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€åˆ†æžé–‹å§‹ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚",
        "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "select_answer": "æ­£è§£ã‚’é¸æŠžã—ã¦ãã ã•ã„",
        "check_answer": "æ­£è§£ã‚’ç¢ºèª",
        "next_question": "æ¬¡ã®è³ªå•",
        "correct_answer": "æ­£è§£ã§ã™! ðŸŽ‰",
        "incorrect_answer": "ä¸æ­£è§£ã§ã™ã€‚ðŸ˜ž",
        "correct_is": "æ­£è§£",
        "explanation": "è§£èª¬",
        "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
        "score": "ã‚¹ã‚³ã‚¢",
        "retake_quiz": "ã‚¯ã‚¤ã‚ºã‚’å†æŒ‘æˆ¦",
        "quiz_error_llm": "LLMãŒæ­£ã—ã„JSONã®å½¢å¼ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã®ã§ã€ã‚¯ã‚¤ã‚ºã®ç”ŸæˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚",
        "quiz_original_response": "LLM åŽŸæœ¬å¿œç­”",
        "firestore_loading": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...",
        "firestore_no_index": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ—¢å­˜ã®RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°ã—ãä½œæˆã—ã¦ãã ã•ã„ã€‚", 
        "db_save_complete": "(DBä¿å­˜å®Œäº†)", 
        "data_analysis_progress": "è³‡æ–™åˆ†æžãŠã‚ˆã³å­¦ç¿’DBæ§‹ç¯‰ä¸­...", 
        "response_generating": "å¿œç­”ç”Ÿæˆä¸­...", 
        "lstm_result_header": "é”æˆåº¦äºˆæ¸¬çµæžœ",
        "lstm_score_metric": "ç¾åœ¨ã®äºˆæ¸¬é”æˆåº¦",
        "lstm_score_info": "æ¬¡ã®ã‚¯ã‚¤ã‚ºã®æŽ¨å®šã‚¹ã‚³ã‚¢ã¯ç´„ **{predicted_score:.1f}ç‚¹**ã§ã™ã€‚å­¦ç¿’ã®æˆæžœã‚’ç¶­æŒã¾ãŸã¯å‘ä¸Šã•ã›ã¦ãã ã•ã„ï¼",
        "lstm_rerun_button": "æ–°ã—ã„ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬",

        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "simulator_desc": "é›£ã—ã„é¡§å®¢ã®å•ã„åˆã‚ã›ã«å¯¾ã—ã¦ã€AIã«ã‚ˆã‚‹å¯¾å¿œæ¡ˆã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚",
        "customer_query_label": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ï¼ˆãƒªãƒ³ã‚¯ä»»æ„ï¼‰",
        "customer_type_label": "é¡§å®¢ã®å‚¾å‘",
        "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "æ‰‹ã”ã‚ã„é¡§å®¢", "éžå¸¸ã«ä¸æº€ãªé¡§å®¢"],
        "button_simulate": "å¯¾å¿œã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¦æ±‚",
        "simulation_warning_query": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "simulation_no_key_warning": "âš ï¸ APIã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å¿œç­”ã®ç”Ÿæˆã¯ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚",
        "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
        "simulation_draft_header": "æŽ¨å¥¨ã•ã‚Œã‚‹å¯¾å¿œè‰æ¡ˆ",
        "button_listen_audio": "éŸ³å£°ã§èžã",
        "tts_status_ready": "éŸ³å£°å†ç”Ÿã®æº–å‚™ãŒã§ãã¾ã—ãŸ",
        "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
        "tts_status_success": "âœ… éŸ³å£°å†ç”Ÿå®Œäº†!",
        "tts_status_error": "âŒ TTS APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "history_expander_title": "ðŸ“ ä»¥å‰ã®å¯¾å¿œå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰ (æœ€æ–° 10ä»¶)", 
        "initial_query_sample": "ãƒ•ãƒ©ãƒ³ã‚¹ã®ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã§è³¼å…¥ã—ãŸeSIMãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚æŽ¥ç¶šã§ããªãã¦å›°ã£ã¦ã„ã¾ã™ã€‚ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ", 
        "button_mic_input": "ðŸŽ™ éŸ³å£°å…¥åŠ›",
        "prompt_customer_end": "ãŠå®¢æ§˜ã‹ã‚‰ã®è¿½åŠ ã®ãŠå•ã„åˆã‚ã›ãŒãªã„ãŸã‚ã€æœ¬ãƒãƒ£ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆã‚’çµ‚äº†ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
        "prompt_survey": "ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚è¿½åŠ ã®ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã„ã¤ã§ã‚‚ã”é€£çµ¡ãã ã•ã„ã€‚",
        "customer_closing_confirm": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
        "customer_positive_response": "è¦ªåˆ‡ãªã”å¯¾å¿œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚",
        "button_end_chat": "å¯¾å¿œçµ‚äº† (ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã‚’ä¾é ¼)",
        "agent_response_header": "âœï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”",
        "agent_response_placeholder": "é¡§å®¢ã«è¿”ä¿¡ (å¿…é ˆæƒ…å ±ã®è¦æ±‚/ç¢ºèªã€ã¾ãŸã¯è§£æ±ºç­–ã®æç¤º)",
        "send_response_button": "å¿œç­”é€ä¿¡",
        "request_rebuttal_button": "é¡§å®¢ã®æ¬¡ã®åå¿œã‚’è¦æ±‚", 
        "new_simulation_button": "æ–°ã—ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹",
        "history_selectbox_label": "å±¥æ­´ã‚’é¸æŠžã—ã¦ãƒ­ãƒ¼ãƒ‰:",
        "history_load_button": "é¸æŠžã•ã‚ŒãŸå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰",
        "delete_history_button": "âŒ å…¨å±¥æ­´ã‚’å‰Šé™¤", 
        "delete_confirm_message": "æœ¬å½“ã«ã™ã¹ã¦ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’å‰Šé™¤ã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿã“ã®æ“ä½œã¯å…ƒã«æˆ»ã›ã¾ã›ã‚“ã€‚", 
        "delete_confirm_yes": "ã¯ã„ã€å‰Šé™¤ã—ã¾ã™", 
        "delete_confirm_no": "ã„ã„ãˆã€ç¶­æŒã—ã¾ã™", 
        "delete_success": "âœ… å‰Šé™¤ãŒå®Œäº†ã•ã‚Œã¾ã—ãŸ!", 
        "deleting_history_progress": "å±¥æ­´å‰Šé™¤ä¸­...", 
        "search_history_label": "å±¥æ­´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", 
        "date_range_label": "æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", 
        "no_history_found": "æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",

        # â­ ìŒì„± ê¸°ë¡ í†µí•© ê´€ë ¨ í‚¤ (Voice/GCS)
        "voice_rec_header": 'éŸ³å£°è¨˜éŒ²ã¨ç®¡ç†',
        "record_help": 'ãƒžã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦éŒ²éŸ³ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚',
        "uploaded_file": 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰',
        "rec_list_title": 'ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ² (Whisper/GCS)',
        "transcribe_btn": 'è»¢å†™(Whisper)',
        "save_btn": 'éŸ³å£°è¨˜éŒ²ã‚’ä¿å­˜',
        "transcribing": 'éŸ³å£°è»¢å†™ä¸­...',
        "transcript_result": 'è»¢å†™çµæžœ:',
        "transcript_text": 'è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆ',
        "openai_missing": 'OpenAI APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Secretsã«OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚',
        "whisper_client_error": "âŒ ã‚¨ãƒ©ãƒ¼: Whisper API ClientãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Secretsã®OPENAI_API_KEYã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "whisper_auth_error": "âŒ Whisper APIèªè¨¼å¤±æ•—: APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "whisper_format_error": "âŒ ã‚¨ãƒ©ãƒ¼: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„éŸ³å£°ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã§ã™ã€‚",
        "whisper_success": "âœ… éŸ³å£°è»¢å†™å®Œäº†ï¼ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚",
        "playback": 'éŒ²éŸ³å†ç”Ÿ',
        "retranscribe": 'å†è»¢å†™',
        "delete": 'å‰Šé™¤',
        "no_records": 'ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚',
        "gcs_missing": 'GCSãƒã‚±ãƒƒãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Secretsã«GCS_BUCKET_NAMEã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚',
        "saved_success": 'ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼',
        "delete_confirm_rec": 'æœ¬å½“ã«ã“ã®éŸ³å£°è¨˜éŒ²ã‚’å‰Šé™¤ã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼ŸGCSãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‰Šé™¤ã•ã‚Œã¾ã™ã€‚',
        "gcs_init_fail": 'GCSã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ¨©é™ã¨ãƒã‚±ãƒƒãƒˆåã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
        "firebase_init_fail": 'Firebase Adminã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚',
        "upload_fail": 'GCSéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ',
        "gcs_not_conf": 'GCSãŒæœªè¨­å®šã‹ã€éŸ³å£°ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“',
        "gcs_playback_fail": 'éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ',
        "gcs_no_audio": 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãªã— (GCSæœªè¨­å®š)',
        "error": 'ã‚¨ãƒ©ãƒ¼:',
    }
}


# -----------------------------
# 1. Firebase Admin, GCS, OpenAI Initialization
# -----------------------------

def _load_service_account_from_secrets():
    # Expect a JSON string in st.secrets['FIREBASE_SERVICE_ACCOUNT_JSON']
    if hasattr(st, 'secrets') and st.secrets and 'FIREBASE_SERVICE_ACCOUNT_JSON' in st.secrets:
        raw = st.secrets['FIREBASE_SERVICE_ACCOUNT_JSON']
        if isinstance(raw, str):
            try:
                return json.loads(raw)
            except Exception:
                return None
        elif isinstance(raw, dict):
            return raw
    return None

@st.cache_resource(ttl=None)
def init_firebase_admin():
    """Secretsì—ì„œ ë¡œë“œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    L = LANG[st.session_state.language] 
    sa_info, error_message = _get_admin_credentials()
    if error_message:
        st.error(f"âŒ Firebase Secret ì˜¤ë¥˜: {error_message}")
        return None

    try:
        get_app()
    except ValueError:
        pass
    else:
        try:
            return firestore.client()
        except Exception as e:
            st.error(f"ðŸ”¥ Firebase í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    try:
        cred = credentials.Certificate(sa_info)
        firebase_admin.initialize_app(cred, {
            'projectId': sa_info.get('project_id')
        })
        db_client = firestore.client()
        st.session_state["db"] = db_client
        return db_client
    except Exception as e:
        st.error(f"ðŸ”¥ {L['firebase_init_fail']}: {e}")
        return None

@st.cache_resource
def init_gcs_client(L):
    sa = _load_service_account_from_secrets()
    if not sa:
        return None
    
    gcs_client = None
    try:
        gcs_bucket_name = st.secrets.get('GCS_BUCKET_NAME') or os.environ.get('GCS_BUCKET_NAME')
        
        if gcs_bucket_name:
            # Set credentials environment variable explicitly for GCS client to use the service account
            # This is critical for environments like Streamlit Cloud
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.json')
            tmp.write(json.dumps(sa).encode('utf-8'))
            tmp.flush()
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = tmp.name
            
            gcs_client = storage.Client()
            # Test bucket access (optional, but good for early warning)
            # gcs_client.bucket(gcs_bucket_name).exists()
            return gcs_client
        else:
            return None
    except Exception as e:
        # st.warning(f"{L['gcs_init_fail']}: {e}") # Suppress verbose warning on every rerun
        return None

@st.cache_resource
def init_openai_client(L):
    openai_key = st.secrets.get('OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')
    if openai_key:
        try:
            return OpenAI(api_key=openai_key)
        except Exception:
            # st.warning(f"OpenAI client init error: {e}") # Suppress verbose warning
            return None
    return None

def get_gcs_bucket_name():
    return st.secrets.get('GCS_BUCKET_NAME') or os.environ.get('GCS_BUCKET_NAME')

# -----------------------------
# 2. GCS, Firestore, Whisper Helpers (í†µí•©ëœ í•¨ìˆ˜)
# -----------------------------

def upload_audio_to_gcs(bucket_name: str, blob_name: str, audio_bytes: bytes, content_type: str = 'audio/webm'):
    L = LANG[st.session_state.language]
    gcs_client = init_gcs_client(L)
    if not gcs_client:
        raise RuntimeError(L['gcs_not_conf'])
    bucket = gcs_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_string(audio_bytes, content_type=content_type)
    return f'gs://{bucket_name}/{blob_name}' 

def download_audio_from_gcs(bucket_name: str, blob_name: str) -> bytes:
    L = LANG[st.session_state.language]
    gcs_client = init_gcs_client(L)
    if not gcs_client:
        raise RuntimeError(L['gcs_not_conf'])
    try:
        bucket = gcs_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        return blob.download_as_bytes()
    except NotFound:
        raise FileNotFoundError(f"GCS Blob not found: {blob_name}")
    except Exception as e:
        raise RuntimeError(f"{L['gcs_playback_fail']}: {e}")

def save_audio_record(db, bucket_name, audio_bytes: bytes, filename: str, transcript_text: str, meta: dict = None, mime_type: str = 'audio/webm'):
    L = LANG[st.session_state.language]
    if not db:
        raise RuntimeError('Firestore not initialized')

    ts = datetime.now(timezone.utc)
    doc_ref = db.collection('voice_records').document()
    blob_name = f"voice_records/{doc_ref.id}/{filename}"

    gcs_path = None
    if bucket_name and init_gcs_client(L):
        try:
            gcs_path = upload_audio_to_gcs(bucket_name, blob_name, audio_bytes, mime_type)
        except Exception as e:
            st.warning(f"{L['upload_fail']}: {e}")
            gcs_path = None
    else:
        st.warning(L['gcs_missing'])

    data = {
        'created_at': ts,
        'filename': filename,
        'size': len(audio_bytes),
        'gcs_path': gcs_path,
        'transcript': transcript_text,
        'mime_type': mime_type, # Add mime_type
        'language': st.session_state.language,
        'meta': meta or {}
    }

    doc_ref.set(data)
    return doc_ref.id

def delete_audio_record(db, bucket_name, doc_id: str):
    L = LANG[st.session_state.language]
    doc_ref = db.collection('voice_records').document(doc_id)
    doc = doc_ref.get()
    if not doc.exists:
        return False
    data = doc.to_dict()
    
    gcs_client = init_gcs_client(L)
    # delete GCS blob
    try:
        if data.get('gcs_path') and gcs_client and bucket_name:
            blob_name = data['gcs_path'].split(f'gs://{bucket_name}/')[-1]
            bucket = gcs_client.bucket(bucket_name)
            blob = bucket.blob(blob_name)
            blob.delete()
    except Exception as e:
        st.warning(f"GCS delete warning: {e}")
    
    # delete firestore doc
    doc_ref.delete()
    return True

def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = 'audio/webm'):
    L = LANG[st.session_state.language]
    openai_client = init_openai_client(L)
    if openai_client is None:
        raise RuntimeError(L['openai_missing'])
    
    # Determine file extension
    ext = mime_type.split('/')[-1].lower() if '/' in mime_type else 'webm'
    
    # write to temp file
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f'.{ext}')
    tmp.write(audio_bytes)
    tmp.flush()
    tmp.close()
    
    try:
        with open(tmp.name, 'rb') as af:
            res = openai_client.audio.transcriptions.create(
                model='whisper-1', 
                file=af,
                response_format='text'
            )
        return res.strip() or ''
    except Exception as e:
        raise RuntimeError(f"{L['error']} Whisper: {e}")
    finally:
        try:
            os.remove(tmp.name)
        except Exception:
            pass


# -----------------------------
# 3. Firestore/RAG/LLM Helpers (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
# -----------------------------

def _get_admin_credentials():
    """Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if "FIREBASE_SERVICE_ACCOUNT_JSON" not in st.secrets:
        return None, "FIREBASE_SERVICE_ACCOUNT_JSON Secretì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    service_account_data = st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"]
    sa_info = None
    if isinstance(service_account_data, str):
        try:
            sa_info = json.loads(service_account_data.strip())
        except json.JSONDecodeError as e:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ JSON êµ¬ë¬¸ ì˜¤ë¥˜ìž…ë‹ˆë‹¤. ê°’ì„ í™•ì¸í•˜ì„¸ìš”. ìƒì„¸ ì˜¤ë¥˜: {e}"
    elif hasattr(service_account_data, 'get'):
        try:
            sa_info = dict(service_account_data)
        except Exception:
             return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì‹¤íŒ¨. íƒ€ìž…: {type(service_account_data)}"
    else:
        return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (Type: {type(service_account_data)})"
    
    if not sa_info.get("project_id") or not sa_info.get("private_key"):
        return None, "JSON ë‚´ 'project_id' ë˜ëŠ” 'private_key' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    return sa_info, None

@st.cache_resource(ttl=None)
def initialize_firestore_admin():
    sa_info, error_message = _get_admin_credentials()
    if error_message:
        st.error(f"âŒ Firebase Secret ì˜¤ë¥˜: {error_message}")
        return None
    try:
        get_app()
    except ValueError:
        pass 
    else:
        try:
            return firestore.client()
        except Exception as e:
            st.error(f"ðŸ”¥ Firebase í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    try:
        cred = credentials.Certificate(sa_info) 
        initialize_app(cred)
        db_client = firestore.client()
        st.session_state["db"] = db_client
        return db_client
    except Exception as e:
        st.error(f"ðŸ”¥ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¬¸ì œ. ì˜¤ë¥˜: {e}")
        return None

def save_index_to_firestore(db, vector_store, index_id="user_portfolio_rag"):
    if not db: return False
    temp_dir = tempfile.mkdtemp()
    try:
        vector_store.save_local(folder_path=temp_dir, index_name="index")
        with open(f"{temp_dir}/index.faiss", "rb") as f: faiss_bytes = f.read()
        with open(f"{temp_dir}/index.pkl", "rb") as f: metadata_bytes = f.read()
        encoded_data = {
            "faiss_data": base64.b64encode(faiss_bytes).decode('utf-8'),
            "metadata_data": base64.b64encode(metadata_bytes).decode('utf-8'),
            "timestamp": gcp_firestore.SERVER_TIMESTAMP 
        }
        db.collection("rag_indices").document(index_id).set(encoded_data)
        return True
    except Exception as e:
        print(f"Error saving index to Firestore: {e}")
        return False

def load_index_from_firestore(db, embeddings, index_id="user_portfolio_rag"):
    if not db: return None
    try:
        doc = db.collection("rag_indices").document(index_id).get()
        if not doc.exists: return None 
        encoded_data = doc.to_dict()
        faiss_bytes = base64.b64decode(encoded_data["faiss_data"])
        metadata_bytes = base64.b64decode(encoded_data["metadata_data"])
        temp_dir = tempfile.mkdtemp()
        with open(f"{temp_dir}/index.faiss", "wb") as f: f.write(faiss_bytes)
        with open(f"{temp_dir}/index.pkl", "wb") as f: f.write(metadata_bytes)
        vector_store = FAISS.load_local(folder_path=temp_dir, embeddings=embeddings, index_name="index")
        return vector_store
    except Exception as e:
        print(f"Error loading index from Firestore: {e}")
        return None

def save_simulation_history(db, initial_query, customer_type, messages):
    L = LANG[st.session_state.language]
    if not db: 
        st.sidebar.warning(L.get("firestore_no_db_connect", "âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ìž¥ ë¶ˆê°€"))
        return False
    history_data = [{k: v for k, v in msg.items()} for msg in messages]
    data = {
        "initial_query": initial_query,
        "customer_type": customer_type,
        "messages": history_data,
        "language_key": st.session_state.language, 
        "timestamp": firestore.SERVER_TIMESTAMP
    }
    try:
        db.collection("simulation_histories").add(data)
        st.sidebar.success(L.get("save_history_success", "âœ… ìƒë‹´ ì´ë ¥ì´ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤."))
        return True
    except Exception as e:
        st.sidebar.error(f"âŒ {L.get('save_history_fail', 'ìƒë‹´ ì´ë ¥ ì €ìž¥ ì‹¤íŒ¨')}: {e}")
        return False

def load_simulation_histories(db):
    current_lang_key = st.session_state.language 
    if not db: return []
    try:
        histories = (
            db.collection("simulation_histories")
            .where("language_key", "==", current_lang_key) 
            .order_by("timestamp", direction=Query.DESCENDING)
            .limit(10)
            .stream()
        )
        results = []
        for doc in histories:
            data = doc.to_dict()
            data['id'] = doc.id
            if 'messages' in data and isinstance(data['messages'], list) and data['messages']:
                results.append(data)
        return results
    except Exception as e:
        print(f"Error loading histories: {e}")
        return []

def delete_all_history(db):
    L = LANG[st.session_state.language] 
    if not db:
        st.error(L["firestore_no_index"])
        return
    try:
        docs = db.collection("simulation_histories").stream()
        for doc in docs:
            doc.reference.delete()
        st.session_state.simulator_messages = []
        st.session_state.simulator_memory.clear()
        st.session_state.show_delete_confirm = False
        st.success(L["delete_success"]) 
        st.rerun()
    except Exception as e:
        st.error(f"{L.get('delete_fail', 'ì´ë ¥ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ')}: {e}")

# -----------------------------
# 4. LLM/Content Helpers (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
# -----------------------------

def clean_and_load_json(text):
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None

def get_mock_response_data(lang_key, customer_type):
    L = LANG[lang_key]
    # (Mock data logic remains the same, using L for localization)
    if lang_key == 'ko':
        initial_check = "ê³ ê°ë‹˜ì˜ ì„±í•¨, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼ ë“± ì •í™•í•œ ì—°ë½ì²˜ ì •ë³´ë¥¼ í™•ì¸í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤."
        tone = "ê³µê° ë° í•´ê²° ì¤‘ì‹¬"
        advice = "ì´ ê³ ê°ì€ {customer_type} ì„±í–¥ì´ì§€ë§Œ, ë¬¸ì œ í•´ê²°ì„ ê°„ì ˆížˆ ì›í•©ë‹ˆë‹¤. ê³µê°ê³¼ í•¨ê»˜, ë¬¸ì œ í•´ê²°ì— í•„ìˆ˜ì ì¸ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ìš”ì²­í•´ì•¼ í•©ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ì‚¬ì¡±ì„ í”¼í•˜ê³  ì‹ ë¢°ë¥¼ ì£¼ë„ë¡ í•˜ì„¸ìš”."
        draft = f"{initial_check}\n\n> ê³ ê°ë‹˜, ë¶ˆíŽ¸ì„ ê²ªê²Œ í•´ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„ížˆ ì´í•´í•˜ê³  ìžˆìŠµë‹ˆë‹¤.\n> ë¬¸ì œ í•´ê²°ì„ ìœ„í•´, ì•„ëž˜ ì„¸ ê°€ì§€ í•„ìˆ˜ ì •ë³´ë¥¼ í™•ì¸í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ì´ ì •ë³´ê°€ ìžˆì–´ì•¼ ê³ ê°ë‹˜ ìƒí™©ì— ë§žëŠ” ì •í™•í•œ í•´ê²°ì±…ì„ ì œì‹œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n> 1. ë¬¸ì œ ë°œìƒê³¼ ê´€ë ¨ëœ ìƒí’ˆ/ì„œë¹„ìŠ¤ì˜ **ì •í™•í•œ ëª…ì¹­ ë° ì˜ˆì•½ ë²ˆí˜¸**\n> 2. í˜„ìž¬ **ë¬¸ì œ ìƒí™©**ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª…\n> 3. ì´ë¯¸ **ì‹œë„í•˜ì‹  í•´ê²° ë‹¨ê³„**\n\n> ê³ ê°ë‹˜ê³¼ì˜ ì›í™œí•œ ì†Œí†µì„ í†µí•´ ì‹ ì†í•˜ê²Œ ë¬¸ì œ í•´ê²°ì„ ë•ê² ìŠµë‹ˆë‹¤. ë‹µë³€ ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤."
    elif lang_key == 'en':
        initial_check = "Could you please confirm your accurate contact details, such as your full name, phone number, and email address?"
        tone = "Empathy and Solution-Focused"
        advice = "This customer is {customer_type} but desperately wants a solution. Show empathy, but clearly request the essential information needed for troubleshooting. Be direct and build trust."
        draft = f"{initial_check}\n\n> Dear Customer, I sincerely apologize for the inconvenience you are facing. I completely understand your frustration.\n> To proceed with troubleshooting, please confirm the three essential pieces of information below. This data is critical for providing you with the correct, tailored solution:\n> 1. The **exact name and booking number** of the product/service concerned.\n> 2. A specific description of the **current issue**.\n> 3. Any **troubleshooting steps already attempted**.\n\n> We aim to resolve your issue as quickly as possible with your cooperation. We await your response."
    elif lang_key == 'ja':
        initial_check = "ãŠå®¢æ§˜ã®æ°åã€ãŠé›»è©±ç•ªå·ã€Eãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ã€æ­£ç¢ºãªé€£çµ¡å…ˆæƒ…å ±ã‚’ç¢ºèªã•ã›ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚"
        tone = "å…±æ„Ÿã¨è§£æ±ºä¸­å¿ƒ"
        advice = "ã“ã®ãŠå®¢æ§˜ã¯{customer_type}å‚¾å‘ã§ã™ãŒã€å•é¡Œã®è§£æ±ºã‚’å¼·ãæœ›ã‚“ã§ã„ã¾ã™ã€‚å…±æ„Ÿã‚’ç¤ºã—ã¤ã¤ã‚‚ã€å•é¡Œè§£æ±ºã«ä¸å¯æ¬ ãªæƒ…å ±ã‚’æ˜Žç¢ºã«å°‹ã­ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å†—é•·ãªèª¬æ˜Žã‚’é¿ã‘ã€ä¿¡é ¼æ„Ÿã‚’ä¸Žãˆã‚‹å¯¾å¿œã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
        draft = f"{initial_check}\n\n> ãŠå®¢æ§˜ã€ã”ä¸ä¾¿ã‚’ãŠã‹ã‘ã—ã€èª ã«ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã®çŠ¶æ³ã€ååˆ†æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚\n> å•é¡Œã‚’è¿…é€Ÿã«è§£æ±ºã™ã‚‹ãŸã‚ã€æã‚Œå…¥ã‚Šã¾ã™ãŒã€ä»¥ä¸‹ã®3ç‚¹ã®å¿…é ˆæƒ…å ±ã«ã¤ã„ã¦ã”ç¢ºèªã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚ã“ã®æƒ…å ±ãŒãªã„ã¨ã€ãŠå®¢æ§˜ã®çŠ¶æ³ã«åˆã‚ã›ãŸçš„ç¢ºãªè§£æ±ºç­–ã‚’ã”æ¡ˆå†…ã§ãã¾ã›ã‚“ã€‚\n> 1. å•é¡Œã®å¯¾è±¡ã¨ãªã‚‹**å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã®æ­£ç¢ºãªåç§°ã¨äºˆç´„ç•ªå·**\n> 2. ç¾åœ¨ã®**å…·ä½“çš„ãªå•é¡ŒçŠ¶æ³**\n> 3. æ—¢ã«**ãŠè©¦ã—ã„ãŸã ã„ãŸè§£æ±ºæ‰‹é †**\n\n> ãŠå®¢æ§˜ã¨ã®å††æ»‘ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€è¿…é€Ÿã«å•é¡Œè§£æ±ºã‚’ã‚µãƒãƒ¼ãƒˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ã”è¿”ä¿¡ã‚’ãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™ã€‚"
    
    advice_text = advice.replace("{customer_type}", customer_type)
    return {
        "advice_header": f"{L['simulation_advice_header']}",
        "advice": advice_text,
        "draft_header": f"{L['simulation_draft_header']} ({tone})",
        "draft": draft
    }

def get_closing_messages(lang_key):
    # (Closing messages logic remains the same, using LANG for localization)
    if lang_key == 'ko':
        return {"additional_query": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?", "chat_closing": LANG['ko']['prompt_survey']}
    elif lang_key == 'en':
        return {"additional_query": "Is there anything else we can assist you with today?", "chat_closing": LANG['en']['prompt_survey']}
    elif lang_key == 'ja':
        return {"additional_query": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ", "chat_closing": LANG['ja']['prompt_survey']}
    return get_closing_messages('ko')

def get_document_chunks(files):
    documents = []
    temp_dir = tempfile.mkdtemp()
    # (Document loading and chunking logic remains the same)
    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        file_extension = uploaded_file.name.split('.')[-1].lower()
        if file_extension == "pdf":
            with open(temp_filepath, "wb") as f: f.write(uploaded_file.getvalue())
            loader = PyPDFLoader(temp_filepath)
            documents.extend(loader.load())
        elif file_extension == "html":
            raw_html = uploaded_file.getvalue().decode('utf-8')
            soup = BeautifulSoup(raw_html, 'html.parser')
            text_content = soup.get_text(separator=' ', strip=True)
            documents.append(Document(page_content=text_content, metadata={"source": uploaded_file.name}))
        elif file_extension == "txt":
            with open(temp_filepath, "wb") as f: f.write(uploaded_file.getvalue())
            loader = TextLoader(temp_filepath, encoding="utf-8")
            documents.extend(loader.load())
        else:
            continue
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    return text_splitter.split_documents(documents)

def get_vector_store(text_chunks):
    cache_key = tuple(doc.page_content for doc in text_chunks)
    if cache_key in st.session_state.embedding_cache: return st.session_state.embedding_cache[cache_key]
    if not st.session_state.is_llm_ready: return None
    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        st.session_state.embedding_cache[cache_key] = vector_store
        return vector_store
    except Exception as e:
        print(f"Vector Store creation failed: {e}") 
        return None

def get_rag_chain(vector_store):
    if vector_store is None: return None
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

@st.cache_resource
def load_or_train_lstm():
    np.random.seed(int(time.time()))
    data = np.cumsum(np.random.normal(loc=5, scale=5, size=50)) + 60
    data = np.clip(data, 50, 95)
    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset) - look_back):
            X.append(dataset[i:(i + look_back)])
            Y.append(dataset[i + look_back])
        return np.array(X), np.array(Y)
    look_back = 5
    X, Y = create_dataset(data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    model = Sequential([
        LSTM(50, activation='relu', input_shape=(look_back, 1)),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)
    return model, data

def force_rerun_lstm():
    st.session_state.lstm_rerun_trigger = time.time()
    st.rerun()

def render_interactive_quiz(quiz_data, current_lang):
    L = LANG[current_lang]
    if not quiz_data or 'quiz_questions' not in quiz_data: return
    questions = quiz_data['quiz_questions']
    num_questions = len(questions)
    # (Quiz rendering logic remains the same)
    if "current_question" not in st.session_state or st.session_state.current_question >= num_questions:
        st.session_state.current_question = 0
        st.session_state.quiz_results = [None] * num_questions
        st.session_state.quiz_submitted = False
    q_index = st.session_state.current_question
    q_data = questions[q_index]
    st.subheader(f"{q_index + 1}. {q_data['question']}")
    options_dict = {}
    try:
        options_dict = {f"{opt['option']}": f"{opt['option']}) {opt['text']}" for opt in q_data['options']}
    except KeyError:
        st.error(L["quiz_fail_structure"])
        if 'quiz_data_raw' in st.session_state: st.code(st.session_state.quiz_data_raw, language="json")
        return
    options_list = list(options_dict.values())
    selected_answer = st.radio(L.get("select_answer", "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”"), options=options_list, key=f"q_radio_{q_index}")
    col1, col2 = st.columns(2)
    if col1.button(L.get("check_answer", "ì •ë‹µ í™•ì¸"), key=f"check_btn_{q_index}", disabled=st.session_state.quiz_submitted):
        user_choice_letter = selected_answer.split(')')[0] if selected_answer else None
        correct_answer_letter = q_data['correct_answer']
        is_correct = (user_choice_letter == correct_answer_letter)
        st.session_state.quiz_results[q_index] = is_correct
        st.session_state.quiz_submitted = True
        if is_correct: st.success(L.get("correct_answer", "ì •ë‹µìž…ë‹ˆë‹¤! ðŸŽ‰"))
        else: st.error(L.get("incorrect_answer", "ì˜¤ë‹µìž…ë‹ˆë‹¤.ðŸ˜ž"))
        st.markdown(f"**{L.get('correct_is', 'ì •ë‹µ')}: {correct_answer_letter}**")
        st.info(f"**{L.get('explanation', 'í•´ì„¤')}:** {q_data['explanation']}")
    if st.session_state.quiz_submitted:
        if q_index < num_questions - 1:
            if col2.button(L.get("next_question", "ë‹¤ìŒ ë¬¸í•­"), key=f"next_btn_{q_index}"):
                st.session_state.current_question += 1
                st.session_state.quiz_submitted = False
                st.rerun()
        else:
            total_correct = st.session_state.quiz_results.count(True)
            total_questions = len(st.session_state.quiz_results)
            st.success(f"**{L.get('quiz_complete', 'í€´ì¦ˆ ì™„ë£Œ!')}** {L.get('score', 'ì ìˆ˜')}: {total_correct}/{total_questions}")
            if st.button(L.get("retake_quiz", "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°"), key="retake"):
                st.session_state.current_question = 0
                st.session_state.quiz_results = [None] * num_questions
                st.session_state.quiz_submitted = False
                st.rerun()

def synthesize_and_play_audio(current_lang_key):
    # (TTS JS injection logic remains the same)
    ko_ready = LANG["ko"]["tts_status_ready"]
    en_ready = LANG["en"]["tts_status_ready"]
    ja_ready = LANG["ja"]["tts_status_ready"]

    tts_js_code = f"""
    <script>
    if (!window.speechSynthesis) {{
        document.getElementById('tts_status').innerText = 'âŒ TTS Not Supported';
    }}

    window.speakText = function(text, langKey) {{
        if (!window.speechSynthesis || !text) return;

        const statusElement = document.getElementById('tts_status');
        const utterance = new SpeechSynthesisUtterance(text);
        
        const langCode = {{ "ko": "ko-KR", "en": "en-US", "ja": "ja-JP" }}[langKey] || "en-US";
        utterance.lang = langCode; 

        const getReadyText = (key) => {{
            if (key === 'ko') return '{ko_ready}';
            if (key === 'en') return '{en_ready}';
            if (key === 'ja') return '{ja_ready}';
            return '{en_ready}';
        }};

        let voicesLoaded = false;
        const setVoiceAndSpeak = () => {{
            const voices = window.speechSynthesis.getVoices();
            if (voices.length > 0) {{
                utterance.voice = voices.find(v => v.lang.startsWith(langCode.substring(0, 2))) || voices[0];
                voicesLoaded = true;
                window.speechSynthesis.speak(utterance);
            }} else if (!voicesLoaded) {{
                setTimeout(setVoiceAndSpeak, 100);
            }}
        }};
        
        utterance.onstart = () => {{
            statusElement.innerText = '{LANG[current_lang_key].get("tts_status_generating", "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªç”Ÿæˆä¸­...")}';
            statusElement.style.backgroundColor = '#fff3e0';
        }};
        
        utterance.onend = () => {{
            statusElement.innerText = '{LANG[current_lang_key].get("tts_status_success", "âœ… ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå†ç”Ÿå®Œäº†!")}';
            statusElement.style.backgroundColor = '#e8f5e9';
             setTimeout(() => {{ 
                 statusElement.innerText = getReadyText(langKey);
                 statusElement.style.backgroundColor = '#f0f0f0';
             }}, 3000);
        }};
        
        utterance.onerror = (event) => {{
            statusElement.innerText = '{LANG[current_lang_key].get("tts_status_error", "âŒ TTSã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ")}';
            statusElement.style.backgroundColor = '#ffebee';
            console.error("SpeechSynthesis Error:", event);
             setTimeout(() => {{ 
                 statusElement.innerText = getReadyText(langKey);
                 statusElement.style.backgroundColor = '#f0f0f0';
             }}, 3999);
        }};

        window.speechSynthesis.cancel(); 
        setVoiceAndSpeak(); 
    }};
    </script>
    """
    st.components.v1.html(tts_js_code, height=5, width=0)

def render_tts_button(text_to_speak, current_lang_key):
    safe_text = re.sub(r'#+\s*', '', text_to_speak)
    safe_text = safe_text.replace('\n', ' ').replace('"', '\\"').replace("'", "\\'")
    js_call = f"window.speakText('{safe_text}', '{current_lang_key}')"

    st.markdown(f"""
        <button onclick="{js_call}"
                style="background-color: #4338CA; color: white; padding: 10px 20px; border-radius: 5px; cursor: pointer; border: none; width: 100%; font-weight: bold; margin-bottom: 10px;">
            {LANG[current_lang_key].get("button_listen_audio", "éŸ³å£°ã§èžã")} ðŸŽ§
        </button>
    """, unsafe_allow_html=True)

# -----------------------------
# 5. Core Initialization & Session State
# -----------------------------

# Initialize core clients and get DB connection
firestore_db = initialize_firestore_admin()
gcs_client = init_gcs_client(LANG[st.session_state.language])
openai_client = init_openai_client(LANG[st.session_state.language])

if 'llm' not in st.session_state:
    API_KEY = os.environ.get("GEMINI_API_KEY")
    if API_KEY:
        try:
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
            
            # Simulator Chain Setup
            SIMULATOR_PROMPT = PromptTemplate(
                template="The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n\n{chat_history}\nHuman: {input}\nAI:",
                input_variables=["input", "chat_history"]
            )
            st.session_state.simulator_chain = ConversationChain(
                llm=st.session_state.llm,
                memory=st.session_state.simulator_memory,
                prompt=SIMULATOR_PROMPT,
                input_key="input", 
            )

        except Exception as e:
            st.session_state.llm_init_error_msg = f"{LANG[st.session_state.language]['llm_error_init']} {e}"
            st.session_state.is_llm_ready = False

# RAG Index Loading Attempt
if st.session_state.get('firestore_db') and 'conversation_chain' not in st.session_state:
    loaded_index = load_index_from_firestore(st.session_state.firestore_db, st.session_state.embeddings)
    if loaded_index:
        st.session_state.conversation_chain = get_rag_chain(loaded_index)
        st.session_state.is_rag_ready = True
        st.session_state.firestore_load_success = True
    else:
        st.session_state.firestore_load_success = False

# Session State for Transcribed Text (used in both recorder and simulator)
if 'last_transcript' not in st.session_state: st.session_state['last_transcript'] = ''
if 'sim_audio_upload_key' not in st.session_state: st.session_state['sim_audio_upload_key'] = 0


# -----------------------------
# 6. Streamlit UI
# -----------------------------

L = LANG[st.session_state.language] 
st.set_page_config(page_title=L["title"], layout="wide")

# Sidebar for Language and RAG/LLM config
with st.sidebar:
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=['ko', 'en', 'ja'],
        index=['ko', 'en', 'ja'].index(st.session_state.language),
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èªž"}[x],
    )
    
    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        st.rerun() 
    
    L = LANG[st.session_state.language] 
    st.title(L["sidebar_title"])
    
    # Initialization status display
    if st.session_state.get('llm_init_error_msg'):
        st.error(st.session_state.llm_init_error_msg)
    elif st.session_state.is_llm_ready:
        st.success("âœ… LLM ë° ìž„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

    if st.session_state.get('firestore_db'):
        st.success("âœ… Firestore DB ì—°ê²° ì„±ê³µ")
    
    if gcs_client:
        st.success("âœ… GCS í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
    else:
        st.warning(L['gcs_missing'])

    st.markdown("---")
    
    # RAG Indexing Section
    uploaded_files_widget = st.file_uploader(
        L["file_uploader"], type=["pdf","txt","html"], accept_multiple_files=True
    )
    if uploaded_files_widget: st.session_state.uploaded_files_state = uploaded_files_widget
    files_to_process = st.session_state.uploaded_files_state if st.session_state.uploaded_files_state else []
    
    if files_to_process and st.session_state.is_llm_ready:
        if st.button(L["button_start_analysis"], key="start_analysis"):
            with st.spinner(L["data_analysis_progress"]): 
                text_chunks = get_document_chunks(files_to_process)
                vector_store = get_vector_store(text_chunks)
                if vector_store:
                    db = st.session_state.firestore_db
                    save_success = False
                    if db: save_success = save_index_to_firestore(db, vector_store)
                    
                    st.success(L["embed_success"].format(count=len(text_chunks)) + (" " + L["db_save_complete"] if save_success else " (DB Save Failed)"))
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                else:
                    st.session_state.is_rag_ready = False
                    st.error(L["embed_fail"])
    elif not files_to_process:
        st.warning(L.get("warning_no_files")) 

    st.markdown("---")
    
    # Feature Selection Radio
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ", 
        [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["simulator_tab"], L["voice_rec_header"]]
    )

st.title(L["title"])

# ================================
# 7. ê¸°ëŠ¥ë³„ íŽ˜ì´ì§€ êµ¬í˜„
# ================================

if feature_selection == L["voice_rec_header"]:
    st.header(L['voice_rec_header'])
    st.caption(L['record_help'])

    col_rec_ui, col_list_ui = st.columns([1, 1])

    with col_rec_ui:
        st.subheader(L['rec_header'])
        
        # Audio Input Widget
        audio_obj = None
        try:
            if hasattr(st, 'audio_input'):
                # Use a dedicated key for the main recorder to avoid conflicts
                audio_obj = st.audio_input(L["button_mic_input"], key='main_recorder_input') 
        except Exception:
            audio_obj = None

        if audio_obj is None:
            st.caption(f"({L['uploaded_file']}ë¡œ ëŒ€ì²´)")
            audio_obj = st.file_uploader(L['uploaded_file'], type=['wav', 'mp3', 'm4a', 'webm'], key='main_file_uploader')

        audio_bytes = None
        audio_mime = 'audio/webm'
        if audio_obj is not None:
            if hasattr(audio_obj, 'getvalue'):
                audio_bytes = audio_obj.getvalue()
                audio_mime = getattr(audio_obj, 'type', 'audio/webm')
        
        if audio_bytes:
            st.audio(audio_bytes, format=audio_mime)
            
            # Transcribe Action
            if st.button(L['transcribe_btn'], key='transcribe_btn_key_rec'):
                if openai_client is None:
                    st.error(L['openai_missing'])
                else:
                    with st.spinner(L['transcribing']):
                        try:
                            transcript_text = transcribe_bytes_with_whisper(audio_bytes, audio_mime)
                            st.session_state['last_transcript'] = transcript_text
                            st.success(L['transcript_result'])
                        except RuntimeError as e:
                            st.error(e)

            st.text_area(L['transcript_text'], value=st.session_state.get('last_transcript', ''), height=150, key='transcript_area_rec')

            # Save Action
            if st.button(L['save_btn'], key='save_btn_key_rec'):
                if firestore_db is None:
                    st.error(L['firebase_init_fail'])
                else:
                    bucket_name = get_gcs_bucket_name()
                    ext = audio_mime.split('/')[-1] if '/' in audio_mime else 'webm'
                    filename = f"record_{int(time.time())}.{ext}"
                    transcript_text = st.session_state.get('last_transcript', '')
                    
                    try:
                        save_audio_record(firestore_db, bucket_name, audio_bytes, filename, transcript_text, mime_type=audio_mime)
                        st.success(L['saved_success'])
                        st.session_state['last_transcript'] = ''
                        st.experimental_rerun()
                    except Exception as e:
                        st.error(f"{L['error']} {e}")

    with col_list_ui:
        st.subheader(L['rec_list_title'])
        if firestore_db is None:
            st.warning(L['firebase_init_fail'] + ' â€” ì´ë ¥ ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€')
        else:
            try:
                docs = list(firestore_db.collection('voice_records').order_by('created_at', direction=firestore.Query.DESCENDING).limit(50).stream())
            except Exception as e:
                st.error(f"Firestore read error: {e}")
                docs = []

            if not docs:
                st.info(L['no_records'])
            else:
                bucket_name = get_gcs_bucket_name()
                for d in docs:
                    data = d.to_dict()
                    doc_id = d.id
                    created_str = data.get('created_at').astimezone(timezone.utc).strftime('%Y-%m-%d %H:%M UTC') if isinstance(data.get('created_at'), datetime) else str(data.get('created_at'))
                    transcript_snippet = (data.get('transcript') or '')[:50].replace('\n', ' ') + '...'

                    with st.expander(f"[{created_str}] {transcript_snippet}"):
                        st.write(f"**{L['transcript_text']}:** {data.get('transcript') or 'N/A'}")
                        st.caption(f"**Size:** {data.get('size')} bytes | **Path:** {data.get('gcs_path', L['gcs_not_conf'])}")

                        colp, colr, cold = st.columns([2, 1, 1])
                        
                        # Playback Button
                        if colp.button(L['playback'], key=f'play_{doc_id}'):
                            if data.get('gcs_path') and gcs_client and bucket_name:
                                with st.spinner(L['playback']):
                                    try:
                                        blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(f'gs://{bucket_name}/')[-1])
                                        mime_type = data.get('mime_type', 'audio/webm')
                                        st.audio(blob_bytes, format=mime_type)
                                    except Exception as e:
                                        st.error(f"{L['gcs_playback_fail']}: {e}")
                            else:
                                st.info(L['gcs_no_audio'])

                        # Re-transcribe Button
                        if colr.button(L['retranscribe'], key=f'retx_{doc_id}'):
                            if openai_client is None: st.error(L['openai_missing'])
                            elif data.get('gcs_path') and gcs_client and bucket_name:
                                with st.spinner(L['transcribing']):
                                    try:
                                        blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(f'gs://{bucket_name}/')[-1])
                                        mime_type = data.get('mime_type', 'audio/webm')
                                        new_text = transcribe_bytes_with_whisper(blob_bytes, mime_type)
                                        firestore_db.collection('voice_records').document(doc_id).update({'transcript': new_text})
                                        st.success(L['retranscribe'] + ' ' + L['saved_success'])
                                        st.experimental_rerun()
                                    except Exception as e:
                                        st.error(f"{L['error']} {e}")
                            else: st.error(L['gcs_not_conf'])

                        # Delete Button
                        if cold.button(L['delete'], key=f'del_{doc_id}'):
                            if st.session_state.get(f'confirm_del_rec_{doc_id}', False):
                                ok = delete_audio_record(firestore_db, bucket_name, doc_id)
                                if ok: st.success(L['delete_success'])
                                else: st.error(L['delete_fail'])
                                st.session_state[f'confirm_del_rec_{doc_id}'] = False
                                st.experimental_rerun()
                            else:
                                st.session_state[f'confirm_del_rec_{doc_id}'] = True
                                st.warning(L['delete_confirm_rec'])

elif feature_selection == L["simulator_tab"]: 
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])
    
    # 1. TTS ìœ í‹¸ë¦¬í‹° (ìƒíƒœ í‘œì‹œê¸° ë° JS í•¨ìˆ˜)ë¥¼ íŽ˜ì´ì§€ ìƒë‹¨ì— ì‚½ìž…
    st.markdown(f'<div id="tts_status" style="padding: 5px; text-align: center; border-radius: 5px; background-color: #f0f0f0; margin-bottom: 10px;">{L["tts_status_ready"]}</div>', unsafe_allow_html=True)
    if "tts_js_loaded" not in st.session_state:
         synthesize_and_play_audio(st.session_state.language) 
         st.session_state.tts_js_loaded = True

    # 1.5 ì´ë ¥ ì‚­ì œ ë²„íŠ¼ ë° ëª¨ë‹¬
    db = st.session_state.get('firestore_db')
    col_delete, _ = st.columns([1, 4])
    with col_delete:
        if st.button(L["delete_history_button"], key="trigger_delete_history_sim"):
            st.session_state.show_delete_confirm = True

    if st.session_state.show_delete_confirm:
        with st.container(border=True):
            st.warning(L["delete_confirm_message"])
            col_yes, col_no = st.columns(2)
            if col_yes.button(L["delete_confirm_yes"], key="confirm_delete_yes", type="primary"):
                with st.spinner(L["deleting_history_progress"]): 
                    delete_all_history(db)
            if col_no.button(L["delete_confirm_no"], key="confirm_delete_no"):
                st.session_state.show_delete_confirm = False
                st.rerun()

    # â­ Firebase ìƒë‹´ ì´ë ¥ ë¡œë“œ ë° ì„ íƒ ì„¹ì…˜
    if db:
        with st.expander(L["history_expander_title"]):
            histories = load_simulation_histories(db)
            search_query = st.text_input(L["search_history_label"], key="history_search_sim", value="")
            today = datetime.now().date()
            default_start_date = today - timedelta(days=7)
            date_range_input = st.date_input(L["date_range_label"], value=[default_start_date, today], key="history_date_range_sim")

            filtered_histories = []
            if histories:
                # (Filtering logic remains the same)
                if isinstance(date_range_input, list) and len(date_range_input) == 2:
                    start_date = min(date_range_input)
                    end_date = max(date_range_input) + timedelta(days=1)
                else:
                    start_date = datetime.min.date()
                    end_date = datetime.max.date()
                for h in histories:
                    search_match = True
                    if search_query:
                        query_lower = search_query.lower()
                        searchable_text = h['initial_query'].lower() + " " + h['customer_type'].lower()
                        if query_lower not in searchable_text: search_match = False
                    date_match = True
                    if h.get('timestamp'):
                        h_date = h['timestamp'].date()
                        if not (start_date <= h_date < end_date): date_match = False
                    if search_match and date_match: filtered_histories.append(h)
            
            if filtered_histories:
                history_options = {f"[{h['timestamp'].strftime('%m-%d %H:%M')}] {h['customer_type']} - {h['initial_query'][:30]}...": h for h in filtered_histories}
                selected_key = st.selectbox(L["history_selectbox_label"], options=list(history_options.keys()))
                
                if st.button(L["history_load_button"], key='load_sim_history'): 
                    selected_history = history_options[selected_key]
                    st.session_state.customer_query_text_area = selected_history['initial_query']
                    st.session_state.initial_advice_provided = True
                    st.session_state.simulator_messages = selected_history['messages']
                    st.session_state.is_chat_ended = selected_history.get('is_chat_ended', False)
                    st.session_state.simulator_memory.clear()
                    for msg in selected_history['messages']:
                         if msg['role'] == 'customer' or msg['role'] == 'agent_response': st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
                         elif msg['role'] in ['supervisor', 'customer_rebuttal', 'customer_end', 'system_end']: st.session_state.simulator_memory.chat_memory.add_ai_message(msg['content'])
                    st.rerun()
            else:
                 st.info(L.get("no_history_found"))

    # LLM and UI logic for Simulation flow
    if st.session_state.is_llm_ready or not os.environ.get("GEMINI_API_KEY"):
        if st.session_state.is_chat_ended:
            st.success(L["prompt_customer_end"] + " " + L["prompt_survey"])
            if st.button(L["new_simulation_button"], key="new_simulation"): 
                 st.session_state.is_chat_ended = False
                 st.session_state.initial_advice_provided = False
                 st.session_state.simulator_messages = []
                 st.session_state.simulator_memory.clear()
                 st.session_state['last_transcript'] = ''
                 st.rerun()
            st.stop()
        
        if 'customer_query_text_area' not in st.session_state: st.session_state.customer_query_text_area = ""

        customer_query = st.text_area(
            L["customer_query_label"], key="customer_query_text_area", height=150, placeholder=L["initial_query_sample"], 
            disabled=st.session_state.initial_advice_provided
        )
        customer_type_options_list = L["customer_type_options"]
        default_index = 1 if len(customer_type_options_list) > 1 else 0
        customer_type_display = st.selectbox(
            L["customer_type_label"], customer_type_options_list, index=default_index, disabled=st.session_state.initial_advice_provided
        )
        current_lang_key = st.session_state.language 

        if st.button(L["button_simulate"], key="start_simulation", disabled=st.session_state.initial_advice_provided):
            if not customer_query: st.warning(L["simulation_warning_query"]); st.stop()
            
            st.session_state.simulator_memory.clear()
            st.session_state.simulator_messages = []
            st.session_state.is_chat_ended = False
            st.session_state.simulator_messages.append({"role": "customer", "content": customer_query})
            st.session_state.simulator_memory.chat_memory.add_user_message(customer_query)
            
            # (Initial prompt generation remains the same)
            initial_prompt = f"""You are an AI Customer Support Supervisor... [CRITICAL RULE FOR DRAFT CONTENT]... When the Agent subsequently asks for information, **Roleplay as the Customer** who is frustrated but **MUST BE HIGHLY COOPERATIVE** and provide the requested details piece by piece (not all at once). The customer MUST NOT argue or ask why the information is needed... The recommended draft MUST be strictly in {LANG[current_lang_key]['lang_select']}."""

            if not os.environ.get("GEMINI_API_KEY"):
                mock_data = get_mock_response_data(current_lang_key, customer_type_display)
                ai_advice_text = f"### {mock_data['advice_header']}\n\n{mock_data['advice']}\n\n### {mock_data['draft_header']}\n\n{mock_data['draft']}"
                st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
                st.session_state.simulator_memory.chat_memory.add_ai_message(ai_advice_text)
                st.session_state.initial_advice_provided = True
                save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
                st.rerun() 
            
            if os.environ.get("GEMINI_API_KEY"):
                with st.spinner(L["response_generating"]):
                    try:
                        response_text = st.session_state.simulator_chain.predict(input=initial_prompt)
                        st.session_state.simulator_messages.append({"role": "supervisor", "content": response_text})
                        st.session_state.initial_advice_provided = True
                        save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
                        st.rerun() 
                    except Exception as e:
                        st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        st.markdown("---")
        for message in st.session_state.simulator_messages:
            if message["role"] == "customer": with st.chat_message("user", avatar="ðŸ™‹"): st.markdown(message["content"])
            elif message["role"] == "supervisor": with st.chat_message("assistant", avatar="ðŸ¤–"): st.markdown(message["content"]); render_tts_button(message["content"], st.session_state.language) 
            elif message["role"] == "agent_response": with st.chat_message("user", avatar="ðŸ§‘â€ðŸ’»"): st.markdown(message["content"])
            elif message["role"] == "customer_rebuttal": with st.chat_message("assistant", avatar="ðŸ˜ "): st.markdown(message["content"])
            elif message["role"] == "customer_end": with st.chat_message("assistant", avatar="ðŸ˜Š"): st.markdown(message["content"])
            elif message["role"] == "system_end": with st.chat_message("assistant", avatar="âœ¨"): st.markdown(message["content"])

        if st.session_state.initial_advice_provided and not st.session_state.is_chat_ended:
            last_role = st.session_state.simulator_messages[-1]['role'] if st.session_state.simulator_messages else None
            
            if last_role in ["customer_rebuttal", "customer_end", "supervisor", "customer"]:
                st.markdown(f"### {L['agent_response_header']}") 
                
                col_audio, col_text_area = st.columns([1, 2])
                
                # --- Whisper Audio Input for Agent Response ---
                with col_audio:
                    # Rerunning the input component ensures it reloads cleanly after a transcription event
                    audio_file = st.audio_input(L["button_mic_input"], key=f"sim_audio_input_{st.session_state['sim_audio_upload_key']}")
                
                if audio_file:
                    if openai_client is None: st.error(L.get("whisper_client_error"))
                    else:
                        with st.spinner(L.get("whisper_processing")):
                            try:
                                # Get mime type from UploadedFile object
                                mime_type = getattr(audio_file, 'type', 'audio/webm')
                                transcribed_text = transcribe_bytes_with_whisper(audio_file.getvalue(), mime_type)
                                st.session_state['last_transcript'] = transcribed_text
                                st.session_state['sim_audio_upload_key'] += 1 # Change key to force widget reset on rerun
                                st.success(L.get("whisper_success"))
                                st.rerun() 
                            except Exception as e: st.error(f"ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); st.session_state['last_transcript'] = ""

                agent_response = col_text_area.text_area(
                    L["agent_response_placeholder"], value=st.session_state['last_transcript'], key="agent_response_area_text", height=150
                )
                
                # JS Enter Key Listener
                st.components.v1.html("""<script>const textarea = document.querySelector('textarea[key="agent_response_area_text"]'); const button = document.querySelector('button[key="send_agent_response_sim"]'); if (textarea && button) { textarea.addEventListener('keydown', function(event) { if (event.key === 'Enter' && (!event.shiftKey && !event.ctrlKey)) { event.preventDefault(); button.click(); } }); }</script>""", height=0, width=0)

                if st.button(L["send_response_button"], key="send_agent_response_sim"): 
                    if agent_response.strip():
                        st.session_state['last_transcript'] = "" # Clear last transcript after sending
                        st.session_state.simulator_messages.append({"role": "agent_response", "content": agent_response})
                        st.session_state.simulator_memory.chat_memory.add_user_message(agent_response)
                        save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                        st.rerun()
                    else: st.warning(L.get("empty_response_warning"))
            
            if last_role == "agent_response":
                col_end, col_next = st.columns([1, 2])
                
                if col_end.button(L["button_end_chat"], key="end_chat_sim"): 
                    closing_messages = get_closing_messages(current_lang_key)
                    st.session_state.simulator_messages.append({"role": "supervisor", "content": closing_messages["additional_query"]})
                    st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["additional_query"])
                    st.session_state.simulator_messages.append({"role": "system_end", "content": closing_messages["chat_closing"]})
                    st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["chat_closing"])
                    st.session_state.is_chat_ended = True
                    save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                    st.rerun()

                if col_next.button(L["request_rebuttal_button"], key="request_rebuttal_sim"):
                    if not os.environ.get("GEMINI_API_KEY"): st.warning("API Keyê°€ ì—†ì–´ LLM ì‹œë®¬ë ˆì´ì…˜ ë¶ˆê°€"); st.stop()
                    
                    next_reaction_prompt = f"""Analyze the entire chat history. Roleplay as the customer ({customer_type_display}). Based on the agent's last message, generate ONE of the following responses... The response MUST be strictly in {LANG[current_lang_key]['lang_select']}."""
                    
                    with st.spinner(L["response_generating"]):
                        try:
                            customer_reaction = st.session_state.simulator_chain.predict(input=next_reaction_prompt)
                            positive_keywords = ["ê°ì‚¬", "thank you", "ã‚ã‚ŠãŒã¨ã†", L['customer_positive_response'].lower().split('/')[-1].strip()]
                            is_positive_close = any(keyword in customer_reaction.lower() for keyword in positive_keywords)
                            
                            if is_positive_close:
                                role = "customer_end"
                                st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                                st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)
                                st.session_state.simulator_messages.append({"role": "supervisor", "content": L["customer_closing_confirm"]})
                                st.session_state.simulator_memory.chat_memory.add_ai_message(L["customer_closing_confirm"])
                            else:
                                role = "customer_rebuttal"
                                st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                                st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)
                                 
                            save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                            st.rerun()
                        except Exception as e: st.error(f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.error(L["llm_error_init"])

elif feature_selection == L["rag_tab"]:
    # (RAG Chatbot UI logic remains the same)
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    if st.session_state.get('is_rag_ready', False) and st.session_state.get('conversation_chain'):
        if "messages" not in st.session_state: st.session_state.messages = []
        for message in st.session_state.messages:
            with st.chat_message(message["role"]): st.markdown(message["content"])
        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"): st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner(L["response_generating"]):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' if st.session_state.language == 'ko' else 'Could not generate response.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e: st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}"); st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ" if st.session_state.language == 'ko' else "An error occurred"})
    else: st.warning(L["warning_rag_not_ready"])

elif feature_selection == L["content_tab"]:
    # (Custom Content Generation UI logic remains the same)
    st.header(L["content_header"])
    st.markdown(L["content_desc"])
    if st.session_state.is_llm_ready:
        topic = st.text_input(L["topic_label"])
        level_map = dict(zip(L["level_options"], ["Beginner", "Intermediate", "Advanced"]))
        content_map = dict(zip(L["content_options"], ["summary", "quiz", "example"]))
        level_display = st.selectbox(L["level_label"], L["level_options"])
        content_type_display = st.selectbox(L["content_type_label"], L["content_options"])
        level = level_map[level_display]
        content_type = content_map[content_type_display]

        if st.button(L["button_generate"]):
            if topic:
                target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]
                if content_type == 'quiz':
                    full_prompt = f"""You are a professional AI coach at the {level} level. Please generate exactly 10 multiple-choice questions about the topic in {target_lang}. Your entire response MUST be a valid JSON object wrapped in ```json tags. The JSON must have a single key named 'quiz_questions', which is an array of objects. Each question object must contain: 'question' (string), 'options' (array of objects with 'option' (A,B,C,D) and 'text' (string)), 'correct_answer' (A,B,C, or D), and 'explanation' (string). Topic: {topic}"""
                else:
                    display_type_text = L["content_options"][L["content_options"].index(content_type_display)]
                    full_prompt = f"""You are a professional AI coach at the {level} level. Please generate clear and educational content in the requested {display_type_text} format based on the topic. The response MUST be strictly in {target_lang}. Topic: {topic}. Requested Format: {display_type_text}"""
                
                with st.spinner(f"Generating {content_type_display} for {topic}..."):
                    quiz_data_raw = None
                    try:
                        response = st.session_state.llm.invoke(full_prompt)
                        quiz_data_raw = response.content
                        st.session_state.quiz_data_raw = quiz_data_raw
                        if content_type == 'quiz':
                            quiz_data = clean_and_load_json(quiz_data_raw)
                            if quiz_data and 'quiz_questions' in quiz_data:
                                st.session_state.quiz_data = quiz_data
                                st.session_state.current_question = 0
                                st.session_state.quiz_submitted = False
                                st.session_state.quiz_results = [None] * len(quiz_data.get('quiz_questions',[]))
                                st.success(f"**{topic}** - **{content_type_display}** Result:")
                            else: st.error(L["quiz_error_llm"]); st.markdown(f"**{L['quiz_original_response']}**:"); st.code(quiz_data_raw, language="json")
                        else: st.success(f"**{topic}** - **{content_type_display}** Result:"); st.markdown(response.content)
                    except Exception as e: st.error(f"Content Generation Error: {e}"); 
            else: st.warning(L["warning_topic"])
    else: st.error(L["llm_error_init"])
    is_quiz_ready = content_type == 'quiz' and 'quiz_data' in st.session_state and st.session_state.quiz_data
    if is_quiz_ready and st.session_state.get('current_question', 0) < len(st.session_state.quiz_data.get('quiz_questions', [])):
        render_interactive_quiz(st.session_state.quiz_data, st.session_state.language)

elif feature_selection == L["lstm_tab"]:
    # (LSTM UI logic remains the same)
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])
    if st.button(L["lstm_rerun_button"], key="rerun_lstm", on_click=force_rerun_lstm): pass
    try:
        model, data = load_or_train_lstm()
        look_back = 5
        X_input = np.reshape(data[-look_back:], (1, look_back, 1))
        predicted_score = model.predict(X_input, verbose=0)[0][0]
        st.markdown("---")
        st.subheader(L["lstm_result_header"])
        col_score, col_chart = st.columns([1, 2])
        with col_score:
            st.metric(L["lstm_score_metric"], f"{predicted_score:.1f}{'ì ' if st.session_state.language == 'ko' else ''}")
            st.info(L["lstm_score_info"].format(predicted_score=predicted_score))
        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label='Past Scores', marker='o')
            ax.plot(len(data), predicted_score, label='Predicted Next Score', marker='*', color='red', markersize=10)
            ax.set_title(L["lstm_header"])
            ax.set_xlabel(f"Time ({L.get('score', 'Score')} attempts)")
            ax.set_ylabel(f"{L.get('score', 'Score')} (0-100)")
            ax.legend()
            st.pyplot(fig)
    except Exception as e:
        st.error(f"LSTM ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜ ë©”ì‹œì§€: {e})")
