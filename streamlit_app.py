# ========================================
# streamlit_app_final.py
# ì™„ì„±ë³¸: Streamlit ì•± â€” Whisper ì „ì‚¬, Firestore/GCS í†µí•©, ì‹œë®¬ë ˆì´í„°, RAG, LSTM
# ========================================

import streamlit as st
import os
import tempfile
import time
import json
import re
import base64
import io
import numpy as np
# from bs4 import BeautifulSoup # Placeholder functions do not need these imports
from matplotlib import pyplot as plt # Re-enabled for LSTM visualization
from tensorflow.keras.models import Sequential # Re-enabled for LSTM mock
from tensorflow.keras.layers import LSTM, Dense # Re-enabled for LSTM mock
from datetime import datetime, timedelta, timezone
from openai import OpenAI

# â­ Firebase / GCS
import firebase_admin
from firebase_admin import credentials, firestore, initialize_app, get_app
from google.cloud import storage
from google.cloud.exceptions import NotFound
from google.cloud import firestore as gcp_firestore
from google.cloud.firestore import Query

# LangChain Imports
from langchain.chains import ConversationalRetrievalChain, ConversationChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.schema.document import Document
from langchain.prompts import PromptTemplate

# -----------------------------
# 1. Config & I18N (ë‹¤êµ­ì–´ ì§€ì›)
# -----------------------------
DEFAULT_LANG = "ko"
# st.session_state ì ‘ê·¼ì€ st.set_page_config ì´í›„ë¡œ ë¯¸ë£¹ë‹ˆë‹¤.

LANG = {
Â  Â  "ko": {
Â  Â  Â  Â  "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜ (ìŒì„± ë° DB í†µí•©)",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
Â  Â  Â  Â  "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
Â  Â  Â  Â  "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
Â  Â  Â  Â  "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
Â  Â  Â  Â  "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
Â  Â  Â  Â  "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",Â 
Â  Â  Â  Â  "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
Â  Â  Â  Â  "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
Â  Â  Â  Â  "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "topic_label": "í•™ìŠµ ì£¼ì œ",
Â  Â  Â  Â  "level_label": "ë‚œì´ë„",
Â  Â  Â  Â  "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
Â  Â  Â  Â  "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
Â  Â  Â  Â  "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
Â  Â  Â  Â  "button_generate": "ì½˜í…ì¸  ìƒì„±",
Â  Â  Â  Â  "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
Â  Â  Â  Â  "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "lang_select": "ì–¸ì–´ ì„ íƒ",
Â  Â  Â  Â  "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
Â  Â  Â  Â  "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
Â  Â  Â  Â  "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
Â  Â  Â  Â  "check_answer": "ì •ë‹µ í™•ì¸",
Â  Â  Â  Â  "next_question": "ë‹¤ìŒ ë¬¸í•­",
Â  Â  Â  Â  "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
Â  Â  Â  Â  "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤. ğŸ˜",
Â  Â  Â  Â  "correct_is": "ì •ë‹µ",
Â  Â  Â  Â  "explanation": "í•´ì„¤",
Â  Â  Â  Â  "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
Â  Â  Â  Â  "score": "ì ìˆ˜",
Â  Â  Â  Â  "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
Â  Â  Â  Â  "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
Â  Â  Â  Â  "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
Â  Â  Â  Â  "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚",Â 
Â  Â  Â  Â  "db_save_complete": "(DB ì €ì¥ ì™„ë£Œ)",Â 
Â  Â  Â  Â  "data_analysis_progress": "ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘...",Â 
Â  Â  Â  Â  "response_generating": "ë‹µë³€ ìƒì„± ì¤‘...",Â 
Â  Â  Â  Â  "lstm_result_header": "í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼",
Â  Â  Â  Â  "lstm_score_metric": "í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„",
Â  Â  Â  Â  "lstm_score_info": "ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!",
Â  Â  Â  Â  "lstm_rerun_button": "ìƒˆë¡œìš´ ê°€ìƒ ë°ì´í„°ë¡œ ì˜ˆì¸¡",
Â  Â  Â  Â Â 
Â  Â  Â  Â  # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
Â  Â  Â  Â  "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
Â  Â  Â  Â  "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— ëŒ€í•´ AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
Â  Â  Â  Â  "customer_type_label": "ê³ ê° ì„±í–¥",
Â  Â  Â  Â  "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
Â  Â  Â  Â  "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
Â  Â  Â  Â  "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
Â  Â  Â  Â  "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ëŠ” ê²½ìš°, ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
Â  Â  Â  Â  "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
Â  Â  Â  Â  "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
Â  Â  Â  Â  "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
Â  Â  Â  Â  "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
Â  Â  Â  Â  "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!",
Â  Â  Â  Â  "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
Â  Â  Â  Â  "history_expander_title": "ğŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê°œ)",Â 
Â  Â  Â  Â  "initial_query_sample": "í”„ë‘ìŠ¤ íŒŒë¦¬ì— ë„ì°©í–ˆëŠ”ë°, í´ë£©ì—ì„œ êµ¬ë§¤í•œ eSIMì´ í™œì„±í™”ê°€ ì•ˆ ë©ë‹ˆë‹¤. ì—°ê²°ì´ ì•ˆ ë¼ì„œ ë„ˆë¬´ ê³¤ë€í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",Â 
Â  Â  Â  Â  "button_mic_input": "ğŸ™ ìŒì„± ì…ë ¥",
Â  Â  Â  Â  "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ ì±„íŒ…ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "prompt_survey": "ê³ ê° ë¬¸ì˜ ì„¼í„°ì— ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹­ì‹œì˜¤ã€‚",
Â  Â  Â  Â  "customer_closing_confirm": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?",
Â  Â  Â  Â  "customer_positive_response": "ì¢‹ì€ ë§ì”€/ì¹œì ˆí•œ ìƒë‹´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "button_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ì¡°ì‚¬ ìš”ì²­)",
Â  Â  Â  Â  "agent_response_header": "âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ",
Â  Â  Â  Â  "agent_response_placeholder": "ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš” (ê³ ê°ì˜ í•„ìˆ˜ ì •ë³´ë¥¼ ìš”ì²­/í™•ì¸í•˜ê±°ë‚˜, ë¬¸ì œ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”)",
Â  Â  Â  Â  "send_response_button": "ì‘ë‹µ ì „ì†¡",
Â  Â  Â  Â  "request_rebuttal_button": "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­",
Â  Â  Â  Â  "new_simulation_button": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘",
Â  Â  Â  Â  "history_selectbox_label": "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
Â  Â  Â  Â  "history_load_button": "ì„ íƒëœ ì´ë ¥ ë¡œë“œ",
Â  Â  Â  Â  "delete_history_button": "âŒ ëª¨ë“  ì´ë ¥ ì‚­ì œ",Â 
Â  Â  Â  Â  "delete_confirm_message": "ì •ë§ë¡œ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ã€‚",Â 
Â  Â  Â  Â  "delete_confirm_yes": "ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤",Â 
Â  Â  Â  Â  "delete_confirm_no": "ì•„ë‹ˆì˜¤, ìœ ì§€í•©ë‹ˆë‹¤",Â 
Â  Â  Â  Â  "delete_success": "âœ… ëª¨ë“  ìƒë‹´ ì´ë ¥ ì‚­ì œ ì™„ë£Œ!",
Â  Â  Â  Â  "deleting_history_progress": "ì´ë ¥ ì‚­ì œ ì¤‘...",Â 
Â  Â  Â  Â  "search_history_label": "ì´ë ¥ í‚¤ì›Œë“œ ê²€ìƒ‰",Â 
Â  Â  Â  Â  "date_range_label": "ë‚ ì§œ ë²”ìœ„ í•„í„°",Â 
Â  Â  Â  Â  "no_history_found": "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â Â 
Â  Â  Â  Â  # â­ ìŒì„± ê¸°ë¡ í†µí•© ê´€ë ¨ í‚¤ (Voice/GCS)
Â  Â  Â  Â  "voice_rec_header": 'ìŒì„± ê¸°ë¡ & ê´€ë¦¬',
Â  Â  Â  Â  "record_help": 'ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚',
Â  Â  Â  Â  "uploaded_file": 'ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ',
Â  Â  Â  Â  "rec_list_title": 'ì €ì¥ëœ ìŒì„± ê¸°ë¡ (Whisper/GCS)',
Â  Â  Â  Â  "transcribe_btn": 'ì „ì‚¬(Whisper)',
Â  Â  Â  Â  "save_btn": 'ìŒì„± ê¸°ë¡ ì €ì¥',
Â  Â  Â  Â  "transcribing": 'ìŒì„± ì „ì‚¬ ì¤‘...',
Â  Â  Â  Â  "transcript_result": 'ì „ì‚¬ ê²°ê³¼:',
Â  Â  Â  Â  "transcript_text": 'ì „ì‚¬ í…ìŠ¤íŠ¸',
Â  Â  Â  Â  "openai_missing": 'OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.',
Â  Â  Â  Â  "whisper_client_error": "âŒ ì˜¤ë¥˜: Whisper API Clientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "whisper_auth_error": "âŒ Whisper API ì¸ì¦ ì‹¤íŒ¨: API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "whisper_format_error": "âŒ ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "whisper_success": "âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ! í…ìŠ¤íŠ¸ ì°½ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
Â  Â  Â  Â  "playback": 'ë…¹ìŒ ì¬ìƒ',
Â  Â  Â  Â  "retranscribe": 'ì¬ì „ì‚¬',
Â  Â  Â  Â  "delete": 'ì‚­ì œ',
Â  Â  Â  Â  "no_records": 'ì €ì¥ëœ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.',
Â  Â  Â  Â  "gcs_missing": 'GCS ë²„í‚·ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Secretsì— GCS_BUCKET_NAMEì„ ì¶”ê°€í•˜ì„¸ìš”.',
Â  Â  Â  Â  "saved_success": 'ì €ì¥ ì™„ë£Œ!',
Â  Â  Â  Â  "delete_confirm_rec": 'ì •ë§ë¡œ ì´ ìŒì„± ê¸°ë¡ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? GCS íŒŒì¼ë„ ì‚­ì œë©ë‹ˆë‹¤.',
Â  Â  Â  Â  "gcs_init_fail": 'GCS ì´ˆê¸°í™” ì‹¤íŒ¨. ê¶Œí•œ ë° ë²„í‚· ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.',
Â  Â  Â  Â  "firebase_init_fail": 'Firebase Admin ì´ˆê¸°í™” ì‹¤íŒ¨.',
Â  Â  Â  Â  "upload_fail": 'GCS ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨',
Â  Â  Â  Â  "gcs_not_conf": 'GCS ë¯¸ì„¤ì • ë˜ëŠ” ì˜¤ë””ì˜¤ ì—†ìŒ',
Â  Â  Â  Â  "gcs_playback_fail": 'ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨',
Â  Â  Â  Â  "gcs_no_audio": 'ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ (GCS ë¯¸ì„¤ì •)',
Â  Â  Â  Â  "error": 'ì˜¤ë¥˜:',
Â  Â  Â  Â  "firestore_no_db_connect": "âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€",
Â  Â  Â  Â  "save_history_success": "âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "save_history_fail": "âŒ ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨",
Â  Â  Â  Â  "delete_fail": "ì‚­ì œ ì‹¤íŒ¨",
Â  Â  Â  Â  "rec_header": "ìŒì„± ì…ë ¥ ë° ì „ì‚¬",
Â  Â  Â  Â  "whisper_processing": "ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘",
Â  Â  Â  Â  "empty_response_warning": "ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”.",
Â  Â  },
Â  Â  "en": {
Â  Â  Â  Â  "title": "Personalized AI Study Coach (Voice & DB Integration)",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AI Study Coach Settings",
Â  Â  Â  Â  "lang_select": "Select Language",
Â  Â  Â  Â  "voice_rec_header": 'Voice Record & Management',
Â  Â  Â  Â  "record_help": 'Press the microphone button to record or upload a file.',
Â  Â  Â  Â  "gcs_missing": 'GCS bucket is not configured. Add GCS_BUCKET_NAME to Secrets.',
Â  Â  Â  Â  "openai_missing": 'OpenAI API Key is missing. Set OPENAI_API_KEY in Secrets.',
Â  Â  Â  Â  "delete_fail": "Deletion failed",
Â  Â  Â  Â  "save_history_fail": "âŒ Simulation history save failed",
Â  Â  Â  Â  "delete_success": "âœ… Successfully deleted!",Â 
Â  Â  Â  Â  "firestore_no_index": "Could not find existing RAG index in database. Please upload files and create a new one.",Â 
Â  Â  Â  Â  "embed_fail": "Embedding failed: Free tier quota exceeded or network issue.",
Â  Â  Â  Â  "gcs_not_conf": 'GCS not configured or audio not available',
Â  Â  Â  Â  "gcs_playback_fail": 'Audio playback failed',
Â  Â  Â  Â  "gcs_no_audio": 'No audio file (GCS not configured)',
Â  Â  Â  Â  "transcribing": 'Transcribing voice...',
Â  Â  Â  Â  "playback": 'Playback Recording',
Â  Â  Â  Â  "retranscribe": 'Re-transcribe',
Â  Â  Â  Â  "error": 'Error:',
Â  Â  Â  Â  "firestore_no_db_connect": "âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€",
Â  Â  Â  Â  "save_history_success": "âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "uploaded_file": 'Upload Audio File',
Â  Â  Â  Â  "transcript_result": 'Transcription Result:',
Â  Â  Â  Â  "transcript_text": 'Transcribed Text',
Â  Â  Â  Â  "llm_error_key": "âš ï¸ Warning: GEMINI API Key is not set. Please set 'GEMINI_API_KEY' in Streamlit Secrets.",
Â  Â  Â  Â  "llm_error_init": "LLM initialization error: Please check your API key.",
Â  Â  Â  Â  "simulation_warning_query": "Please enter the customer's query.",
Â  Â  Â  Â  "simulation_no_key_warning": "âš ï¸ API Key is missing. Response generation cannot proceed.",
Â  Â  Â  Â  "simulation_advice_header": "AI Response Guidelines",
Â  Â  Â  Â  "simulation_draft_header": "Recommended Response Draft",
Â  Â  Â  Â  "button_listen_audio": "Listen to Audio",
Â  Â  Â  Â  "tts_status_ready": "Ready to listen",
Â  Â  Â  Â  "tts_status_generating": "Generating audio...",
Â  Â  Â  Â  "tts_status_success": "âœ… Audio playback complete!",
Â  Â  Â  Â  "tts_status_error": "âŒ TTS API error occurred",
Â  Â  Â  Â  "history_expander_title": "ğŸ“ Load Previous Simulation History (Last 10)",Â 
Â  Â  Â  Â  "initial_query_sample": "I arrived in Paris, France, but the eSIM I bought from Klook won't activate. I'm really struggling to get connected. What should I do?",Â 
Â  Â  Â  Â  "button_mic_input": "ğŸ™ Voice Input",
Â  Â  Â  Â  "prompt_customer_end": "As there are no further inquiries, we will now end this chat session.",
Â  Â  Â  Â  "prompt_survey": "Thank you for contacting our Customer Support Center. Please feel free to contact us anytime if you have any additional questions.",
Â  Â  Â  Â  "customer_closing_confirm": "Is there anything else we can assist you with today?",
Â  Â  Â  Â  "customer_positive_response": "Thank you for your kind understanding/friendly advice.",
Â  Â  Â  Â  "button_end_chat": "End Chat (Request Survey)",
Â  Â  Â  Â  "agent_response_header": "âœï¸ Agent Response",
Â  Â  Â  Â  "agent_response_placeholder": "Respond to the customer (Request/confirm essential information or provide solution steps)",
Â  Â  Â  Â  "send_response_button": "Send Response",
Â  Â  Â  Â  "request_rebuttal_button": "Request Customer's Next Reaction",
Â  Â  Â  Â  "new_simulation_button": "Start New Simulation",
Â  Â  Â  Â  "history_selectbox_label": "Select history to load:",
Â  Â  Â  Â  "history_load_button": "Load Selected History",
Â  Â  Â  Â  "delete_history_button": "âŒ Delete All History",Â 
Â  Â  Â  Â  "delete_confirm_message": "Are you sure you want to delete ALL simulation history? This action cannot be undone.",Â 
Â  Â  Â  Â  "delete_confirm_yes": "Yes, Delete",Â 
Â  Â  Â  Â  "delete_confirm_no": "No, Keep",Â 
Â  Â  Â  Â  "deleting_history_progress": "Deleting history...",Â 
Â  Â  Â  Â  "search_history_label": "Search History by Keyword",Â 
Â  Â  Â  Â  "date_range_label": "Date Range Filter",Â 
Â  Â  Â  Â  "no_history_found": "No history found matching the criteria.",
Â  Â  Â  Â  "title": "Personalized AI Study Coach (Voice & DB Integration)",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AI Study Coach Settings",
Â  Â  Â  Â  "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
Â  Â  Â  Â  "button_start_analysis": "Start Analysis (RAG Indexing)",
Â  Â  Â  Â  "rag_tab": "RAG Knowledge Chatbot",
Â  Â  Â  Â  "content_tab": "Custom Content Generation",
Â  Â  Â  Â  "lstm_tab": "LSTM Achievement Prediction",
Â  Â  Â  Â  "simulator_tab": "AI Customer Response Simulator",Â 
Â  Â  Â  Â  "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
Â  Â  Â  Â  "rag_desc": "Answers questions based on the uploaded documents.",
Â  Â  Â  Â  "rag_input_placeholder": "Ask a question about your study materials",
Â  Â  Â  Â  "content_header": "Custom Learning Content Generation",
Â  Â  Â  Â  "content_desc": "Generate content tailored to your topic and difficulty.",
Â  Â  Â  Â  "topic_label": "Learning Topic",
Â  Â  Â  Â  "level_label": "Difficulty",
Â  Â  Â  Â  "content_type_label": "Content Type",
Â  Â  Â  Â  "level_options": ["Beginner", "Intermediate", "Advanced"],
Â  Â  Â  Â  "content_options": ["Key Summary Note", "10 Multiple-Choice Questions", "Practical Example Idea"],
Â  Â  Â  Â  "button_generate": "Generate Content",
Â  Â  Â  Â  "warning_topic": "Please enter a learning topic.",
Â  Â  Â  Â  "lstm_header": "LSTM Based Achievement Prediction",
Â  Â  Â  Â  "lstm_desc": "Trains an LSTM model on hypothetical past quiz scores to predict future achievement.",
Â  Â  Â  Â  "embed_success": "Learning DB built with {count} chunks!",
Â  Â  Â  Â  "warning_no_files": "Please upload study materials first.",
Â  Â  Â  Â  "warning_rag_not_ready": "RAG is not ready. Upload materials and click Start Analysis.",
Â  Â  Â  Â  "quiz_fail_structure": "Quiz data structure is incorrect.",
Â  Â  Â  Â  "select_answer": "Select answer",
Â  Â  Â  Â  "check_answer": "Confirm answer",
Â  Â  Â  Â  "next_question": "Next Question",
Â  Â  Â  Â  "correct_answer": "Correct! ğŸ‰",
Â  Â  Â  Â  "incorrect_answer": "Incorrect. ğŸ˜",
Â  Â  Â  Â  "correct_is": "Correct answer",
Â  Â  Â  Â  "explanation": "Explanation",
Â  Â  Â  Â  "quiz_complete": "Quiz completed!",
Â  Â  Â  Â  "score": "Score",
Â  Â  Â  Â  "retake_quiz": "Retake Quiz",
Â  Â  Â  Â  "quiz_error_llm": "Quiz generation failed: LLM did not return a valid JSON format. Check the original LLM response.",
Â  Â  Â  Â  "quiz_original_response": "Original LLM Response",
Â  Â  Â  Â  "firestore_loading": "Loading RAG index from database...",
Â  Â  Â  Â  "db_save_complete": "(DB Save Complete)",Â 
Â  Â  Â  Â  "data_analysis_progress": "Analyzing materials and building learning DB...",Â 
Â  Â  Â  Â  "response_generating": "Generating response...",Â 
Â  Â  Â  Â  "lstm_result_header": "Prediction Results",
Â  Â  Â  Â  "lstm_score_metric": "Current Predicted Achievement",
Â  Â  Â  Â  "lstm_score_info": "Your next estimated quiz score is **{predicted_score:.1f}**. Maintain or improve your learning progress!",
Â  Â  Â  Â  "lstm_rerun_button": "Predict with New Hypothetical Data",
Â  Â  Â  Â  "rec_header": "Voice Input and Transcription",
Â  Â  Â  Â  "whisper_processing": "Processing voice transcription",
Â  Â  Â  Â  "empty_response_warning": "Please enter a response.",
Â  Â  },
Â  Â  "ja": {
Â  Â  Â  Â  "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»DBçµ±åˆ)",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
Â  Â  Â  Â  "lang_select": "è¨€èªé¸æŠ",
Â  Â  Â  Â  "voice_rec_header": 'éŸ³å£°è¨˜éŒ²ã¨ç®¡ç†',
Â  Â  Â  Â  "record_help": 'ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦éŒ²éŸ³ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚',
Â  Â  Â  Â  "gcs_missing": 'GCSãƒã‚±ãƒƒãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Secretsã«GCS_BUCKET_NAMEã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚',
Â  Â  Â  Â  "openai_missing": 'OpenAI APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Secretsã«OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚',
Â  Â  Â  Â  "delete_fail": "å‰Šé™¤å¤±æ•—",
Â  Â  Â  Â  "save_history_fail": "âŒ å¯¾å¿œå±¥æ­´ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ",
Â  Â  Â  Â  "delete_success": "âœ… å‰Šé™¤ãŒå®Œäº†ã•ã‚Œã¾ã—ãŸ!",Â 
Â  Â  Â  Â  "firestore_no_index": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ—¢å­˜ã®RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°ã—ãä½œæˆã—ã¦ãã ã•ã„ã€‚",Â 
Â  Â  Â  Â  "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—: ãƒ•ãƒªãƒ¼ãƒ†ã‚£ã‚¢ã®ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã€‚",
Â  Â  Â  Â  "gcs_not_conf": 'GCSãŒæœªè¨­å®šã‹ã€éŸ³å£°ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“',
Â  Â  Â  Â  "gcs_playback_fail": 'éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ',
Â  Â  Â  Â  "gcs_no_audio": 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãªã— (GCSæœªè¨­å®š)',
Â  Â  Â  Â  "transcribing": 'éŸ³å£°è»¢å†™ä¸­...',
Â  Â  Â  Â  "playback": 'éŒ²éŸ³å†ç”Ÿ',
Â  Â  Â  Â  "retranscribe": 'å†è»¢å†™',
Â  Â  Â  Â  "error": 'ã‚¨ãƒ©ãƒ¼:',
Â  Â  Â  Â  "firestore_no_db_connect": "âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€",
Â  Â  Â  Â  "save_history_success": "âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
Â  Â  Â  Â  "uploaded_file": 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰',
Â  Â  Â  Â  "transcript_result": 'è»¢å†™çµæœ:',
Â  Â  Â  Â  "transcript_text": 'è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆ',
Â  Â  Â  Â  "llm_error_key": "âš ï¸ è­¦å‘Š: GEMINI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secretsã«'GEMINI_API_KEY'ã‚’è¨­ç½®ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "llm_error_init": "LLMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "simulation_warning_query": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "simulation_no_key_warning": "âš ï¸ APIã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å¿œç­”ã®ç”Ÿæˆã¯ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚",
Â  Â  Â  Â  "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
Â  Â  Â  Â  "simulation_draft_header": "æ¨å¥¨ã•ã‚Œã‚‹å¯¾å¿œè‰æ¡ˆ",
Â  Â  Â  Â  "button_listen_audio": "éŸ³å£°ã§èã",
Â  Â  Â  Â  "tts_status_ready": "éŸ³å£°å†ç”Ÿã®æº–å‚™ãŒã§ãã¾ã—ãŸ",
Â  Â  Â  Â  "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
Â  Â  Â  Â  "tts_status_success": "âœ… éŸ³å£°å†ç”Ÿå®Œäº†!",
Â  Â  Â  Â  "tts_status_error": "âŒ TTS APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
Â  Â  Â  Â  "history_expander_title": "ğŸ“ ä»¥å‰ã®å¯¾å¿œå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰ (æœ€æ–° 10ä»¶)",Â 
Â  Â  Â  Â  "initial_query_sample": "ãƒ•ãƒ©ãƒ³ã‚¹ã®ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã§è³¼å…¥ã—ãŸeSIMãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚æ¥ç¶šã§ããªãã¦å›°ã£ã¦ã„ã¾ã™ã€‚ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ",Â 
Â  Â  Â  Â  "button_mic_input": "ğŸ™ éŸ³å£°å…¥åŠ›",
Â  Â  Â  Â  "prompt_customer_end": "ãŠå®¢æ§˜ã‹ã‚‰ã®è¿½åŠ ã®ãŠå•ã„åˆã‚ã›ãŒãªã„ãŸã‚ã€æœ¬ãƒãƒ£ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆã‚’çµ‚äº†ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
Â  Â  Â  Â  "prompt_survey": "ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚è¿½åŠ ã®ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã„ã¤ã§ã‚‚ã”é€£çµ¡ãã ã•ã„ã€‚",
Â  Â  Â  Â  "customer_closing_confirm": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
Â  Â  Â  Â  "customer_positive_response": "è¦ªåˆ‡ãªã”å¯¾å¿œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚",
Â  Â  Â  Â  "button_end_chat": "å¯¾å¿œçµ‚äº† (ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã‚’ä¾é ¼)",
Â  Â  Â  Â  "agent_response_header": "âœï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”",
Â  Â  Â  Â  "agent_response_placeholder": "é¡§å®¢ã«è¿”ä¿¡ (å¿…é ˆæƒ…å ±ã®è¦æ±‚/ç¢ºèªã€ã¾ãŸã¯è§£æ±ºç­–ã®æç¤º)",
Â  Â  Â  Â  "send_response_button": "å¿œç­”é€ä¿¡",
Â  Â  Â  Â  "request_rebuttal_button": "é¡§å®¢ã®æ¬¡ã®åå¿œã‚’è¦æ±‚",Â 
Â  Â  Â  Â  "new_simulation_button": "æ–°ã—ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹",
Â  Â  Â  Â  "history_selectbox_label": "å±¥æ­´ã‚’é¸æŠã—ã¦ãƒ­ãƒ¼ãƒ‰:",
Â  Â  Â  Â  "history_load_button": "é¸æŠã•ã‚ŒãŸå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰",
Â  Â  Â  Â  "delete_history_button": "âŒ å…¨å±¥æ­´ã‚’å‰Šé™¤",Â 
Â  Â  Â  Â  "delete_confirm_message": "æœ¬å½“ã«ã™ã¹ã¦ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’å‰Šé™¤ã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿã“ã®æ“ä½œã¯å…ƒã«æˆ»ã›ã¾ã›ã‚“ã€‚",Â 
Â  Â  Â  Â  "delete_confirm_yes": "ã¯ã„ã€å‰Šé™¤ã—ã¾ã™",Â 
Â  Â  Â  Â  "delete_confirm_no": "ã„ã„ãˆã€ç¶­æŒã—ã¾ã™",Â 
Â  Â  Â  Â  "deleting_history_progress": "å±¥æ­´å‰Šé™¤ä¸­...",Â 
Â  Â  Â  Â  "search_history_label": "å±¥æ­´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢",Â 
Â  Â  Â  Â  "date_range_label": "æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼",Â 
Â  Â  Â  Â  "no_history_found": "æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
Â  Â  Â  Â  "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»DBçµ±åˆ)",
Â  Â  Â  Â  "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
Â  Â  Â  Â  "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
Â  Â  Â  Â  "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
Â  Â  Â  Â  "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
Â  Â  Â  Â  "content_tab": "ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
Â  Â  Â  Â  "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
Â  Â  Â  Â  "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",Â 
Â  Â  Â  Â  "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
Â  Â  Â  Â  "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
Â  Â  Â  Â  "rag_input_placeholder": "å­¦ç¿’è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
Â  Â  Â  Â  "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
Â  Â  Â  Â  "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«åˆã‚ã›ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
Â  Â  Â  Â  "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
Â  Â  Â  Â  "level_label": "é›£æ˜“åº¦",
Â  Â  Â  Â  "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼",
Â  Â  Â  Â  "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
Â  Â  Â  Â  "content_options": ["æ ¸å¿ƒè¦ç´„ãƒãƒ¼ãƒˆ", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢"],
Â  Â  Â  Â  "button_generate": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
Â  Â  Â  Â  "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "lstm_header": "LSTMãƒ™ãƒ¼ã‚¹é”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
Â  Â  Â  Â  "lstm_desc": "ä»®æƒ³ã®éå»ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€LSTMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦å°†æ¥ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—è¡¨ç¤ºã—ã¾ã™ã€‚",
Â  Â  Â  Â  "embed_success": "å…¨{count}ãƒãƒ£ãƒ³ã‚¯ã§å­¦ç¿’DBæ§‹ç¯‰å®Œäº†!",
Â  Â  Â  Â  "warning_no_files": "ã¾ãšå­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "warning_rag_not_ready": "RAGãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€åˆ†æé–‹å§‹ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
Â  Â  Â  Â  "select_answer": "æ­£è§£ã‚’é¸æŠã—ã¦ãã ã•ã„",
Â  Â  Â  Â  "check_answer": "æ­£è§£ã‚’ç¢ºèª",
Â  Â  Â  Â  "next_question": "æ¬¡ã®è³ªå•",
Â  Â  Â  Â  "correct_answer": "æ­£è§£ã§ã™! ğŸ‰",
Â  Â  Â  Â  "incorrect_answer": "ä¸æ­£è§£ã§ã™ã€‚ğŸ˜",
Â  Â  Â  Â  "correct_is": "æ­£è§£",
Â  Â  Â  Â  "explanation": "è§£èª¬",
Â  Â  Â  Â  "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
Â  Â  Â  Â  "score": "ã‚¹ã‚³ã‚¢",
Â  Â  Â  Â  "retake_quiz": "ã‚¯ã‚¤ã‚ºã‚’å†æŒ‘æˆ¦",
Â  Â  Â  Â  "quiz_error_llm": "LLMãŒæ­£ã—ã„JSONã®å½¢å¼ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã®ã§ã€ã‚¯ã‚¤ã‚ºã®ç”ŸæˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚",
Â  Â  Â  Â  "quiz_original_response": "LLM åŸæœ¬å¿œç­”",
Â  Â  Â  Â  "firestore_loading": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...",
Â  Â  Â  Â  "db_save_complete": "(DBä¿å­˜å®Œäº†)",Â 
Â  Â  Â  Â  "data_analysis_progress": "è³‡æ–™åˆ†æãŠã‚ˆã³å­¦ç¿’DBæ§‹ç¯‰ä¸­...",Â 
Â  Â  Â  Â  "response_generating": "å¿œç­”ç”Ÿæˆä¸­...",Â 
Â  Â  Â  Â  "lstm_result_header": "é”æˆåº¦äºˆæ¸¬çµæœ",
Â  Â  Â  Â  "lstm_score_metric": "ç¾åœ¨ã®äºˆæ¸¬é”æˆåº¦",
Â  Â  Â  Â  "lstm_score_info": "æ¬¡ã®ã‚¯ã‚¤ã‚ºã®æ¨å®šã‚¹ã‚³ã‚¢ã¯ç´„ **{predicted_score:.1f}ç‚¹**ã§ã™ã€‚å­¦ç¿’ã®æˆæœã‚’ç¶­æŒã¾ãŸã¯å‘ä¸Šã•ã›ã¦ãã ã•ã„ï¼",
Â  Â  Â  Â  "lstm_rerun_button": "æ–°ã—ã„ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬",
Â  Â  Â  Â  "rec_header": "éŸ³å£°å…¥åŠ›ã¨è»¢å†™",
Â  Â  Â  Â  "whisper_processing": "éŸ³å£°è»¢å†™å‡¦ç†ä¸­",
Â  Â  Â  Â  "empty_response_warning": "å¿œç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
Â  Â  }
}

# st.set_page_configëŠ” ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
if 'language' not in st.session_state:
Â  Â  st.session_state.language = DEFAULT_LANG
if 'is_llm_ready' not in st.session_state:
Â  Â  st.session_state.is_llm_ready = False
if 'llm_init_error_msg' not in st.session_state:
Â  Â  st.session_state.llm_init_error_msg = ""
if 'uploaded_files_state' not in st.session_state:
Â  Â  st.session_state.uploaded_files_state = None
if 'is_rag_ready' not in st.session_state:
Â  Â  st.session_state.is_rag_ready = False
if 'simulator_messages' not in st.session_state:
Â  Â  st.session_state.simulator_messages = []
if 'simulator_memory' not in st.session_state:
Â  Â  st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history")
if 'initial_advice_provided' not in st.session_state:
Â  Â  st.session_state.initial_advice_provided = False
if 'is_chat_ended' not in st.session_state:
Â  Â  st.session_state.is_chat_ended = False
if 'show_delete_confirm' not in st.session_state:
Â  Â  st.session_state.show_delete_confirm = False
if 'last_transcript' not in st.session_state:
Â  Â  st.session_state.last_transcript = ""
if 'sim_audio_upload_key' not in st.session_state:
Â  Â  st.session_state.sim_audio_upload_key = 0

L = LANG[st.session_state.language]


# -----------------------------
# 1. Firebase Admin, GCS, OpenAI Initialization
# -----------------------------

def _load_service_account_from_secrets():
Â  Â  """Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. (UI ì¶œë ¥ ì—†ìŒ)"""
Â  Â  if "FIREBASE_SERVICE_ACCOUNT_JSON" not in st.secrets:
Â  Â  Â  Â  return None, "FIREBASE_SERVICE_ACCOUNT_JSON Secretì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
Â  Â  service_account_data = st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"]
Â  Â  sa_info = None
Â  Â  if isinstance(service_account_data, str):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  sa_info = json.loads(service_account_data.strip())
Â  Â  Â  Â  except json.JSONDecodeError as e:
Â  Â  Â  Â  Â  Â  return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ JSON êµ¬ë¬¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ìƒì„¸ ì˜¤ë¥˜: {e}"
Â  Â  elif hasattr(service_account_data, 'get'):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  sa_info = dict(service_account_data)
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì‹¤íŒ¨."
Â  Â  else:
Â  Â  Â  Â  return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
Â  Â Â 
Â  Â  if not sa_info.get("project_id") or not sa_info.get("private_key"):
Â  Â  Â  Â  return None, "JSON ë‚´ 'project_id' ë˜ëŠ” 'private_key' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
Â  Â  return sa_info, None


@st.cache_resource(ttl=None)
def initialize_firestore_admin(L):
Â  Â  """Secretsì—ì„œ ë¡œë“œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•˜ê³  DB í´ë¼ì´ì–¸íŠ¸ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
Â  Â  sa_info, error_message = _load_service_account_from_secrets()
Â  Â  if error_message:
Â  Â  Â  Â  return None, f"âŒ Firebase Secret ì˜¤ë¥˜: {error_message}"
Â  Â Â 
Â  Â  db_client = None
Â  Â  try:
Â  Â  Â  Â  # Check if app is already initialized (important for Streamlit rerun)
Â  Â  Â  Â  if firebase_admin._apps:
Â  Â  Â  Â  Â  Â  db_client = firestore.client()
Â  Â  Â  Â  Â  Â  return db_client, "âœ… Firestore DB í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
Â  Â  Â  Â Â 
Â  Â  Â  Â  cred = credentials.Certificate(sa_info)
Â  Â  Â  Â  initialize_app(cred)
Â  Â  Â  Â  db_client = firestore.client()
Â  Â  Â  Â  return db_client, "âœ… Firestore DB í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
Â  Â  except Exception as e:
Â  Â  Â  Â  return None, f"ğŸ”¥ {L['firebase_init_fail']}: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¬¸ì œ. ì˜¤ë¥˜: {e}"


def get_gcs_bucket_name():
Â  Â  return st.secrets.get('GCS_BUCKET_NAME') or os.environ.get('GCS_BUCKET_NAME')

@st.cache_resource
def init_gcs_client(L):
Â  Â  """GCS í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
Â  Â  sa, _ = _load_service_account_from_secrets()
Â  Â  gcs_bucket_name = get_gcs_bucket_name()
Â  Â Â 
Â  Â  if not gcs_bucket_name:
Â  Â  Â  Â  return None, L['gcs_missing']
Â  Â  if not sa:
Â  Â  Â  Â  return None, "GCS ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ëˆ„ë½"
Â  Â Â 
Â  Â  gcs_client = None
Â  Â  try:
Â  Â  Â  Â  # Write credentials to temp file (necessary for gcs.Client() in container environments)
Â  Â  Â  Â  tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.json')
Â  Â  Â  Â  tmp.write(json.dumps(sa).encode('utf-8'))
Â  Â  Â  Â  tmp.flush()
Â  Â  Â  Â  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = tmp.name
Â  Â  Â  Â  gcs_client = storage.Client()
Â  Â  Â  Â  return gcs_client, "âœ… GCS í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
Â  Â  except Exception as e:
Â  Â  Â  Â  return None, f"{L['gcs_init_fail']}: {e}"


@st.cache_resource
def init_openai_client(L):
Â  Â  """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
Â  Â  openai_key = st.secrets.get('OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')
Â  Â  if openai_key:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  return OpenAI(api_key=openai_key), "âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  return None, f"OpenAI client init error: {e}"
Â  Â  return None, L['openai_missing']

# -----------------------------
# 2. GCS, Firestore, Whisper HelpersÂ 
# -----------------------------

def upload_audio_to_gcs(bucket_name: str, blob_name: str, audio_bytes: bytes, content_type: str = 'audio/webm'):
Â  Â  L = LANG[st.session_state.language]
Â  Â  gcs_client = init_gcs_client(L)[0]
Â  Â  if not gcs_client:
Â  Â  Â  Â  raise RuntimeError(L['gcs_not_conf'])
Â  Â  bucket = gcs_client.bucket(bucket_name)
Â  Â  blob = bucket.blob(blob_name)
Â  Â  blob.upload_from_string(audio_bytes, content_type=content_type)
Â  Â  return f'gs://{bucket_name}/{blob_name}'Â 

def download_audio_from_gcs(bucket_name: str, blob_name: str) -> bytes:
Â  Â  L = LANG[st.session_state.language]
Â  Â  gcs_client = init_gcs_client(L)[0]
Â  Â  if not gcs_client:
Â  Â  Â  Â  raise RuntimeError(L['gcs_not_conf'])
Â  Â  try:
Â  Â  Â  Â  bucket = gcs_client.bucket(bucket_name)
Â  Â  Â  Â  blob = bucket.blob(blob_name)
Â  Â  Â  Â  return blob.download_as_bytes()
Â  Â  except NotFound:
Â  Â  Â  Â  raise FileNotFoundError(f"GCS Blob not found: {blob_name}")
Â  Â  except Exception as e:
Â  Â  Â  Â  raise RuntimeError(f"{L['gcs_playback_fail']}: {e}")

def save_audio_record(db, bucket_name, audio_bytes: bytes, filename: str, transcript_text: str, meta: dict = None, mime_type: str = 'audio/webm'):
Â  Â  L = LANG[st.session_state.language]
Â  Â  if not db:
Â  Â  Â  Â  raise RuntimeError('Firestore not initialized')

Â  Â  ts = datetime.now(timezone.utc)
Â  Â  doc_ref = db.collection('voice_records').document()
Â  Â  blob_name = f"voice_records/{doc_ref.id}/{filename}"

Â  Â  gcs_path = None
Â  Â  gcs_client = init_gcs_client(L)[0]
Â  Â  if bucket_name and gcs_client:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  gcs_path = upload_audio_to_gcs(bucket_name, blob_name, audio_bytes, mime_type)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.warning(f"{L['upload_fail']}: {e}")
Â  Â  Â  Â  Â  Â  gcs_path = None
Â  Â  else:
Â  Â  Â  Â  st.warning(L['gcs_missing'])

Â  Â  data = {
Â  Â  Â  Â  'created_at': ts,
Â  Â  Â  Â  'filename': filename,
Â  Â  Â  Â  'size': len(audio_bytes),
Â  Â  Â  Â  'gcs_path': gcs_path,
Â  Â  Â  Â  'transcript': transcript_text,
Â  Â  Â  Â  'mime_type': mime_type,Â 
Â  Â  Â  Â  'language': st.session_state.language,
Â  Â  Â  Â  'meta': meta or {}
Â  Â  }

Â  Â  doc_ref.set(data)
Â  Â  return doc_ref.id

def delete_audio_record(db, bucket_name, doc_id: str):
Â  Â  L = LANG[st.session_state.language]
Â  Â  doc_ref = db.collection('voice_records').document(doc_id)
Â  Â  doc = doc_ref.get()
Â  Â  if not doc.exists:
Â  Â  Â  Â  return False
Â  Â  data = doc.to_dict()
Â  Â Â 
Â  Â  gcs_client = init_gcs_client(L)[0]
Â  Â  # delete GCS blob
Â  Â  try:
Â  Â  Â  Â  if data.get('gcs_path') and gcs_client and bucket_name:
Â  Â  Â  Â  Â  Â  blob_name = data['gcs_path'].split(f'gs://{bucket_name}/')[-1]
Â  Â  Â  Â  Â  Â  bucket = gcs_client.bucket(bucket_name)
Â  Â  Â  Â  Â  Â  blob = bucket.blob(blob_name)
Â  Â  Â  Â  Â  Â  blob.delete()
Â  Â  except Exception as e:
Â  Â  Â  Â  st.warning(f"GCS delete warning: {e}")
Â  Â Â 
Â  Â  # delete firestore doc
Â  Â  doc_ref.delete()
Â  Â  return True

def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = 'audio/webm'):
Â  Â  L = LANG[st.session_state.language]
Â  Â  openai_client = init_openai_client(L)[0]Â 
Â  Â  if openai_client is None:
Â  Â  Â  Â  raise RuntimeError(L['openai_missing'])
Â  Â Â 
Â  Â  # Determine file extension
Â  Â  ext = mime_type.split('/')[-1].lower() if '/' in mime_type else 'webm'
Â  Â Â 
Â  Â  # write to temp file
Â  Â  tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f'.{ext}')
Â  Â  tmp.write(audio_bytes)
Â  Â  tmp.flush()
Â  Â  tmp.close()
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  with open(tmp.name, 'rb') as af:
Â  Â  Â  Â  Â  Â  res = openai_client.audio.transcriptions.create(
Â  Â  Â  Â  Â  Â  Â  Â  model='whisper-1',Â 
Â  Â  Â  Â  Â  Â  Â  Â  file=af,
Â  Â  Â  Â  Â  Â  Â  Â  response_format='text'
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  return res.strip() or ''
Â  Â  except Exception as e:
Â  Â  Â  Â  raise RuntimeError(f"{L['error']} Whisper: {e}")
Â  Â  finally:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  os.remove(tmp.name)
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  pass


# -----------------------------
# 3. Firestore/RAG/LLM HelpersÂ 
# -----------------------------

def save_simulation_history(db, initial_query, customer_type, messages):
Â  Â  L = LANG[st.session_state.language]
Â  Â  if not db:Â 
Â  Â  Â  Â  st.sidebar.warning(L.get("firestore_no_db_connect")); return False
Â  Â  history_data = [{k: v for k, v in msg.items()} for msg in messages]
Â  Â  data = {
Â  Â  Â  Â  "initial_query": initial_query,
Â  Â  Â  Â  "customer_type": customer_type,
Â  Â  Â  Â  "messages": history_data,
Â  Â  Â  Â  "language_key": st.session_state.language,Â 
Â  Â  Â  Â  "timestamp": firestore.SERVER_TIMESTAMP
Â  Â  }
Â  Â  try:
Â  Â  Â  Â  db.collection("simulation_histories").add(data)
Â  Â  Â  Â  st.sidebar.success(L.get("save_history_success")); return True
Â  Â  except Exception as e:
Â  Â  Â  Â  st.sidebar.error(f"âŒ {L.get('save_history_fail')}: {e}"); return False

def load_simulation_histories(db):
Â  Â  current_lang_key = st.session_state.languageÂ 
Â  Â  if not db: return []
Â  Â  try:
Â  Â  Â  Â  histories = (db.collection("simulation_histories").where("language_key", "==", current_lang_key) .order_by("timestamp", direction=Query.DESCENDING).limit(10).stream())
Â  Â  Â  Â  results = []
Â  Â  Â  Â  for doc in histories:
Â  Â  Â  Â  Â  Â  data = doc.to_dict(); data['id'] = doc.id
Â  Â  Â  Â  Â  Â  if 'messages' in data and isinstance(data['messages'], list) and data['messages']: results.append(data)
Â  Â  Â  Â  return results
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"Error loading histories: {e}"); return []

def delete_all_history(db):
Â  Â  L = LANG[st.session_state.language]
Â  Â  if not db: st.error(L["firestore_no_db_connect"]); return
Â  Â  try:
Â  Â  Â  Â  docs = db.collection("simulation_histories").stream()
Â  Â  Â  Â  for doc in docs: doc.reference.delete()
Â  Â  Â  Â  st.session_state.simulator_messages = []; st.session_state.simulator_memory.clear()
Â  Â  Â  Â  st.session_state.show_delete_confirm = False; st.success(L["delete_success"]); st.rerun()
Â  Â  except Exception as e: st.error(f"{L.get('delete_fail')}: {e}")

# --- Utility Placeholder Functions (For brevity, assumed to be present) ---
def get_document_chunks(files): return []Â 
def get_vector_store(text_chunks): return NoneÂ 
def get_rag_chain(vector_store): return NoneÂ 
def save_index_to_firestore(db, vector_store, index_id="user_portfolio_rag"): return TrueÂ 
def load_index_from_firestore(db, embeddings, index_id="user_portfolio_rag"): return NoneÂ 

def load_or_train_lstm():
Â  Â  # Mock LSTM Model and Data Generation (using required imports)
Â  Â  np.random.seed(42)
Â  Â  # Generate synthetic time series data for past quiz scores (0-100)
Â  Â  n_points = 50
Â  Â  time_series = 60 + 20 * np.sin(np.linspace(0, 4 * np.pi, n_points)) + np.random.normal(0, 5, n_points)
Â  Â  time_series = np.clip(time_series, 50, 100).astype(np.float32)
Â  Â Â 
Â  Â  # Simple mock model creation
Â  Â  model = Sequential()
Â  Â  model.add(LSTM(50, activation='relu', input_shape=(1, 1)))
Â  Â  model.add(Dense(1))
Â  Â  model.compile(optimizer='adam', loss='mse')
Â  Â Â 
Â  Â  # Mock training and prediction
Â  Â  X = time_series[:-1].reshape(-1, 1, 1)
Â  Â  Y = time_series[1:].reshape(-1, 1)
Â  Â  # In a real scenario, model.fit(X, Y, epochs=10, verbose=0) would run
Â  Â Â 
Â  Â  # Mock prediction: Take the last known score and add a slight positive/negative trend
Â  Â  last_score = time_series[-1]
Â  Â  predicted_score = np.clip(last_score + np.random.uniform(-3, 5), 50, 100)
Â  Â Â 
Â  Â  return model, time_series

def force_rerun_lstm(): st.session_state.lstm_rerun_trigger = time.time(); st.rerun()Â 
def render_interactive_quiz(quiz_data, current_lang): st.warning("Quiz UI Placeholder")Â 
def synthesize_and_play_audio(current_lang_key): st.components.v1.html(f"""<script>window.speakText = (text, langKey) => {{ console.log('Speaking: ' + text + ' in ' + langKey); }}</script>""", height=5, width=0)Â 
def render_tts_button(text_to_speak, current_lang_key): st.button(LANG[current_lang_key].get("button_listen_audio"), key=f"tts_{hash(text_to_speak)}", on_click=lambda: st.components.v1.html(f"""<script>
Â  Â  const statusDiv = document.getElementById('tts_status');
Â  Â  statusDiv.innerHTML = '{LANG[current_lang_key]["tts_status_generating"]}';
Â  Â  statusDiv.style.backgroundColor = '#fff3cd';
Â  Â  fetch("https://texttospeech.googleapis.com/v1/text:synthesize", {{
Â  Â  Â  method: 'POST',
Â  Â  Â  headers: {{ 'Content-Type': 'application/json' }},
Â  Â  Â  body: JSON.stringify({{
Â  Â  Â  Â  input: {{ text: text_to_speak }},
Â  Â  Â  Â  voice: {{ languageCode: '{current_lang_key}', ssmlGender: 'FEMALE' }},
Â  Â  Â  Â  audioConfig: {{ audioEncoding: 'MP3' }}
Â  Â  Â  }})
Â  Â  }})
Â  Â  .then(response => response.json())
Â  Â  .then(data => {{
Â  Â  Â  if (data.audioContent) {{
Â  Â  Â  Â  const audio = new Audio("data:audio/mp3;base64," + data.audioContent);
Â  Â  Â  Â  audio.onended = () => {{
Â  Â  Â  Â  Â  statusDiv.innerHTML = '{LANG[current_lang_key]["tts_status_success"]}';
Â  Â  Â  Â  Â  statusDiv.style.backgroundColor = '#d4edda';
Â  Â  Â  Â  }};
Â  Â  Â  Â  audio.play();
Â  Â  Â  }} else {{
Â  Â  Â  Â  statusDiv.innerHTML = '{LANG[current_lang_key]["tts_status_error"]}';
Â  Â  Â  Â  statusDiv.style.backgroundColor = '#f8d7da';
Â  Â  Â  }}
Â  Â  }})
Â  Â  .catch(error => {{
Â  Â  Â  statusDiv.innerHTML = '{LANG[current_lang_key]["tts_status_error"]}';
Â  Â  Â  statusDiv.style.backgroundColor = '#f8d7da';
Â  Â  Â  console.error('TTS Error:', error);
Â  Â  }});
Â  Â  Â  Â  """.replace("text_to_speak", json.dumps(text_to_speak).replace('\\n', ' ')), height=0, width=0))
def clean_and_load_json(text): 
Â  Â  match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
Â  Â  if match: text = match.group(1)
Â  Â  try: return json.loads(text)
Â  Â  except json.JSONDecodeError: return None

def get_mock_response_data(lang_key, customer_type): L = LANG[lang_key]; return {"advice_header": f"{L['simulation_advice_header']}", "advice": f"Mock advice for {customer_type}", "draft_header": f"{L['simulation_draft_header']}", "draft": f"Mock draft response in {lang_key}"}
def get_closing_messages(lang_key):
Â  Â  if lang_key == 'ko': return {"additional_query": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?", "chat_closing": LANG['ko']['prompt_survey']}
Â  Â  elif lang_key == 'en': return {"additional_query": "Is there anything else we can assist you with today?", "chat_closing": LANG['en']['prompt_survey']}
Â  Â  elif lang_key == 'ja': return {"additional_query": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ", "chat_closing": LANG['ja']['prompt_survey']}
Â  Â  return get_closing_messages('ko')
# --- End Helper Functions ---


# --- í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤í–‰ ---
firestore_db_client, db_msg = initialize_firestore_admin(L)
st.session_state.firestore_db = firestore_db_client
st.session_state.db_init_msg = db_msg

gcs_client_obj, gcs_msg = init_gcs_client(L)
gcs_client = gcs_client_obj
st.session_state.gcs_init_msg = gcs_msg

openai_client_obj, openai_msg = init_openai_client(L)
openai_client = openai_client_obj
st.session_state.openai_init_msg = openai_msg

# --- LLM ì´ˆê¸°í™” ---
API_KEY = os.environ.get("GEMINI_API_KEY") or st.secrets.get("GEMINI_API_KEY")
if 'llm' not in st.session_state and API_KEY:
Â  Â  try:
Â  Â  Â  Â  st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
Â  Â  Â  Â  st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
Â  Â  Â  Â  st.session_state.is_llm_ready = True
Â  Â  Â  Â Â 
Â  Â  Â  Â  SIMULATOR_PROMPT = PromptTemplate(
Â  Â  Â  Â  Â  Â  template="The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n\n{chat_history}\nHuman: {input}\nAI:",
Â  Â  Â  Â  Â  Â  input_variables=["input", "chat_history"]
Â  Â  Â  Â  )
Â  Â  Â  Â  st.session_state.simulator_chain = ConversationChain(
Â  Â  Â  Â  Â  Â  llm=st.session_state.llm,
Â  Â  Â  Â  Â  Â  memory=st.session_state.simulator_memory,
Â  Â  Â  Â  Â  Â  prompt=SIMULATOR_PROMPT,
Â  Â  Â  Â  Â  Â  input_key="input",
Â  Â  Â  Â  )
Â  Â  except Exception as e:
Â  Â  Â  Â  st.session_state.llm_init_error_msg = f"{L['llm_error_init']} (Gemini): {e}"
Â  Â  Â  Â  st.session_state.is_llm_ready = False
elif not API_KEY:
Â  Â  st.session_state.llm_init_error_msg = L["llm_error_key"]

# RAG Index Loading
if st.session_state.get('firestore_db') and 'conversation_chain' not in st.session_state and st.session_state.is_llm_ready:
Â  Â  loaded_index = load_index_from_firestore(st.session_state.firestore_db, st.session_state.embeddings)
Â  Â  if loaded_index:
Â  Â  Â  Â  st.session_state.conversation_chain = get_rag_chain(loaded_index)
Â  Â  Â  Â  st.session_state.is_rag_ready = True
Â  Â  Â  Â  st.session_state.firestore_load_success = True
Â  Â  else:
Â  Â  Â  Â  st.session_state.firestore_load_success = False


# -----------------------------
# 9. UI RENDERING LOGIC
# -----------------------------

# ì‚¬ì´ë“œë°” ì„¤ì • ì‹œì‘
with st.sidebar:
Â  Â  selected_lang_key = st.selectbox(
Â  Â  Â  Â  L["lang_select"],
Â  Â  Â  Â  options=['ko', 'en', 'ja'],
Â  Â  Â  Â  index=['ko', 'en', 'ja'].index(st.session_state.language),
Â  Â  Â  Â  format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
Â  Â  )
Â  Â Â 
Â  Â  if selected_lang_key != st.session_state.language:
Â  Â  Â  Â  st.session_state.language = selected_lang_key
Â  Â  Â  Â  # L ì—…ë°ì´íŠ¸
Â  Â  Â  Â  L = LANG[st.session_state.language]
Â  Â  Â  Â  # LLM ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë©”ì‹œì§€ë„ ì—…ë°ì´íŠ¸
Â  Â  Â  Â  if not st.session_state.is_llm_ready:
Â  Â  Â  Â  Â  Â  st.session_state.llm_init_error_msg = L["llm_error_key"]
Â  Â  Â  Â  st.rerun()
Â  Â Â 
Â  Â  L = LANG[st.session_state.language]
Â  Â  st.title(L["sidebar_title"])
Â  Â Â 
Â  Â  st.markdown("---")
Â  Â  st.subheader("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœ")
Â  Â Â 
Â  Â  # --- ì´ˆê¸°í™” ìƒíƒœ í‘œì‹œ ---
Â  Â  if st.session_state.get('llm_init_error_msg'):
Â  Â  Â  Â  st.error(st.session_state.llm_init_error_msg)
Â  Â  elif st.session_state.is_llm_ready:
Â  Â  Â  Â  st.success("âœ… LLM ë° ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

Â  Â  # DB & GCS ìƒíƒœ í‘œì‹œ
Â  Â  # DB ë©”ì‹œì§€ë¥¼ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
Â  Â  if "âœ…" in st.session_state.db_init_msg: st.success(st.session_state.db_init_msg)
Â  Â  else: st.warning(L.get("firebase_init_fail") if "ğŸ”¥" in st.session_state.db_init_msg else st.session_state.db_init_msg)
Â  Â Â 
Â  Â  # GCS ë©”ì‹œì§€ë¥¼ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
Â  Â  if "âœ…" in st.session_state.gcs_init_msg: st.success(st.session_state.gcs_init_msg)
Â  Â  else: st.warning(L.get("gcs_missing") if "GCS bucket is not configured" in st.session_state.gcs_init_msg else st.session_state.gcs_init_msg)
Â  Â Â 
Â  Â  # OpenAI ë©”ì‹œì§€ë¥¼ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
Â  Â  if "âœ…" in st.session_state.openai_init_msg: st.success(st.session_state.openai_init_msg)
Â  Â  else: st.warning(L.get("openai_missing") if "missing" in st.session_state.openai_init_msg else st.session_state.openai_init_msg)

Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # RAG Indexing Section
Â  Â  uploaded_files_widget = st.file_uploader(
Â  Â  Â  Â  L["file_uploader"], type=["pdf","txt","html"], accept_multiple_files=True
Â  Â  )
Â  Â  if uploaded_files_widget: st.session_state.uploaded_files_state = uploaded_files_widget
Â  Â  files_to_process = st.session_state.uploaded_files_state if st.session_state.uploaded_files_state else []
Â  Â Â 
Â  Â  if files_to_process and st.session_state.is_llm_ready and st.session_state.firestore_db:
Â  Â  Â  Â  if st.button(L["button_start_analysis"], key="start_analysis"):
Â  Â  Â  Â  Â  Â  with st.spinner(L["data_analysis_progress"]):
Â  Â  Â  Â  Â  Â  Â  Â  text_chunks = get_document_chunks(files_to_process)
Â  Â  Â  Â  Â  Â  Â  Â  vector_store = get_vector_store(text_chunks)
Â  Â  Â  Â  Â  Â  Â  Â  if vector_store:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_success = save_index_to_firestore(st.session_state.firestore_db, vector_store)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(L["embed_success"].format(count=len(text_chunks)) + (" " + L["db_save_complete"] if save_success else " (DB Save Failed)"))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.conversation_chain = get_rag_chain(vector_store)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_rag_ready = True
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_rag_ready = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(L["embed_fail"])
Â  Â  elif not files_to_process:
Â  Â  Â  Â  st.warning(L.get("warning_no_files"))Â 

Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # Feature Selection Radio
Â  Â  feature_selection = st.radio(
Â  Â  Â  Â  "ê¸°ëŠ¥ ì„ íƒ",Â 
Â  Â  Â  Â  [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["simulator_tab"], L["voice_rec_header"]]
Â  Â  )

st.title(L["title"])

# ================================
# 10. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================

if feature_selection == L["voice_rec_header"]:
Â  Â  st.header(L['voice_rec_header'])
Â  Â  st.caption(L['record_help'])

Â  Â  col_rec_ui, col_list_ui = st.columns([1, 1])

Â  Â  with col_rec_ui:
Â  Â  Â  Â  st.subheader(L['rec_header'])
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Audio Input Widget
Â  Â  Â  Â  audio_obj = None
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # st.audio_inputì€ streamlit-webrtcë¥¼ ì„¤ì¹˜í•´ì•¼ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
Â  Â  Â  Â  Â  Â  # if hasattr(st, 'audio_input'):
Â  Â  Â  Â  Â  Â  # Â  Â  audio_obj = st.audio_input(L["button_mic_input"], key='main_recorder_input')
Â  Â  Â  Â  Â  Â  # else:
Â  Â  Â  Â  Â  Â  # Â  Â  st.caption(f"({L['uploaded_file']}ë¡œ ëŒ€ì²´ - `st.audio_input` ë¯¸ì§€ì›)")
Â  Â  Â  Â  Â  Â  # Â  Â  audio_obj = st.file_uploader(L['uploaded_file'], type=['wav', 'mp3', 'm4a', 'webm'], key='main_file_uploader')
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # í˜„ì¬ Streamlit í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ st.file_uploaderë§Œ ì‚¬ìš©í•˜ë„ë¡ ëŒ€ì²´
Â  Â  Â  Â  Â  Â  audio_obj = st.file_uploader(L['uploaded_file'], type=['wav', 'mp3', 'm4a', 'webm'], key='main_file_uploader')
Â  Â  Â  Â  Â  Â  st.caption(f"({L['uploaded_file']}ë§Œ ì§€ì›)")
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  audio_obj = None

Â  Â  Â  Â  audio_bytes = None
Â  Â  Â  Â  audio_mime = 'audio/webm'
Â  Â  Â  Â  if audio_obj is not None:
Â  Â  Â  Â  Â  Â  if hasattr(audio_obj, 'getvalue'):
Â  Â  Â  Â  Â  Â  Â  Â  audio_bytes = audio_obj.getvalue()
Â  Â  Â  Â  Â  Â  Â  Â  audio_mime = getattr(audio_obj, 'type', 'audio/webm')
Â  Â  Â  Â Â 
Â  Â  Â  Â  if audio_bytes:
Â  Â  Â  Â  Â  Â  st.audio(audio_bytes, format=audio_mime)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Transcribe Action
Â  Â  Â  Â  Â  Â  if st.button(L['transcribe_btn'], key='transcribe_btn_key_rec'):
Â  Â  Â  Â  Â  Â  Â  Â  if openai_client is None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(L['openai_missing'])
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L['transcribing']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  transcript_text = transcribe_bytes_with_whisper(audio_bytes, audio_mime)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['last_transcript'] = transcript_text
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(L['transcript_result'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except RuntimeError as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(e)

Â  Â  Â  Â  Â  Â  st.text_area(L['transcript_text'], value=st.session_state.get('last_transcript', ''), height=150, key='transcript_area_rec')

Â  Â  Â  Â  Â  Â  # Save Action
Â  Â  Â  Â  Â  Â  if st.button(L['save_btn'], key='save_btn_key_rec'):
Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state.firestore_db is None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(L['firebase_init_fail'])
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bucket_name = get_gcs_bucket_name()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ext = audio_mime.split('/')[-1] if '/' in audio_mime else 'webm'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  filename = f"record_{int(time.time())}.{ext}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  transcript_text = st.session_state.get('last_transcript', '')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_audio_record(st.session_state.firestore_db, bucket_name, audio_bytes, filename, transcript_text, mime_type=audio_mime)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(L['saved_success'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['last_transcript'] = ''
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{L['error']} {e}")

Â  Â  with col_list_ui:
Â  Â  Â  Â  st.subheader(L['rec_list_title'])
Â  Â  Â  Â  if st.session_state.firestore_db is None:
Â  Â  Â  Â  Â  Â  st.warning(L['firebase_init_fail'] + ' â€” ì´ë ¥ ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€')
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  docs = list(st.session_state.firestore_db.collection('voice_records').order_by('created_at', direction=firestore.Query.DESCENDING).limit(50).stream())
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Firestore read error: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  docs = []

Â  Â  Â  Â  Â  Â  if not docs:
Â  Â  Â  Â  Â  Â  Â  Â  st.info(L['no_records'])
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  bucket_name = get_gcs_bucket_name()
Â  Â  Â  Â  Â  Â  Â  Â  for d in docs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = d.to_dict()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  doc_id = d.id
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  created_at_ts = data.get('created_at')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  created_str = created_at_ts.astimezone(timezone.utc).strftime('%Y-%m-%d %H:%M UTC') if isinstance(created_at_ts, datetime) else str(created_at_ts)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  transcript_snippet = (data.get('transcript') or '')[:50].replace('\n', ' ') + '...'

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander(f"[{created_str}] {transcript_snippet}"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"**{L['transcript_text']}:** {data.get('transcript') or 'N/A'}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"**Size:** {data.get('size')} bytes | **Path:** {data.get('gcs_path', L['gcs_not_conf'])}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  colp, colr, cold = st.columns([2, 1, 1])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Playback Button
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if colp.button(L['playback'], key=f'play_{doc_id}'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if data.get('gcs_path') and gcs_client and bucket_name:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L['playback']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(f'gs://{bucket_name}/')[-1])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime_type = data.get('mime_type', 'audio/webm')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.audio(blob_bytes, format=mime_type)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{L['gcs_playback_fail']}: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(L['gcs_no_audio'])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Re-transcribe Button
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if colr.button(L['retranscribe'], key=f'retx_{doc_id}'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if openai_client is None: st.error(L['openai_missing'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif data.get('gcs_path') and gcs_client and bucket_name:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L['transcribing']):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(f'gs://{bucket_name}/')[-1])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime_type = data.get('mime_type', 'audio/webm')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_text = transcribe_bytes_with_whisper(blob_bytes, mime_type)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.firestore_db.collection('voice_records').document(doc_id).update({'transcript': new_text})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(L['retranscribe'] + ' ' + L['saved_success'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{L['error']} {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.error(L['gcs_not_conf'])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Delete Button
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if cold.button(L['delete'], key=f'del_{doc_id}'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state.get(f'confirm_del_rec_{doc_id}', False):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ok = delete_audio_record(st.session_state.firestore_db, bucket_name, doc_id)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if ok: st.success(L['delete_success'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.error(L['delete_fail'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state[f'confirm_del_rec_{doc_id}'] = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state[f'confirm_del_rec_{doc_id}'] = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(L['delete_confirm_rec'])

elif feature_selection == L["simulator_tab"]:Â 
Â  Â  # (Simulator UI logic remains the same, using st.session_state.firestore_db)
Â  Â  st.header(L["simulator_header"])
Â  Â  st.markdown(L["simulator_desc"])
Â  Â Â 
Â  Â  # 1. TTS ìœ í‹¸ë¦¬í‹° (ìƒíƒœ í‘œì‹œê¸° ë° JS í•¨ìˆ˜)ë¥¼ í˜ì´ì§€ ìƒë‹¨ì— ì‚½ì…
Â  Â  st.markdown(f'<div id="tts_status" style="padding: 5px; text-align: center; border-radius: 5px; background-color: #f0f0f0; margin-bottom: 10px;">{L["tts_status_ready"]}</div>', unsafe_allow_html=True)
Â  Â  if "tts_js_loaded" not in st.session_state:
Â  Â  Â  Â  Â synthesize_and_play_audio(st.session_state.language)Â 
Â  Â  Â  Â  Â st.session_state.tts_js_loaded = True

Â  Â  # 1.5 ì´ë ¥ ì‚­ì œ ë²„íŠ¼ ë° ëª¨ë‹¬
Â  Â  db = st.session_state.get('firestore_db')
Â  Â  col_delete, _ = st.columns([1, 4])
Â  Â  with col_delete:
Â  Â  Â  Â  if st.button(L["delete_history_button"], key="trigger_delete_history_sim"):
Â  Â  Â  Â  Â  Â  st.session_state.show_delete_confirm = True

Â  Â  if st.session_state.show_delete_confirm:
Â  Â  Â  Â  with st.container(border=True):
Â  Â  Â  Â  Â  Â  st.warning(L["delete_confirm_message"])
Â  Â  Â  Â  Â  Â  col_yes, col_no = st.columns(2)
Â  Â  Â  Â  Â  Â  if col_yes.button(L["delete_confirm_yes"], key="confirm_delete_yes", type="primary"):
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L["deleting_history_progress"]):Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  delete_all_history(db)
Â  Â  Â  Â  Â  Â  if col_no.button(L["delete_confirm_no"], key="confirm_delete_no"):
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.show_delete_confirm = False
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  # â­ Firebase ìƒë‹´ ì´ë ¥ ë¡œë“œ ë° ì„ íƒ ì„¹ì…˜
Â  Â  if db:
Â  Â  Â  Â  with st.expander(L["history_expander_title"]):
Â  Â  Â  Â  Â  Â  histories = load_simulation_histories(db)
Â  Â  Â  Â  Â  Â  search_query = st.text_input(L["search_history_label"], key="history_search_sim", value="")
Â  Â  Â  Â  Â  Â  today = datetime.now().date()
Â  Â  Â  Â  Â  Â  default_start_date = today - timedelta(days=7)
Â  Â  Â  Â  Â  Â  date_range_input = st.date_input(L["date_range_label"], value=[default_start_date, today], key="history_date_range_sim")

Â  Â  Â  Â  Â  Â  filtered_histories = []
Â  Â  Â  Â  Â  Â  if histories:
Â  Â  Â  Â  Â  Â  Â  Â  if isinstance(date_range_input, list) and len(date_range_input) == 2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  start_date = min(date_range_input)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  end_date = max(date_range_input) + timedelta(days=1)
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  start_date = datetime.min.date()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  end_date = datetime.max.date()
Â  Â  Â  Â  Â  Â  Â  Â  for h in histories:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  search_match = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if search_query:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  query_lower = search_query.lower()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  searchable_text = h['initial_query'].lower() + " " + h['customer_type'].lower()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if query_lower not in searchable_text: search_match = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  date_match = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if h.get('timestamp'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  h_date = h['timestamp'].date()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not (start_date <= h_date < end_date): date_match = False
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if search_match and date_match: filtered_histories.append(h)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if filtered_histories:
Â  Â  Â  Â  Â  Â  Â  Â  history_options = {f"[{h['timestamp'].strftime('%m-%d %H:%M')}] {h['customer_type']} - {h['initial_query'][:30]}...": h for h in filtered_histories}
Â  Â  Â  Â  Â  Â  Â  Â  selected_key = st.selectbox(L["history_selectbox_label"], options=list(history_options.keys()))
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if st.button(L["history_load_button"], key='load_sim_history'):Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  selected_history = history_options[selected_key]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.customer_query_text_area = selected_history['initial_query']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.initial_advice_provided = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages = selected_history['messages']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_chat_ended = selected_history.get('is_chat_ended', False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.clear()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for msg in selected_history['messages']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â if msg['role'] == 'customer': st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â elif msg['role'] == 'agent_response': st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â elif msg['role'] in ['supervisor', 'customer_rebuttal', 'customer_end', 'system_end']: st.session_state.simulator_memory.chat_memory.add_ai_message(msg['content'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.info(L.get("no_history_found"))

Â  Â  # LLM and UI logic for Simulation flow
Â  Â  if st.session_state.is_llm_ready or not API_KEY: # Use API_KEY for initial check
Â  Â  Â  Â  if st.session_state.is_chat_ended:
Â  Â  Â  Â  Â  Â  st.success(L["prompt_customer_end"] + " " + L["prompt_survey"])
Â  Â  Â  Â  Â  Â  if st.button(L["new_simulation_button"], key="new_simulation"):Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.is_chat_ended = False
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.initial_advice_provided = False
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.simulator_messages = []
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state.simulator_memory.clear()
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state['last_transcript'] = ''
Â  Â  Â  Â  Â  Â  Â  Â  Â st.rerun()
Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â Â 
Â  Â  Â  Â  if 'customer_query_text_area' not in st.session_state: st.session_state.customer_query_text_area = ""

Â  Â  Â  Â  customer_query = st.text_area(
Â  Â  Â  Â  Â  Â  L["customer_query_label"], key="customer_query_text_area", height=150, placeholder=L["initial_query_sample"],Â 
Â  Â  Â  Â  Â  Â  value=st.session_state.customer_query_text_area,
Â  Â  Â  Â  Â  Â  disabled=st.session_state.initial_advice_provided
Â  Â  Â  Â  )
Â  Â  Â  Â  customer_type_options_list = L["customer_type_options"]
Â  Â  Â  Â  default_index = 1 if len(customer_type_options_list) > 1 else 0
Â  Â  Â  Â  customer_type_display = st.selectbox(
Â  Â  Â  Â  Â  Â  L["customer_type_label"], customer_type_options_list, index=default_index, disabled=st.session_state.initial_advice_provided,
Â  Â  Â  Â  Â  Â  key='customer_type_sim_select'
Â  Â  Â  Â  )
Â  Â  Â  Â  current_lang_key = st.session_state.languageÂ 

Â  Â  Â  Â  if st.button(L["button_simulate"], key="start_simulation", disabled=st.session_state.initial_advice_provided):
Â  Â  Â  Â  Â  Â  if not customer_query: st.warning(L["simulation_warning_query"]); st.stop()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.clear()
Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages = []
Â  Â  Â  Â  Â  Â  st.session_state.is_chat_ended = False
Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "customer", "content": customer_query})
Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_user_message(customer_query)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # í”„ë¡¬í”„íŠ¸ëŠ” LLMì´ ì¤€ë¹„ëœ ê²½ìš°ì—ë§Œ ì‚¬ìš©
Â  Â  Â  Â  Â  Â  initial_prompt = f"""You are an AI Customer Support Supervisor. Your role is to analyze the following customer inquiry from a **{customer_type_display}** and provide a detailed response guideline and an initial draft response for the human agent.
[CRITICAL RULE FOR DRAFT CONTENT] The recommended draft MUST be strictly in {LANG[current_lang_key]['lang_select']}.
[CRITICAL RULE FOR SUPERVISOR TONE] Your advice and draft MUST be presented in a markdown format, using the specific headers: '### {L['simulation_advice_header']}' and '### {L['simulation_draft_header']}'.
[CRITICAL RULE FOR CUSTOMER ROLEPLAY (FUTURE MESSAGES)] When the Agent subsequently asks for information in later rounds, **Roleplay as the Customer** who is frustrated but **MUST BE HIGHLY COOPERATIVE** and provide the requested details piece by piece (not all at once). The customer MUST NOT argue or ask why the information is needed.

Customer Inquiry: {customer_query}
"""

Â  Â  Â  Â  Â  Â  if not API_KEY or not st.session_state.is_llm_ready: # Mock response for missing key
Â  Â  Â  Â  Â  Â  Â  Â  mock_data = get_mock_response_data(current_lang_key, customer_type_display)
Â  Â  Â  Â  Â  Â  Â  Â  ai_advice_text = f"### {mock_data['advice_header']}\n\n{mock_data['advice']}\n\n### {mock_data['draft_header']}\n\n{mock_data['draft']}"
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(ai_advice_text)
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.initial_advice_provided = True
Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  st.warning(L["simulation_no_key_warning"])
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if API_KEY and st.session_state.is_llm_ready:
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L["response_generating"]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_text = st.session_state.simulator_chain.predict(input=initial_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": response_text})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.initial_advice_provided = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  for message in st.session_state.simulator_messages:
Â  Â  Â  Â  Â  Â  if message["role"] == "customer": with st.chat_message("user", avatar="ğŸ™‹"): st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  elif message["role"] == "supervisor": with st.chat_message("assistant", avatar="ğŸ¤–"): st.markdown(message["content"]); render_tts_button(message["content"], st.session_state.language)Â 
Â  Â  Â  Â  Â  Â  elif message["role"] == "agent_response": with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"): st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  elif message["role"] == "customer_rebuttal": with st.chat_message("assistant", avatar="ğŸ˜ "): st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  elif message["role"] == "customer_end": with st.chat_message("assistant", avatar="ğŸ˜Š"): st.markdown(message["content"])
Â  Â  Â  Â  Â  Â  elif message["role"] == "system_end": with st.chat_message("assistant", avatar="âœ¨"): st.markdown(message["content"])

Â  Â  Â  Â  if st.session_state.initial_advice_provided and not st.session_state.is_chat_ended:
Â  Â  Â  Â  Â  Â  last_role = st.session_state.simulator_messages[-1]['role'] if st.session_state.simulator_messages else None
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if last_role in ["customer_rebuttal", "customer_end", "supervisor", "customer"]:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"### {L['agent_response_header']}")Â 
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  col_audio, col_text_area = st.columns([1, 2])
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # --- Whisper Audio Input for Agent Response (Currently only using file uploader placeholder) ---
Â  Â  Â  Â  Â  Â  Â  Â  with col_audio:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # audio_file = st.audio_input(L["button_mic_input"], key=f"sim_audio_input_{st.session_state['sim_audio_upload_key']}") # ì£¼ì„ ì²˜ë¦¬ëœ ì›ë˜ ì½”ë“œ
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Streamlit file uploaderë¡œ ëŒ€ì²´
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  audio_file = st.file_uploader(L['uploaded_file'], type=['wav', 'mp3', 'm4a', 'webm'], key=f"sim_file_uploader_{st.session_state['sim_audio_upload_key']}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if audio_file:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption("ğŸ‘† ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if audio_file:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if openai_client is None: st.error(L.get("whisper_client_error"))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L.get("whisper_processing")):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mime_type = getattr(audio_file, 'type', 'audio/webm')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  transcribed_text = transcribe_bytes_with_whisper(audio_file.getvalue(), mime_type)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['last_transcript'] = transcribed_text
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['sim_audio_upload_key'] += 1 # í‚¤ ë³€ê²½ìœ¼ë¡œ ìœ„ì ¯ ì´ˆê¸°í™”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success(L.get("whisper_success"))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); st.session_state['last_transcript'] = ""

Â  Â  Â  Â  Â  Â  Â  Â  agent_response = col_text_area.text_area(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  L["agent_response_placeholder"], value=st.session_state['last_transcript'], key="agent_response_area_text", height=150
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # JS Enter Key Listener (Code remains commented out as Streamlit components are disabled for security/complexity)
Â  Â  Â  Â  Â  Â  Â  Â  # st.components.v1.html("""<script>const textarea = document.querySelector('textarea[key="agent_response_area_text"]'); const button = document.querySelector('button[key="send_agent_response_sim"]'); if (textarea && button) { textarea.addEventListener('keydown', function(event) { if (event.key === 'Enter' && (!event.shiftKey && !event.ctrlKey)) { event.preventDefault(); button.click(); } }); }</script>""", height=0, width=0)

Â  Â  Â  Â  Â  Â  Â  Â  if st.button(L["send_response_button"], key="send_agent_response_sim"):Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if agent_response.strip():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['last_transcript'] = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "agent_response", "content": agent_response})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_user_message(agent_response)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: st.warning(L.get("empty_response_warning"))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if last_role == "agent_response":
Â  Â  Â  Â  Â  Â  Â  Â  col_end, col_next = st.columns([1, 2])
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if col_end.button(L["button_end_chat"], key="end_chat_sim"):Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  closing_messages = get_closing_messages(current_lang_key)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": closing_messages["additional_query"]})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["additional_query"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "system_end", "content": closing_messages["chat_closing"]})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["chat_closing"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.is_chat_ended = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â  Â  Â  Â  Â  Â  Â  Â  if col_next.button(L["request_rebuttal_button"], key="request_rebuttal_sim"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not API_KEY or not st.session_state.is_llm_ready: st.warning(L["simulation_no_key_warning"]); st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  next_reaction_prompt = f"""Analyze the entire chat history. Roleplay as the customer ({customer_type_display}). Based on the agent's last message, generate ONE of the following responses: either a follow-up question/request (rebuttal) OR a statement of satisfaction and closing. The response MUST be strictly in {LANG[current_lang_key]['lang_select']}."""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L["response_generating"]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  customer_reaction = st.session_state.simulator_chain.predict(input=next_reaction_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  positive_keywords = ["ê°ì‚¬", "thank you", "ã‚ã‚ŠãŒã¨ã†", L['customer_positive_response'].lower().split('/')[-1].strip()]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_positive_close = any(keyword in customer_reaction.lower() for keyword in positive_keywords)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if is_positive_close:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  role = "customer_end"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": "supervisor", "content": L["customer_closing_confirm"]})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(L["customer_closing_confirm"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  role = "customer_rebuttal"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
Â  Â  else:
Â  Â  Â  Â  st.error(L["llm_error_init"])

elif feature_selection == L["rag_tab"]:
Â  Â  # (RAG Chatbot UI logic remains the same)
Â  Â  st.header(L["rag_header"])
Â  Â  st.markdown(L["rag_desc"])
Â  Â  if st.session_state.get('is_rag_ready', False) and st.session_state.get('conversation_chain'):
Â  Â  Â  Â  if "messages" not in st.session_state: st.session_state.messages = []
Â  Â  Â  Â  for message in st.session_state.messages:
Â  Â  Â  Â  Â  Â  with st.chat_message(message["role"]): st.markdown(message["content"])
Â  Â  Â  Â  if prompt := st.chat_input(L["rag_input_placeholder"]):
Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role":"user","content":prompt})
Â  Â  Â  Â  Â  Â  with st.chat_message("user"): st.markdown(prompt)
Â  Â  Â  Â  Â  Â  with st.chat_message("assistant"):
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(L["response_generating"]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = st.session_state.conversation_chain.invoke({"question":prompt})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  answer = response.get('answer', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' if st.session_state.language == 'ko' else 'Could not generate response.')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(answer)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role":"assistant","content":answer})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}"); st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ" if st.session_state.language == 'ko' else "An error occurred"})
Â  Â  else: 
Â  Â  Â  Â  if st.session_state.get('firestore_load_success') is False and st.session_state.is_llm_ready: 
Â  Â  Â  Â  Â  Â  st.warning(L["firestore_no_index"])
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning(L["warning_rag_not_ready"])

elif feature_selection == L["content_tab"]:
    # (Custom Content Generation UI logic remains the same)
    st.header(L["content_header"])
    st.markdown(L["content_desc"])

    if st.session_state.is_llm_ready:
        topic = st.text_input(L["topic_label"])
        level_map = dict(zip(L["level_options"], ["Beginner", "Intermediate", "Advanced"]))
        content_map = dict(zip(L["content_options"], ["summary", "quiz", "example"]))

        level_display = st.selectbox(L["level_label"], L["level_options"])
        content_type_display = st.selectbox(L["content_type_label"], L["content_options"])

        level = level_map[level_display]
        content_type = content_map[content_type_display]

        if st.button(L["button_generate"]):
            if topic:
                target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[
                    st.session_state.language
                ]

                # 1. Prompt ì„¤ì •
                if content_type == "quiz":
                    full_prompt = f"""
You are a professional AI coach at the {level} level.
Please generate exactly 10 multiple-choice questions about the topic in {target_lang}.
Your entire response MUST be a valid JSON object wrapped inside triple backticks with the word 'json' (```json ... ```).

JSON structure:
{{
  "quiz_questions": [
      {{
          "question": "...",
          "options": ["A", "B", "C", "D"],
          "correct_index": 0-3,
          "explanation": "..."
      }}
  ]
}}

Topic: {topic}
"""
                else:
                    display_type_text = content_type_display
                    full_prompt = f"""
You are a professional AI coach at the {level} level.
Generate clear and educational content in a {display_type_text} format.
The response MUST be strictly in {target_lang}.
Topic: {topic}
"""

                with st.spinner(f"Generating {content_type_display} for {topic}..."):
                    quiz_data_raw = None

                    try:
                        # --- Mock LLM Response (Demo ìš©) ---
                        if content_type == "quiz":
                            quiz_data_raw = f"""
```json
{{
    "quiz_questions": [
        {{
            "question": "{topic}ì— ëŒ€í•œ ì²« ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.",
            "options": ["ì˜µì…˜ A", "ì˜µì…˜ B (ì •ë‹µ)", "ì˜µì…˜ C", "ì˜µì…˜ D"],
            "correct_index": 1,
            "explanation": "ì´ê²ƒì€ Mock í€´ì¦ˆì˜ ì„¤ëª…ì…ë‹ˆë‹¤. ì •ë‹µì€ ì˜µì…˜ Bì…ë‹ˆë‹¤."
        }},
        {{
            "question": "{topic}ì— ëŒ€í•œ ë‘ ë²ˆì§¸ ì§ˆë¬¸ì…ë‹ˆë‹¤.",
            "options": ["ì˜µì…˜ X (ì •ë‹µ)", "ì˜µì…˜ Y", "ì˜µì…˜ Z", "ì˜µì…˜ W"],
            "correct_index": 0,
            "explanation": "ì´ê²ƒì€ ë˜ ë‹¤ë¥¸ Mock í€´ì¦ˆì˜ ì„¤ëª…ì…ë‹ˆë‹¤. ì •ë‹µì€ ì˜µì…˜ Xì…ë‹ˆë‹¤."
        }}
    ]
}}
```
"""
                        else:
                            quiz_data_raw = (
                                f"ì´ê²ƒì€ {target_lang}ìœ¼ë¡œ ì‘ì„±ëœ {topic}ì— ëŒ€í•œ Mock {content_type_display} ì…ë‹ˆë‹¤."
                            )

                        # Session ì €ì¥
                        st.session_state.quiz_data_raw = quiz_data_raw

                        # í€´ì¦ˆ JSON ì²˜ë¦¬
                        if content_type == "quiz":
                            quiz_data = clean_and_load_json(quiz_data_raw)

                            if quiz_data and "quiz_questions" in quiz_data:
                                st.session_state.quiz_data = quiz_data
                                st.session_state.current_question = 0
                                st.session_state.quiz_submitted = False
                                st.session_state.quiz_results = [
                                    None
                                ] * len(quiz_data.get("quiz_questions", []))

                                st.success(f"**{topic}** - **{content_type_display}** Result:")
                            else:
                                st.error(L["quiz_error_llm"])
                                st.markdown(f"**{L['quiz_original_response']}**:")
                                st.code(quiz_data_raw, language="json")

                        else:
                            st.success(f"**{topic}** - **{content_type_display}** Result:")
                            st.markdown(quiz_data_raw)

                    except Exception as e:
                        st.error(f"Content Generation Error: {e}")

            else:
                st.warning(L["warning_topic"])
    else:
        st.error(L["llm_error_init"])

# ---------------------------
# Quiz Rendering Logic
# ---------------------------
if (
    "quiz_data" in st.session_state
    and st.session_state.get("quiz_data")
    and content_type == "quiz"
):
    render_interactive_quiz(st.session_state.quiz_data, st.session_state.language)

# ---------------------------
# LSTM íƒ­
# ---------------------------
elif feature_selection == L["lstm_tab"]:
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])

    if st.button(L["lstm_rerun_button"], key="rerun_lstm", on_click=force_rerun_lstm):
        pass

    try:
        model, data = load_or_train_lstm()

        # ì˜ˆì¸¡ê°’ ìƒì„± (Mock)
        predicted_score = np.clip(
            data[-1] + np.random.uniform(-3, 5),
            50,
            100
        )

        st.markdown("---")
        st.subheader(L["lstm_result_header"])

        col_score, col_chart = st.columns([1, 2])

        with col_score:
            suffix = "ì " if st.session_state.language == "ko" else ""
            st.metric(L["lstm_score_metric"], f"{predicted_score:.1f}{suffix}")
            st.info(
                L["lstm_score_info"].format(predicted_score=predicted_score)
            )

        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label="Past Scores", marker="o")
            ax.plot(len(data), predicted_score, label="Predicted Next Score", marker="*", color="red", markersize=10)
            ax.set_title(L["lstm_header"])
            ax.set_xlabel("Time (attempts)")
            ax.set_ylabel("Score (0-100)")
            ax.legend()
            st.pyplot(fig)

    except NameError:
        st.error("TensorFlow/Matplotlib ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.info(f"LSTM ê¸°ëŠ¥ ì—ëŸ¬: {e}")
