# ========================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ (ìµœì¢… Firebase ì˜êµ¬ ì €ì¥ì†Œ í†µí•©)
# ========================================
import streamlit as st
import os
import tempfile
import time
import json
import re
import base64
import io

# â­ Admin SDK ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from firebase_admin import credentials, firestore, initialize_app, get_app
# Admin SDKì˜ firestoreì™€ Google Cloud SDKì˜ firestoreë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ alias ì‚¬ìš©
from google.cloud import firestore as gcp_firestore

from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.schema.document import Document
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense


# ================================
# 1. Firebase Admin SDK ì´ˆê¸°í™” ë° Secrets ì²˜ë¦¬ í•¨ìˆ˜
# ================================

# ì´ í•¨ìˆ˜ëŠ” ì´ˆê¸°í™” ê°ì²´ ìì²´ë¥¼ ìƒì„±í•˜ëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
def _get_admin_credentials():
    """
    Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if "FIREBASE_SERVICE_ACCOUNT_JSON" not in st.secrets:
        return None, "FIREBASE_SERVICE_ACCOUNT_JSON Secretì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    service_account_data = st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"]
    
    # AttrDict íƒ€ì…ì„ í‘œì¤€ dictë¡œ ê°•ì œ ë³€í™˜í•˜ëŠ” ìµœì¢… ë¡œì§
    sa_info = None

    if isinstance(service_account_data, str):
        # 1. ë¬¸ìì—´ì¸ ê²½ìš°: JSON ë¡œë“œ
        try:
            sa_info = json.loads(service_account_data.strip())
        except json.JSONDecodeError as e:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ JSON êµ¬ë¬¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ê°’ì„ í™•ì¸í•˜ì„¸ìš”. ìƒì„¸ ì˜¤ë¥˜: {e}"
    elif hasattr(service_account_data, 'get'):
        # 2. AttrDict (secrets.toml ë”•ì…”ë„ˆë¦¬ í˜•ì‹)ì¸ ê²½ìš°: dictë¡œ ë³€í™˜
        try:
            sa_info = dict(service_account_data) # AttrDictë¥¼ í‘œì¤€ dictë¡œ ë³€í™˜
        except Exception:
             return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì‹¤íŒ¨. íƒ€ì…: {type(service_account_data)}"
    else:
        return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (Type: {type(service_account_data)})"
    
    if not sa_info.get("project_id") or not sa_info.get("private_key"):
        return None, "JSON ë‚´ 'project_id' ë˜ëŠ” 'private_key' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."

    return sa_info, None

@st.cache_resource(ttl=None)
def initialize_firestore_admin():
    """
    Secretsì—ì„œ ë¡œë“œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    sa_info, error_message = _get_admin_credentials()

    if error_message:
        st.error(f"âŒ Firebase Secret ì˜¤ë¥˜: {error_message}")
        return None

    # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸ (Streamlit Rerun ë¬¸ì œ ë°©ì§€)
    try:
        get_app()
    except ValueError:
        pass # ì•±ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
    else:
        # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° í´ë¼ì´ì–¸íŠ¸ë§Œ ë°˜í™˜
        try:
            return firestore.client()
        except Exception as e:
            st.error(f"ğŸ”¥ Firebase í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None


    try:
        # ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì§ì ‘ ì „ë‹¬í•˜ì—¬ ì¸ì¦ ê°ì²´ ìƒì„±
        cred = credentials.Certificate(sa_info) 
        initialize_app(cred)
        
        db_client = firestore.client()
        st.session_state["db"] = db_client
        st.success("âœ… Firebase Admin SDK ì´ˆê¸°í™” ì™„ë£Œ! (Secrets ê¸°ë°˜)")
        return db_client
    except Exception as e:
        st.error(f"ğŸ”¥ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¬¸ì œ. ì˜¤ë¥˜: {e}")
        return None


def save_index_to_firestore(db, vector_store, index_id="user_portfolio_rag"):
    """FAISS ì¸ë±ìŠ¤ë¥¼ Firestoreì— Base64 í˜•íƒœë¡œ ì§ë ¬í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    if not db: return False
    temp_dir = tempfile.mkdtemp()
    
    try:
        vector_store.save_local(folder_path=temp_dir, index_name="index")
        
        with open(f"{temp_dir}/index.faiss", "rb") as f: faiss_bytes = f.read()
        with open(f"{temp_dir}/index.pkl", "rb") as f: metadata_bytes = f.read()
        
        encoded_data = {
            "faiss_data": base64.b64encode(faiss_bytes).decode('utf-8'),
            "metadata_data": base64.b64encode(metadata_bytes).decode('utf-8'),
            "timestamp": gcp_firestore.SERVER_TIMESTAMP 
        }
        
        db.collection("rag_indices").document(index_id).set(encoded_data)
        return True
    
    except Exception as e:
        # DB ì €ì¥ ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ í‘œì‹œ
        st.error(f"DB ì €ì¥ ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"Error saving index to Firestore: {e}")
        return False

def load_index_from_firestore(db, embeddings, index_id="user_portfolio_rag"):
    """Firestoreì—ì„œ Base64 ë¬¸ìì—´ì„ ë¡œë“œí•˜ì—¬ FAISS ì¸ë±ìŠ¤ë¡œ ì—­ì§ë ¬í™”í•©ë‹ˆë‹¤."""
    if not db: return False

    try:
        doc = db.collection("rag_indices").document(index_id).get()
        if not doc.exists:
            return None 

        encoded_data = doc.to_dict()
        
        faiss_bytes = base64.b64decode(encoded_data["faiss_data"])
        metadata_bytes = base64.b64decode(encoded_data["metadata_data"])
        
        temp_dir = tempfile.mkdtemp()
        with open(f"{temp_dir}/index.faiss", "wb") as f: f.write(faiss_bytes)
        with open(f"{temp_dir}/index.pkl", "wb") as f: f.write(metadata_bytes)
        
        vector_store = FAISS.load_local(folder_path=temp_dir, embeddings=embeddings, index_name="index")
        return vector_store
        
    except Exception as e:
        print(f"Error loading index from Firestore: {e}")
        return None

# ================================
# 2. JSON/RAG/LSTM í•¨ìˆ˜ ì •ì˜
# (ì´ ì„¹ì…˜ì˜ ë‚˜ë¨¸ì§€ í•¨ìˆ˜ë“¤ì€ ê¸°ì¡´ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.)
# ================================
def clean_and_load_json(text):
    """LLM ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë§Œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ë¡œë“œ"""
    match = re.search(r'\{.*\}', text, re.DOTALL)
    
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None

def render_interactive_quiz(quiz_data, current_lang):
    """ìƒì„±ëœ í€´ì¦ˆ ë°ì´í„°ë¥¼ Streamlit UIë¡œ ë Œë”ë§í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤."""
    L = LANG[current_lang]
    
    if not quiz_data or 'quiz_questions' not in quiz_data:
        return

    questions = quiz_data['quiz_questions']
    num_questions = len(questions)

    if "current_question" not in st.session_state or st.session_state.current_question >= num_questions:
        st.session_state.current_question = 0
        st.session_state.quiz_results = [None] * num_questions
        st.session_state.quiz_submitted = False
        
    q_index = st.session_state.current_question
    q_data = questions[q_index]
    
    st.subheader(f"{q_index + 1}. {q_data['question']}")
    
    options_dict = {}
    try:
        # ì•ˆì „í•˜ê²Œ ì˜µì…˜ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì‹œë„
        options_dict = {f"{opt['option']}": f"{opt['option']}) {opt['text']}" for opt in q_data['options']}
    except KeyError:
        st.error(L["quiz_fail_structure"])
        st.markdown(f"**{L['quiz_original_response']}**:")
        if 'quiz_data_raw' in st.session_state:
            st.code(st.session_state.quiz_data_raw, language="json")
        return

    options_list = list(options_dict.values())
    
    selected_answer = st.radio(
        L.get("select_answer", "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”"),
        options=options_list,
        key=f"q_radio_{q_index}"
    )

    col1, col2 = st.columns(2)

    if col1.button(L.get("check_answer", "ì •ë‹µ í™•ì¸"), key=f"check_btn_{q_index}", disabled=st.session_state.quiz_submitted):
        user_choice_letter = selected_answer.split(')')[0] if selected_answer else None
        correct_answer_letter = q_data['correct_answer']

        is_correct = (user_choice_letter == correct_answer_letter)
        
        st.session_state.quiz_results[q_index] = is_correct
        st.session_state.quiz_submitted = True
        
        if is_correct:
            st.success(L.get("correct_answer", "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰"))
        else:
            st.error(L.get("incorrect_answer", "ì˜¤ë‹µì…ë‹ˆë‹¤. ğŸ˜"))
        
        st.markdown(f"**{L.get('correct_is', 'ì •ë‹µ')}: {correct_answer_letter}**")
        st.info(f"**{L.get('explanation', 'í•´ì„¤')}:** {q_data['explanation']}")

    if st.session_state.quiz_submitted:
        if q_index < num_questions - 1:
            if col2.button(L.get("next_question", "ë‹¤ìŒ ë¬¸í•­"), key=f"next_btn_{q_index}"):
                st.session_state.current_question += 1
                st.session_state.quiz_submitted = False
                st.rerun()
        else:
            total_correct = st.session_state.quiz_results.count(True)
            total_questions = len(st.session_state.quiz_results)
            st.success(f"**{L.get('quiz_complete', 'í€´ì¦ˆ ì™„ë£Œ!')}** {L.get('score', 'ì ìˆ˜')}: {total_correct}/{total_questions}")
            if st.button(L.get("retake_quiz", "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°"), key="retake"):
                st.session_state.current_question = 0
                st.session_state.quiz_results = [None] * num_questions
                st.session_state.quiz_submitted = False
                st.rerun()


def get_document_chunks(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
    documents = []
    temp_dir = tempfile.mkdtemp()

    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        if file_extension == "pdf":
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = PyPDFLoader(temp_filepath)
            documents.extend(loader.load())
        
        elif file_extension == "html":
            raw_html = uploaded_file.getvalue().decode('utf-8')
            soup = BeautifulSoup(raw_html, 'html.parser')
            text_content = soup.get_text(separator=' ', strip=True)
            
            documents.append(Document(page_content=text_content, metadata={"source": uploaded_file.name}))


        elif file_extension == "txt":
            with open(temp_filepath, "wb") as f:
                f.write(uploaded_file.getvalue())
            loader = TextLoader(temp_filepath, encoding="utf-8")
            documents.extend(loader.load())
            
        else:
            print(f"File '{uploaded_file.name}' not supported.")
            continue

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    return text_splitter.split_documents(documents)


def get_vector_store(text_chunks):
    """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  Vector Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    cache_key = tuple(doc.page_content for doc in text_chunks)
    if cache_key in st.session_state.embedding_cache:
        return st.session_state.embedding_cache[cache_key]
    
    if not st.session_state.is_llm_ready:
        return None

    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        st.session_state.embedding_cache[cache_key] = vector_store
        return vector_store
    
    except Exception as e:
        if "429" in str(e):
             return None
        else:
            print(f"Vector Store creation failed: {e}") 
            return None


def get_rag_chain(vector_store):
    """ê²€ìƒ‰ ì²´ì¸(ConversationalRetrievalChain)ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if vector_store is None:
        return None
        
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )


@st.cache_resource
def load_or_train_lstm():
    """ê°€ìƒì˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ì„ ìœ„í•œ LSTM ëª¨ë¸ì„ ìƒì„±í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤."""
    np.random.seed(42)
    data = np.cumsum(np.random.normal(loc=5, scale=5, size=50)) + 60
    data = np.clip(data, 50, 95)

    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset) - look_back):
            X.append(dataset[i:(i + look_back)])
            Y.append(dataset[i + look_back])
        return np.array(X), np.array(Y)

    look_back = 5
    X, Y = create_dataset(data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    model = Sequential([
        LSTM(50, activation='relu', input_shape=(look_back, 1)),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)

    return model, data

# ================================
# 3. ë‹¤êµ­ì–´ ì§€ì› ë”•ì…”ë„ˆë¦¬ (Language Dictionary)
# ================================
LANG = {
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°", # â­ ì‹œë®¬ë ˆì´í„° íƒ­ ì¶”ê°€
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
        "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
        "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
        "lstm_disabled_error": "The LSTM feature is temporarily disabled due to build environment issues. Please use the 'Custom Content Generation' feature first.",
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
        "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
        "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
        "check_answer": "ì •ë‹µ í™•ì¸",
        "next_question": "ë‹¤ìŒ ë¬¸í•­",
        "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
        "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤. ğŸ˜",
        "correct_is": "ì •ë‹µ",
        "explanation": "í•´ì„¤",
        "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
        "score": "ì ìˆ˜",
        "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ ì›ë³¸ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
        "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
        
        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— ëŒ€í•´ AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
        "customer_type_label": "ê³ ê° ì„±í–¥",
        "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
        "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
        "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
    },
    "en": {
        "title": "Personalized AI Study Coach",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction",
        "simulator_tab": "AI Customer Response Simulator", # â­ ì‹œë®¬ë ˆì´í„° íƒ­ ì¶”ê°€
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answers questions based on the uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "llm_error_key": "âš ï¸ Warning: GEMINI API Key is not set. Please set 'GEMINI_API_KEY' in Streamlit Secrets.",
        "llm_error_init": "LLM initialization error: Please check your API key.",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content tailored to your topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "10 Multiple-Choice Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Based Achievement Prediction",
        "lstm_desc": "Trains an LSTM model on hypothetical past quiz scores to predict future achievement.",
        "lstm_disabled_error": "The LSTM feature is temporarily disabled due to build environment issues. Please use the 'Custom Content Generation' feature first.",
        "lang_select": "Select Language",
        "embed_success": "Learning DB built with {count} chunks!",
        "embed_fail": "Embedding failed: Free tier quota exceeded or network issue.",
        "warning_no_files": "Please upload study materials first.",
        "warning_rag_not_ready": "RAG is not ready. Upload materials and click Start Analysis.",
        "quiz_fail_structure": "Quiz data structure is incorrect.",
        "select_answer": "Select answer",
        "check_answer": "Confirm answer",
        "next_question": "Next Question",
        "correct_answer": "Correct! ğŸ‰",
        "incorrect_answer": "Incorrect. ğŸ˜",
        "correct_is": "Correct answer",
        "explanation": "Explanation",
        "quiz_complete": "Quiz completed!",
        "score": "Score",
        "retake_quiz": "Retake Quiz",
        "quiz_error_llm": "Quiz generation failed: LLM did not return a valid JSON format. Check the original LLM response.",
        "quiz_original_response": "Original LLM Response",
        "firestore_loading": "Loading RAG index from database...",
        
        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AI Customer Response Simulator",
        "simulator_desc": "Provides AI-generated response drafts and guidelines for handling challenging customer inquiries.",
        "customer_query_label": "Customer Query (Link optional)",
        "customer_type_label": "Customer Sentiment",
        "customer_type_options": ["General Inquiry", "Challenging Customer", "Highly Dissatisfied Customer"],
        "button_simulate": "Request Response Advice",
        "simulation_warning_query": "Please enter the customer's query.",
    },
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
        "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
        "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "content_tab": "ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼", # â­ ã‚·ãƒŸãƒ¥ë ˆì´í„° íƒ­ ì¶”ê°€
        "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
        "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
        "rag_input_placeholder": "å­¦ç¿’è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
        "llm_error_key": "âš ï¸ è­¦å‘Š: GEMINI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secretsã«'GEMINI_API_KEY'ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚",
        "llm_error_init": "LLMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«åˆã‚ã›ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
        "level_label": "é›£æ˜“åº¦",
        "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼",
        "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
        "content_options": ["æ ¸å¿ƒè¦ç´„ãƒãƒ¼ãƒˆ", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢"],
        "button_generate": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "lstm_header": "LSTMãƒ™ãƒ¼ã‚¹é”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "lstm_desc": "ä»®æƒ³ã®éå»ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€LSTMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦å°†æ¥ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—è¡¨ç¤ºã—ã¾ã™ã€‚",
        "lstm_disabled_error": "ç¾åœ¨ã€ãƒ“ãƒ«ãƒ‰ç’°å¢ƒã®å•é¡Œã«ã‚ˆã‚ŠLSTMæ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã€Œã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã€æ©Ÿèƒ½ã‚’å…ˆã«ã”åˆ©ç”¨ãã ã•ã„ã€‚ã€",
        "lang_select": "è¨€èªé¸æŠ",
        "embed_success": "å…¨{count}ãƒãƒ£ãƒ³ã‚¯ã§å­¦ç¿’DBæ§‹ç¯‰å®Œäº†!",
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—: ãƒ•ãƒªãƒ¼ãƒ†ã‚£ã‚¢ã®ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã€‚",
        "warning_no_files": "ã¾ãšå­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "warning_rag_not_ready": "RAGãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€åˆ†æé–‹å§‹ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚",
        "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "select_answer": "æ­£è§£ã‚’é¸æŠã—ã¦ãã ã•ã„",
        "check_answer": "æ­£è§£ã‚’ç¢ºèª",
        "next_question": "æ¬¡ã®è³ªå•",
        "correct_answer": "æ­£è§£ã§ã™! ğŸ‰",
        "incorrect_answer": "ä¸æ­£è§£ã§ã™ã€‚ğŸ˜",
        "correct_is": "æ­£è§£",
        "explanation": "è§£èª¬",
        "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
        "score": "ã‚¹ã‚³ã‚¢",
        "retake_quiz": "ã‚¯ã‚¤ã‚ºã‚’å†æŒ‘æˆ¦",
        "quiz_error_llm": "LLMãŒæ­£ã—ã„JSONã®å½¢å¼ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã®ã§ã€ã‚¯ã‚¤ã‚ºã®ç”ŸæˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚",
        "quiz_original_response": "LLM åŸæœ¬å¿œç­”",
        "firestore_loading": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...",
        
        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "simulator_desc": "é›£ã—ã„é¡§å®¢ã®å•ã„åˆã‚ã›ã«å¯¾ã—ã¦ã€AIã«ã‚ˆã‚‹å¯¾å¿œæ¡ˆã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚",
        "customer_query_label": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ï¼ˆãƒªãƒ³ã‚¯ä»»æ„ï¼‰",
        "customer_type_label": "é¡§å®¢ã®å‚¾å‘",
        "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "æ‰‹ã”ã‚ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"],
        "button_simulate": "å¯¾å¿œã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¦æ±‚",
        "simulation_warning_query": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
    }
}


# ================================
# 4. Streamlit í•µì‹¬ Config ì„¤ì • ë° Session State ì´ˆê¸°í™” (CRITICAL ZONE)
# ================================

if 'language' not in st.session_state: st.session_state.language = 'ko'
if 'uploaded_files_state' not in st.session_state: st.session_state.uploaded_files_state = None
if 'is_llm_ready' not in st.session_state: st.session_state.is_llm_ready = False
if 'is_rag_ready' not in st.session_state: st.session_state.is_rag_ready = False
if 'firestore_db' not in st.session_state: st.session_state.firestore_db = None
if 'llm_init_error_msg' not in st.session_state: st.session_state.llm_init_error_msg = None
if 'firestore_load_success' not in st.session_state: st.session_state.firestore_load_success = False

# ì–¸ì–´ ì„¤ì • ë¡œë“œ (UI ì¶œë ¥ ì „ í•„ìˆ˜)
L = LANG[st.session_state.language] 
API_KEY = os.environ.get("GEMINI_API_KEY")

# =======================================================
# 5. Streamlit UI í˜ì´ì§€ ì„¤ì • (ìŠ¤í¬ë¦½íŠ¸ ë‚´ ì²« ë²ˆì§¸ ST ëª…ë ¹)
# =======================================================
st.set_page_config(page_title=L["title"], layout="wide")

# =======================================================
# 6. ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë° LLM/DB ë¡œì§ (í˜ì´ì§€ ì„¤ì • í›„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰)
# =======================================================

if 'llm' not in st.session_state: 
    llm_init_error = None
    if not API_KEY:
        llm_init_error = L["llm_error_key"]
    else:
        try:
            # LLM ë° Embeddings ì´ˆê¸°í™”
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
            
            # Admin SDK í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” 
            sa_info, error_message = _get_admin_credentials()
            
            if error_message:
                llm_init_error = f"{L['llm_error_init']} (DB Auth Error: {error_message})" 
            elif sa_info:
                db = initialize_firestore_admin() 
                st.session_state.firestore_db = db
                
                if not db:
                    llm_init_error = f"{L['llm_error_init']} (DB Client Error: Firebase Admin Init Failed)" 

            # DB ë¡œë”© ë¡œì§
            if st.session_state.firestore_db and 'conversation_chain' not in st.session_state:
                # DB ë¡œë”© ì‹œë„
                loaded_index = load_index_from_firestore(st.session_state.firestore_db, st.session_state.embeddings)
                
                if loaded_index:
                    st.session_state.conversation_chain = get_rag_chain(loaded_index)
                    st.session_state.is_rag_ready = True
                    st.session_state.firestore_load_success = True
                else:
                    st.session_state.firestore_load_success = False
            
        except Exception as e:
            llm_init_error = f"{L['llm_error_init']} {e}" 
            st.session_state.is_llm_ready = False
    
    if llm_init_error:
        st.session_state.is_llm_ready = False
        st.session_state.llm_init_error_msg = llm_init_error # ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì €ì¥

# ë‚˜ë¨¸ì§€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

if "embedding_cache" not in st.session_state:
    st.session_state.embedding_cache = {}

# ================================
# 7. ì´ˆê¸°í™” ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ ë° DB ìƒíƒœ ì•Œë¦¼
# ================================

if st.session_state.llm_init_error_msg:
    st.error(st.session_state.llm_init_error_msg)
    
if st.session_state.get('firestore_db'):
    if st.session_state.get('firestore_load_success', False):
        st.success("âœ… RAG ì¸ë±ìŠ¤ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    elif not st.session_state.get('is_rag_ready', False):
        st.info("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”.")


# ================================
# 8. Streamlit UI ì‹œì‘
# ================================

with st.sidebar:
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=['ko', 'en', 'ja'],
        index=['ko', 'en', 'ja'].index(st.session_state.language),
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
    )
    
    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        st.rerun() 
    
    L = LANG[st.session_state.language] 
    
    st.title(L["sidebar_title"])
    st.markdown("---")
    
    uploaded_files_widget = st.file_uploader(
        L["file_uploader"],
        type=["pdf","txt","html"],
        accept_multiple_files=True
    )
    
    if uploaded_files_widget:
        st.session_state.uploaded_files_state = uploaded_files_widget
    elif 'uploaded_files_state' not in st.session_state:
        st.session_state.uploaded_files_state = None
    
    files_to_process = st.session_state.uploaded_files_state if st.session_state.uploaded_files_state else []
    
    if files_to_process and st.session_state.is_llm_ready:
        if st.button(L["button_start_analysis"], key="start_analysis"):
            with st.spinner(f"ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘..."):
                text_chunks = get_document_chunks(files_to_process)
                vector_store = get_vector_store(text_chunks)
                
                if vector_store:
                    # RAG ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ë©´ Firestoreì— ì €ì¥ ì‹œë„
                    db = st.session_state.firestore_db
                    save_success = False
                    if db:
                        save_success = save_index_to_firestore(db, vector_store)
                    
                    if save_success:
                        st.success(L["embed_success"].format(count=len(text_chunks)) + " (DB ì €ì¥ ì™„ë£Œ)")
                    else:
                        st.success(L["embed_success"].format(count=len(text_chunks)) + " (DB ì €ì¥ ì‹¤íŒ¨)")

                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                else:
                    st.session_state.is_rag_ready = False
                    st.error(L["embed_fail"])

    else:
        st.session_state.is_rag_ready = False
        st.warning(L.get("warning_no_files", "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")) 

    st.markdown("---")
    # â­ ìƒˆë¡œìš´ íƒ­(ì‹œë®¬ë ˆì´í„°)ì„ í¬í•¨í•˜ì—¬ ë¼ë””ì˜¤ ë²„íŠ¼ ì—…ë°ì´íŠ¸
    feature_selection = st.radio(
        L["content_tab"], 
        [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["simulator_tab"]]
    )

st.title(L["title"])

# ================================
# 9. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================

if feature_selection == L["simulator_tab"]: # â­ ì‹œë®¬ë ˆì´í„° íƒ­ êµ¬í˜„ ì‹œì‘
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])

    if st.session_state.is_llm_ready:
        # 1. ê³ ê° ë¬¸ì˜ ì…ë ¥ í•„ë“œ
        customer_query = st.text_area(
            L["customer_query_label"],
            height=150,
            placeholder="ì˜ˆ: ì œê°€ ì–´ì œ ì£¼ë¬¸í•œ ìƒí’ˆì´ ì•„ì§ë„ ë°°ì†¡ ì¶œë°œ ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤. ë„ˆë¬´ ëŠ¦ëŠ” ê²ƒ ì•„ë‹Œê°€ìš”? ë¹ ë¥´ê²Œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
        )

        # 2. ê³ ê° ì„±í–¥ ì„ íƒ
        customer_type = st.selectbox(
            L["customer_type_label"],
            L["customer_type_options"]
        )

        if st.button(L["button_simulate"]):
            if customer_query:
                st.info(f"ì„ íƒëœ ê³ ê° ì„±í–¥: {customer_type}")
                st.warning("âš ï¸ API Keyê°€ ì—†ëŠ” ê²½ìš°, ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (API Keyê°€ ì—†ì–´ë„ UI êµ¬ì„±ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.)")
                
                if API_KEY:
                    with st.spinner(f"{customer_type} ê³ ê° ì‘ëŒ€ ê°€ì´ë“œë¼ì¸ ìƒì„± ì¤‘..."):
                        # ì—¬ê¸°ì— LLM í˜¸ì¶œ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤ (API Key ë°œê¸‰ í›„)
                        # í˜„ì¬ëŠ” API Keyê°€ ì—†ê±°ë‚˜ LLMì´ ì‘ë‹µí•˜ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ í•˜ë“œì½”ë”©ëœ ì˜ˆì‹œë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
                        st.success("AIì˜ ì‘ëŒ€ ì¡°ì–¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                        st.markdown("### AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸")
                        st.info("ì´ ê³ ê°ì€ ë°°ì†¡ ì§€ì—°ì— ëŒ€í•œ **ë¶ˆë§Œì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤**. ë¨¼ì € ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼í•˜ê³ , í˜„ì¬ ìƒíƒœë¥¼ íˆ¬ëª…í•˜ê²Œ ì„¤ëª…í•˜ë©°, ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ **êµ¬ì²´ì ì¸ ë‹¤ìŒ í–‰ë™**ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.")

                        st.markdown("### ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ (Tone: ê³µê° ë° ì§„ì •)")
                        st.markdown(f"""
                        > ê³ ê°ë‹˜, ë¨¼ì € ì£¼ë¬¸í•˜ì‹  ìƒí’ˆ ë°°ì†¡ì´ ëŠ¦ì–´ì ¸ ë§ì´ ë¶ˆí¸í•˜ì…¨ì„ ì  ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.
                        > í˜„ì¬ ì‹œìŠ¤í…œ ìƒ í™•ì¸ëœ ë°”ë¡œëŠ” [ë°°ì†¡ ì§€ì—° ì‚¬ìœ  ì„¤ëª…]. 
                        > ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ì €í¬ê°€ [êµ¬ì²´ì ì¸ í•´ê²°ì±… 1: ì˜ˆ: ë‹´ë‹¹ íŒ€ì— ì§ì ‘ ì—°ë½] ë° [êµ¬ì²´ì ì¸ í•´ê²°ì±… 2: ì˜ˆ: ì˜¤ëŠ˜ ì¤‘ìœ¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¬í™•ì¸]ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
                        > ì²˜ë¦¬ë˜ëŠ” ëŒ€ë¡œ ì˜¤ëŠ˜ ì˜¤í›„ [ì‹œê°„]ê¹Œì§€ ê³ ê°ë‹˜ê»˜ **ê°œë³„ì ìœ¼ë¡œ** ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê¸°ë‹¤ë ¤ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.
                        """)
                else:
                     st.error(f"{L['llm_error_key']} (ì‘ë‹µ ìƒì„± ë¶ˆê°€)")


            else:
                st.warning(L["simulation_warning_query"])

    else:
        st.error(L["llm_error_init"])

elif feature_selection == L["rag_tab"]:
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    if st.session_state.get('is_rag_ready', False) and st.session_state.get('conversation_chain'):
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner(f"ë‹µë³€ ìƒì„± ì¤‘..." if st.session_state.language == 'ko' else "Generating response..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer','ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' if st.session_state.language == 'ko' else 'Could not generate response.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ" if st.session_state.language == 'ko' else "An error occurred"})
    else:
        st.warning(L["warning_rag_not_ready"])

elif feature_selection == L["content_tab"]:
    st.header(L["content_header"])
    st.markdown(L["content_desc"])

    if st.session_state.is_llm_ready:
        topic = st.text_input(L["topic_label"])
        
        level_map = dict(zip(L["level_options"], ["Beginner", "Intermediate", "Advanced"]))
        content_map = dict(zip(L["content_options"], ["summary", "quiz", "example"]))
        
        level_display = st.selectbox(L["level_label"], L["level_options"])
        content_type_display = st.selectbox(L["content_type_label"], L["content_options"])

        level = level_map[level_display]
        content_type = content_map[content_type_display]

        if st.button(L["button_generate"]):
            if topic:
                target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]
                
                if content_type == 'quiz':
                    # 10ë¬¸í•­ìœ¼ë¡œ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸
                    full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate exactly 10 multiple-choice questions about the topic in {target_lang}.
Your entire response MUST be a valid JSON object wrapped in ```json tags.
The JSON must have a single key named 'quiz_questions', which is an array of objects.
Each question object must contain: 'question' (string), 'options' (array of objects with 'option' (A,B,C,D) and 'text' (string)), 'correct_answer' (A,B,C, or D), and 'explanation' (string).

Topic: {topic}"""
                else:
                    display_type_text = L["content_options"][L["content_options"].index(content_type_display)]
                    full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate clear and educational content in the requested {display_type_text} format based on the topic.
The response MUST be strictly in {target_lang}.

Topic: {topic}
Requested Format: {display_type_text}"""
                
                
                with st.spinner(f"Generating {content_type_display} for {topic}..."):
                    
                    quiz_data_raw = None
                    try:
                        response = st.session_state.llm.invoke(full_prompt)
                        quiz_data_raw = response.content
                        st.session_state.quiz_data_raw = quiz_data_raw # ë””ë²„ê¹…ì„ ìœ„í•´ raw data ì €ì¥
                        
                        if content_type == 'quiz':
                            quiz_data = clean_and_load_json(quiz_data_raw)
                            
                            if quiz_data and 'quiz_questions' in quiz_data:
                                st.session_state.quiz_data = quiz_data
                                st.session_state.current_question = 0
                                st.session_state.quiz_submitted = False
                                st.session_state.quiz_results = [None] * len(quiz_data.get('quiz_questions',[]))
                                
                                st.success(f"**{topic}** - **{content_type_display}** Result:")
                            else:
                                st.error(L["quiz_error_llm"])
                                st.markdown(f"**{L['quiz_original_response']}**:")
                                st.code(quiz_data_raw, language="json")

                        else: # ì¼ë°˜ ì½˜í…ì¸  (ìš”ì•½, ì˜ˆì œ)
                            st.success(f"**{topic}** - **{content_type_display}** Result:")
                            st.markdown(response.content)

                    except Exception as e:
                        st.error(f"Content Generation Error: {e}")
                        if quiz_data_raw:
                            st.markdown(f"**{L['quiz_original_response']}**: {quiz_data_raw}")

            else:
                st.warning(L["warning_topic"])
    else:
        st.error(L["llm_error_init"])
        
    # í€´ì¦ˆ í’€ì´ ë Œë”ë§ì„ ë©”ì¸ ë£¨í”„ì—ì„œ ì¡°ê±´ë¶€ë¡œ ë‹¨ í•œ ë²ˆ í˜¸ì¶œ
    is_quiz_ready = content_type == 'quiz' and 'quiz_data' in st.session_state and st.session_state.quiz_data
    if is_quiz_ready and st.session_state.get('current_question', 0) < len(st.session_state.quiz_data.get('quiz_questions', [])):
        render_interactive_quiz(st.session_state.quiz_data, st.session_state.language)


elif feature_selection == L["lstm_tab"]:
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])

    with st.spinner(f"LSTM model loading/training..." if st.session_state.language != 'ko' else "LSTM ëª¨ë¸ì„ ë¡œë“œ/í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            # 1. ëª¨ë¸ ë¡œë“œ ë° ë°ì´í„° ìƒì„±
            lstm_model, historical_scores = load_or_train_lstm()
            st.success("LSTM Model Ready!")

            # 2. ì˜ˆì¸¡ ë¡œì§
            look_back = 5
            last_sequence = historical_scores[-look_back:]
            input_sequence = np.reshape(last_sequence, (1, look_back, 1))
            
            future_predictions = []
            current_input = input_sequence

            for i in range(5):
                next_score = lstm_model.predict(current_input, verbose=0)[0]
                future_predictions.append(next_score[0])

                next_input = np.append(current_input[:, 1:, :], next_score[0]).reshape(1, look_back, 1)
                current_input = next_input

            # 3. ì‹œê°í™”
            fig, ax = plt.subplots(figsize=(10, 6))

            ax.plot(range(len(historical_scores)), historical_scores, label="Past Quiz Scores (Hypothetical)", marker='o', linestyle='-', color='blue')
            future_indices = range(len(historical_scores), len(historical_scores) + len(future_predictions))
            ax.plot(future_indices, future_predictions, label="Predicted Achievement (Next 5 Days)", marker='x', linestyle='--', color='red')

            ax.set_title(L["lstm_header"])
            ax.set_xlabel(L["topic_label"])
            ax.set_ylabel("Achievement Score (0-100)")
            ax.legend()
            st.pyplot(fig)

            # 4. LLM ë¶„ì„ ì½”ë©˜íŠ¸
            st.markdown("---")
            st.markdown(f"#### {L.get('coach_analysis', 'AI Coach Analysis Comment')}")
            
            avg_recent = np.mean(historical_scores[-5:])
            avg_predict = np.mean(future_predictions)
            
            if st.session_state.language == 'ko':
                if avg_predict > avg_recent:
                    comment = "ìµœê·¼ í•™ìŠµ ë°ì´í„°ì™€ LSTM ì˜ˆì¸¡ ê²°ê³¼ì— ë”°ë¥´ë©´, **ì•ìœ¼ë¡œì˜ í•™ìŠµ ì„±ì·¨ë„ê°€ ê¸ì •ì ìœ¼ë¡œ í–¥ìƒë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡**ë©ë‹ˆë‹¤. í˜„ì¬ í•™ìŠµ ë°©ì‹ì„ ìœ ì§€í•˜ì‹œê±°ë‚˜, ë‚œì´ë„ë¥¼ í•œ ë‹¨ê³„ ë†’ì—¬ ë„ì „í•´ ë³´ì„¸ìš”!"
                elif avg_predict < avg_recent - 5:
                    comment = "LSTM ì˜ˆì¸¡ ê²°ê³¼, **ì„±ì·¨ë„ê°€ ë‹¤ì†Œ í•˜ë½í•  ìˆ˜ ìˆë‹¤ëŠ” ì‹ í˜¸**ê°€ ë³´ì…ë‹ˆë‹¤. í•™ìŠµì— ì‚¬ìš©ëœ ìë£Œë‚˜ ë°©ë²•ë¡ ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. RAG ì±—ë´‡ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ê¸°ì´ˆ ê°œë…ì„ ë‹¤ì‹œ í™•ì¸í•´ ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤."
                else:
                    comment = "ì„±ì·¨ë„ëŠ” í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤. ì •ì²´ê¸°ê°€ ë  ìˆ˜ ìˆìœ¼ë‹ˆ, **ìƒˆë¡œìš´ í•™ìŠµ ì½˜í…ì¸  í˜•ì‹(ì˜ˆ: ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´)ì„ ìƒì„±**í•˜ì—¬ í•™ìŠµì— í™œë ¥ì„ ë”í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ ë³´ì„¸ìš”ã€‚"
            elif st.session_state.language == 'en': # English
                if avg_predict > avg_recent:
                    comment = "Based on recent learning data and LSTM prediction, **your achievement is projected to improve positively**. Maintain your current study methods or consider increasing the difficulty level."
                elif avg_predict < avg_recent - 5:
                    comment = "LSTM prediction suggests a **potential drop in achievement**. Your understanding of fundamental concepts may be lacking. Use the RAG Chatbot to review foundational knowledge."
                else:
                    comment = "Achievement is expected to remain stable. Consider generating **new content types (e.g., Practical Example Ideas)** to revitalize your learning during this plateau."
            else: # Japanese
                if avg_predict > avg_recent:
                    comment = "æœ€è¿‘ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨LSTMäºˆæ¸¬çµæœã«åŸºã¥ãã€**ä»Šå¾Œã®é”æˆåº¦ã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ã«å‘ä¸Šã™ã‚‹ã¨äºˆæ¸¬**ã•ã‚Œã¾ã™ã€‚ç¾åœ¨ã®å­¦ç¿’æ–¹æ³•ã‚’ç¶­æŒã™ã‚‹ã‹ã€é›£æ˜“åº¦ã‚’ä¸€æ®µéšä¸Šã’ã¦æŒ‘æˆ¦ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                elif avg_predict < avg_recent - 5:
                    comment = "LSTMäºˆæ¸¬ã®çµæœã€**é”æˆåº¦ãŒã‚„ã‚„ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§**ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚å­¦ç¿’è³‡æ–™ã‚„æ–¹æ³•è«–ã®åŸºç¤ç†è§£ãŒä¸è¶³ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã—ã¦ã€åŸºæœ¬æ¦‚å¿µã‚’å†ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
                else:
                    comment = "é”æˆåº¦ã¯ç¾çŠ¶ç¶­æŒã¨äºˆæ¸¬ã•ã‚Œã¾ã™ã€‚åœæ»æœŸã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚**æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼ï¼ˆä¾‹ï¼šå®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢ï¼‰ã‚’ç”Ÿæˆ**ã—ã€å­¦ç¿’ã«æ´»åŠ›ã‚’ä¸ãˆã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"


            st.info(comment)

        except Exception as e:
            st.error(f"LSTM Model Processing Error: {e}")
            st.markdown(f'<div style="background-color: #fce4e4; color: #cc0000; padding: 10px; border-radius: 5px;">{L["lstm_disabled_error"]}</div>', unsafe_allow_html=True)
