# ========================================
# streamlit_app.py (ì „ì²´ ìˆ˜ì •ëœ ì½”ë“œ)
#
# ì£¼ìš” ê°œì„  ì‚¬í•­:
# 1. ì±„íŒ…/ì´ë©”ì¼ íƒ­ì— 'ì „í™” ë°œì‹  (í˜„ì§€ ì—…ì²´/ê³ ê°)' ë²„íŠ¼ ë° ê¸°ëŠ¥ ì¶”ê°€ (ì˜ˆì™¸ ì²˜ë¦¬ ëŒ€ì‘)
# 2. ì „í™” íƒ­ì— 'ì „í™” ë°œì‹ ' ë²„íŠ¼ ì¶”ê°€ ë° ë°œì‹  í†µí™” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì§€ì›
# 3. ê´€ë ¨ ì–¸ì–´ íŒ© ì¶”ê°€ ë° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
# 4. í€´ì¦ˆ ê¸°ëŠ¥ì˜ ì •ë‹µ í™•ì¸, í•´ì„¤, ì ìˆ˜ í‘œì‹œ ë¡œì§ ì™„ì„±
# 5. [BUG FIX] ì–¸ì–´ ì´ê´€ ì‹œ 'ë²ˆì—­ ë‹¤ì‹œ ì‹œë„' ë²„íŠ¼ì˜ DuplicateWidgetID ì˜¤ë¥˜ í•´ê²°
# 6. [BUG FIX] ì½˜í…ì¸  ìƒì„± íƒ­ì˜ LLM ì‘ë‹µ ë° ë¼ë””ì˜¤ ë²„íŠ¼ ì´ˆê¸°í™” ì˜¤ë¥˜ í•´ê²°
# â­ [ì „í™” ì•„ë°”íƒ€ ë²„ê·¸ ìˆ˜ì •]
# 7. ì „í™” ì‘ë‹µ í›„ ì¸ì‚¬ë§ ë¯¸ì¶œë ¥ ì˜¤ë¥˜ ìˆ˜ì • (just_entered_call í”Œë˜ê·¸ ìœ„ì¹˜ ìˆ˜ì •)
# 8. ì•„ë°”íƒ€ Lottie íŒŒì¼ ë¡œë”© ê²½ë¡œ ìˆ˜ì • (ì—…ë¡œë“œëœ íŒŒì¼ëª… ì°¸ì¡°)
# ========================================

import os
import io
import json
import time
import uuid
import base64
import tempfile
import hashlib
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Union
from typing import List, Dict, Any
import google.generativeai as genai
import numpy as np
import streamlit as st
from matplotlib import pyplot as plt
import requests  # â­ ì¶”ê°€: requests ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots

    IS_PLOTLY_AVAILABLE = True
except ImportError:
    IS_PLOTLY_AVAILABLE = False

from openai import OpenAI
from anthropic import Anthropic

# mic_recorder (0.0.8) - returns dict with key "bytes"
from streamlit_mic_recorder import mic_recorder

# LangChain / RAG ê´€ë ¨
from langchain_core.documents import Document
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    raise ImportError(
        "âŒ 'langchain-text-splitters' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install langchain-text-splitters\n"
        "ë˜ëŠ” requirements.txtì˜ ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜: pip install -r requirements.txt"
    )
from langchain_core.prompts import PromptTemplate
try:
    from langchain.memory import ConversationBufferMemory
except ImportError:
    raise ImportError(
        "âŒ 'langchain' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ 'langchain.memory' ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install langchain\n"
        "ë˜ëŠ” requirements.txtì˜ ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜: pip install -r requirements.txt"
    )
try:
    from langchain.chains import ConversationChain
except ImportError:
    raise ImportError(
        "âŒ 'langchain' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ 'langchain.chains' ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install langchain\n"
        "ë˜ëŠ” requirements.txtì˜ ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜: pip install -r requirements.txt"
    )

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

# Word, PPTX, PDF ìƒì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from docx import Document as DocxDocument
    from docx.shared import Pt, RGBColor, Inches
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    IS_DOCX_AVAILABLE = True
except ImportError:
    IS_DOCX_AVAILABLE = False

try:
    from pptx import Presentation
    from pptx.util import Inches, Pt
    IS_PPTX_AVAILABLE = True
except ImportError:
    IS_PPTX_AVAILABLE = False

try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.lib import colors
    from reportlab.lib.colors import black
    IS_REPORTLAB_AVAILABLE = True
except ImportError:
    IS_REPORTLAB_AVAILABLE = False



try:
    from langchain_google_genai import GoogleGenerativeAIEmbeddings

    IS_GEMINI_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_GEMINI_EMBEDDING_AVAILABLE = False

try:
    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

    IS_NVIDIA_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_NVIDIA_EMBEDDING_AVAILABLE = False

# ========================================
# Streamlit í˜ì´ì§€ ì„¤ì • (ë°˜ë“œì‹œ ìµœìƒë‹¨ì— ìœ„ì¹˜)
# ========================================
st.set_page_config(
    page_title="AI Study Coach & Customer Service Simulator",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ========================================
# 0. ê¸°ë³¸ ê²½ë¡œ/ë¡œì»¬ DB ì„¤ì •
# ========================================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "local_db")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
RAG_INDEX_DIR = os.path.join(DATA_DIR, "rag_index")

VOICE_META_FILE = os.path.join(DATA_DIR, "voice_records.json")
SIM_META_FILE = os.path.join(DATA_DIR, "simulation_histories.json")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(RAG_INDEX_DIR, exist_ok=True)




# ----------------------------------------
# JSON Helper
# ----------------------------------------
def _load_json(path: str, default: Any):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    except (FileNotFoundError, json.JSONDecodeError):
        return default


def _save_json(path: str, data: Any):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)





# ========================================
# 1. ë‹¤êµ­ì–´ ì„¤ì • (ì „í™” ë°œì‹  ê´€ë ¨ í…ìŠ¤íŠ¸ ì¶”ê°€)
# ========================================
DEFAULT_LANG = "ko"

LANG: Dict[str, Dict[str, str]] = {
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜ (ìŒì„± ë° DB í†µí•©)",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "sim_tab_chat_email": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì±„íŒ…/ì´ë©”ì¼)",
        "sim_tab_phone": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì „í™”)",
        "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
        "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
        "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
        "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
        "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
        "check_answer": "ì •ë‹µ í™•ì¸",
        "next_question": "ë‹¤ìŒ ë¬¸í•­",
        "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
        "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤ã€‚ğŸ˜",
        "correct_is": "ì •ë‹µ",
        "explanation": "í•´ì„¤",
        "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
        "score": "ì ìˆ˜",
        "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
        "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
        "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚",
        "db_save_complete": "(DB ì €ì¥ ì™„ë£Œ)",
        "data_analysis_progress": "ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘...",
        "response_generating": "ë‹µë³€ ìƒì„± ì¤‘...",
        "lstm_result_header": "í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼",
        "lstm_score_metric": "í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„",
        "lstm_score_info": "ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!",
        "lstm_rerun_button": "ìƒˆë¡œìš´ ê°€ìƒ ë°ì´í„°ë¡œ ì˜ˆì¸¡",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ ---
        "toast_like": "ğŸ”¥ ì»¨í…ì¸ ê°€ ë§˜ì— ë“œì…¨êµ°ìš”! (ì¢‹ì•„ìš” ì¹´ìš´íŠ¸ +1)",
        "toast_dislike": "ğŸ˜” ë” ë‚˜ì€ ì½˜í…ì¸ ë¥¼ ìœ„í•´ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ê² ìŠµë‹ˆë‹¤ã€‚",
        "toast_share": "ğŸŒ ì½˜í…ì¸  ë§í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "toast_copy": "âœ… ì½˜í…ì¸ ê°€ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!",
        "toast_more": "â„¹ï¸ ì¶”ê°€ ì˜µì…˜ (PDF, ì¸ì‡„ë³¸ ì €ì¥ ë“±)",
        "mock_pdf_save": "ğŸ“¥ PDF ì €ì¥",
        "mock_word_open": "ğŸ“‘ Wordë¡œ ì—´ê¸°",
        "mock_print": "ğŸ–¨ ì¸ì‡„",

        # --- ì‹œë®¬ë ˆì´í„° ---
        "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤ã€‚",
        "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
        "customer_type_label": "ê³ ê° ì„±í–¥",
        "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
        "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
        "customer_generate_response_button": "ê³ ê° ë°˜ì‘ ìƒì„±",
        "send_closing_confirm_button": "ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸°",
        "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ê¸° ë•Œë¬¸ì— ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
        "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
        "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
        "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
        "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
        "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!",
        "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
        "history_expander_title": "ğŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê±´)",
        "initial_query_sample": "í”„ë‘ìŠ¤ íŒŒë¦¬ì— ë„ì°©í–ˆëŠ”ë°, í´ë£©ì—ì„œ êµ¬ë§¤í•œ eSIMì´ í™œì„±í™”ê°€ ì•ˆ ë©ë‹ˆë‹¤...",
        "button_mic_input": "ğŸ™ ìŒì„± ì…ë ¥",
        "button_mic_stop": "â¹ï¸ ë…¹ìŒ ì¢…ë£Œ",
        "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤ã€‚",
        "prompt_survey": "ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› 000ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì‹œê¸° ë°”ëë‹ˆë‹¤. [ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬]",
        "customer_closing_confirm": "ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹­ë‹ˆê¹Œ?",
        "customer_positive_response": "ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤ã€‚",
        "button_email_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ìš”ì²­)",
        "error_mandatory_contact": "ì´ë©”ì¼ê³¼ ì „í™”ë²ˆí˜¸ ì…ë ¥ì€ í•„ìˆ˜ì…ë‹ˆë‹¤ã€‚",
        "customer_attachment_label": "ğŸ“ ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ",
        "attachment_info_llm": "[ê³ ê° ì²¨ë¶€ íŒŒì¼: {filename}ì´(ê°€) í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”.]",
        "button_retry_translation": "ë²ˆì—­ ë‹¤ì‹œ ì‹œë„",
        "button_request_hint": "ğŸ’¡ ì‘ëŒ€ íŒíŠ¸ ìš”ì²­ (AHT ëª¨ë‹ˆí„°ë§ ì¤‘)",
        "button_generate_draft": "ğŸ¤– AI ì‘ë‹µ ì´ˆì•ˆ ìƒì„±",
        "draft_generating": "AIê°€ ì‘ë‹µ ì´ˆì•ˆì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...",
        "draft_success": "âœ… AI ì‘ë‹µ ì´ˆì•ˆì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.",
        "hint_placeholder": "ë¬¸ì˜ ì‘ëŒ€ì— ëŒ€í•œ íŒíŠ¸:",
        "survey_sent_confirm": "ğŸ“¨ ì„¤ë¬¸ì¡°ì‚¬ ë§í¬ê°€ ì „ì†¡ë˜ì—ˆìœ¼ë©°, ì´ ìƒë‹´ì€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "new_simulation_ready": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ã€‚",
        "agent_response_header": "âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ",
        "agent_response_placeholder": "ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš”...",
        "send_response_button": "ì‘ë‹µ ì „ì†¡",
        "customer_turn_info": "ì—ì´ì „íŠ¸ ì‘ë‹µ ì „ì†¡ ì™„ë£Œ. ê³ ê° ë°˜ì‘ì„ ìë™ìœ¼ë¡œ ìƒì„± ì¤‘ì…ë‹ˆë‹¤ã€‚",
        "generating_customer_response": "ê³ ê° ë°˜ì‘ ìƒì„± ì¤‘...",
        "customer_escalation_start": "ìƒê¸‰ìì™€ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
        "request_rebuttal_button": "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­",
        "new_simulation_button": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘",
        "history_selectbox_label": "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
        "history_load_button": "ì„ íƒëœ ì´ë ¥ ë¡œë“œ",
        "delete_history_button": "âŒ ëª¨ë“  ì´ë ¥ ì‚­ì œ",
        "delete_confirm_message": "ì •ë§ë¡œ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        "delete_confirm_yes": "ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤",
        "download_history_word": "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (Word)",
        "download_history_pptx": "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PPTX)",
        "download_history_pdf": "ğŸ“¥ ì´ë ¥ ë‹¤ìš´ë¡œë“œ (PDF)",
        "download_current_session": "ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ë‹¤ìš´ë¡œë“œ",
        "delete_confirm_no": "ì•„ë‹ˆì˜¤, ìœ ì§€í•©ë‹ˆë‹¤",
        "delete_success": "âœ… ì‚­ì œ ì™„ë£Œ!",
        "deleting_history_progress": "ì´ë ¥ ì‚­ì œ ì¤‘...",
        "search_history_label": "ì´ë ¥ ê²€ìƒ‰",
        "date_range_label": "ë‚ ì§œ ë²”ìœ„ í•„í„°",
        "history_search_button": "ğŸ” ê²€ìƒ‰",
        "no_history_found": "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "customer_email_label": "ê³ ê° ì´ë©”ì¼ (í•„ìˆ˜)",
        "customer_phone_label": "ê³ ê° ì—°ë½ì²˜ / ì „í™”ë²ˆí˜¸ (í•„ìˆ˜)",
        "transfer_header": "ì–¸ì–´ ì´ê´€ ìš”ì²­ (ë‹¤ë¥¸ íŒ€)",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ ì˜ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ ì¼ë³¸ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_system_msg": "ğŸ“Œ ì‹œìŠ¤í…œ ë©”ì‹œì§€: ê³ ê° ìš”ì²­ì— ë”°ë¼ ìƒë‹´ ì–¸ì–´ê°€ {target_lang} íŒ€ìœ¼ë¡œ ì´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìƒë‹´ì›(AI)ì´ ì‘ëŒ€í•©ë‹ˆë‹¤ã€‚",
        "transfer_loading": "ì´ê´€ ì²˜ë¦¬ ì¤‘: ì´ì „ ëŒ€í™” ì´ë ¥ ë²ˆì—­ ë° ê²€í†  (ê³ ê°ë‹˜ê»˜ 3~10ë¶„ ì–‘í•´ ìš”ì²­)",
        "transfer_summary_header": "ğŸ” ì´ê´€ëœ ìƒë‹´ì›ì„ ìœ„í•œ ìš”ì•½ (ë²ˆì—­ë¨)",
        "transfer_summary_intro": "ê³ ê°ë‹˜ê³¼ì˜ ì´ì „ ëŒ€í™” ì´ë ¥ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ëŒ€ë¥¼ ì´ì–´ë‚˜ê°€ì„¸ìš”ã€‚",
        "llm_translation_error": "âŒ ë²ˆì—­ ì‹¤íŒ¨: LLM ì‘ë‹µ ì˜¤ë¥˜",
        "timer_metric": "ìƒë‹´ ê²½ê³¼ ì‹œê°„ (AHT)",
        "timer_info_ok": "AHT (15ë¶„ ê¸°ì¤€)",
        "timer_info_warn": "AHT (10ë¶„ ì´ˆê³¼)",
        "timer_info_risk": "ğŸš¨ 15ë¶„ ì´ˆê³¼: ë†’ì€ ë¦¬ìŠ¤í¬",
        "solution_check_label": "âœ… ì´ ì‘ë‹µì— ì†”ë£¨ì…˜/í•´ê²°ì±…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ã€‚",
        "sentiment_score_label": "ê³ ê° ê°ì • ì ìˆ˜",
        "urgency_score_label": "ê¸´ê¸‰ë„ ì ìˆ˜",
        "similarity_chart_title": "ìœ ì‚¬ ì¼€ì´ìŠ¤ ìœ ì‚¬ë„",
        "scores_comparison_title": "ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ",
        "similarity_score_label": "ìœ ì‚¬ë„",
        "satisfaction_score_label": "ë§Œì¡±ë„",
        "sentiment_trend_label": "ê°ì • ì ìˆ˜ ì¶”ì´",
        "satisfaction_trend_label": "ë§Œì¡±ë„ ì ìˆ˜ ì¶”ì´",
        "case_trends_title": "ê³¼ê±° ì¼€ì´ìŠ¤ ì ìˆ˜ ì¶”ì´",
        "date_label": "ë‚ ì§œ",
        "score_label": "ì ìˆ˜ (0-100)",
        "customer_characteristics_title": "ê³ ê° íŠ¹ì„± ë¶„í¬",
        "language_label": "ì–¸ì–´",
        "email_provided_label": "ì´ë©”ì¼ ì œê³µ",
        "phone_provided_label": "ì „í™”ë²ˆí˜¸ ì œê³µ",
        "region_label": "ì§€ì—­",
        "btn_request_phone_summary": "ì´ë ¥ ìš”ì•½ ìš”ì²­",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "ì „í™” ë°œì‹ ",
        "call_outbound_system_msg": "ğŸ“Œ ì‹œìŠ¤í…œ ë©”ì‹œì§€: ì—ì´ì „íŠ¸ê°€ {target}ì—ê²Œ ì „í™” ë°œì‹ ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤ã€‚",
        "call_outbound_simulation_header": "ğŸ“ ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼",
        "call_outbound_summary_header": "ğŸ“ í˜„ì§€ ì—…ì²´/ê³ ê°ê³¼ì˜ í†µí™” ìš”ì•½",
        "call_outbound_loading": "ì „í™” ì—°ê²° ë° í†µí™” ê²°ê³¼ ì •ë¦¬ ì¤‘... (LLM í˜¸ì¶œ)",
        "call_target_customer": "ê³ ê°ì—ê²Œ ë°œì‹ ",
        "call_target_partner": "í˜„ì§€ ì—…ì²´ ë°œì‹ ",

        # --- ìŒì„± ê¸°ë¡ ---
        "voice_rec_header": "ìŒì„± ê¸°ë¡ & ê´€ë¦¬",
        "record_help": "ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "uploaded_file": "ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ",
        "rec_list_title": "ì €ì¥ëœ ìŒì„± ê¸°ë¡",
        "transcribe_btn": "ì „ì‚¬(Whisper)",
        "save_btn": "ìŒì„± ê¸°ë¡ ì €ì¥",
        "transcribing": "ìŒì„± ì „ì‚¬ ì¤‘...",
        "transcript_result": "ì „ì‚¬ ê²°ê³¼:",
        "transcript_text": "ì „ì‚¬ í…ìŠ¤íŠ¸",
        "openai_missing": "OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "whisper_client_error": "âŒ Whisper API Client ì´ˆê¸°í™” ì‹¤íŒ¨",
        "whisper_auth_error": "âŒ Whisper API ì¸ì¦ ì‹¤íŒ¨",
        "whisper_format_error": "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤ã€‚",
        "whisper_success": "âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ!",
        "playback": "ë…¹ìŒ ì¬ìƒ",
        "retranscribe": "ì¬ì „ì‚¬",
        "delete": "ì‚­ì œ",
        "no_records": "ì €ì¥ëœ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "saved_success": "ì €ì¥ ì™„ë£Œ!",
        "delete_confirm_rec": "ì •ë§ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        "gcs_not_conf": "GCS ë¯¸ì„¤ì •",
        "gcs_playback_fail": "ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨",
        "gcs_no_audio": "ì˜¤ë””ì˜¤ ì—†ìŒ",
        "error": "ì˜¤ë¥˜:",
        "firestore_no_db_connect": "DB ì—°ê²° ì‹¤íŒ¨",
        "save_history_success": "ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "save_history_fail": "ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨",
        "delete_fail": "ì‚­ì œ ì‹¤íŒ¨",
        "rec_header": "ìŒì„± ì…ë ¥ ë° ì „ì‚¬",
        "whisper_processing": "ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘..",
        "empty_response_warning": "ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”.",
        "customer_no_more_inquiries": "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.",
        "customer_has_additional_inquiries": "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤.",
        "sim_end_chat_button": "ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë° ì‘ëŒ€ ì¢…ë£Œ",
        "delete_mic_record": "âŒ ë…¹ìŒ ì‚­ì œ",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ (ìŠ¤í¬ë¦°ìƒ· ë“±)",
        "attachment_placeholder": "íŒŒì¼ì„ ì²¨ë¶€í•˜ì—¬ ìƒí™©ì„ ì„¤ëª…í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­)",
        "attachment_info_llm": "[ê³ ê° ì²¨ë¶€ íŒŒì¼: {filename}ì´(ê°€) í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”.]",
        "agent_attachment_label": "ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ (ìŠ¤í¬ë¦°ìƒ· ë“±)",
        "agent_attachment_placeholder": "ì‘ë‹µì— ì²¨ë¶€í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­)",
        "agent_attachment_status": "ğŸ“ ì—ì´ì „íŠ¸ê°€ **{filename}** íŒŒì¼ì„ ì‘ë‹µì— ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. (íŒŒì¼ íƒ€ì…: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG ì„ë² ë”© ì‹¤íŒ¨: OpenAI API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_gemini": "RAG ì„ë² ë”© ì‹¤íŒ¨: Gemini API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_nvidia": "RAG ì„ë² ë”© ì‹¤íŒ¨: NVIDIA API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_none": "RAG ì„ë² ë”©ì— í•„ìš”í•œ ëª¨ë“  í‚¤(OpenAI, Gemini, NVIDIA)ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”ã€‚",

        # --- ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ì¶”ê°€ ---
        "phone_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì „í™”)",
        "call_status_waiting": "ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...",
        "call_status_ringing": "ì „í™” ìˆ˜ì‹  ì¤‘: {number}",
        "button_answer": "ğŸ“ ì „í™” ì‘ë‹µ",
        "button_hangup": "ğŸ”´ ì „í™” ëŠê¸°",
        "button_hold": "â¸ï¸ Hold (ì†ŒìŒ ì°¨ë‹¨)",
        "button_resume": "â–¶ï¸ í†µí™” ì¬ê°œ",
        "hold_status": "í†µí™” Hold ì¤‘ (ëˆ„ì  Hold ì‹œê°„: {duration})",
        "cc_live_transcript": "ğŸ¤ ì‹¤ì‹œê°„ CC ìë§‰ / ì „ì‚¬",
        "mic_input_status": "ğŸ™ï¸ ì—ì´ì „íŠ¸ ìŒì„± ì…ë ¥",
        "customer_audio_playback": "ğŸ—£ï¸ ê³ ê° ìŒì„± ì¬ìƒ",
        "agent_response_prompt": "ê³ ê°ì—ê²Œ ë§í•  ì‘ë‹µì„ ë…¹ìŒí•˜ì„¸ìš”ã€‚",
        "agent_response_stop_and_send": "â¹ï¸ ë…¹ìŒ ì¢…ë£Œ ë° ì‘ë‹µ ì „ì†¡",
        "call_end_message": "í†µí™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. AHT ë° ì´ë ¥ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "call_query_placeholder": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”ã€‚",
        "call_number_placeholder": "+82 10-xxxx-xxxx (ê°€ìƒ ë²ˆí˜¸)",
        "call_summary_header": "AI í†µí™” ìš”ì•½",
        "customer_audio_header": "ê³ ê° ìµœì´ˆ ë¬¸ì˜ (ìŒì„±)",
        "aht_not_recorded": "âš ï¸ í†µí™” ì‹œì‘ ì‹œê°„ì´ ê¸°ë¡ë˜ì§€ ì•Šì•„ AHTë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "no_audio_record": "ê³ ê°ì˜ ìµœì´ˆ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
    },

    # --- â­ ì˜ì–´ ë²„ì „ (í•œêµ­ì–´ 100% ë§¤ì¹­) ---
    "en": {
        "title": "Personalized AI Study Coach (Voice & Local DB)",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction Dashboard",
        "sim_tab_chat_email": "AI Customer Support Simulator (Chat / Email)",
        "sim_tab_phone": "AI Customer Support Simulator (Phone)",
        "simulator_tab": "AI Customer Support Simulator",
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answer questions based on uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "llm_error_key": "âš ï¸ Warning: GEMINI_API_KEY is not set.",
        "llm_error_init": "LLM initialization error. Please check your API key.",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content based on the topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "10 MCQ Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Achievement Prediction Dashboard",
        "lstm_desc": "Train an LSTM model on hypothetical quiz scores and predict performance.",
        "lang_select": "Select Language",
        "embed_success": "Learning DB built with {count} chunks!",
        "embed_fail": "Embedding failed: quota exceeded or network issue.",
        "warning_no_files": "Please upload study materials first.",
        "warning_rag_not_ready": "RAG is not ready. Upload materials and analyze.",
        "quiz_fail_structure": "Quiz data structure is invalid.",
        "select_answer": "Select answer",
        "check_answer": "Check answer",
        "next_question": "Next question",
        "correct_answer": "Correct! ğŸ‰",
        "incorrect_answer": "Incorrect ğŸ˜",
        "correct_is": "Correct answer",
        "explanation": "Explanation",
        "quiz_complete": "Quiz Complete!",
        "score": "Score",
        "retake_quiz": "Retake Quiz",
        "quiz_error_llm": "Quiz generation failed: invalid JSON.",
        "quiz_original_response": "Original LLM Response",
        "firestore_loading": "Loading RAG index...",
        "firestore_no_index": "No existing RAG index found.",
        "db_save_complete": "(DB Save Complete)",
        "data_analysis_progress": "Analyzing materials and building DB...",
        "response_generating": "Generating response...",
        "lstm_result_header": "Achievement Prediction",
        "lstm_score_metric": "Predicted Achievement",
        "lstm_score_info": "Estimated next quiz score: **{predicted_score:.1f}**.",
        "lstm_rerun_button": "Predict with New Data",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ ---
        "toast_like": "ğŸ”¥ Content liked! (+1 Count Reflected)",
        "toast_dislike": "ğŸ˜” Feedback recorded for better content.",
        "toast_share": "ğŸŒ Content link generated.",
        "toast_copy": "âœ… Content copied to clipboard!",
        "toast_more": "â„¹ï¸ Additional options (Print, PDF Save, etc.)",
        "mock_pdf_save": "ğŸ“¥ Save as PDF",
        "mock_word_open": "ğŸ“‘ Open via Word",
        "mock_print": "ğŸ–¨ Print",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ë ---

        # Simulator
        "simulator_header": "AI Customer Response Simulator",
        "simulator_desc": "AI generates draft responses and guidelines for customer inquiries.",
        "customer_query_label": "Customer Message (links allowed)",
        "customer_type_label": "Customer Type",
        "customer_type_options": ["General Inquiry", "Difficult Customer", "Highly Dissatisfied Customer"],
        "button_simulate": "Generate Response",
        "customer_generate_response_button": "Generate Customer Response",
        "send_closing_confirm_button": "Send Closing Confirmation",
        "simulation_warning_query": "Please enter the customerâ€™s message.",
        "simulation_no_key_warning": "âš ï¸ API Key missing. Simulation cannot proceed.",
        "simulation_advice_header": "AI Response Guidelines",
        "simulation_draft_header": "Recommended Response Draft",
        "button_listen_audio": "Play as Audio",
        "tts_status_ready": "Ready to generate audio",
        "tts_status_generating": "Generating audio...",
        "tts_status_success": "Audio ready!",
        "tts_status_error": "TTS error occurred",
        "history_expander_title": "ğŸ“ Load Previous Sessions (Last 10)",
        "initial_query_sample": "I arrived in Paris but my Klook eSIM won't activateâ€¦",
        "button_mic_input": "ğŸ™ Voice Input",
        "button_mic_stop": "â¹ï¸ Stop recording",
        "prompt_customer_end": "No further inquiries. Ending chat.",
        "prompt_survey": "This was Agent 000. Have a nice day. [Survey Link]",
        "customer_closing_confirm": "Is there anything else we can assist you with?",
        "customer_positive_response": "I understand. Thank you.",
        "button_email_end_chat": "End supports (Survey Request)",
        "error_mandatory_contact": "Email and Phone number input are mandatory.",
        "customer_attachment_label": "ğŸ“ Customer Attachment Upload",
        "attachment_info_llm": "[Customer Attachment: {filename} is confirmed. Reference this file in your response.]",
        "button_retry_translation": "Retry Translation",
        "button_request_hint": "ğŸ’¡ Request Response Hint (AHT Monitored)",
        "button_generate_draft": "ğŸ¤– Generate AI Response Draft",
        "draft_generating": "AI is generating a response draft...",
        "draft_success": "âœ… AI response draft has been generated. Please review and modify below.",
        "hint_placeholder": "Hints for responses",
        "survey_sent_confirm": "ğŸ“¨ The survey link has been sent. This chat session is now closedã€‚",
        "new_simulation_ready": "You can now start a new simulation.",
        "agent_response_header": "âœï¸ Agent Response",
        "agent_response_placeholder": "Write a response...",
        "send_response_button": "Send Response",
        "customer_turn_info": "Agent response sent. Generating customer reaction automaticallyã€‚",
        "generating_customer_response": "Generating customer response...",
        "customer_escalation_start": "I want to speak to a supervisor",
        "request_rebuttal_button": "Request Customer Reaction",
        "new_simulation_button": "Start New Simulation",
        "history_selectbox_label": "Choose a record to load:",
        "history_load_button": "Load Selected Record",
        "delete_history_button": "âŒ Delete All History",
        "delete_confirm_message": "Are you sure you want to delete all records?",
        "delete_confirm_yes": "Yes, Delete",
        "delete_confirm_no": "Cancel",
        "download_history_word": "ğŸ“¥ Download History (Word)",
        "download_history_pptx": "ğŸ“¥ Download History (PPTX)",
        "download_history_pdf": "ğŸ“¥ Download History (PDF)",
        "download_current_session": "ğŸ“¥ Download Current Session",
        "delete_success": "Deleted successfully!",
        "deleting_history_progress": "Deleting history...",
        "search_history_label": "Search History",
        "date_range_label": "Date Filter",
        "history_search_button": "ğŸ” Search",
        "no_history_found": "No matching history found.",
        "customer_email_label": "Customer Email (Mandatory)",
        "customer_phone_label": "Customer Phone / WhatsApp (Mandatory)",
        "transfer_header": "Language Transfer Request (To Other Teams)",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ English Team Transfer",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ Japanese Team Transfer",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· Korean Team Transfer",
        "transfer_system_msg": "ğŸ“Œ System Message: The session language has been transferred to the {target_lang} team per customer request. A new agent (AI) will now respondã€‚",
        "transfer_loading": "Transferring: Translating and reviewing chat history (3-10 minute wait requested from customer)",
        "transfer_summary_header": "ğŸ” Summary for Transferred Agent (Translated)",
        "transfer_summary_intro": "This is the previous chat history. Please continue the support based on this summaryã€‚",
        "llm_translation_error": "âŒ Translation failed: LLM response error",
        "timer_metric": "Elapsed Time",
        "timer_info_ok": "AHT (15 min standard)",
        "timer_info_warn": "AHT (Over 10 min)",
        "timer_info_risk": "ğŸš¨ Over 15 min: High Risk",
        "solution_check_label": "âœ… This response includes a solution/fixã€‚",
        "sentiment_score_label": "Customer Sentiment Score",
        "urgency_score_label": "Urgency Score",
        "similarity_chart_title": "Case Similarity",
        "scores_comparison_title": "Sentiment & Satisfaction Scores",
        "similarity_score_label": "Similarity",
        "satisfaction_score_label": "Satisfaction",
        "sentiment_trend_label": "Sentiment Trend",
        "satisfaction_trend_label": "Satisfaction Trend",
        "case_trends_title": "Case Score Trends",
        "date_label": "Date",
        "score_label": "Score (0-100)",
        "customer_characteristics_title": "Customer Characteristics",
        "language_label": "Language",
        "email_provided_label": "Email Provided",
        "phone_provided_label": "Phone Provided",
        "region_label": "Region",
        "btn_request_phone_summary": "Request to summarize histories",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "Call Outbound",
        "call_outbound_system_msg": "ğŸ“Œ System Message: Agent attempted an outbound call to {target}ã€‚",
        "call_outbound_simulation_header": "ğŸ“ Outbound Call Simulation Result",
        "call_outbound_summary_header": "ğŸ“ Summary of Call with Local Partner/Customer",
        "call_outbound_loading": "Connecting call and summarizing outcome... (LLM Call)",
        "call_target_customer": "Call Customer",
        "call_target_partner": "Call Local Partner",

        # --- ìŒì„± ê¸°ë¡ ---
        "voice_rec_header": "Voice Record & Management",
        "record_help": "Record using the microphone or upload a fileã€‚",
        "uploaded_file": "Upload Audio File",
        "rec_list_title": "Saved Voice Records",
        "transcribe_btn": "Transcribe (Whisper)",
        "save_btn": "Save Record",
        "transcribing": "Transcribing...",
        "transcript_result": "Transcription:",
        "transcript_text": "Transcribed Text",
        "openai_missing": "OpenAI API Key is missingã€‚",
        "whisper_client_error": "âŒ Error: Whisper API client not initializedã€‚",
        "whisper_auth_error": "âŒ Whisper API authentication failedã€‚",
        "whisper_format_error": "âŒ Error: Unsupported audio formatã€‚",
        "whisper_success": "âœ… Voice Transcription Complete!",
        "playback": "Playback Recording",
        "retranscribe": "Re-transcribe",
        "delete": "Delete",
        "no_records": "No saved voice recordsã€‚",
        "saved_success": "Saved successfully!",
        "delete_confirm_rec": "Are you sure you want to delete this voice record?",
        "gcs_not_conf": "GCS not configured or no audio available",
        "gcs_playback_fail": "Failed to play audio",
        "gcs_no_audio": "No audio file found",
        "error": "Error:",
        "firestore_no_db_connect": "DB connection failed",
        "save_history_success": "Saved successfullyã€‚",
        "save_history_fail": "Save failedã€‚",
        "delete_fail": "Delete failed",
        "rec_header": "Voice Input & Transcription",
        "whisper_processing": "Processing...",
        "empty_response_warning": "Please enter a responseã€‚",
        "customer_no_more_inquiries": "No, that will be all, thank youã€‚",
        "customer_has_additional_inquiries": "Yes, I have an additional questionã€‚",
        "sim_end_chat_button": "Send Survey Link and End Consultations",
        "delete_mic_record": "âŒ Delete recordings",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "Customer Attachment Upload (Screenshot, etcã€‚)",
        "attachment_placeholder": "Attach a file to explain the situation (optional)",
        "attachment_info_llm": "[Customer Attachment: {filename} is confirmed. Reference this file in your responseã€‚]",
        "agent_attachment_label": "Agent Attachment (Screenshot, etcã€‚)",
        "agent_attachment_placeholder": "Select a file to attach to the response (optional)",
        "agent_attachment_status": "ğŸ“ Agent attached **{filename}** file to the responseã€‚ (File type: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG embedding failed: OpenAI API Key is invalid or not setã€‚",
        "rag_embed_error_gemini": "RAG embedding failed: Gemini API Key is invalid or not setã€‚",
        "rag_embed_error_nvidia": "RAG embedding failed: NVIDIA API Key is invalid or not setã€‚",
        "rag_embed_error_none": "RAG embedding failed: All required keys (OpenAI, Gemini, NVIDIA) are invalid or not setã€‚ Please configure a keyã€‚",

        # --- ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ì¶”ê°€ ---
        "phone_header": "AI Customer Support Simulator (Phone)",
        "call_status_waiting": "Waiting for incoming call...",
        "call_status_ringing": "Incoming Call from: {number}",
        "button_answer": "ğŸ“ Answer Call",
        "button_hangup": "ğŸ”´ Hang Up",
        "button_hold": "â¸ï¸ Hold (Mute)",
        "button_resume": "â–¶ï¸ Resume Call",
        "hold_status": "On Hold (Total Hold Time: {duration})",
        "cc_live_transcript": "ğŸ¤ Live CC Transcript",
        "mic_input_status": "ğŸ™ï¸ Agent Voice Input",
        "customer_audio_playback": "ğŸ—£ï¸ Customer Audio Playback",
        "agent_response_prompt": "Record your response to the customerã€‚",
        "agent_response_stop_and_send": "â¹ï¸ Stop and share recording to customers",
        "call_end_message": "Call ended. Check AHT and historyã€‚",
        "call_query_placeholder": "Enter customer's initial queryã€‚",
        "call_number_placeholder": "+1 (555) 123-4567 (Mock Number)",
        "call_summary_header": "AI Call Summary",
        "customer_audio_header": "Customer Initial Query (Voice)",
        "aht_not_recorded": "âš ï¸ Call start time not recordedã€‚ Cannot calculate AHTã€‚",
        "no_audio_record": "No initial customer voice recordã€‚",

    },

    # --- â­ ì¼ë³¸ì–´ ë²„ì „ (í•œêµ­ì–´ 100% ë§¤ì¹­) ---
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«DB)",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
        "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
        "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "content_tab": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "sim_tab_chat_email": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(ãƒãƒ£ãƒƒãƒˆãƒ»ãƒ¡ãƒ¼ãƒ«)",
        "sim_tab_phone": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(é›»è©±)",
        "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
        "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
        "rag_input_placeholder": "è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
        "llm_error_key": "âš ï¸ æ³¨æ„: GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "llm_error_init": "LLM åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«å¿œã˜ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
        "level_label": "é›£æ˜“åº¦",
        "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¨®é¡",
        "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
        "content_options": ["è¦ç‚¹ã‚µãƒãƒªãƒ¼", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã‚¢ã‚¤ãƒ‡ã‚¢"],
        "button_generate": "ç”Ÿæˆã™ã‚‹",
        "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "lstm_header": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "lstm_desc": "ä»®æƒ³ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨ã—ã¦é”æˆåº¦ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚",
        "lang_select": "è¨€èªé¸æŠ",
        "embed_success": "{count}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã§DBæ§‹ç¯‰å®Œäº†!",
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—ï¼šã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œã€‚",
        "warning_no_files": "è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "warning_rag_not_ready": "RAGãŒæº–å‚™ã§ãã¦ã„ã¾ã›ã‚“ã€‚",
        "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "select_answer": "å›ç­”ã‚’é¸æŠã—ã¦ãã ã•ã„",
        "check_answer": "å›ç­”ã‚’ç¢ºèª",
        "next_question": "æ¬¡ã®è³ªå•",
        "correct_answer": "æ­£è§£ï¼ ğŸ‰",
        "incorrect_answer": "ä¸æ­£è§£ ğŸ˜",
        "correct_is": "æ­£è§£",
        "explanation": "è§£èª¬",
        "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
        "score": "ã‚¹ã‚³ã‚¢",
        "retake_quiz": "å†æŒ‘æˆ¦",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨ï¼šJSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "quiz_original_response": "LLM åŸæœ¬å›ç­”",
        "firestore_loading": "RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ä¸­...",
        "firestore_no_index": "ä¿å­˜ã•ã‚ŒãŸRAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
        "db_save_complete": "(DBä¿å­˜å®Œäº†)",
        "data_analysis_progress": "è³‡æ–™åˆ†æä¸­...",
        "response_generating": "å¿œç­”ç”Ÿæˆä¸­...",
        "lstm_result_header": "é”æˆåº¦äºˆæ¸¬çµæœ",
        "lstm_score_metric": "äºˆæ¸¬é”æˆåº¦",
        "lstm_score_info": "æ¬¡ã®ã‚¹ã‚³ã‚¢äºˆæ¸¬: **{predicted_score:.1f}ì **",
        "lstm_rerun_button": "æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§å†äºˆæ¸¬",

        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ ---
        "toast_like": "ğŸ”¥ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ°—ã«å…¥ã£ã¦ã„ãŸã ã‘ã¾ã—ãŸï¼ (+1 ã‚«ã‚¦ãƒ³ãƒˆåæ˜ )",
        "toast_dislike": "ğŸ˜” ã‚ˆã‚Šè‰¯ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãŸã‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚",
        "toast_share": "ğŸŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªãƒ³ã‚¯ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚",
        "toast_copy": "âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã«ã‚³ãƒ”ãƒ¼ã•ã‚Œã¾ã—ãŸï¼",
        "toast_more": "â„¹ï¸ ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå°åˆ·ã€PDFä¿å­˜ãªã©ï¼‰",
        "mock_pdf_save": "ğŸ“¥ PDFã§ä¿å­˜",
        "mock_word_open": "ğŸ“‘ Wordã§é–‹ã",
        "mock_print": "ğŸ–¨ å°åˆ·",
        # --- í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ë ---

        # --- Simulator ---
        "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "simulator_desc": "é›£ã—ã„é¡§å®¢å•ã„åˆã‚ã›ã«å¯¾ã™ã‚‹AIã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨è‰æ¡ˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "customer_query_label": "é¡§å®¢ã‹ã‚‰ã®å•ã„åˆã‚ã›å†…å®¹ (ãƒªãƒ³ã‚¯å¯)",
        "customer_type_label": "é¡§å®¢ã‚¿ã‚¤ãƒ—",
        "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "é›£ã—ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"],
        "button_simulate": "å¿œå¯¾ã‚¬ã‚¤ãƒ‰ç”Ÿæˆ",
        "customer_generate_response_button": "é¡§å®¢ã®è¿”ä¿¡ã‚’ç”Ÿæˆ",
        "send_closing_confirm_button": "è¿½åŠ ã®ã”è³ªå•æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡",
        "simulation_warning_query": "ãŠå•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "simulation_no_key_warning": "âš ï¸ APIã‚­ãƒ¼ä¸è¶³ã®ãŸã‚å¿œå¯¾ç”Ÿæˆä¸å¯ã€‚",
        "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
        "simulation_draft_header": "æ¨å¥¨å¿œå¯¾è‰æ¡ˆ",
        "button_listen_audio": "éŸ³å£°ã§èã",
        "tts_status_ready": "éŸ³å£°ç”Ÿæˆæº–å‚™å®Œäº†",
        "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
        "tts_status_success": "éŸ³å£°æº–å‚™å®Œäº†ï¼",
        "tts_status_error": "TTS ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "history_expander_title": "ğŸ“ éå»ã®å¯¾å¿œå±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ (æœ€æ–°10ä»¶)",
        "initial_query_sample": "ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã®eSIMãŒä½¿ãˆã¾ã›ã‚“â€¦",
        "button_mic_input": "ğŸ™ éŸ³å£°å…¥åŠ›",
        "button_mic_stop": "â¹ï¸ éŒ²éŸ³çµ‚äº†",
        "prompt_customer_end": "è¿½åŠ ã®è³ªå•ãŒãªã„ãŸã‚ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚",
        "prompt_survey": "æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ000ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚ [ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯]",
        "customer_closing_confirm": "ä»–ã®ãŠå•åˆã›ã¯ã”ã–ã„ã¾ã›ã‚“ã§ã—ã‚‡ã†ã‹ã€‚",
        "customer_positive_response": "æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
        "button_email_end_chat": "å¿œå¯¾çµ‚äº†ï¼ˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆï¼‰",
        "error_mandatory_contact": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨é›»è©±ç•ªå·ã®å…¥åŠ›ã¯å¿…é ˆã§ã™ã€‚",
        "customer_attachment_label": "ğŸ“ é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "attachment_info_llm": "[é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {filename}ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦å¯¾å¿œã—ã¦ãã ã•ã„ã€‚]",
        "button_retry_translation": "ç¿»è¨³ã‚’å†è©¦è¡Œ",
        "button_request_hint": "ğŸ’¡ å¿œå¯¾ãƒ’ãƒ³ãƒˆã‚’è¦è«‹ (AHT ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ä¸­)",
        "button_generate_draft": "ğŸ¤– AIå¿œç­”è‰æ¡ˆç”Ÿæˆ",
        "draft_generating": "AIãŒå¿œç­”è‰æ¡ˆã‚’ç”Ÿæˆä¸­ã§ã™...",
        "draft_success": "âœ… AIå¿œç­”è‰æ¡ˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚ä»¥ä¸‹ã§ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚",
        "hint_placeholder": "ãŠå•åˆã›ã®å¿œå¯¾ã«å¯¾ã™ã‚‹ãƒ’ãƒ³ãƒˆï¼š",
        "new_simulation_ready": "æ–°ã—ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã§ãã¾ã™ã€‚",
        "survey_sent_confirm": "ğŸ“¨ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚ã“ã®ãƒãƒ£ãƒƒãƒˆã¯çµ‚äº†ã—ã¾ã—ãŸã€‚",
        "agent_response_header": "âœï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”",
        "agent_response_placeholder": "é¡§å®¢ã¸è¿”ä¿¡å†…å®¹ã‚’å…¥åŠ›â€¦",
        "send_response_button": "è¿”ä¿¡é€ä¿¡",
        "customer_turn_info": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”é€ä¿¡å®Œäº†ã€‚é¡§å®¢ã®åå¿œã‚’è‡ªå‹•ç”Ÿæˆä¸­ã§ã™ã€‚",
        "generating_customer_response": "é¡§å®¢ã®åå¿œã‚’ç”Ÿæˆä¸­...",
        "customer_escalation_start": "ä¸Šç´šã®æ‹…å½“è€…ã¨è©±ã—ãŸã„",
        "request_rebuttal_button": "é¡§å®¢ã®åå¿œã‚’ç”Ÿæˆ",
        "new_simulation_button": "æ–°è¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
        "history_selectbox_label": "å±¥æ­´ã‚’é¸æŠ:",
        "history_load_button": "å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€",
        "delete_history_button": "âŒ å…¨å±¥æ­´å‰Šé™¤",
        "delete_confirm_message": "ã™ã¹ã¦ã®å±¥æ­´ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ",
        "delete_confirm_yes": "ã¯ã„ã€å‰Šé™¤ã—ã¾ã™ã€‚",
        "download_history_word": "ğŸ“¥ å±¥æ­´ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Word)",
        "download_history_pptx": "ğŸ“¥ å±¥æ­´ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PPTX)",
        "download_history_pdf": "ğŸ“¥ å±¥æ­´ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (PDF)",
        "download_current_session": "ğŸ“¥ ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        "delete_confirm_no": "ã„ã„ãˆã€ç¶­æŒã—ã¾ã™ã€‚",
        "delete_success": "å‰Šé™¤å®Œäº†ï¼",
        "deleting_history_progress": "å‰Šé™¤ä¸­...",
        "search_history_label": "å±¥æ­´æ¤œç´¢",
        "date_range_label": "æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼",
        "history_search_button": "ğŸ” æ¤œç´¢",
        "no_history_found": "è©²å½“ã™ã‚‹å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "customer_email_label": "é¡§å®¢ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆå¿…ä¿®ï¼‰",
        "customer_phone_label": "é¡§å®¢é€£çµ¡å…ˆ / é›»è©±ç•ªå·ï¼ˆå¿…ä¿®ï¼‰",
        "transfer_header": "è¨€èªåˆ‡ã‚Šæ›¿ãˆè¦è«‹ï¼ˆä»–ãƒãƒ¼ãƒ ã¸ï¼‰",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ è‹±èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· éŸ“å›½èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_system_msg": "ğŸ“Œ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: é¡§å®¢ã®è¦è«‹ã«ã‚ˆã‚Šã€å¯¾å¿œè¨€èªãŒ {target_lang} ãƒãƒ¼ãƒ ã¸åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã—ãŸã€‚æ–°ã—ã„æ‹…å½“è€…(AI)ãŒå¯¾å¿œã—ã¾ã™ã€‚",
        "transfer_loading": "è»¢é€ä¸­: éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç¿»è¨³ãŠã‚ˆã³ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ã„ã¾ã™ (ãŠå®¢æ§˜ã«ã¯3ã€œ10åˆ†ã®ãŠæ™‚é–“ã‚’ã„ãŸã ã„ã¦ã„ã¾ã™)",
        "transfer_summary_header": "ğŸ” è»¢é€ã•ã‚ŒãŸæ‹…å½“è€…å‘ã‘ã®è¦ç´„ (ç¿»è¨³æ¸ˆã¿)",
        "transfer_summary_intro": "ã“ã‚ŒãŒé¡§å®¢ã¨ã®éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã§ã™ã€‚ã“ã®è¦ç´„ã«åŸºã¥ã„ã¦ã‚µãƒãƒ¼ãƒˆã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚",
        "llm_translation_error": "âŒ ç¿»è¨³å¤±æ•—: LLMå¿œç­”ã‚¨ãƒ©ãƒ¼",
        "timer_metric": "çµŒéæ™‚é–“",
        "timer_info_ok": "AHT (15åˆ†åŸºæº–)",
        "timer_info_warn": "AHT (10åˆ†çµŒé)",
        "timer_info_risk": "ğŸš¨ 15åˆ†çµŒé: é«˜ã„ãƒªã‚¹ã‚¯",
        "solution_check_label": "âœ… ã“ã®å¿œç­”ã«è§£æ±ºç­–/å¯¾å¿œç­–ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚",
        "sentiment_score_label": "é¡§å®¢ã®æ„Ÿæƒ…ã‚¹ã‚³ã‚¢",  # <--- ì¶”ê°€/ìˆ˜ì •
        "urgency_score_label": "ç·Šæ€¥åº¦ã‚¹ã‚³ã‚¢",
        "similarity_chart_title": "é¡ä¼¼æ€§ã‚±ãƒ¼ã‚¹ã®æ¯”ç‡",
        "scores_comparison_title": "æ„Ÿæƒ…åŠã³æº€è¶³åº¦ã®ã‚¹ã‚³ã‚¢",
        "similarity_score_label": "é¡ä¼¼æ€§",
        "satisfaction_score_label": "æº€è¶³åº¦",
        "sentiment_trend_label": "æ„Ÿæƒ…ã®ã‚¹ã‚³ã‚¢ã®æ¨æ¸¬",
        "satisfaction_trend_label": "æº€è¶³åº¦ã®ã‚¹ã‚³ã‚¢ã®æ¨æ¸¬",
        "case_trends_title": "éå»ã«æ¨å®šã•ã‚ŒãŸã‚¹ã‚³ã‚¢",
        "date_label": "æ—¥ä»˜",
        "score_label": "ã‚¹ã‚³ã‚¢ (0-100)",
        "customer_characteristics_title": "é¡§å®¢ã®æ€§æ ¼",
        "language_label": "è¨€èª",
        "email_provided_label": "æä¾›ã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹",
        "phone_provided_label": "æä¾›ã•ã‚ŒãŸé›»è©±ç•ªå·",
        "region_label": "åœ°åŸŸ",
        "btn_request_phone_summary": "å±¥æ­´ã‚’è¦ç´„ã™ã‚‹",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "é›»è©±ç™ºä¿¡",
        "call_outbound_system_msg": "ğŸ“Œ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ{target}ã¸é›»è©±ç™ºä¿¡ã‚’è©¦ã¿ã¾ã—ãŸã€‚",
        "call_outbound_simulation_header": "ğŸ“ é›»è©±ç™ºä¿¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ",
        "call_outbound_summary_header": "ğŸ“ ç¾åœ°æ¥­è€…/é¡§å®¢ã¨ã®é€šè©±è¦ç´„",
        "call_outbound_loading": "é›»è©±æ¥ç¶šã¨é€šè©±çµæœã®æ•´ç†ä¸­... (LLMã‚³ãƒ¼ãƒ«)",
        "call_target_customer": "é¡§å®¢ã¸é›»è©±ç™ºä¿¡",
        "call_target_partner": "ç¾åœ°æ¥­è€…ã¸é›»è©±ç™ºä¿¡",

        # --- Voice ---
        "voice_rec_header": "éŸ³å£°è¨˜éŒ²ï¼†ç®¡ç†",
        "record_help": "éŒ²éŸ³ã™ã‚‹ã‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚",
        "uploaded_file": "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "rec_list_title": "ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ²",
        "transcribe_btn": "è»¢å†™ (Whisper)",
        "save_btn": "éŸ³å£°è¨˜éŒ²ã‚’ä¿å­˜",
        "transcribing": "éŸ³å£°ã‚’è»¢å†™ä¸­...",
        "transcript_result": "è»¢å†™çµæœ:",
        "transcript_text": "è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆ",
        "openai_missing": "OpenAI APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "whisper_client_error": "âŒ ã‚¨ãƒ©ãƒ¼: Whisper APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "whisper_auth_error": "âŒ Whisper APIèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "whisper_format_error": "âŒ ã‚¨ãƒ©ãƒ¼: ã“ã®éŸ³å£°å½¢å¼ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "whisper_success": "âœ… éŸ³å£°è»¢å†™å®Œäº†ï¼",
        "playback": "éŒ²éŸ³å†ç”Ÿ",
        "retranscribe": "å†è»¢å†™",
        "delete": "å‰Šé™¤",
        "no_records": "ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "saved_success": "ä¿å­˜ã—ã¾ã—ãŸï¼",
        "delete_confirm_rec": "ã“ã®éŸ³å£°è¨˜éŒ²ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ",
        "gcs_not_conf": "GCSãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€éŸ³å£°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "gcs_playback_fail": "éŸ³å£°ã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "gcs_no_audio": "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "error": "ã‚¨ãƒ©ãƒ¼:",
        "firestore_no_db_connect": "DBæ¥ç¶šå¤±æ•—",
        "save_history_success": "ä¿å­˜å®Œäº†ã€‚",
        "save_history_fail": "ä¿å­˜å¤±æ•—ã€‚",
        "delete_fail": "å‰Šé™¤å¤±æ•—",
        "rec_header": "éŸ³å£°å…¥åŠ›ï¼†è»¢å†™",
        "whisper_processing": "å‡¦ç†ä¸­...",
        "empty_response_warning": "å¿œç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "customer_no_more_inquiries": "ã„ã„ãˆã€çµæ§‹ã§ã™ã€‚å¤§ä¸ˆå¤«ã§ã™ã€‚æœ‰é›£ã†å¾¡åº§ã„ã¾ã—ãŸã€‚",
        "customer_has_additional_inquiries": "ã¯ã„ã€è¿½åŠ ã®å•ã„åˆã‚ã›ãŒã‚ã‚Šã¾ã™ã€‚",
        "sim_end_chat_button": "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¦å¿œå¯¾çµ‚äº†",
        "delete_mic_record": "éŒ²éŸ³ã‚’å‰Šé™¤ã™ã‚‹",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "é¡§å®¢ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãªã©)",
        "attachment_placeholder": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜ã—ã¦çŠ¶æ³ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
        "attachment_status_llm": "é¡§å®¢ãŒ **{filename}** íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ìŠ¤í¬ë¦°ìƒ·ì´ë¼ê³  ê°€ì •í•˜ê³  ì‘ëŒ€ ì´ˆì•ˆê³¼ ê°€ì´ë“œë¼ì¸ì— ë°˜ì˜í•´ì£¼ì„¸ìš”. (ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {filetype})",
        "agent_attachment_label": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãªã©)",
        "agent_attachment_placeholder": "å¿œç­”ã«æ·»ä»˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
        "agent_attachment_status": "ğŸ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ **{filename}** ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¿œç­”ã«æ·»ä»˜ã—ã¾ã—ãŸã€‚(ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG embedding failed: OpenAI API Key is invalid or not setã€‚",
        "rag_embed_error_gemini": "RAG embedding failed: Gemini API Key is invalid or not setã€‚",
        "rag_embed_error_nvidia": "RAG embedding failed: NVIDIA API Key is invalid or not setã€‚",
        "rag_embed_error_none": "RAG embedding failed: All required keys (OpenAI, Gemini, NVIDIA) are invalid or not setã€‚ Please configure a keyã€‚",

        # --- é›»è©±æ©Ÿèƒ½é–¢é€£è¿½åŠ  ---
        "phone_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(é›»è©±)",
        "call_status_waiting": "ç€ä¿¡å¾…ã¡...",
        "call_status_ringing": "ç€ä¿¡ä¸­: {number}",
        "button_answer": "ğŸ“ é›»è©±ã«å‡ºã‚‹",
        "button_hangup": "ğŸ”´ é›»è©±ã‚’åˆ‡ã‚‹",
        "button_hold": "â¸ï¸ ä¿ç•™ (ãƒã‚¤ã‚ºé®æ–­)",
        "button_resume": "â–¶ï¸ é€šè©±å†é–‹",
        "hold_status": "ä¿ç•™ä¸­ (ç´¯è¨ˆä¿ç•™æ™‚é–“: {duration})",
        "cc_live_transcript": "ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ CCå­—å¹• / è»¢å†™",
        "mic_input_status": "ğŸ™ï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®éŸ³å£°å…¥åŠ›",
        "customer_audio_playback": "ğŸ—£ï¸ é¡§å®¢ã®éŸ³å£°å†ç”Ÿ",
        "agent_response_prompt": "é¡§å®¢ã¸ã®å¿œç­”ã‚’éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚",
        "agent_response_stop_and_send": "â¹ï¸éŒ²éŸ³ã‚’çµ‚äº†ã—ã¦ã€é¡§å®¢ã¸è»¢é€ã™ã‚‹",
        "call_end_message": "é€šè©±ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚AHTã¨å±¥æ­´ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "call_query_placeholder": "é¡§å®¢ã‹ã‚‰ã®æœ€åˆã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "call_number_placeholder": "+81 90-xxxx-xxxx (ä»®æƒ³ç•ªå·)",
        "call_summary_header": "AI é€šè©±è¦ç´„",
        "customer_audio_header": "é¡§å®¢ã®æœ€åˆã®å•ã„åˆã‚ã› (éŸ³å£°)",
        "aht_not_recorded": "âš ï¸ é€šè©±é–‹å§‹æ™‚é–“ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€AHTã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã€‚",
        "no_audio_record": "é¡§å®¢ã®æœ€åˆã®éŸ³å£°è¨˜éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
    }
}

# ========================================
# 1-1. Session State ì´ˆê¸°í™” (ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì¶”ê°€)
# ========================================
# â­ ì‚¬ì´ë“œë°” ë²„íŠ¼ì€ ì‚¬ì´ë“œë°” ë¸”ë¡ ì•ˆìœ¼ë¡œ ì´ë™í•´ì•¼ í•¨
# ì—¬ê¸°ì„œëŠ” ì„¸ì…˜ ìƒíƒœë§Œ ì´ˆê¸°í™”

if "language" not in st.session_state:
    st.session_state.language = DEFAULT_LANG
if "is_llm_ready" not in st.session_state:
    st.session_state.is_llm_ready = False
if "llm_init_error_msg" not in st.session_state:
    st.session_state.llm_init_error_msg = ""
if "uploaded_files_state" not in st.session_state:
    st.session_state.uploaded_files_state = None
if "is_rag_ready" not in st.session_state:
    st.session_state.is_rag_ready = False
if "rag_vectorstore" not in st.session_state:
    st.session_state.rag_vectorstore = None
if "rag_messages" not in st.session_state:
    st.session_state.rag_messages = []
if "agent_input" not in st.session_state:
    st.session_state.agent_input = ""
if "last_audio" not in st.session_state:
    st.session_state.last_audio = None
if "simulator_messages" not in st.session_state:
    st.session_state.simulator_messages = []
if "simulator_memory" not in st.session_state:
    st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history")
if "simulator_chain" not in st.session_state:
    st.session_state.simulator_chain = None
if "initial_advice_provided" not in st.session_state:
    st.session_state.initial_advice_provided = False
if "is_chat_ended" not in st.session_state:
    st.session_state.is_chat_ended = False
if "show_delete_confirm" not in st.session_state:
    st.session_state.show_delete_confirm = False
if "customer_query_text_area" not in st.session_state:
    st.session_state.customer_query_text_area = ""
if "agent_response_area_text" not in st.session_state:
    st.session_state.agent_response_area_text = ""
if "reset_agent_response_area" not in st.session_state:
    st.session_state.reset_agent_response_area = False
if "last_transcript" not in st.session_state:
    st.session_state.last_transcript = ""
if "sim_audio_bytes" not in st.session_state:
    st.session_state.sim_audio_bytes = None
if "chat_state" not in st.session_state:
    st.session_state.chat_state = "idle"
    # idle â†’ initial_customer â†’ supervisor_advice â†’ agent_turn â†’ customer_turn â†’ closing
if "openai_client" not in st.session_state:
    st.session_state.openai_client = None
if "openai_init_msg" not in st.session_state:
    st.session_state.openai_init_msg = ""
if "sim_stage" not in st.session_state:
    st.session_state.sim_stage = "WAIT_FIRST_QUERY"
    # WAIT_FIRST_QUERY (ì´ˆê¸° ë¬¸ì˜ ì…ë ¥)
    # AGENT_TURN (ì—ì´ì „íŠ¸ ì‘ë‹µ ì…ë ¥)
    # CUSTOMER_TURN (ê³ ê° ë°˜ì‘ ìƒì„± ìš”ì²­)
    # WAIT_CLOSING_CONFIRMATION_FROM_AGENT (ê³ ê°ì´ ê°ì‚¬, ì—ì´ì „íŠ¸ê°€ ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸° ëŒ€ê¸°)
    # WAIT_CUSTOMER_CLOSING_RESPONSE (ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ë³´ëƒ„, ê³ ê°ì˜ ë§ˆì§€ë§‰ ì‘ë‹µ ëŒ€ê¸°)
    # FINAL_CLOSING_ACTION (ìµœì¢… ì¢…ë£Œ ë²„íŠ¼ ëŒ€ê¸°)
    # CLOSING (ì±„íŒ… ì¢…ë£Œ)
    # â­ ì¶”ê°€: OUTBOUND_CALL_IN_PROGRESS (ì „í™” ë°œì‹  ì§„í–‰ ì¤‘)
if "start_time" not in st.session_state:  # AHT íƒ€ì´ë¨¸ ì‹œì‘ ì‹œê°„
    st.session_state.start_time = None
if "is_solution_provided" not in st.session_state:  # ì†”ë£¨ì…˜ ì œê³µ ì—¬ë¶€ í”Œë˜ê·¸
    st.session_state.is_solution_provided = False
if "transfer_summary_text" not in st.session_state:  # ì´ê´€ ì‹œ ë²ˆì—­ëœ ìš”ì•½
    st.session_state.transfer_summary_text = ""
if "language_transfer_requested" not in st.session_state:  # ê³ ê°ì˜ ì–¸ì–´ ì´ê´€ ìš”ì²­ ì—¬ë¶€
    st.session_state.language_transfer_requested = False
if "customer_attachment_file" not in st.session_state:  # ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´
    st.session_state.customer_attachment_file = None
if "language_at_transfer" not in st.session_state:  # í˜„ì¬ ì–¸ì–´ì™€ ë¹„êµë¥¼ ìœ„í•œ ë³€ìˆ˜
    st.session_state.language_at_transfer = st.session_state.language
if "language_at_transfer_start" not in st.session_state:  # ë²ˆì—­ ì¬ì‹œë„ë¥¼ ìœ„í•œ ì›ë³¸ ì–¸ì–´
    st.session_state.language_at_transfer_start = st.session_state.language
if "transfer_retry_count" not in st.session_state:
    st.session_state.transfer_retry_count = 0
if "customer_type_sim_select" not in st.session_state:  # FIX: Attribute Error í•´ê²°
    # LANGì´ ì •ì˜ë˜ê¸° ì „ì´ë¯€ë¡œ ê¸°ë³¸ê°’ì„ ì§ì ‘ ì„¤ì •
    default_customer_type = "ê¹Œë‹¤ë¡œìš´ ê³ ê°"  # í•œêµ­ì–´ ê¸°ë³¸ê°’
    if st.session_state.language == "en":
        default_customer_type = "Difficult Customer"
    elif st.session_state.language == "ja":
        default_customer_type = "é›£ã—ã„é¡§å®¢"
    st.session_state.customer_type_sim_select = default_customer_type
if "customer_email" not in st.session_state:  # FIX: customer_email ì´ˆê¸°í™”
    st.session_state.customer_email = ""
if "customer_phone" not in st.session_state:  # FIX: customer_phone ì´ˆê¸°í™”
    st.session_state.customer_phone = ""
if "agent_response_input_box_widget" not in st.session_state:  # FIX: customer_phone ì´ˆê¸°í™”
    st.session_state.agent_response_input_box_widget = ""
if "sim_instance_id" not in st.session_state:  # FIX: DuplicateWidgetID ë°©ì§€ìš© ì¸ìŠ¤í„´ìŠ¤ ID ì´ˆê¸°í™”
    st.session_state.sim_instance_id = str(uuid.uuid4())
if "sim_attachment_context_for_llm" not in st.session_state:
    st.session_state.sim_attachment_context_for_llm = ""
if "realtime_hint_text" not in st.session_state:
    st.session_state.realtime_hint_text = ""
# â­ ì¶”ê°€: ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ
if "sim_call_outbound_summary" not in st.session_state:
    st.session_state.sim_call_outbound_summary = ""
if "sim_call_outbound_target" not in st.session_state:
    st.session_state.sim_call_outbound_target = None
# ----------------------------------------------------------------------
# â­ ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ìƒíƒœ ì¶”ê°€
if "call_sim_stage" not in st.session_state:
    st.session_state.call_sim_stage = "WAITING_CALL"  # WAITING_CALL, RINGING, IN_CALL, CALL_ENDED
if "call_sim_mode" not in st.session_state:
    st.session_state.call_sim_mode = "INBOUND"  # INBOUND or OUTBOUND
if "incoming_phone_number" not in st.session_state:
    st.session_state.incoming_phone_number = "+82 10-1234-5678"
if "is_on_hold" not in st.session_state:
    st.session_state.is_on_hold = False
if "hold_start_time" not in st.session_state:
    st.session_state.hold_start_time = None
if "total_hold_duration" not in st.session_state:
    st.session_state.total_hold_duration = timedelta(0)
if "current_customer_audio_text" not in st.session_state:
    st.session_state.current_customer_audio_text = ""
if "current_agent_audio_text" not in st.session_state:
    st.session_state.current_agent_audio_text = ""
if "agent_response_input_box_widget_call" not in st.session_state:  # ì „í™” íƒ­ ì „ìš© ì…ë ¥ì°½
    st.session_state.agent_response_input_box_widget_call = ""
if "call_initial_query" not in st.session_state:  # ì „í™” íƒ­ ì „ìš© ì´ˆê¸° ë¬¸ì˜
    st.session_state.call_initial_query = ""
# â­ ì¶”ê°€: í†µí™” ìš”ì•½ ë° ì´ˆê¸° ê³ ê° ìŒì„± ì €ì¥ì†Œ
if "call_summary_text" not in st.session_state:
    st.session_state.call_summary_text = ""
if "customer_initial_audio_bytes" not in st.session_state:  # ê³ ê°ì˜ ì²« ìŒì„± (TTS ê²°ê³¼) ì €ì¥
    st.session_state.customer_initial_audio_bytes = None
if "supervisor_policy_context" not in st.session_state:
    # Supervisorê°€ ì—…ë¡œë“œí•œ ì˜ˆì™¸ ì •ì±… í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    st.session_state.supervisor_policy_context = ""
if "agent_policy_attachment_content" not in st.session_state:
    # ì—ì´ì „íŠ¸ê°€ ì—…ë¡œë“œí•œ ì •ì±… íŒŒì¼ ê°ì²´(ë˜ëŠ” ë‚´ìš©)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    st.session_state.agent_policy_attachment_content = ""
if "customer_attachment_b64" not in st.session_state:
    st.session_state.customer_attachment_b64 = ""
if "customer_history_summary" not in st.session_state:
    st.session_state.customer_history_summary = ""
if "customer_avatar" not in st.session_state:
    st.session_state.customer_avatar = {
        "gender": "male",  # ê¸°ë³¸ê°’
        "state": "NEUTRAL",  # ê¸°ë³¸ ì•„ë°”íƒ€ ìƒíƒœ
    }
# â­ ì¶”ê°€: ì „ì‚¬í•  ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ì„ì‹œ ì €ì¥ì†Œ
if "bytes_to_process" not in st.session_state:
    st.session_state.bytes_to_process = None

L = LANG[st.session_state.language]

# â­ 2-A. Gemini í‚¤ ì´ˆê¸°í™” (ì˜ëª»ëœ í‚¤ ì”ì¡´ ë°©ì§€)
if "user_gemini_key" in st.session_state and st.session_state["user_gemini_key"].startswith("AIza"):
    pass

# ========================================
# 0. ë©€í‹° ëª¨ë¸ API Key ì•ˆì „ êµ¬ì¡° (Secrets + Env Varë§Œ ì‚¬ìš©)
# ========================================

# 1) ì§€ì›í•˜ëŠ” API ëª©ë¡ ì •ì˜
SUPPORTED_APIS = {
    "openai": {
        "label": "OpenAI API Key",
        "secret_key": "OPENAI_API_KEY",
        "session_key": "user_openai_key",
        "placeholder": "sk-proj-**************************",
    },
    "gemini": {
        "label": "Google Gemini API Key",
        "secret_key": "GEMINI_API_KEY",
        "session_key": "user_gemini_key",
        "placeholder": "AIza***********************************",
    },
    "nvidia": {
        "label": "NVIDIA NIM API Key",
        "secret_key": "NVIDIA_API_KEY",
        "session_key": "user_nvidia_key",
        "placeholder": "nvapi-**************************",
    },
    "claude": {
        "label": "Anthropic Claude API Key",
        "secret_key": "CLAUDE_API_KEY",
        "session_key": "user_claude_key",
        "placeholder": "sk-ant-api-**************************",
    },
    "groq": {
        "label": "Groq API Key",
        "secret_key": "GROQ_API_KEY",
        "session_key": "user_groq_key",
        "placeholder": "gsk_**************************",
    },
}

# 2) ì„¸ì…˜ ì´ˆê¸°í™”
for api, cfg in SUPPORTED_APIS.items():
    if cfg["session_key"] not in st.session_state:
        st.session_state[cfg["session_key"]] = ""

if "selected_llm" not in st.session_state:
    st.session_state.selected_llm = "openai_gpt4"


def get_api_key(api):
    cfg = SUPPORTED_APIS[api]

    # â­ 1. Streamlit Secrets (.streamlit/secrets.toml) - ìµœìš°ì„ 
    try:
        if hasattr(st, "secrets") and cfg["secret_key"] in st.secrets:
            return st.secrets[cfg["secret_key"]]
    except Exception:
        pass

    # 2. Environment Variable (os.environ)
    env_key = os.environ.get(cfg["secret_key"])
    if env_key:
        return env_key

    # 3. User Input (Session State - ì œê±°ë¨)
    user_key = st.session_state.get(cfg["session_key"], "")
    if user_key:
        return user_key

    return ""


# ========================================
# 1. Sidebar UI: API Key ì…ë ¥ ì œê±°
# ========================================
# API Key ì…ë ¥ UIëŠ” ì œê±°í•˜ê³ , í™˜ê²½ë³€ìˆ˜ì™€ Streamlit Secretsë§Œ ì‚¬ìš©í•˜ë„ë¡ í•¨.


# ========================================
# 2. LLM í´ë¼ì´ì–¸íŠ¸ ë¼ìš°íŒ… & ì‹¤í–‰
# ========================================
def get_llm_client():
    """ì„ íƒëœ ëª¨ë¸ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ + ëª¨ë¸ì½”ë“œ ë°˜í™˜"""
    model_key = st.session_state.get("selected_llm", "openai_gpt4")

    # --- OpenAI ---
    if model_key.startswith("openai"):
        key = get_api_key("openai")
        if not key: return None, None
        try:
            client = OpenAI(api_key=key)
            model_name = "gpt-4o" if model_key == "openai_gpt4" else "gpt-3.5-turbo"
            return client, ("openai", model_name)
        except Exception:
            return None, None

    # --- Gemini ---
    if model_key.startswith("gemini"):
        key = get_api_key("gemini")
        if not key: return None, None
        try:
            genai.configure(api_key=key)
            model_name = "gemini-2.5-pro" if model_key == "gemini_pro" else "gemini-2.5-flash"
            return genai, ("gemini", model_name)
        except Exception:
            return None, None

    # --- Claude ---
    if model_key.startswith("claude"):
        key = get_api_key("claude")
        if not key: return None, None
        try:
            client = Anthropic(api_key=key)
            model_name = "claude-3-5-sonnet-latest"
            return client, ("claude", model_name)
        except Exception:
            return None, None

    # --- Groq ---
    if model_key.startswith("groq"):
        from groq import Groq
        key = get_api_key("groq")
        if not key: return None, None
        try:
            client = Groq(api_key=key)
            model_name = (
                "llama3-70b-8192"
                if "llama3" in model_key
                else "mixtral-8x7b-32768"
            )
            return client, ("groq", model_name)
        except Exception:
            return None, None

    return None, None


def run_llm(prompt: str) -> str:
    """ì„ íƒëœ LLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (Gemini ìš°ì„ ìˆœìœ„ ë³€ê²½ ì ìš©)"""
    client, info = get_llm_client()

    # Note: infoëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒëœ ì£¼ë ¥ ëª¨ë¸ì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    provider, model_name = info if info else (None, None)

    # Fallback ìˆœì„œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. (Gemini ìš°ì„ )
    llm_attempts = []

    # 1. Geminië¥¼ ìµœìš°ì„  Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    gemini_key = get_api_key("gemini")
    if gemini_key:
        llm_attempts.append(("gemini", gemini_key, "gemini-2.5-pro" if "pro" in model_name else "gemini-2.5-flash"))

    # 2. OpenAIë¥¼ 2ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    openai_key = get_api_key("openai")
    if openai_key:
        llm_attempts.append(("openai", openai_key, "gpt-4o" if "4" in model_name else "gpt-3.5-turbo"))

    # 3. Claudeë¥¼ 3ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    claude_key = get_api_key("claude")
    if claude_key:
        llm_attempts.append(("claude", claude_key, "claude-3-5-sonnet-latest"))

    # 4. Groqë¥¼ 4ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    groq_key = get_api_key("groq")
    if groq_key:
        groq_model = "llama3-70b-8192" if "llama3" in model_name else "mixtral-8x7b-32768"
        llm_attempts.append(("groq", groq_key, groq_model))

    # â­ ìˆœì„œ ì¡°ì •: ì£¼ë ¥ ëª¨ë¸(ì‚¬ìš©ìê°€ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•œ ëª¨ë¸)ì„ ê°€ì¥ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.
    # ë§Œì•½ ì£¼ë ¥ ëª¨ë¸ì´ Fallback ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, ê·¸ ëª¨ë¸ì„ ì²« ìˆœì„œë¡œ ì˜¬ë¦½ë‹ˆë‹¤.
    if provider and provider in [attempt[0] for attempt in llm_attempts]:
        # ì£¼ë ¥ ëª¨ë¸ì„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ì•„ ì œê±°
        primary_attempt = next((attempt for attempt in llm_attempts if attempt[0] == provider), None)
        if primary_attempt:
            llm_attempts.remove(primary_attempt)
            # ì£¼ë ¥ ëª¨ë¸ì´ Geminië‚˜ OpenAIê°€ ì•„ë‹ˆë¼ë©´, Fallback ìˆœì„œì™€ ê´€ê³„ì—†ì´ ê°€ì¥ ë¨¼ì € ì‹œë„í•˜ë„ë¡ ì‚½ì…
            llm_attempts.insert(0, primary_attempt)

    # LLM ìˆœì°¨ ì‹¤í–‰
    for provider, key, model in llm_attempts:
        if not key: continue

        try:
            if provider == "gemini":
                genai.configure(api_key=key)
                gen_model = genai.GenerativeModel(model)
                resp = gen_model.generate_content(prompt)
                return resp.text

            elif provider == "openai":
                o_client = OpenAI(api_key=key)
                resp = o_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.choices[0].message.content

            elif provider == "claude":
                c_client = Anthropic(api_key=key)
                resp = c_client.messages.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.content[0].text

            elif provider == "groq":
                from groq import Groq
                g_client = Groq(api_key=key)
                resp = g_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.choices[0].message.content

        except Exception as e:
            # í•´ë‹¹ APIê°€ ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ APIë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
            print(f"LLM {provider} ({model}) failed: {e}")
            continue

    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ
    return "âŒ ëª¨ë“  LLM API í‚¤ê°€ ì‘ë™í•˜ì§€ ì•Šê±°ë‚˜ í• ë‹¹ëŸ‰ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤."


# ========================================
# 2-A. Whisper / TTS ìš© OpenAI Client ë³„ë„ë¡œ ì´ˆê¸°í™”
# ========================================

def init_openai_audio_client():
    key = get_api_key("openai")
    if not key:
        return None
    try:
        return OpenAI(api_key=key)
    except:
        return None


# â­ ìµœì í™”: LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìºì‹± (ë§¤ë²ˆ ì¬ìƒì„±í•˜ì§€ ì•Šë„ë¡)
# OpenAI í´ë¼ì´ì–¸íŠ¸ ìºì‹±
# â­ ìˆ˜ì •: ì´ˆê¸°í™” ì‹œ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ try-except ì¶”ê°€
if "openai_client" not in st.session_state or st.session_state.openai_client is None:
    try:
        st.session_state.openai_client = init_openai_audio_client()
    except Exception as e:
        st.session_state.openai_client = None
        print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

# LLM ì¤€ë¹„ ìƒíƒœ ìºì‹± (API í‚¤ ë³€ê²½ ì‹œì—ë§Œ ì¬í™•ì¸)
# â­ ìˆ˜ì •: ì´ˆê¸°í™” ì‹œ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ try-except ì¶”ê°€
if "is_llm_ready" not in st.session_state or "llm_ready_checked" not in st.session_state:
    try:
        probe_client, _ = get_llm_client()
        st.session_state.is_llm_ready = probe_client is not None
    except Exception as e:
        # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œì—ë„ ì•±ì´ ê³„ì† ì‹¤í–‰ë˜ë„ë¡ Falseë¡œ ì„¤ì •
        st.session_state.is_llm_ready = False
        print(f"LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    st.session_state.llm_ready_checked = True

# API í‚¤ ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ì²´í¬
current_api_keys_hash = hashlib.md5(
    f"{get_api_key('openai')}{get_api_key('gemini')}{get_api_key('claude')}{get_api_key('groq')}".encode()
).hexdigest()

if "api_keys_hash" not in st.session_state:
    st.session_state.api_keys_hash = current_api_keys_hash
elif st.session_state.api_keys_hash != current_api_keys_hash:
    # API í‚¤ê°€ ë³€ê²½ëœ ê²½ìš°ë§Œ ì¬í™•ì¸
    # â­ ìˆ˜ì •: ì´ˆê¸°í™” ì‹œ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ try-except ì¶”ê°€
    try:
        probe_client, _ = get_llm_client()
        st.session_state.is_llm_ready = probe_client is not None
    except Exception as e:
        st.session_state.is_llm_ready = False
        print(f"LLM ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
    st.session_state.api_keys_hash = current_api_keys_hash
    # OpenAI í´ë¼ì´ì–¸íŠ¸ë„ ì¬ì´ˆê¸°í™”
    try:
        st.session_state.openai_client = init_openai_audio_client()
    except Exception as e:
        st.session_state.openai_client = None
        print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

if st.session_state.openai_client:
    # í‚¤ë¥¼ ì°¾ì•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ëŠ” ìƒì„±ë˜ì—ˆìœ¼ë‚˜, ì‹¤ì œ ì¸ì¦ì€ API í˜¸ì¶œ ì‹œ ì´ë£¨ì–´ì§ (401 ì˜¤ë¥˜ëŠ” ì—¬ê¸°ì„œ ë°œìƒ)
    st.session_state.openai_init_msg = "âœ… OpenAI TTS/Whisper í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ (Key í™•ì¸ë¨)"
else:
    # í‚¤ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
    st.session_state.openai_init_msg = L["openai_missing"]

if not st.session_state.is_llm_ready:
    st.session_state.llm_init_error_msg = L["simulation_no_key_warning"]
else:
    st.session_state.llm_init_error_msg = ""


# ----------------------------------------
# LLM ë²ˆì—­ í•¨ìˆ˜ (Gemini í´ë¼ì´ì–¸íŠ¸ ì˜ì¡´ì„± ì œê±° ë° ê°•í™”)
# ----------------------------------------
def translate_text_with_llm(text_content: str, target_lang_code: str, source_lang_code: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. (ì•ˆì •í™”ëœ í…ìŠ¤íŠ¸ ì¶œë ¥)
    **ìˆ˜ì • ì‚¬í•­:** LLM Fallback ìˆœì„œë¥¼ OpenAI ìš°ì„ ìœ¼ë¡œ ì¡°ì •í•˜ê³ , ì‘ë‹µì´ ë¹„ì–´ìˆì„ ê²½ìš° ëª…ì‹œì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½
    """
    target_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(target_lang_code, "English")
    source_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(source_lang_code, "English")

    # ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì œ
    system_prompt = (
        f"You are a professional translation AI. Translate the entire following customer support chat history "
        f"from '{source_lang_name}' to '{target_lang_name}'. "
        f"You MUST translate the content to {target_lang_name} ONLY. "
        f"Do not include any mixed languages, the source text, or any introductory/concluding remarks. "
        f"Output ONLY the translated chat history text. "
    )
    prompt = f"Original Chat History:\n\n{text_content}"

    # LLM Fallback ìˆœì„œ: OpenAI -> Gemini -> Claude (OpenAIë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì¡°ì •)
    llm_attempts = [
        ("openai", get_api_key("openai"), "gpt-4o"),  # 1ìˆœìœ„: OpenAI (ê°€ì¥ ì•ˆì •ì )
        ("gemini", get_api_key("gemini"), "gemini-2.5-flash"),  # 2ìˆœìœ„
        ("claude", get_api_key("claude"), "claude-3-5-sonnet-latest"),  # 3ìˆœìœ„
    ]

    last_error = ""

    for provider, key, model_name in llm_attempts:
        if not key: continue

        try:
            translated_text = ""

            if provider == "openai":
                o_client = OpenAI(api_key=key)
                resp = o_client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}],
                    temperature=0.1
                )
                translated_text = resp.choices[0].message.content.strip()

            elif provider == "gemini":
                genai.configure(api_key=key)
                g_model = genai.GenerativeModel(model_name)
                resp = g_model.generate_content(
                    contents=prompt,
                    config=genai.types.GenerateContentConfig(system_instruction=system_prompt, temperature=0.1)
                )
                translated_text = resp.text.strip()

            elif provider == "claude":
                from anthropic import Anthropic
                c_client = Anthropic(api_key=key)
                resp = c_client.messages.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt}],
                    system=system_prompt
                )
                translated_text = resp.content[0].text.strip()

            # ë²ˆì—­ ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸
            if translated_text:
                return translated_text
            else:
                last_error = f"Translation failed: {provider} returned empty response."
                continue  # ë‹¤ìŒ LLM ì‹œë„

        except Exception as e:
            last_error = f"Translation API call failed with {provider} ({model_name}): {e}"  # ëª¨ë¸ëª… ì¶”ê°€
            print(last_error)
            continue  # ë‹¤ìŒ LLM ì‹œë„

    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ, ìƒì„¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
    return f"âŒ Translation Error: All LLM attempts failed. Last error: {last_error or 'No active API key found.'}"


# ----------------------------------------
# Realtime Hint Generation (ìš”ì²­ 2 ë°˜ì˜)
# ----------------------------------------
def generate_realtime_hint(current_lang_key: str, is_call: bool = False):
    """í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ì—ê²Œ ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸(í‚¤ì›Œë“œ/ì •ì±…/ì•¡ì…˜)ë¥¼ ì œê³µ"""
    L = LANG[current_lang_key]
    # ì±„íŒ…/ì „í™” êµ¬ë¶„í•˜ì—¬ ì´ë ¥ ì‚¬ìš©
    if is_call:
        # ì „í™” ì‹œë®¬ë ˆì´í„°ì—ì„œëŠ” í˜„ì¬ CC ì˜ì—­ì— í‘œì‹œëœ í…ìŠ¤íŠ¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ í•¨ê»˜ ì‚¬ìš©
        history_text = (
            f"Initial Query: {st.session_state.call_initial_query}\n"
            f"Previous Customer Utterance: {st.session_state.current_customer_audio_text}\n"
            f"Previous Agent Utterance: {st.session_state.current_agent_audio_text}"
        )
    else:
        history_text = get_chat_history_for_prompt(include_attachment=True)

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    hint_prompt = f"""
You are an AI Supervisor providing an **urgent, internal hint** to a human agent whose AHT is being monitored.
Analyze the conversation history, especially the customer's last message, which might be about complex issues like JR Pass, Universal Studio Japan (USJ), or a complex refund policy.

Provide ONE concise, actionable hint for the agent. The purpose is to save AHT time.

Output MUST be a single paragraph/sentence in {lang_name} containing actionable advice.
DO NOT use markdown headers or titles.
Do NOT direct the agent to check the general website.
Provide an actionable fact or the next specific step (e.g., check policy section, confirm coverage).

Examples of good hints (based on the content):
- Check the official JR Pass site for current exchange rates.
- The 'Universal Express Pass' is non-refundable; clearly cite policy section 3.2.
- Ask for the order confirmation number before proceeding with any action.
- The solution lies in the section of the Klook site titled '~'.

Conversation History:
{history_text}

HINT:
"""
    if not st.session_state.is_llm_ready:
        return "(Mock Hint: LLM Key is missing. Ask the customer for the booking number.)"

    with st.spinner(f"ğŸ’¡ {L['button_request_hint']}..."):
        try:
            return run_llm(hint_prompt).strip()
        except Exception as e:
            return f"âŒ Hint Generation Error. (Try again or check API Key: {e})"


def generate_agent_response_draft(current_lang_key: str) -> str:
    """ê³ ê° ì‘ë‹µì„ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ì—ì´ì „íŠ¸ ì‘ë‹µ ì´ˆì•ˆì„ ìƒì„± (ìš”ì²­ 1 ë°˜ì˜)"""
    L = LANG[current_lang_key]
    history_text = get_chat_history_for_prompt(include_attachment=True)
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ê³ ê°ì˜ ìµœì‹  ë¬¸ì˜ ë‚´ìš© ì¶”ì¶œ ë° ë¶„ì„ (ê°•í™”)
    latest_customer_message = ""
    initial_customer_query = st.session_state.get('customer_query_text_area', '')
    customer_query_analysis = ""
    
    # ëª¨ë“  ê³ ê° ë©”ì‹œì§€ ìˆ˜ì§‘
    all_customer_messages = []
    if st.session_state.simulator_messages:
        all_customer_messages = [msg["content"] for msg in st.session_state.simulator_messages 
                                if msg.get("role") in ["customer", "customer_rebuttal", "initial_query"]]
    
    # ì´ˆê¸° ë¬¸ì˜ë„ í¬í•¨
    if initial_customer_query and initial_customer_query not in all_customer_messages:
        all_customer_messages.insert(0, initial_customer_query)
    
    if all_customer_messages:
        latest_customer_message = all_customer_messages[-1]
        
        # ë¬¸ì˜ ë‚´ìš©ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        inquiry_keywords = []
        inquiry_text = " ".join(all_customer_messages).lower()
        
        # ì¼ë°˜ì ì¸ ë¬¸ì˜ í‚¤ì›Œë“œ íŒ¨í„´
        important_patterns = [
            r'\b\d{4,}\b',  # ì£¼ë¬¸ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ ë“± ìˆ«ì
            r'\b(ì£¼ë¬¸|order|æ³¨æ–‡)\b',
            r'\b(í™˜ë¶ˆ|refund|è¿”é‡‘)\b',
            r'\b(ì·¨ì†Œ|cancel|ã‚­ãƒ£ãƒ³ã‚»ãƒ«)\b',
            r'\b(ë°°ì†¡|delivery|é…é€)\b',
            r'\b(ë³€ê²½|change|å¤‰æ›´)\b',
            r'\b(ë¬¸ì œ|problem|issue|å•é¡Œ)\b',
            r'\b(ë„ì›€|help|åŠ©ã‘)\b',
        ]
        
        # í•µì‹¬ ë¬¸ì˜ ë‚´ìš© ìš”ì•½
        inquiry_summary = f"""
**CUSTOMER INQUIRY DETAILS:**

Initial Query: "{initial_customer_query if initial_customer_query else 'Not provided'}"

Latest Customer Message: "{latest_customer_message}"

All Customer Messages Context:
{chr(10).join([f"- {msg[:150]}..." if len(msg) > 150 else f"- {msg}" for msg in all_customer_messages[-3:]])}

**YOUR RESPONSE MUST DIRECTLY ADDRESS:**

1. **SPECIFIC ISSUE IDENTIFICATION**: 
   - What EXACT problem or question did the customer mention?
   - Extract and reference specific details: order numbers, dates, product names, locations, error messages, etc.
   - If multiple issues were mentioned, address EACH one specifically

2. **CONCRETE SOLUTION PROVIDED**:
   - Provide STEP-BY-STEP instructions tailored to their EXACT situation
   - Include specific actions they need to take (e.g., "Go to Settings > Account > Order History and click on order #12345")
   - Reference the exact products/services they mentioned
   - If they mentioned a location, reference it in your solution

3. **PERSONALIZATION**:
   - Use the customer's specific words/phrases when appropriate
   - Reference their exact situation (e.g., "Since you mentioned your eSIM isn't activating in Paris...")
   - Acknowledge their specific concern or frustration point

4. **COMPLETENESS**:
   - Answer ALL questions they asked
   - Address ALL problems they mentioned
   - If they asked "why", explain the specific reason for their situation
   - If they asked "how", provide detailed steps for their exact case

**CRITICAL REQUIREMENTS:**
- DO NOT use generic templates like "Thank you for contacting us" without addressing their specific issue
- DO NOT give vague answers like "Please check your settings" - be SPECIFIC about which settings and what to do
- DO NOT ignore specific details they mentioned (order numbers, dates, locations, etc.)
- Your response must read as if it was written SPECIFICALLY for this customer's exact inquiry
- If the customer mentioned "eSIM activation in Paris", your response MUST specifically address eSIM activation and Paris
- If the customer mentioned an order number, your response MUST reference that order number

**EXAMPLE OF GOOD RESPONSE:**
Bad: "Thank you for contacting us. We understand your concern and will help you resolve this issue."
Good: "I understand you're having trouble activating your eSIM in Paris. Let me help you resolve this step by step. First, please check if your phone's APN settings are configured correctly for the Paris network..."

**NOW GENERATE YOUR RESPONSE** following these requirements:
"""
        
        customer_query_analysis = inquiry_summary

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"\n[ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´: {attachment_context}]\n"
    else:
        attachment_context = ""

    # ê³ ê° ìœ í˜• ë° ë°˜ë³µ ë¶ˆë§Œ íŒ¨í„´ ë¶„ì„
    customer_type = st.session_state.get('customer_type_sim_select', 'ì¼ë°˜ì ì¸ ë¬¸ì˜')
    is_difficult_customer = customer_type in ["ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°", "Difficult Customer",
                                              "Highly Dissatisfied Customer", "é›£ã—ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"]

    # ê³ ê° ë©”ì‹œì§€ ìˆ˜ ë° ê°ì • ë¶„ì„
    customer_message_count = sum(
        1 for msg in st.session_state.simulator_messages if msg.get("role") in ["customer", "customer_rebuttal"])
    agent_message_count = sum(1 for msg in st.session_state.simulator_messages if msg.get("role") == "agent_response")

    # ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µë“¤ ì¶”ì¶œ (ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•´)
    previous_agent_responses = [msg["content"] for msg in st.session_state.simulator_messages 
                                if msg.get("role") == "agent_response"]
    previous_responses_context = ""
    if previous_agent_responses:
        previous_responses_context = f"\n[ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µë“¤ (ì°¸ê³ ìš©, ë™ì¼í•˜ê²Œ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ):\n"
        for i, prev_resp in enumerate(previous_agent_responses[-3:], 1):  # ìµœê·¼ 3ê°œë§Œ
            prev_resp_preview = prev_resp[:200] + "..." if len(prev_resp) > 200 else prev_resp
            previous_responses_context += f"{i}. {prev_resp_preview}\n"
        previous_responses_context += "]\n"

    # ê³ ê°ì´ ê³„ì† ë”°ì§€ê±°ë‚˜ í™”ë‚´ëŠ” íŒ¨í„´ ê°ì§€ (ê³ ê° ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ë³´ë‹¤ ë§ê±°ë‚˜, ë°˜ë³µì ì¸ ë¶ˆë§Œ í‘œí˜„)
    is_repeating_complaints = False
    if customer_message_count > agent_message_count and customer_message_count >= 2:
        # ë§ˆì§€ë§‰ 2ê°œ ê³ ê° ë©”ì‹œì§€ ë¶„ì„
        recent_customer_messages = [msg["content"].lower() for msg in st.session_state.simulator_messages if
                                    msg.get("role") in ["customer", "customer_rebuttal"]][-2:]
        complaint_keywords = ["ì™œ", "ì´ìœ ", "ì„¤ëª…", "ë§ì´ ì•ˆ", "ì´í•´ê°€ ì•ˆ", "í™”ë‚˜", "ì§œì¦", "ë¶ˆë§Œ", "ì™œ", "why", "reason", "explain",
                              "angry", "frustrated", "complaint", "ãªãœ", "ç†ç”±", "èª¬æ˜", "æ€’ã‚Š", "ä¸æº€"]
        if any(any(keyword in msg for keyword in complaint_keywords) for msg in recent_customer_messages):
            is_repeating_complaints = True

    # ëŒ€ì²˜ë²• í¬ë©”ì´ì…˜ ì¶”ê°€ ì—¬ë¶€ ê²°ì •
    needs_coping_strategy = is_difficult_customer or (is_repeating_complaints and customer_message_count >= 2)

    # ëŒ€ì²˜ë²• ê°€ì´ë“œë¼ì¸ ìƒì„±
    coping_guidance = ""
    if needs_coping_strategy:
        coping_guidance = f"""

[CRITICAL: Handling Difficult Customer Situation]
The customer type is "{customer_type}" and the customer has sent {customer_message_count} messages while the agent has sent {agent_message_count} messages.
The customer may be showing signs of continued frustration or dissatisfaction.

**INCLUDE THE FOLLOWING COPING STRATEGY FORMAT IN YOUR RESPONSE:**

1. **Immediate Acknowledgment** (1-2 sentences):
   - Acknowledge their frustration/specific concern explicitly
   - Show deep empathy and understanding
   - Example formats:
     * "{'ì£„ì†¡í•©ë‹ˆë‹¤. ë¶ˆí¸ì„ ë“œë ¤ ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I sincerely apologize for the inconvenience. I fully understand your situation and frustration.' if current_lang_key == 'en' else 'å¤§å¤‰ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãŠå®¢æ§˜ã®çŠ¶æ³ã¨ã”ä¸ä¾¿ã‚’ååˆ†ã«ç†è§£ã—ã¦ãŠã‚Šã¾ã™ã€‚')}"
     * "{'ê³ ê°ë‹˜ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ì˜ ë“£ê³  ìˆìŠµë‹ˆë‹¤. ì •ë§ ë‹µë‹µí•˜ì…¨ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I hear your concerns clearly. This must have been very frustrating for you.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã”æ„è¦‹ã‚’ã—ã£ã‹ã‚Šã¨å—ã‘æ­¢ã‚ã¦ã„ã¾ã™ã€‚æœ¬å½“ã«ãŠå›°ã‚Šã ã£ãŸã¨æ€ã„ã¾ã™ã€‚')}"

2. **Specific Solution Recap** (2-3 sentences):
   - Clearly restate the solution/step provided previously (if any)
   - Offer a NEW concrete action or alternative solution
   - Be specific and actionable
   - Example formats:
     * "{'ì•ì„œ ì•ˆë‚´ë“œë¦° [êµ¬ì²´ì  í•´ê²°ì±…] ì™¸ì—ë„, [ìƒˆë¡œìš´ ëŒ€ì•ˆ/ì¶”ê°€ ì¡°ì¹˜]ë¥¼ ì§„í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('In addition to the [specific solution] I mentioned earlier, I can also [new alternative/additional action] for you.' if current_lang_key == 'en' else 'å…ˆã»ã©ã”æ¡ˆå†…ã—ãŸ[å…·ä½“çš„è§£æ±ºç­–]ã«åŠ ãˆã¦ã€[æ–°ã—ã„ä»£æ›¿æ¡ˆ/è¿½åŠ æªç½®]ã‚‚é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚')}"
     * "{'í˜¹ì‹œ [êµ¬ì²´ì  ë¬¸ì œì ] ë•Œë¬¸ì— ë¶ˆí¸í•˜ì…¨ë‹¤ë©´, [êµ¬ì²´ì  í•´ê²° ë°©ë²•]ì„ ë°”ë¡œ ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If you are experiencing [specific issue], I can immediately proceed with [specific solution].' if current_lang_key == 'en' else 'ã‚‚ã—[å…·ä½“çš„å•é¡Œ]ã§ã”ä¸ä¾¿ã§ã—ãŸã‚‰ã€[å…·ä½“çš„è§£æ±ºæ–¹æ³•]ã‚’ã™ãã«é€²ã‚ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"

3. **Escalation or Follow-up Offer** (1-2 sentences):
   - Offer to escalate to supervisor/higher level support
   - Promise immediate follow-up within specific time
   - Example formats:
     * "{'ë§Œì•½ ì—¬ì „íˆ ë¶ˆë§Œì´ í•´ì†Œë˜ì§€ ì•Šìœ¼ì‹ ë‹¤ë©´, ì¦‰ì‹œ ìƒê¸‰ ê´€ë¦¬ìì—ê²Œ ì´ê´€í•˜ì—¬ ë” ë‚˜ì€ í•´ê²°ì±…ì„ ì°¾ì•„ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If your concern is still not resolved, I can immediately escalate this to a supervisor to find a better solution.' if current_lang_key == 'en' else 'ã‚‚ã—ã”ä¸æº€ãŒè§£æ¶ˆã•ã‚Œãªã„å ´åˆã¯ã€ã™ãã«ä¸Šç´šç®¡ç†è€…ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ãƒˆã—ã¦ã€ã‚ˆã‚Šè‰¯ã„è§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"
     * "{'24ì‹œê°„ ì´ë‚´ì— [êµ¬ì²´ì  ì¡°ì¹˜/ê²°ê³¼]ë¥¼ í™•ì¸í•˜ì—¬ ê³ ê°ë‹˜ê»˜ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will follow up with you within 24 hours regarding [specific action/result].' if current_lang_key == 'en' else '24æ™‚é–“ä»¥å†…ã«[å…·ä½“çš„æªç½®/çµæœ]ã‚’ç¢ºèªã—ã€ãŠå®¢æ§˜ã«å†åº¦ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚')}"

4. **Closing with Assurance** (1 sentence):
   - Reassure that their concern is being taken seriously
   - Example formats:
     * "{'ê³ ê°ë‹˜ì˜ ëª¨ë“  ë¬¸ì˜ì‚¬í•­ì„ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will prioritize resolving all of your concerns.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã™ã¹ã¦ã®ã”è³ªå•ã‚’æœ€å„ªå…ˆã§å‡¦ç†ã„ãŸã—ã¾ã™ã€‚')}"

**IMPORTANT NOTES:**
- DO NOT repeat the exact same solution that was already provided
- DO NOT sound dismissive or automated
- DO sound genuinely concerned and willing to go the extra mile
- If policy restrictions exist, acknowledge them but still offer alternatives
- Use warm, respectful tone while being firm about what can/cannot be done

**RESPONSE STRUCTURE:**
[Immediate Acknowledgment]
[Specific Solution Recap + New Action]
[Escalation/Follow-up Offer]
[Closing with Assurance]

Now generate the agent's response draft following this structure:
"""

    # ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ì¶”ê°€ ì§€ì‹œì‚¬í•­ (ë” ê°•í™”)
    diversity_instruction = ""
    if previous_agent_responses:
        # ì´ì „ ì‘ë‹µë“¤ì˜ ì£¼ìš” í‚¤ì›Œë“œ/êµ¬ë¬¸ ì¶”ì¶œ (ë°˜ë³µ ë°©ì§€)
        previous_keywords = []
        for prev_resp in previous_agent_responses[-3:]:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (2-3ë‹¨ì–´ êµ¬ë¬¸)
            words = prev_resp.split()[:20]  # ì²˜ìŒ 20ë‹¨ì–´ë§Œ
            for i in range(len(words) - 1):
                if len(words[i]) > 3 and len(words[i+1]) > 3:
                    previous_keywords.append(f"{words[i]} {words[i+1]}")
        
        keywords_warning = ""
        if previous_keywords:
            unique_keywords = list(set(previous_keywords))[:10]  # ìµœëŒ€ 10ê°œë§Œ
            keywords_warning = f"\n- AVOID using these exact phrases from previous responses: {', '.join(unique_keywords[:5])}"
        
        diversity_instruction = f"""
**CRITICAL DIVERSITY REQUIREMENT - STRICTLY ENFORCED:**
- You MUST generate a COMPLETELY DIFFERENT response from ALL previous agent responses shown above
- Use COMPLETELY DIFFERENT wording, phrasing, sentence structures, and vocabulary
- If similar solutions are needed, present them in a COMPLETELY DIFFERENT way or from a COMPLETELY DIFFERENT angle
- Vary your opening sentences, transition phrases, and closing statements - NO REPETITION
- DO NOT copy, paraphrase, or reuse ANY phrases from previous responses - be CREATIVE and UNIQUE while maintaining professionalism
- Consider COMPLETELY different approaches: 
  * If previous responses were formal, try a warmer, more personal tone (or vice versa)
  * If previous responses focused on one aspect, emphasize a COMPLETELY different aspect this time
  * Use different examples, analogies, or explanations
  * Change the order of information presentation
  * Use different sentence lengths and structures
{keywords_warning}
- IMPORTANT: Read ALL previous responses carefully and ensure your response is DISTINCTLY different in style, tone, structure, and content
- If you find yourself writing something similar to a previous response, STOP and rewrite it completely differently
"""

    # ëœë¤ ìš”ì†Œ ì¶”ê°€ë¥¼ ìœ„í•œ ë³€í˜• ì§€ì‹œì‚¬í•­
    variation_approaches = [
        "Start with a different greeting or acknowledgment style",
        "Use a different problem-solving approach or framework",
        "Present information in a different order",
        "Use different examples or analogies",
        "Vary the level of formality or warmth",
        "Focus on different aspects of the solution",
        "Use different transition words and phrases",
        "Change the length and complexity of sentences"
    ]
    selected_approaches = random.sample(variation_approaches, min(3, len(variation_approaches)))
    variation_note = "\n".join([f"- {approach}" for approach in selected_approaches])

    draft_prompt = f"""
You are an AI assistant helping a customer support agent write a professional, tailored response.

**PRIMARY OBJECTIVE:**
Generate a response draft that is SPECIFICALLY tailored to the customer's EXACT inquiry, providing concrete, actionable solutions that directly address their specific situation. The response must read as if it was written personally for this customer's unique case.

**CRITICAL REQUIREMENTS (IN ORDER OF PRIORITY):**
1. **MOST CRITICAL**: Address the customer's SPECIFIC inquiry/question with DETAILED, ACTIONABLE solutions
   - Extract and reference specific details from their message (order numbers, dates, product names, locations, error messages, etc.)
   - Provide step-by-step instructions tailored to their EXACT situation
   - Answer ALL questions they asked completely
   - Address ALL problems they mentioned specifically

2. The response MUST be in {lang_name}

3. Be professional, empathetic, and solution-oriented

4. If the customer asked a question, provide a COMPLETE and SPECIFIC answer - do NOT give vague or generic responses
   - Bad: "Please check your settings"
   - Good: "Please go to Settings > Mobile Network > APN Settings and ensure the APN is set to 'internet'"

5. If the customer mentioned a problem, acknowledge it SPECIFICALLY and provide STEP-BY-STEP solutions
   - Reference their exact problem description
   - Provide solutions that directly address their specific issue

6. Reference specific details from their inquiry (order numbers, dates, products, locations, etc.) if mentioned
   - If they mentioned "order #12345", your response MUST include "order #12345"
   - If they mentioned "Paris", your response should reference Paris specifically
   - If they mentioned "eSIM", address eSIM specifically, not just "SIM card"

7. Keep the tone appropriate for the customer type: {customer_type}

8. Do NOT include any markdown formatting, just plain text

9. {f'**FOLLOW THE COPING STRATEGY FORMAT BELOW**' if needs_coping_strategy else 'Use natural, conversational flow'}

10. **CRITICAL**: Generate a COMPLETELY UNIQUE and VARIED response - avoid repeating ANY similar phrases, structures, or approaches from previous responses

11. **CRITICAL**: Your response must be HIGHLY RELEVANT to the customer's specific inquiry - generic template responses are NOT acceptable
    - DO NOT start with generic greetings without immediately addressing their specific issue
    - DO NOT use placeholder text like "[specific solution]" - provide ACTUAL specific solutions
    - Your response should make the customer feel like you read and understood THEIR specific message

**VARIATION TECHNIQUES TO APPLY:**
{variation_note}

{customer_query_analysis}

**FULL CONVERSATION HISTORY:**
{history_text}
{attachment_context}

**PREVIOUS RESPONSES TO AVOID REPEATING:**
{previous_responses_context if previous_responses_context else "No previous responses to compare against."}

**DIVERSITY REQUIREMENTS:**
{diversity_instruction if diversity_instruction else "This is the first response, so no previous responses to avoid."}

{coping_guidance if needs_coping_strategy else ''}

**YOUR TASK:**
Generate the agent's response draft NOW. The response must:

1. **FIRST**: Read the customer inquiry analysis above CAREFULLY and identify:
   - What is their EXACT problem or question?
   - What specific details did they mention (order numbers, dates, locations, products)?
   - What do they need help with specifically?

2. **SECOND**: Write a response that:
   - Addresses their EXACT problem/question (not a generic version)
   - References the specific details they mentioned
   - Provides concrete, actionable steps tailored to their situation
   - Answers ALL their questions completely
   - Makes them feel understood

3. **THIRD**: Ensure the response is:
   - COMPLETELY DIFFERENT and UNIQUE from any previous responses
   - Professional, empathetic, and solution-oriented
   - Written in {lang_name}
   - Free of markdown formatting

**BEFORE YOU WRITE**: Ask yourself:
- "Does this response address the customer's SPECIFIC inquiry?"
- "Would a generic template response work here?" (If yes, rewrite it to be more specific)
- "Does this response reference specific details from the customer's message?"
- "Would the customer feel like I read and understood THEIR specific message?"

**NOW GENERATE THE RESPONSE:**
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        draft = run_llm(draft_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if draft.startswith("```"):
            lines = draft.split("\n")
            draft = "\n".join(lines[1:-1]) if len(lines) > 2 else draft
        return draft
    except Exception as e:
        return f"âŒ ì‘ë‹µ ì´ˆì•ˆ ìƒì„± ì˜¤ë¥˜: {e}"


# â­ ìƒˆë¡œìš´ í•¨ìˆ˜: ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ ìš”ì•½ ìƒì„±
def generate_outbound_call_summary(customer_query: str, current_lang_key: str, target: str) -> str:
    """
    Simulates an outbound call to a local partner or customer and generates a summary of the outcome.
    """
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # Get the current chat history for context
    history_text = get_chat_history_for_prompt(include_attachment=True)
    if not history_text:
        history_text = f"Initial Customer Query: {customer_query}"

    # Policy context (from supervisor) should be included to guide the outcome
    policy_context = st.session_state.supervisor_policy_context or ""

    summary_prompt = f"""
You are an AI simulating a quick, high-stakes phone call placed by the customer support agent to a '{target}' (either a local partner/vendor or the customer).

The purpose of the call is to resolve a complex, policy-restricted issue (like an exceptional refund for a non-refundable item, or urgent confirmation of an airport transfer change).

Analyze the conversation history, the initial query, and any provided supervisor policy.
Generate a concise summary of the OUTCOME of this simulated phone call.
The summary MUST be professional and strictly in {lang_name}.

[CRITICAL RULE]: For non-refundable items (e.g., Universal Studio Express Pass, non-refundable hotel/transfer), the local partner should only grant an exception IF the customer has provided strong, unavoidable proof (like a flight cancellation notice, doctor's note, or natural disaster notice). If no such proof is evident in the chat history, the outcome should usually be a denial or a request for more proof, but keep the tone professional.
If the customer's query is about Airport Transfer change, the outcome should be: 'Confirmation complete. Change is approved/denied based on partner policy.'

Conversation History:
{history_text}

Supervisor Policy Context (If any):
{policy_context}

Target of Call: {target}

Generate the phone call summary (Outcome ONLY):
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key missing. (Simulated Outcome: The {target} requested the agent to send proof via email.)"

    try:
        summary = run_llm(summary_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if summary.startswith("```"):
            lines = summary.split("\n")
            summary = "\n".join(lines[1:-1]) if len(lines) > 2 else summary
        return summary
    except Exception as e:
        return f"âŒ Phone call simulation error: {e}"


# ========================================
# 3. Whisper / TTS Helper
# ========================================

def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = "audio/webm", lang_code: str = "ko") -> str:
    """
    OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•©ë‹ˆë‹¤.
    """
    L = LANG[st.session_state.language]
    client = st.session_state.openai_client
    if client is None:
        return f"âŒ {L['openai_missing']}"

    whisper_lang = {"ko": "ko", "en": "en", "ja": "ja"}.get(lang_code, "en")

    # ì„ì‹œ íŒŒì¼ ì €ì¥ (Whisper API í˜¸í™˜ì„±)
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp.write(audio_bytes)
    tmp.flush()
    tmp.close()

    try:
        with open(tmp.name, "rb") as f:
            res = client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                response_format="text",
                language=whisper_lang,
            )
        # res.text ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ res ìì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        return res.text.strip() if hasattr(res, 'text') else str(res).strip()
    except Exception as e:
        # íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜ ë“± ìƒì„¸ ì˜¤ë¥˜ ì²˜ë¦¬
        return f"âŒ {L['error']} Whisper: {e}"
    finally:
        try:
            os.remove(tmp.name)
        except OSError:
            pass


def transcribe_audio(audio_bytes, filename="audio.wav"):
    client = st.session_state.openai_client

    # 1ï¸âƒ£ OpenAI Whisper ì‹œë„
    if client:
        try:
            import io
            bio = io.BytesIO(audio_bytes)
            bio.name = filename
            resp = client.audio.transcriptions.create(
                model="whisper-1",
                file=bio,
            )
            return resp.text
        except Exception as e:
            print("Whisper OpenAI failed:", e)

    # 2ï¸âƒ£ Gemini STT fallback
    try:
        genai.configure(api_key=get_api_key("gemini"))
        model = genai.GenerativeModel("gemini-2.5-flash")
        text = model.generate_content("Transcribe this audio:").text
        return text or ""
    except Exception as e:
        print("Gemini STT failed:", e)

    return "âŒ STT not available"


# ì—­í• ë³„ TTS ìŒì„± ìŠ¤íƒ€ì¼ ì„¤ì •
TTS_VOICES = {
    "customer": {
        "gender": "male",
        "voice": "alloy"  # Distinct Male, Generic/Customer
    },
    "agent": {
        "gender": "female",
        "voice": "shimmer"  # Distinct Female, Professional/Agent
    },
    "supervisor": {
        "gender": "female",
        "voice": "nova"  # Another Distinct Female, Informative/Supervisor
    }
}


def synthesize_tts(text: str, lang_key: str, role: str = "agent"):
    L = LANG[lang_key]
    client = st.session_state.openai_client
    if client is None:
        return None, L["openai_missing"]

    if role not in TTS_VOICES:
        role = "agent"

    voice_name = TTS_VOICES[role]["voice"]

    try:
        # tts-1 ëª¨ë¸ ì‚¬ìš© (ì•ˆì •ì„±)
        resp = client.audio.speech.create(
            model="tts-1",
            voice=voice_name,
            input=text
            # format="mp3"ì€ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.
        )
        return resp.read(), L["tts_status_success"]

    except Exception as e:
        return None, f"{L['tts_status_error']}: {e}"


# ----------------------------------------
# TTS Helper
# ----------------------------------------

def render_tts_button(text, lang_key, role="customer", prefix="", index: int = -1):
    L = LANG[lang_key]

    # â­ ìˆ˜ì •: index=-1ì¸ ê²½ìš°, UUIDë¥¼ ì‚¬ìš©í•˜ì—¬ safe_key ìƒì„±
    if index == -1:
        # ì´ê´€ ìš”ì•½ì²˜ëŸ¼ ì¸ë±ìŠ¤ê°€ ê³ ì •ë˜ì§€ ì•ŠëŠ” ê²½ìš°, í…ìŠ¤íŠ¸ í•´ì‹œì™€ ì„¸ì…˜ ì¸ìŠ¤í„´ìŠ¤ IDë¥¼ ì¡°í•©
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        session_id_part = st.session_state.get('sim_instance_id', 'default_session')
        # â­ ìˆ˜ì •: ì´ê´€ ìš”ì•½ì˜ ê²½ìš° ì•ˆì •ì ì¸ í‚¤ë¥¼ ìƒì„± (time.time_ns() ì œê±°í•˜ì—¬ ë§¤ë²ˆ ê°™ì€ í‚¤ ìƒì„±)
        # ì–¸ì–´ ì½”ë“œë„ ì¶”ê°€í•˜ì—¬ ì´ê´€ í›„ ì–¸ì–´ ë³€ê²½ ì‹œì—ë„ ê³ ìœ ì„± ë³´ì¥
        lang_code = st.session_state.get('language', lang_key)
        safe_key = f"{prefix}_SUMMARY_{session_id_part}_{lang_code}_{content_hash}"
    else:
        # ëŒ€í™” ë¡œê·¸ì²˜ëŸ¼ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        safe_key = f"{prefix}_{index}_{content_hash}"

    # ì¬ìƒ ë²„íŠ¼ì„ ëˆ„ë¥¼ ë•Œë§Œ TTS ìš”ì²­
    if st.button(L["button_listen_audio"], key=safe_key):
        if not st.session_state.openai_client:
            st.error(L["openai_missing"])
            return  # í‚¤ ì—†ìœ¼ë©´ ì¢…ë£Œ

        with st.spinner(L["tts_status_generating"]):
            try:
                audio_bytes, msg = synthesize_tts(text, lang_key, role=role)
                if audio_bytes:
                    # â­ st.audio í˜¸ì¶œ ì‹œ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì¬ìƒ ì‹œê°„ì„ í™•ë³´
                    # Streamlit ë¬¸ì„œ: autoplayëŠ” ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì—†ì´ëŠ” ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                    try:
                        st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                        st.success(msg)
                        # â­ ìˆ˜ì •: ì¬ìƒì´ ì‹œì‘ë  ì¶©ë¶„í•œ ì‹œê°„ì„ í™•ë³´í•˜ê¸° ìœ„í•´ ëŒ€ê¸° ì‹œê°„ì„ 3ì´ˆë¡œ ëŠ˜ë¦¼
                        time.sleep(3)
                    except Exception as e:
                        st.warning(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ ì˜¤ë¥˜: {e}. ì˜¤ë””ì˜¤ íŒŒì¼ì€ ìƒì„±ë˜ì—ˆì§€ë§Œ ìë™ ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                        st.success(msg)
                else:
                    st.error(msg)
                    time.sleep(1)  # ì—ëŸ¬ ë°œìƒ ì‹œë„ ì ì‹œ ëŒ€ê¸°
            except Exception as e:
                # TTS API í˜¸ì¶œ ìì²´ì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ (ë„¤íŠ¸ì›Œí¬ ë“±)
                st.error(f"âŒ TTS ìƒì„± ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                time.sleep(1)

            # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸ í›„, ë¶ˆí•„ìš”í•œ ì¬ì‹¤í–‰ì„ ë§‰ê¸° ìœ„í•´ ì—¬ê¸°ì„œ í•¨ìˆ˜ ì¢…ë£Œ
            return
        # [ì¤‘ëµ: TTS Helper ë]


# ========================================
# 4. ë¡œì»¬ ìŒì„± ê¸°ë¡ Helper
# ========================================

def load_voice_records() -> List[Dict[str, Any]]:
    return _load_json(VOICE_META_FILE, [])


def save_voice_records(records: List[Dict[str, Any]]):
    _save_json(VOICE_META_FILE, records)


def save_audio_record_local(
        audio_bytes: bytes,
        filename: str,
        transcript_text: str,
        mime_type: str = "audio/webm",
        meta: Dict[str, Any] = None,
) -> str:
    records = load_voice_records()
    rec_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    ext = filename.split(".")[-1] if "." in filename else "webm"
    audio_filename = f"{rec_id}.{ext}"
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "wb") as f:
        f.write(audio_bytes)

    rec = {
        "id": rec_id,
        "created_at": ts,
        "filename": filename,
        "audio_filename": audio_filename,
        "size": len(audio_bytes),
        "transcript": transcript_text,
        "mime_type": mime_type,
        "language": st.session_state.language,
        "meta": meta or {},
    }
    records.insert(0, rec)
    save_voice_records(records)
    return rec_id


def delete_audio_record_local(rec_id: str) -> bool:
    records = load_voice_records()
    idx = next((i for i, r in enumerate(records) if r.get("id") == rec_id), None)
    if idx is None:
        return False
    rec = records.pop(idx)
    audio_filename = rec.get("audio_filename")
    if audio_filename:
        audio_path = os.path.join(AUDIO_DIR, audio_filename)
        try:
            os.remove(audio_path)
        except FileNotFoundError:
            pass
    save_voice_records(records)
    return True


def get_audio_bytes_local(rec_id: str):
    records = load_voice_records()
    rec = next((r for r in records if r.get("id") == rec_id), None)
    if not rec:
        raise FileNotFoundError("record not found")
    audio_filename = rec["audio_filename"]
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "rb") as f:
        b = f.read()
    return b, rec


# ========================================
# 5. ë¡œì»¬ ì‹œë®¬ë ˆì´ì…˜ ì´ë ¥ Helper (ìš”ì²­ 4 ë°˜ì˜)
# ========================================

def load_simulation_histories_local(lang_key: str) -> List[Dict[str, Any]]:
    histories = _load_json(SIM_META_FILE, [])
    # í˜„ì¬ ì–¸ì–´ì™€ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ì´ë ¥ë§Œ í•„í„°ë§
    return [
        h for h in histories
        if h.get("language_key") == lang_key and (isinstance(h.get("messages"), list) or h.get("summary"))
    ]


def generate_chat_summary(messages: List[Dict[str, Any]], initial_query: str, customer_type: str,
                          current_lang_key: str) -> Dict[str, Any]:
    """ì±„íŒ… ë‚´ìš©ì„ AIë¡œ ìš”ì•½í•˜ì—¬ ì£¼ìš” ì •ë³´ì™€ ì ìˆ˜ë¥¼ ì¶”ì¶œ (ìš”ì²­ 4)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ëŒ€í™” ë‚´ìš© ì¶”ì¶œ
    conversation_text = f"Initial Query: {initial_query}\n\n"
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role in ["customer", "customer_rebuttal", "phone_exchange"]:
            conversation_text += f"Customer: {content}\n"
        elif role == "agent_response" or role == "agent":
            conversation_text += f"Agent: {content}\n"
        # supervisor ë©”ì‹œì§€ëŠ” LLMì— ì „ë‹¬í•˜ì§€ ì•Šì•„ ì—­í•  í˜¼ë™ ë°©ì§€

    # í° êµí™˜ ë¡œê·¸ëŠ” ì´ë¯¸ "Agent: ... | Customer: ..." í˜•íƒœë¡œ ê¸°ë¡ë˜ë¯€ë¡œ,
    # generate_summary_for_call í•¨ìˆ˜ì—ì„œ ë³„ë„ë¡œ ì²˜ë¦¬í•  í•„ìš” ì—†ì´,
    # ì—¬ê¸°ì„œëŠ” ë²”ìš© ì±„íŒ… ìš”ì•½ ë¡œì§ì„ ë”°ë¥´ë„ë¡ ë©”ì‹œì§€ë¥¼ ì •ì œí•©ë‹ˆë‹¤.

    summary_prompt = f"""
You are an AI analyst summarizing a customer support conversation.

Analyze the conversation and provide a structured summary in JSON format (ONLY JSON, no markdown).

Extract and score:
1. Main inquiry topic (what the customer asked about)
2. Key responses provided by the agent (list of max 3 core actions/solutions)
3. Customer sentiment score (0-100, where 0=very negative, 50=neutral, 100=very positive)
4. Customer satisfaction score (0-100, based on final response)
5. Customer characteristics:
   - Language preference (if mentioned)
   - Cultural background hints (if any)
   - Location/region (if mentioned, but anonymize specific addresses)
   - Communication style (formal/casual, brief/detailed)
6. Privacy-sensitive information (anonymize: names, emails, phone numbers, specific addresses)
   - Extract patterns only (e.g., "email provided", "phone number provided", "resides in Asia region")

Output format (JSON only):
{{
  "main_inquiry": "brief description of main issue",
  "key_responses": ["response 1", "response 2"],
  "customer_sentiment_score": 75,
  "customer_satisfaction_score": 80,
  "customer_characteristics": {{
    "language": "ko/en/ja or unknown",
    "cultural_hints": "brief description or unknown",
    "region": "general region or unknown",
    "communication_style": "formal/casual/brief/detailed"
  }},
  "privacy_info": {{
    "has_email": true/false,
    "has_phone": true/false,
    "has_address": true/false,
    "region_hint": "general region or unknown"
  }},
  "summary": "overall conversation summary in {lang_name}"
}}

Conversation:
{conversation_text}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        # Fallback summary
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Customer inquiry about: {initial_query[:100]}"
        }

    try:
        summary_text = run_llm(summary_prompt).strip()
        # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if "```json" in summary_text:
            summary_text = summary_text.split("```json")[1].split("```")[0].strip()
        elif "```" in summary_text:
            summary_text = summary_text.split("```")[1].split("```")[0].strip()

        import json
        summary_data = json.loads(summary_text)
        return summary_data
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback
        st.warning(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Customer inquiry about: {initial_query[:100]}"
        }
        # Fallback on error
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Error generating summary: {str(e)}"
        }


def save_simulation_history_local(initial_query: str, customer_type: str, messages: List[Dict[str, Any]],
                                  is_chat_ended: bool, attachment_context: str, is_call: bool = False):
    """AI ìš”ì•½ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì´ë ¥ì„ ì €ì¥ (ìš”ì²­ 4 ë°˜ì˜)"""
    histories = _load_json(SIM_META_FILE, [])
    doc_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    # AI ìš”ì•½ ìƒì„± (ì±„íŒ… ì¢…ë£Œ ì‹œ ë˜ëŠ” ì¶©ë¶„í•œ ëŒ€í™”ê°€ ìˆì„ ë•Œ)
    summary_data = None
    if is_chat_ended or len(messages) > 4 or is_call:  # ì „í™” í†µí™”ëŠ” ë°”ë¡œ ìš”ì•½ ì‹œë„
        summary_data = generate_chat_summary(messages, initial_query, customer_type, st.session_state.language)

    # ìš”ì•½ ë°ì´í„°ê°€ ìƒì„±ëœ ê²½ìš°ì—ë§Œ ì €ì¥ (ìš”ì•½ ì¤‘ì‹¬ ì €ì¥)
    if summary_data:
        # ìš”ì•½ ë°ì´í„°ì— ì´ˆê¸° ë¬¸ì˜ì™€ í•µì‹¬ ì •ë³´ í¬í•¨
        data = {
            "id": doc_id,
            "initial_query": initial_query,  # ì´ˆê¸° ë¬¸ì˜ëŠ” ìœ ì§€
            "customer_type": customer_type,
            "messages": [],  # ì „ì²´ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ìš”ì•½ë§Œ ì €ì¥)
            "summary": summary_data,  # AI ìš”ì•½ ë°ì´í„° (ì£¼ìš” ì €ì¥ ë‚´ìš©)
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",  # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸
            "is_call": is_call,  # ì „í™” ì—¬ë¶€ í”Œë˜ê·¸
        }
    else:
        # ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° (ì§„í–‰ ì¤‘ì¸ ëŒ€í™”), ìµœì†Œí•œì˜ ì •ë³´ë§Œ ì €ì¥
        data = {
            "id": doc_id,
            "initial_query": initial_query,
            "customer_type": customer_type,
            "messages": messages[:10] if len(messages) > 10 else messages,  # ìµœê·¼ 10ê°œë§Œ ì €ì¥
            "summary": None,  # ìš”ì•½ ì—†ìŒ
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",
            "is_call": is_call,
        }

    # ê¸°ì¡´ ì´ë ¥ì— ì¶”ê°€ (ìµœì‹ ìˆœ)
    histories.insert(0, data)
    # ë„ˆë¬´ ë§ì€ ì´ë ¥ ë°©ì§€ (ì˜ˆ: 100ê°œë¡œ ì¦ê°€ - ìš”ì•½ë§Œ ì €ì¥í•˜ë¯€ë¡œ ìš©ëŸ‰ ë¶€ë‹´ ì ìŒ)
    _save_json(SIM_META_FILE, histories[:100])
    return doc_id


def delete_all_history_local():
    _save_json(SIM_META_FILE, [])


# ========================================
# DB ì €ì¥ ê¸°ëŠ¥ (Word/PPTX/PDF)
# ========================================
def export_history_to_word(histories: List[Dict[str, Any]], filename: str = None) -> str:
    """ì´ë ¥ì„ Word íŒŒì¼ë¡œ ì €ì¥"""
    if not IS_DOCX_AVAILABLE:
        raise ImportError("Word ì €ì¥ì„ ìœ„í•´ python-docxê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install python-docx")
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
    filepath = os.path.join(DATA_DIR, filename)
    
    doc = DocxDocument()
    
    # ì œëª© ì¶”ê°€
    title = doc.add_heading('ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½', 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # ê° ì´ë ¥ ì¶”ê°€
    for i, hist in enumerate(histories, 1):
        # ì´ë ¥ ì œëª©
        doc.add_heading(f'ì´ë ¥ #{i}', level=1)
        
        # ê¸°ë³¸ ì •ë³´
        doc.add_paragraph(f'ID: {hist.get("id", "N/A")}')
        doc.add_paragraph(f'ë‚ ì§œ: {hist.get("timestamp", "N/A")}')
        doc.add_paragraph(f'ì´ˆê¸° ë¬¸ì˜: {hist.get("initial_query", "N/A")}')
        doc.add_paragraph(f'ê³ ê° ìœ í˜•: {hist.get("customer_type", "N/A")}')
        doc.add_paragraph(f'ì–¸ì–´: {hist.get("language_key", "N/A")}')
        
        summary = hist.get('summary', {})
        if summary:
            # ìš”ì•½ ì„¹ì…˜
            doc.add_heading('ìš”ì•½', level=2)
            doc.add_paragraph(f'ì£¼ìš” ë¬¸ì˜: {summary.get("main_inquiry", "N/A")}')
            doc.add_paragraph(f'í•µì‹¬ ì‘ë‹µ: {", ".join(summary.get("key_responses", []))}')
            doc.add_paragraph(f'ê³ ê° ê°ì • ì ìˆ˜: {summary.get("customer_sentiment_score", "N/A")}/100')
            doc.add_paragraph(f'ê³ ê° ë§Œì¡±ë„ ì ìˆ˜: {summary.get("customer_satisfaction_score", "N/A")}/100')
            
            # ê³ ê° íŠ¹ì„±
            characteristics = summary.get('customer_characteristics', {})
            doc.add_heading('ê³ ê° íŠ¹ì„±', level=2)
            doc.add_paragraph(f'ì–¸ì–´: {characteristics.get("language", "N/A")}')
            doc.add_paragraph(f'ë¬¸í™”ì  ë°°ê²½: {characteristics.get("cultural_hints", "N/A")}')
            doc.add_paragraph(f'ì§€ì—­: {characteristics.get("region", "N/A")}')
            doc.add_paragraph(f'ì†Œí†µ ìŠ¤íƒ€ì¼: {characteristics.get("communication_style", "N/A")}')
            
            # ê°œì¸ì •ë³´ ìš”ì•½
            privacy = summary.get('privacy_info', {})
            doc.add_heading('ê°œì¸ì •ë³´ ìš”ì•½', level=2)
            doc.add_paragraph(f'ì´ë©”ì¼ ì œê³µ: {"ì˜ˆ" if privacy.get("has_email") else "ì•„ë‹ˆì˜¤"}')
            doc.add_paragraph(f'ì „í™”ë²ˆí˜¸ ì œê³µ: {"ì˜ˆ" if privacy.get("has_phone") else "ì•„ë‹ˆì˜¤"}')
            doc.add_paragraph(f'ì£¼ì†Œ ì œê³µ: {"ì˜ˆ" if privacy.get("has_address") else "ì•„ë‹ˆì˜¤"}')
            doc.add_paragraph(f'ì§€ì—­ íŒíŠ¸: {privacy.get("region_hint", "N/A")}')
            
            # ì „ì²´ ìš”ì•½
            doc.add_paragraph(f'ì „ì²´ ìš”ì•½: {summary.get("summary", "N/A")}')
        
        # êµ¬ë¶„ì„ 
        if i < len(histories):
            doc.add_paragraph('-' * 80)
    
    doc.save(filepath)
    return filepath


def export_history_to_pptx(histories: List[Dict[str, Any]], filename: str = None) -> str:
    """ì´ë ¥ì„ PPTX íŒŒì¼ë¡œ ì €ì¥"""
    if not IS_PPTX_AVAILABLE:
        raise ImportError("PPTX ì €ì¥ì„ ìœ„í•´ python-pptxê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install python-pptx")
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx"
    filepath = os.path.join(DATA_DIR, filename)
    
    prs = Presentation()
    prs.slide_width = Inches(10)
    prs.slide_height = Inches(7.5)
    
    # ì œëª© ìŠ¬ë¼ì´ë“œ
    title_slide_layout = prs.slide_layouts[0]
    slide = prs.slides.add_slide(title_slide_layout)
    title = slide.shapes.title
    subtitle = slide.placeholders[1]
    title.text = "ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½"
    subtitle.text = f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    
    # ê° ì´ë ¥ì— ëŒ€í•´ ìŠ¬ë¼ì´ë“œ ìƒì„±
    for i, hist in enumerate(histories, 1):
        # ì œëª© ë° ë‚´ìš© ë ˆì´ì•„ì›ƒ
        bullet_slide_layout = prs.slide_layouts[1]
        slide = prs.slides.add_slide(bullet_slide_layout)
        shapes = slide.shapes
        
        title_shape = shapes.title
        body_shape = shapes.placeholders[1]
        
        title_shape.text = f"ì´ë ¥ #{i}"
        
        tf = body_shape.text_frame
        tf.text = f"ID: {hist.get('id', 'N/A')}"
        
        p = tf.add_paragraph()
        p.text = f"ë‚ ì§œ: {hist.get('timestamp', 'N/A')}"
        p.level = 0
        
        p = tf.add_paragraph()
        p.text = f"ì´ˆê¸° ë¬¸ì˜: {hist.get('initial_query', 'N/A')}"
        p.level = 0
        
        p = tf.add_paragraph()
        p.text = f"ê³ ê° ìœ í˜•: {hist.get('customer_type', 'N/A')}"
        p.level = 0
        
        summary = hist.get('summary', {})
        if summary:
            p = tf.add_paragraph()
            p.text = f"ì£¼ìš” ë¬¸ì˜: {summary.get('main_inquiry', 'N/A')}"
            p.level = 0
            
            p = tf.add_paragraph()
            p.text = f"ê³ ê° ê°ì • ì ìˆ˜: {summary.get('customer_sentiment_score', 'N/A')}/100"
            p.level = 0
            
            p = tf.add_paragraph()
            p.text = f"ê³ ê° ë§Œì¡±ë„ ì ìˆ˜: {summary.get('customer_satisfaction_score', 'N/A')}/100"
            p.level = 0
    
    prs.save(filepath)
    return filepath


def export_history_to_pdf(histories: List[Dict[str, Any]], filename: str = None) -> str:
    """ì´ë ¥ì„ PDF íŒŒì¼ë¡œ ì €ì¥"""
    if not IS_REPORTLAB_AVAILABLE:
        raise ImportError("PDF ì €ì¥ì„ ìœ„í•´ reportlabì´ í•„ìš”í•©ë‹ˆë‹¤: pip install reportlab")
    
    if filename is None:
        filename = f"customer_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
    filepath = os.path.join(DATA_DIR, filename)
    
    doc = SimpleDocTemplate(filepath, pagesize=A4)
    story = []
    styles = getSampleStyleSheet()
    
    # ì œëª© ìŠ¤íƒ€ì¼
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=24,
        textColor=black,
        spaceAfter=30,
        alignment=1  # ì¤‘ì•™ ì •ë ¬
    )
    
    # ì œëª© ì¶”ê°€
    story.append(Paragraph('ê³ ê° ì‘ëŒ€ ì´ë ¥ ìš”ì•½', title_style))
    story.append(Spacer(1, 0.2*inch))
    
    # ê° ì´ë ¥ ì¶”ê°€
    for i, hist in enumerate(histories, 1):
        # ì´ë ¥ ì œëª©
        story.append(Paragraph(f'ì´ë ¥ #{i}', styles['Heading1']))
        story.append(Spacer(1, 0.1*inch))
        
        # ê¸°ë³¸ ì •ë³´
        story.append(Paragraph(f'ID: {hist.get("id", "N/A")}', styles['Normal']))
        story.append(Paragraph(f'ë‚ ì§œ: {hist.get("timestamp", "N/A")}', styles['Normal']))
        story.append(Paragraph(f'ì´ˆê¸° ë¬¸ì˜: {hist.get("initial_query", "N/A")}', styles['Normal']))
        story.append(Paragraph(f'ê³ ê° ìœ í˜•: {hist.get("customer_type", "N/A")}', styles['Normal']))
        story.append(Paragraph(f'ì–¸ì–´: {hist.get("language_key", "N/A")}', styles['Normal']))
        
        summary = hist.get('summary', {})
        if summary:
            story.append(Spacer(1, 0.1*inch))
            story.append(Paragraph('ìš”ì•½', styles['Heading2']))
            story.append(Paragraph(f'ì£¼ìš” ë¬¸ì˜: {summary.get("main_inquiry", "N/A")}', styles['Normal']))
            story.append(Paragraph(f'í•µì‹¬ ì‘ë‹µ: {", ".join(summary.get("key_responses", []))}', styles['Normal']))
            story.append(Paragraph(f'ê³ ê° ê°ì • ì ìˆ˜: {summary.get("customer_sentiment_score", "N/A")}/100', styles['Normal']))
            story.append(Paragraph(f'ê³ ê° ë§Œì¡±ë„ ì ìˆ˜: {summary.get("customer_satisfaction_score", "N/A")}/100', styles['Normal']))
            
            characteristics = summary.get('customer_characteristics', {})
            story.append(Spacer(1, 0.1*inch))
            story.append(Paragraph('ê³ ê° íŠ¹ì„±', styles['Heading2']))
            story.append(Paragraph(f'ì–¸ì–´: {characteristics.get("language", "N/A")}', styles['Normal']))
            story.append(Paragraph(f'ë¬¸í™”ì  ë°°ê²½: {characteristics.get("cultural_hints", "N/A")}', styles['Normal']))
            story.append(Paragraph(f'ì§€ì—­: {characteristics.get("region", "N/A")}', styles['Normal']))
            story.append(Paragraph(f'ì†Œí†µ ìŠ¤íƒ€ì¼: {characteristics.get("communication_style", "N/A")}', styles['Normal']))
            
            privacy = summary.get('privacy_info', {})
            story.append(Spacer(1, 0.1*inch))
            story.append(Paragraph('ê°œì¸ì •ë³´ ìš”ì•½', styles['Heading2']))
            story.append(Paragraph(f'ì´ë©”ì¼ ì œê³µ: {"ì˜ˆ" if privacy.get("has_email") else "ì•„ë‹ˆì˜¤"}', styles['Normal']))
            story.append(Paragraph(f'ì „í™”ë²ˆí˜¸ ì œê³µ: {"ì˜ˆ" if privacy.get("has_phone") else "ì•„ë‹ˆì˜¤"}', styles['Normal']))
            story.append(Paragraph(f'ì£¼ì†Œ ì œê³µ: {"ì˜ˆ" if privacy.get("has_address") else "ì•„ë‹ˆì˜¤"}', styles['Normal']))
            story.append(Paragraph(f'ì§€ì—­ íŒíŠ¸: {privacy.get("region_hint", "N/A")}', styles['Normal']))
            
            story.append(Paragraph(f'ì „ì²´ ìš”ì•½: {summary.get("summary", "N/A")}', styles['Normal']))
        
        # êµ¬ë¶„ì„ 
        if i < len(histories):
            story.append(Spacer(1, 0.2*inch))
            story.append(Paragraph('-' * 80, styles['Normal']))
            story.append(Spacer(1, 0.2*inch))
    
    doc.build(story)
    return filepath


# ========================================
# 6. RAG Helper (FAISS)
# ========================================
# RAG ê´€ë ¨ í•¨ìˆ˜ëŠ” ì‹œë®¬ë ˆì´í„°ì™€ ë¬´ê´€í•˜ë¯€ë¡œ ê¸°ì¡´ ì½”ë“œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

def load_documents(files) -> List[Document]:
    docs: List[Document] = []
    for f in files:
        name = f.name
        lower = name.lower()
        if lower.endswith(".pdf"):
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
            tmp.write(f.read())
            tmp.flush()
            tmp.close()
            loader = PyPDFLoader(tmp.name)
            file_docs = loader.load()
            for d in file_docs:
                d.metadata["source"] = name
            docs.extend(file_docs)
            try:
                os.remove(tmp.name)
            except OSError:
                pass
        elif lower.endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
        elif lower.endswith(".html") or lower.endswith(".htm"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
    return docs


def split_documents(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " ", ""],
    )
    return splitter.split_documents(docs)


def get_embedding_model():
    if get_api_key("openai"):
        try:
            return OpenAIEmbeddings(model="text-embedding-3-small")
        except:
            pass
    if get_api_key("gemini"):
        try:
            return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        except:
            pass
    return None


def get_embedding_function():
    """
    RAG ì„ë² ë”©ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ê²°ì •í•©ë‹ˆë‹¤.
    API í‚¤ ìœ íš¨ì„± ìˆœì„œ: OpenAI (ì‚¬ìš©ì ì„¤ì • ì‹œ) -> Gemini -> NVIDIA -> HuggingFace (fallback)
    API ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì´ë™í•˜ë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    # 1. OpenAI ì„ë² ë”© ì‹œë„ (ì‚¬ìš©ìê°€ ìœ íš¨í•œ í‚¤ë¥¼ ì„¤ì •í–ˆì„ ê²½ìš°)
    openai_key = get_api_key("openai")
    if openai_key:
        try:
            st.info("ğŸ”¹ RAG: OpenAI Embedding ì‚¬ìš© ì¤‘")
            return OpenAIEmbeddings(openai_api_key=openai_key)
        except Exception as e:
            st.warning(f"OpenAI ì„ë² ë”© ì‹¤íŒ¨ â†’ Geminië¡œ Fallback: {e}")

    # 2. Gemini ì„ë² ë”© ì‹œë„
    gemini_key = get_api_key("gemini")
    if IS_GEMINI_EMBEDDING_AVAILABLE and gemini_key:
        try:
            st.info("ğŸ”¹ RAG: Gemini Embedding ì‚¬ìš© ì¤‘")
            # â­ ìˆ˜ì •: ëª¨ë¸ ì´ë¦„ í˜•ì‹ì„ 'models/model-name'ìœ¼ë¡œ ìˆ˜ì •
            return GoogleGenerativeAIEmbeddings(google_api_key=gemini_key, model="models/text-embedding-004")
        except Exception as e:
            st.warning(f"Gemini ì„ë² ë”© ì‹¤íŒ¨ â†’ NVIDIAë¡œ Fallback: {e}")

    # 3. NVIDIA ì„ë² ë”© ì‹œë„
    nvidia_key = get_api_key("nvidia")
    if IS_NVIDIA_EMBEDDING_AVAILABLE and nvidia_key:
        try:
            st.info("ğŸ”¹ RAG: NVIDIA Embedding ì‚¬ìš© ì¤‘")
            # NIM ëª¨ë¸ ì‚¬ìš© (ì‹¤ì œ í‚¤ê°€ ìœ íš¨í•´ì•¼ í•¨)
            return NVIDIAEmbeddings(api_key=nvidia_key, model="ai-embed-qa-4")
        except Exception as e:
            st.warning(f"NVIDIA ì„ë² ë”© ì‹¤íŒ¨ â†’ HuggingFace Fallback: {e}")

    # 4. HuggingFace Embeddings (Local Fallback)
    try:
        st.info("ğŸ”¹ RAG: Local HuggingFace Embedding ì‚¬ìš© ì¤‘")
        # ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    except Exception as e:
        st.warning(f"ìµœì¢… Fallback ì„ë² ë”© ì‹¤íŒ¨: {e}")

    st.error("âŒ RAG ì„ë² ë”© ì‹¤íŒ¨: ì‚¬ìš© ê°€ëŠ¥í•œ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    return None


def build_rag_index(files):
    L = LANG[st.session_state.language]
    if not files: return None, 0

    # ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‹œë„í•˜ëŠ” ê³¼ì •ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ try-exceptë¡œ ê°ìŒ‰ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception as e:
        st.error(f"RAG ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, 0

    if embeddings is None:
        # ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ë„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŒì„ ì•Œë¦¼
        error_msg = L["rag_embed_error_none"]

        # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ êµ¬ì„± (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)
        if not get_api_key("openai"):
            error_msg += f"\n- {L['rag_embed_error_openai']}"
        if not get_api_key("gemini"):
            error_msg += f"\n- {L['rag_embed_error_gemini']}"
        if not get_api_key("nvidia"):
            error_msg += f"\n- {L['rag_embed_error_nvidia']}"

        st.error(error_msg)
        return None, 0

    # ì„ë² ë”© ê°ì²´ ì´ˆê¸°í™” ì„±ê³µ í›„, ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    docs = load_documents(files)
    if not docs: return None, 0

    chunks = split_documents(docs)
    if not chunks: return None, 0

    try:
        vectorstore = FAISS.from_documents(chunks, embeddings)
        # ì €ì¥
        vectorstore.save_local(RAG_INDEX_DIR)
    except Exception as e:
        # API ì¸ì¦ ì‹¤íŒ¨ ë“± ì‹¤ì œ API í˜¸ì¶œ ì˜¤ë¥˜ ì²˜ë¦¬
        st.error(f"RAG ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, 0

    return vectorstore, len(chunks)


def load_rag_index():
    # RAG ì¸ë±ìŠ¤ ë¡œë“œ ì‹œì—ë„ ìœ íš¨í•œ ì„ë² ë”© í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception:
        # get_embedding_function ë‚´ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ìŠ¤í‚µí•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
        return None

    if embeddings is None:
        return None

    try:
        # allow_dangerous_deserialization=TrueëŠ” í•„ìˆ˜
        vs = FAISS.load_local(RAG_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception:
        return None


def rag_answer(question: str, vectorstore: FAISS, lang_key: str) -> str:
    # RAG AnswerëŠ” LLM í´ë¼ì´ì–¸íŠ¸ ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    llm_client, info = get_llm_client()
    if llm_client is None:
        return LANG[lang_key]["simulation_no_key_warning"]

    # Langchain ChatOpenAI ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ promptë¥¼ ì§ì ‘ êµ¬ì„±
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    docs = retriever.get_relevant_documents(question)
    context = "\n\n".join(d.page_content[:1500] for d in docs)

    # â­ RAG ë‹¤êµ­ì–´ ì¸ì‹ ì˜¤ë¥˜ í•´ê²°: ë‹µë³€ ìƒì„± ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ ì–¸ì–´ë¡œ ì¼ê´€ë˜ê²Œ ë‹µí•˜ë„ë¡ ê°•ë ¥íˆ ì§€ì‹œ
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(lang_key, "English")

    prompt = (
            f"You are a helpful AI tutor. Answer the question using ONLY the provided context.\n"
            f"The answer MUST be STRICTLY in {lang_name}, which is the language of the question.\n"
            f"If you cannot find the answer in the context, say you don't know in {lang_name}.\n"
            f"Note: The context may be in a different language, but you must still answer in {lang_name}.\n\n"
            "Question:\n" + question + "\n\n"
                                       "Context:\n" + context + "\n\n"
                                                                f"Answer (in {lang_name}):"
    )
    return run_llm(prompt)


# ========================================
# 7. LSTM Helper (ê°„ë‹¨ Mock + ì‹œê°í™”)
# ========================================

def load_or_train_lstm():
    # ì‹¤ì œ LSTM ëŒ€ì‹  ëœë¤ + sin íŒŒí˜• ê¸°ë°˜ Mock
    np.random.seed(42)
    n_points = 50
    ts = 60 + 20 * np.sin(np.linspace(0, 4 * np.pi, n_points)) + np.random.normal(0, 5, n_points)
    ts = np.clip(ts, 50, 100).astype(np.float32)
    return ts





# ========================================
# 8. LLM (ChatOpenAI) for Simulator / Content
# (RAGì™€ ë™ì¼í•˜ê²Œ run_llmìœ¼ë¡œ í†µí•©)
# ========================================

# ConversationChain ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„
# st.session_state.simulator_memoryëŠ” ìœ ì§€í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

def get_chat_history_for_prompt(include_attachment=False):
    """ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ì¶”ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜ (ì±„íŒ…ìš©)"""
    history_str = ""
    for msg in st.session_state.simulator_messages:
        role = msg["role"]
        content = msg["content"]
        if role == "customer" or role == "customer_rebuttal":
            history_str += f"Customer: {content}\n"
        elif role == "agent_response":
            history_str += f"Agent: {content}\n"
        # supervisor ë©”ì‹œì§€ëŠ” LLMì— ì „ë‹¬í•˜ì§€ ì•Šì•„ ì—­í•  í˜¼ë™ ë°©ì§€
    return history_str


def generate_customer_reaction(current_lang_key: str, is_call: bool = False) -> str:
    """
    ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ì„ ìƒì„±í•˜ëŠ” LLM í˜¸ì¶œ (ì±„íŒ… ì „ìš©)
    **ìˆ˜ì • ì‚¬í•­:** ì—ì´ì „íŠ¸ ì •ë³´ ìš”ì²­ ì‹œ í•„ìˆ˜ ì •ë³´ (ì£¼ë¬¸ë²ˆí˜¸, eSIM, ìë…€ ë§Œ ë‚˜ì´, ì·¨ì†Œ ì‚¬ìœ ) ì œê³µ ì˜ë¬´ë¥¼ ê°•í™”í•¨.
    """
    history_text = get_chat_history_for_prompt()
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        # LLMì—ê²Œ ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì—ì´ì „íŠ¸ì—ê²Œ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    next_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

Read the following conversation and respond naturally in {lang_name}.

Conversation so far:
{history_text}

RULES:
1. You are only the customer. Do not write as the agent.
2. **[CRITICAL: Mandatory Information Submission for Problem Resolution]** If the agent requested any of the following critical information, you MUST provide it:
    - Order/Booking Number (e.g., ABC123, 123456)
    - eSIM related details (e.g., Host device compatibility, local status/location, time of activation)
    - Child-related product details (e.g., Child's Date of Birth or Current Age)
    - Exception/Refund Reason (e.g., flight cancellation/delay, illness, local natural disaster)
    - **If you are a difficult customer and the agent requests this information, you MUST still provide it, but you may express frustration or impatience while doing so.**
3. **[Crucial Rule for Repetition/New Inquiry]** After the agent has provided an attempt at a solution or answer:
    - If you are still confused or the problem is not fully solved, you MUST state the remaining confusion/problem clearly and briefly. DO NOT REPEAT THE INITIAL QUERY. Focus only on the unresolved aspect or the new inquiry.
4. **[CRITICAL: Solution Acknowledgment]** If the agent provided a clear and accurate solution/confirmation:
    - You MUST respond with appreciation and satisfaction, like "{L_local['customer_positive_response']}" or similar positive acknowledgment. This applies even if you are a difficult customer.
5. If the agent's LAST message was the closing confirmation: "{L_local['customer_closing_confirm']}"
    - If you have NO additional questions: You MUST reply with "{L_local['customer_no_more_inquiries']}".
    - If you DO have additional questions: You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS IMMEDIATELY. DO NOT just repeat that you have an additional question.
6. Do NOT repeat your initial message or previous responses unless necessary.
7. Output ONLY the customer's next message.
"""
    try:
        reaction = run_llm(next_prompt)

        # â­ LLMì´ ì‘ë‹µí–ˆì§€ë§Œ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš°, ê¸ì • ì¢…ë£Œ ë¬¸êµ¬ë¥¼ ë°˜í™˜
        if not reaction or len(reaction.strip()) < 5:
            print("LLM returned insufficient response. Using positive closing fallback.")
            return L_local['customer_positive_response']

        return reaction.strip()
    except Exception as e:
        # â­ LLM í˜¸ì¶œ ìì²´ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ (API í‚¤, í• ë‹¹ëŸ‰) ê¸ì • ì¢…ë£Œ ë¬¸êµ¬ë¥¼ ê°•ì œ ë°˜í™˜
        print(f"LLM Customer Reaction generation failed: {e}. Falling back to positive closing.")
        return L_local['customer_positive_response']  # ê°•ì œ ì•ˆì „ì¥ì¹˜


def summarize_history_with_ai(current_lang_key: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ë¥¼ ì •ë¦¬í•˜ì—¬ LLMì— ì „ë‹¬í•˜ê³  ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ë°›ëŠ” í•¨ìˆ˜."""
    # ì „í™” ë¡œê·¸ëŠ” 'phone_exchange' ì—­í• ì„ ê°€ì§€ê±°ë‚˜, 'initial_query'ì— í¬í•¨ë˜ì–´ ìˆìŒ

    # 1. ë¡œê·¸ ì¶”ì¶œ
    conversation_text = ""
    initial_query = st.session_state.get("call_initial_query", "N/A")
    if initial_query and initial_query != "N/A":
        conversation_text += f"Initial Query: {initial_query}\n"

    for msg in st.session_state.simulator_messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "phone_exchange":
            # phone_exchangeëŠ” "Agent: ... | Customer: ..." í˜•íƒœë¡œ ì´ë¯¸ ì •ë¦¬ë˜ì–´ ìˆìŒ
            conversation_text += f"{content}\n"

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    summary_prompt = f"""
You are an AI Analyst specialized in summarizing customer phone calls. 
Analyze the full conversation log below, identify the main issue, the steps taken by the agent, and the customer's sentiment.

Provide a concise, easy-to-read summary of the key exchange STRICTLY in {lang_name}.

--- Conversation Log ---
{conversation_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return "LLM Keyê°€ ì—†ì–´ ìš”ì•½ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤."

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}"


def generate_customer_reaction_for_call(current_lang_key: str, last_agent_response: str) -> str:
    """ì „í™” ì‹œë®¬ë ˆì´í„° ì „ìš© ê³ ê° ë°˜ì‘ ìƒì„± (ê°„ê²°í™”)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]  # â­ ìˆ˜ì •: í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ íŒ©

    # ì „í™” ì‹œë®¬ë ˆì´í„°ì—ì„œëŠ” ì „ì²´ simulator_messages ëŒ€ì‹ ,
    # st.session_state.call_initial_queryì™€ st.session_state.current_customer_audio_text
    # ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µ(ì „ì‚¬ í…ìŠ¤íŠ¸)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    history_text = (
        f"Initial Query: {st.session_state.call_initial_query}\n"
        f"Previous Customer Utterance: {st.session_state.current_customer_audio_text}\n"
        f"Agent's Last Response (Transcribed): {last_agent_response}"
    )

    call_prompt = f"""
You are now ROLEPLAYING as the CUSTOMER in a PHONE CALL.
Your goal is to respond naturally and briefly (like a real person on the phone) in {lang_name}.

Conversation context:
{history_text}

RULES:
1. Respond to the Agent's Last Response. Your reply MUST be short and conversational.
2. If the agent's response is satisfactory: Acknowledge and state you are fine, or ask for closing confirmation (e.g., "{L_local['customer_positive_response']}").
3. If the agent requested information or provided an unsatisfactory answer: Briefly state the remaining problem or provide the requested information.
4. **NEVER** output the agent's response, supervisor advice, or full context. Output ONLY the next customer utterance.
5. If the agent said the call is on hold, you MUST wait silently or acknowledge briefly. (Simulate this by just outputting a very short confirmation like "Okay.")
6. If the agent's last response was the closing confirmation: You MUST reply with "{L_local['customer_no_more_inquiries']}" or "{L_local['customer_has_additional_inquiries']}" (followed by the new query).

Customer's next brief spoken response:
"""
    try:
        reaction = run_llm(call_prompt)
        return reaction.strip()
    except Exception as e:
        return f"âŒ ê³ ê° ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}"


def summarize_history_for_call(call_logs: List[Dict[str, str]], initial_query: str, current_lang_key: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ë³¸ì„ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ë¡œê·¸ ì¬êµ¬ì„± (phone_exchange ì—­í• ë§Œ ì‚¬ìš©)
    full_log_text = f"--- Initial Customer Query ---\nCustomer: {initial_query}\n"
    for log in call_logs:
        if log["role"] == "phone_exchange":
            full_log_text += f"{log['content']}\n"
        elif log["role"] == "agent" and "content" in log:
            # ìµœì´ˆ ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ì€ ì—¬ê¸°ì— í¬í•¨
            full_log_text += f"Agent (Greeting): {log['content']}\n"

    summary_prompt = f"""
You are an AI Supervisor. Analyze the following telephone support conversation log.
Provide a concise, neutral summary of the key issue, the steps taken by the agent, and the final outcome.
The summary MUST be STRICTLY in {lang_name}.

--- Conversation Log ---
{full_log_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key is missing. Cannot generate summary. Log length: {len(full_log_text.splitlines())}"

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ Summary Generation Error: {e}"


def generate_customer_closing_response(current_lang_key: str) -> str:
    """ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ê³ ê°ì˜ ìµœì¢… ë‹µë³€ ìƒì„± (ì±„íŒ…ìš©)"""
    history_text = get_chat_history_for_prompt()
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]  # â­ ìˆ˜ì •: í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ íŒ©

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ì˜ ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ì¸ì§€ í™•ì¸ (í”„ë¡¬í”„íŠ¸ì— í¬í•¨)
    closing_msg = L_local['customer_closing_confirm']

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    final_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

The agent's final message was the closing confirmation: "{closing_msg}".
You MUST respond to this confirmation based on the overall conversation.

Conversation history:
{history_text}

RULES:
1. If the conversation seems resolved and you have NO additional questions:
    - You MUST reply with "{L_local['customer_no_more_inquiries']}".
2. If the conversation is NOT fully resolved and you DO have additional questions (or the agent provided a cancellation denial that you want to appeal):
    - You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS. DO NOT just repeat that you have an additional question.
3. Your reply MUST be ONLY one of the two options above, in {lang_name}.
4. Output ONLY the customer's next message (must be one of the two rule options).
"""
    try:
        reaction = run_llm(final_prompt)
        # LLMì˜ ì¶œë ¥ì´ ê·œì¹™ì„ ë”°ë¥´ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê°•ì œ ì ìš©
        reaction_text = reaction.strip()
        # "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤"ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìƒì„¸ ë‚´ìš© í¬í•¨ ê°€ì •)
        if L_local['customer_no_more_inquiries'] in reaction_text:
            return L_local['customer_no_more_inquiries']
        elif L_local['customer_has_additional_inquiries'] in reaction_text:
            return reaction_text
        else:
            # LLMì´ ê·œì¹™ì„ ì–´ê²¼ì„ ê²½ìš°, "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆë‹¤"ê³  ê°€ì •í•˜ê³  ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ë„˜ê¹€
            return L_local['customer_has_additional_inquiries']
    except Exception as e:
        st.error(f"ê³ ê° ìµœì¢… ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}")
        return L_local['customer_has_additional_inquiries']  # ì˜¤ë¥˜ ì‹œ ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ë„


# ----------------------------------------
# Initial Advice/Draft Generation (ì´ê´€ í›„ ì¬ì‚¬ìš©) (ìš”ì²­ 4 ë°˜ì˜)
# ----------------------------------------
def generate_agent_first_greeting(lang_key: str, initial_query: str) -> str:
    """ì „í™” í†µí™” ì‹œì‘ ì‹œ ì—ì´ì „íŠ¸ì˜ ì²« ì¸ì‚¬ë§ì„ ìƒì„± (ì„ì‹œ í•¨ìˆ˜)"""
    L_local = LANG[lang_key]
    # ë¬¸ì˜ ë‚´ìš©ì˜ ì²« 10ìë§Œ ì‚¬ìš© (too long)
    topic = initial_query.strip()[:15].replace('\n', ' ')
    if len(initial_query.strip()) > 15:
        topic += "..."

    if lang_key == 'ko':
        return f"ì•ˆë…•í•˜ì„¸ìš”, {topic} ê´€ë ¨ ë¬¸ì˜ ì£¼ì…¨ì£ ? ìƒë‹´ì› 000ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    elif lang_key == 'en':
        return f"Hello, thank you for calling. I see you're calling about {topic}. My name is 000. How may I help you today?"
    elif lang_key == 'ja':
        return f"ãŠé›»è©±ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚{topic}ã®ä»¶ã§ã™ã­ã€‚æ‹…å½“ã®000ã¨ç”³ã—ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹?"
    return "Hello, how may I help you?"


def analyze_customer_profile(customer_query: str, current_lang_key: str) -> Dict[str, Any]:
    """ì‹ ê·œ ê³ ê°ì˜ ë¬¸ì˜ì‚¬í•­ê³¼ ë§íˆ¬ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ê°ì„±í–¥ ì ìˆ˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì‚° (ìš”ì²­ 4)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    analysis_prompt = f"""
You are an AI analyst analyzing a customer's inquiry to determine their profile and sentiment.

Analyze the following customer inquiry and provide a structured analysis in JSON format (ONLY JSON, no markdown).

Analyze:
1. Customer sentiment score (0-100, where 0=very negative/angry, 50=neutral, 100=very positive/happy)
2. Communication style (formal/casual, brief/detailed, polite/direct)
3. Urgency level (low/medium/high)
4. Customer type prediction (normal/difficult/very_dissatisfied)
5. Language and cultural hints (if any)
6. Key concerns or pain points

Output format (JSON only):
{{
  "sentiment_score": 45,
  "communication_style": "brief, direct, slightly frustrated",
  "urgency_level": "high",
  "predicted_customer_type": "difficult",
  "cultural_hints": "unknown",
  "key_concerns": ["issue 1", "issue 2"],
  "tone_analysis": "brief description of tone"
}}

Customer Inquiry:
{customer_query}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        return {
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": "Unable to analyze"
        }

    try:
        analysis_text = run_llm(analysis_prompt).strip()
        # JSON ì¶”ì¶œ
        if "```json" in analysis_text:
            analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
        elif "```" in analysis_text:
            analysis_text = analysis_text.split("```")[1].split("```")[0].strip()

        import json
        analysis_data = json.loads(analysis_text)
        return analysis_data
    except Exception as e:
        return {
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": f"Analysis error: {str(e)}"
        }


def find_similar_cases(customer_query: str, customer_profile: Dict[str, Any], current_lang_key: str,
                       limit: int = 5) -> List[Dict[str, Any]]:
    """ì €ì¥ëœ ìš”ì•½ ë°ì´í„°ì—ì„œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì•„ ë°˜í™˜ (ìš”ì²­ 4)"""
    histories = load_simulation_histories_local(current_lang_key)

    if not histories:
        return []

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
           and not h.get("is_call", False)  # ì „í™” ì´ë ¥ ì œì™¸
    ]

    if not cases_with_summary:
        return []

    # ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ + ì ìˆ˜ ìœ ì‚¬ë„)
    similar_cases = []
    query_lower = customer_query.lower()
    customer_sentiment = customer_profile.get("sentiment_score", 50)
    customer_style = customer_profile.get("communication_style", "")

    for case in cases_with_summary:
        summary = case.get("summary", {})
        main_inquiry = summary.get("main_inquiry", "").lower()
        case_sentiment = summary.get("customer_sentiment_score", 50)
        case_satisfaction = summary.get("customer_satisfaction_score", 50)

        # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        similarity_score = 0

        # 1. ë¬¸ì˜ ë‚´ìš© ìœ ì‚¬ë„ (í‚¤ì›Œë“œ ë§¤ì¹­)
        query_words = set(query_lower.split())
        inquiry_words = set(main_inquiry.split())
        if query_words and inquiry_words:
            word_overlap = len(query_words & inquiry_words) / len(query_words | inquiry_words)
            similarity_score += word_overlap * 40

        # 2. ê°ì • ì ìˆ˜ ìœ ì‚¬ë„
        sentiment_diff = abs(customer_sentiment - case_sentiment)
        sentiment_similarity = max(0, 1 - (sentiment_diff / 100)) * 30
        similarity_score += sentiment_similarity

        # 3. ë§Œì¡±ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì¼€ì´ìŠ¤)
        satisfaction_bonus = (case_satisfaction / 100) * 30
        similarity_score += satisfaction_bonus

        if similarity_score > 30:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            similar_cases.append({
                "case": case,
                "similarity_score": similarity_score,
                "summary": summary
            })

    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similar_cases.sort(key=lambda x: x["similarity_score"], reverse=True)
    return similar_cases[:limit]


def visualize_customer_profile_scores(customer_profile: Dict[str, Any], current_lang_key: str):
    """ê³ ê° í”„ë¡œí•„ ì ìˆ˜ë¥¼ ì‹œê°í™” (ê°ì • ì ìˆ˜, ê¸´ê¸‰ë„)"""
    if not IS_PLOTLY_AVAILABLE:
        return None

    L = LANG[current_lang_key]

    sentiment_score = customer_profile.get("sentiment_score", 50)
    urgency_map = {"low": 25, "medium": 50, "high": 75}
    urgency_level = customer_profile.get("urgency_level", "medium")
    urgency_score = urgency_map.get(urgency_level.lower(), 50)

    # ê²Œì´ì§€ ì°¨íŠ¸ ìƒì„±
    fig = make_subplots(
        rows=1, cols=2,
        specs=[[{"type": "indicator"}, {"type": "indicator"}]],
        subplot_titles=(
            L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜"),
            L.get("urgency_score_label", "ê¸´ê¸‰ë„ ì ìˆ˜")
        )
    )

    # ê°ì • ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number+delta",
            value=sentiment_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("sentiment_score_label", "ê°ì • ì ìˆ˜")},
            delta={'reference': 50},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 33], 'color': "lightgray"},
                    {'range': [33, 66], 'color': "gray"},
                    {'range': [66, 100], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 90
                }
            }
        ),
        row=1, col=1
    )

    # ê¸´ê¸‰ë„ ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number",
            value=urgency_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("urgency_score_label", "ê¸´ê¸‰ë„")},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkred"},
                'steps': [
                    {'range': [0, 50], 'color': "lightgreen"},
                    {'range': [50, 75], 'color': "yellow"},
                    {'range': [75, 100], 'color': "lightcoral"}
                ],
            }
        ),
        row=1, col=2
    )

    fig.update_layout(height=300, margin=dict(l=20, r=20, t=50, b=20))
    return fig


def visualize_similarity_cases(similar_cases: List[Dict[str, Any]], current_lang_key: str):
    """ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ì²œì„ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not similar_cases:
        return None

    L = LANG[current_lang_key]

    case_labels = []
    similarity_scores = []
    sentiment_scores = []
    satisfaction_scores = []

    for idx, similar_case in enumerate(similar_cases, 1):
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]
        case_labels.append(f"Case {idx}")
        similarity_scores.append(similarity)
        sentiment_scores.append(summary.get("customer_sentiment_score", 50))
        satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))

    # ìœ ì‚¬ë„ ì°¨íŠ¸
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=(
            L.get("similarity_chart_title", "ìœ ì‚¬ ì¼€ì´ìŠ¤ ìœ ì‚¬ë„"),
            L.get("scores_comparison_title",
                  "ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ")
        ),
        vertical_spacing=0.15
    )

    # ìœ ì‚¬ë„ ë°” ì°¨íŠ¸
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=similarity_scores,
            name=L.get("similarity_score_label", "ìœ ì‚¬ë„"),
            marker_color='lightblue',
            text=[f"{s:.1f}%" for s in similarity_scores],
            textposition='outside'
        ),
        row=1, col=1
    )

    # ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=sentiment_scores,
            name=L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
            marker_color='lightcoral'
        ),
        row=2, col=1
    )

    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=satisfaction_scores,
            name=L.get("satisfaction_score_label", "ë§Œì¡±ë„"),
            marker_color='lightgreen'
        ),
        row=2, col=1
    )

    fig.update_layout(
        height=600,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20),
        barmode='group'
    )
    fig.update_yaxes(title_text="ì ìˆ˜", row=2, col=1)
    fig.update_yaxes(title_text="ìœ ì‚¬ë„ (%)", row=1, col=1)

    return fig


def visualize_case_trends(histories: List[Dict[str, Any]], current_lang_key: str):
    """ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ íŠ¸ë Œë“œë¥¼ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not histories:
        return None

    L = LANG[current_lang_key]

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
    ]

    if not cases_with_summary:
        return None

    # ë‚ ì§œë³„ë¡œ ì •ë ¬
    cases_with_summary.sort(key=lambda x: x.get("timestamp", ""))

    dates = []
    sentiment_scores = []
    satisfaction_scores = []

    for case in cases_with_summary:
        summary = case.get("summary", {})
        timestamp = case.get("timestamp", "")
        try:
            dt = datetime.fromisoformat(timestamp)
            dates.append(dt)
            sentiment_scores.append(summary.get("customer_sentiment_score", 50))
            satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))
        except Exception:
            continue

    if not dates:
        return None

    # íŠ¸ë Œë“œ ë¼ì¸ ì°¨íŠ¸
    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=dates,
        y=sentiment_scores,
        mode='lines+markers',
        name=L.get("sentiment_trend_label", "ê°ì • ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightcoral', width=2),
        marker=dict(size=6)
    ))

    fig.add_trace(go.Scatter(
        x=dates,
        y=satisfaction_scores,
        mode='lines+markers',
        name=L.get("satisfaction_trend_label", "ë§Œì¡±ë„ ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightgreen', width=2),
        marker=dict(size=6)
    ))

    fig.update_layout(
        title=L.get("case_trends_title", "ê³¼ê±° ì¼€ì´ìŠ¤ ì ìˆ˜ ì¶”ì´"),
        xaxis_title=L.get("date_label", "ë‚ ì§œ"),
        yaxis_title=L.get("score_label", "ì ìˆ˜ (0-100)"),
        height=400,
        hovermode='x unified',
        legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
    )

    return fig


def visualize_customer_characteristics(summary: Dict[str, Any], current_lang_key: str):
    """ê³ ê° íŠ¹ì„±ì„ ì‹œê°í™” (ì–¸ì–´, ë¬¸í™”ê¶Œ, ì§€ì—­ ë“±)"""
    if not IS_PLOTLY_AVAILABLE or not summary:
        return None

    L = LANG[current_lang_key]

    characteristics = summary.get("customer_characteristics", {})
    privacy_info = summary.get("privacy_info", {})

    # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
    labels = []
    values = []

    # ì–¸ì–´ ì •ë³´
    language = characteristics.get("language", "unknown")
    if language != "unknown":
        labels.append(L.get("language_label", "ì–¸ì–´"))
        lang_map = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}
        values.append(lang_map.get(language, language))

    # ê°œì¸ì •ë³´ ì œê³µ ì—¬ë¶€
    if privacy_info.get("has_email"):
        labels.append(L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ"))
        values.append("Yes")
    if privacy_info.get("has_phone"):
        labels.append(L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ"))
        values.append("Yes")

    # ì§€ì—­ ì •ë³´
    region = privacy_info.get("region_hint", characteristics.get("region", "unknown"))
    if region != "unknown":
        labels.append(L.get("region_label", "ì§€ì—­"))
        values.append(region)

    if not labels:
        return None

    # íŒŒì´ ì°¨íŠ¸
    fig = go.Figure(data=[go.Pie(
        labels=labels,
        values=[1] * len(labels),
        hole=0.4,
        marker_colors=px.colors.qualitative.Set3[:len(labels)]
    )])

    fig.update_layout(
        title=L.get("customer_characteristics_title",
                    "ê³ ê° íŠ¹ì„± ë¶„í¬"),
        height=300,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20)
    )

    return fig


def generate_guideline_from_past_cases(customer_query: str, customer_profile: Dict[str, Any],
                                       similar_cases: List[Dict[str, Any]], current_lang_key: str) -> str:
    """ê³¼ê±° ìœ ì‚¬ ì¼€ì´ìŠ¤ì˜ ì„±ê³µì ì¸ í•´ê²° ë°©ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¼ì¸ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    if not similar_cases:
        return ""

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìš”ì•½
    past_cases_text = ""
    for idx, similar_case in enumerate(similar_cases, 1):
        case = similar_case["case"]
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]

        past_cases_text += f"""
[Case {idx}] (Similarity: {similarity:.1f}%)
- Inquiry: {summary.get('main_inquiry', 'N/A')}
- Customer Sentiment: {summary.get('customer_sentiment_score', 50)}/100
- Customer Satisfaction: {summary.get('customer_satisfaction_score', 50)}/100
- Key Responses: {', '.join(summary.get('key_responses', [])[:3])}
- Summary: {summary.get('summary', 'N/A')[:200]}
"""

    guideline_prompt = f"""
You are an AI Customer Support Supervisor analyzing past successful cases to provide guidance.

Based on the following similar past cases and their successful resolution strategies, provide actionable guidelines for handling the current customer inquiry.

Current Customer Inquiry:
{customer_query}

Current Customer Profile:
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}

Similar Past Cases (Successful Resolutions):
{past_cases_text}

Provide a concise guideline in {lang_name} that:
1. Identifies what worked well in similar past cases
2. Suggests specific approaches based on successful patterns
3. Warns about potential pitfalls based on past experiences
4. Recommends response strategies that led to high customer satisfaction

Guideline (in {lang_name}):
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        guideline = run_llm(guideline_prompt).strip()
        return guideline
    except Exception as e:
        return f"ê°€ì´ë“œë¼ì¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"


def _generate_initial_advice(customer_query, customer_type_display, customer_email, customer_phone, current_lang_key,
                             customer_attachment_file):
    """Supervisor ê°€ì´ë“œë¼ì¸ê³¼ ì´ˆì•ˆì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì €ì¥ëœ ë°ì´í„° í™œìš©)"""
    L = LANG[current_lang_key]
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    contact_info_block = ""
    if customer_email or customer_phone:
        contact_info_block = (
            f"\n\n[Customer contact info for reference (DO NOT use these in your reply draft!)]"
            f"\n- Email: {customer_email or 'N/A'}"
            f"\n- Phone: {customer_phone or 'N/A'}"
        )

    attachment_block = ""
    if customer_attachment_file:
        file_name = customer_attachment_file.name
        attachment_block = f"\n\n[ATTACHMENT NOTE]: {L['attachment_info_llm'].format(filename=file_name)}"

    # ê³ ê° í”„ë¡œí•„ ë¶„ì„
    customer_profile = analyze_customer_profile(customer_query, current_lang_key)

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì°¾ê¸°
    similar_cases = find_similar_cases(customer_query, customer_profile, current_lang_key, limit=5)

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ìƒì„±
    past_cases_guideline = ""
    if similar_cases:
        past_cases_guideline = generate_guideline_from_past_cases(
            customer_query, customer_profile, similar_cases, current_lang_key
        )

    # ê³ ê° í”„ë¡œí•„ ì •ë³´
    profile_block = f"""
[Customer Profile Analysis]
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency Level: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}
- Key Concerns: {', '.join(customer_profile.get('key_concerns', []))}
- Tone: {customer_profile.get('tone_analysis', 'unknown')}
"""

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ë¸”ë¡
    past_cases_block = ""
    if past_cases_guideline:
        past_cases_block = f"""
[Guidelines Based on {len(similar_cases)} Similar Past Cases]
{past_cases_guideline}
"""
    elif similar_cases:
        past_cases_block = f"""
[Note: Found {len(similar_cases)} similar past cases, but unable to generate detailed guidelines.
Consider reviewing past cases manually for patterns.]
"""

    # Output ALL text (guidelines and draft) STRICTLY in {lang_name}. <--- ê°•ë ¥í•œ ì–¸ì–´ ê°•ì œ ì§€ì‹œ
    initial_prompt = f"""
Output ALL text (guidelines and draft) STRICTLY in {lang_name}.

You are an AI Customer Support Supervisor. Your role is to analyze the following customer inquiry
from a **{st.session_state.customer_type_sim_select}** and provide:

1) A detailed **response guideline for the human agent** (step-by-step).
2) A **ready-to-send draft reply** in {lang_name}.

[FORMAT]
- Use the exact markdown headers:
  - "### {L['simulation_advice_header']}"
  - "### {L['simulation_draft_header']}"

[CRITICAL GUIDELINE RULES]
1. **Initial Information Collection (Req 3):** The first step in the guideline MUST be to request the necessary initial diagnostic information (e.g., device compatibility, local status/location, order number) BEFORE attempting to troubleshoot or solve the problem.
2. **Empathy for Difficult Customers (Req 5):** If the customer type is 'Difficult Customer' or 'Highly Dissatisfied Customer', the guideline MUST emphasize extreme politeness, empathy, and apologies, even if the policy (e.g., no refund) must be enforced.
3. **24-48 Hour Follow-up (Req 6):** If the issue cannot be solved immediately or requires confirmation from a local partner/supervisor, the guideline MUST state the procedure:
   - Acknowledge the issue.
   - Inform the customer they will receive a definite answer within 24 or 48 hours.
   - Request the customer's email or phone number for follow-up contact. (Use provided contact info if available)
4. **Past Cases Learning:** If past cases guidelines are provided, incorporate successful strategies from those cases into your recommendations.

Customer Inquiry:
{customer_query}
{contact_info_block}
{attachment_block}
{profile_block}
{past_cases_block}
"""
    if not st.session_state.is_llm_ready:
        mock_text = (
            f"### {L['simulation_advice_header']}\n\n"
            f"- (Mock) {st.session_state.customer_type_sim_select} ìœ í˜• ê³ ê° ì‘ëŒ€ ê°€ì´ë“œì…ë‹ˆë‹¤. (ìš”ì²­ 3, 5, 6 ë°˜ì˜)\n\n"
            f"### {L['simulation_draft_header']}\n\n"
            f"(Mock) ì—ì´ì „íŠ¸ ì‘ëŒ€ ì´ˆì•ˆì´ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤ã€‚\n\n"
        )
        return mock_text
    else:
        with st.spinner(L["response_generating"]):
            try:
                return run_llm(initial_prompt)
            except Exception as e:
                st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return f"âŒ AI Advice Generation Error: {e}"


# ========================================
# 9. ì‚¬ì´ë“œë°”
# ========================================

with st.sidebar:
    # â­ íšŒì‚¬ë³„ ì–¸ì–´ ìš°ì„ ìˆœìœ„ ì„¤ì •
    st.subheader("ğŸŒ íšŒì‚¬ë³„ ì–¸ì–´ ì„¤ì •")
    
    if "company_language_priority" not in st.session_state:
        st.session_state.company_language_priority = {
            "default": ["ko", "en", "ja"],
            "companies": {}
        }
    
    # íšŒì‚¬ëª… ì…ë ¥ ë° ì¶”ê°€
    with st.expander("íšŒì‚¬ ì¶”ê°€/ê´€ë¦¬"):
        new_company = st.text_input("íšŒì‚¬ëª…", key="new_company_input", placeholder="ì˜ˆ: Company A")
        if st.button("íšŒì‚¬ ì¶”ê°€", key="add_company_btn") and new_company:
            if new_company not in st.session_state.company_language_priority["companies"]:
                st.session_state.company_language_priority["companies"][new_company] = ["ko", "en", "ja"]
                st.success(f"{new_company} ì¶”ê°€ë¨")
                # st.rerun()
    
    # í˜„ì¬ íšŒì‚¬ ì„ íƒ
    company_list = list(st.session_state.company_language_priority["companies"].keys())
    if company_list:
        selected_company = st.selectbox(
            "íšŒì‚¬ ì„ íƒ",
            options=["ê¸°ë³¸ ì„¤ì •"] + company_list,
            key="selected_company"
        )
        
        if selected_company != "ê¸°ë³¸ ì„¤ì •":
            # íšŒì‚¬ë³„ ì–¸ì–´ ìš°ì„ ìˆœìœ„ ì„¤ì • (ê°„ë‹¨í•œ ë“œë˜ê·¸ ì•¤ ë“œë¡­ ëŒ€ì‹  ìˆœì„œ ì„ íƒ)
            st.write(f"**{selected_company}** ì–¸ì–´ ìš°ì„ ìˆœìœ„:")
            current_priority = st.session_state.company_language_priority["companies"].get(selected_company, ["ko", "en", "ja"])
            
            # ì–¸ì–´ ìˆœì„œë¥¼ multiselectë¡œ ì„¤ì • (ì²« ë²ˆì§¸ê°€ ìµœìš°ì„ )
            lang_order = st.multiselect(
                "ì–¸ì–´ ìš°ì„ ìˆœìœ„ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)",
                options=["ko", "en", "ja"],
                default=current_priority,
                format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
                key=f"lang_order_{selected_company}"
            )
            
            if st.button("ì €ì¥", key=f"save_priority_{selected_company}"):
                if len(lang_order) == 3:
                    st.session_state.company_language_priority["companies"][selected_company] = lang_order
                    st.success("ì–¸ì–´ ìš°ì„ ìˆœìœ„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    # st.rerun()
                else:
                    st.warning("ëª¨ë“  ì–¸ì–´ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        selected_company = None
    
    st.markdown("---")
    
    # ì–¸ì–´ ì„ íƒ (íšŒì‚¬ë³„ ìš°ì„ ìˆœìœ„ ë°˜ì˜)
    # â­ L ë³€ìˆ˜ë¥¼ ë¨¼ì € ì •ì˜ (ê¸°ë³¸ ì–¸ì–´ë¡œ)
    if "language" not in st.session_state:
        st.session_state.language = "ko"
    L = LANG[st.session_state.language]
    
    if selected_company and selected_company != "ê¸°ë³¸ ì„¤ì •" and selected_company in st.session_state.company_language_priority["companies"]:
        lang_priority = st.session_state.company_language_priority["companies"][selected_company]
    else:
        lang_priority = st.session_state.company_language_priority["default"]
    
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=lang_priority,
        index=lang_priority.index(st.session_state.language) if st.session_state.language in lang_priority else 0,
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
    )

    # ğŸ”¹ ì–¸ì–´ ë³€ê²½ ê°ì§€
    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        # ì±„íŒ…/ì „í™” ê³µí†µ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.simulator_messages = []
        # â­ ì•ˆì „í•œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        try:
            if hasattr(st.session_state, 'simulator_memory') and st.session_state.simulator_memory is not None:
                st.session_state.simulator_memory.clear()
        except Exception:
            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ ìƒì„±
            try:
                st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history")
            except Exception:
                pass  # ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
        st.session_state.initial_advice_provided = False
        st.session_state.is_chat_ended = False
        # â­ ìˆ˜ì •: ìœ„ì ¯ì´ ìƒì„±ëœ í›„ì—ëŠ” session_stateë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í”Œë˜ê·¸ ì‚¬ìš©
        st.session_state.reset_agent_response_area = True
        st.session_state.customer_query_text_area = ""
        st.session_state.last_transcript = ""
        st.session_state.sim_audio_bytes = None
        st.session_state.sim_stage = "WAIT_FIRST_QUERY"
        st.session_state.customer_attachment_file = []  # ì–¸ì–´ ë³€ê²½ ì‹œ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
        st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
        # ì „í™” ì‹œë®¬ë ˆì´í„° ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.call_sim_stage = "WAITING_CALL"
        st.session_state.call_sim_mode = "INBOUND"
        st.session_state.is_on_hold = False
        st.session_state.total_hold_duration = timedelta(0)
        st.session_state.hold_start_time = None
        st.session_state.current_customer_audio_text = ""
        st.session_state.current_agent_audio_text = ""
        st.session_state.agent_response_input_box_widget_call = ""
        st.session_state.call_initial_query = ""
        # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.sim_call_outbound_summary = ""
        st.session_state.sim_call_outbound_target = None
        # â­ ì–¸ì–´ ë³€ê²½ ì‹œ ì¬ì‹¤í–‰ - ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•´ í”Œë˜ê·¸ ì‚¬ìš©
        if "language_changed" not in st.session_state or not st.session_state.language_changed:
            st.session_state.language_changed = True
            # st.rerun()
        else:
            # ì´ë¯¸ í•œ ë²ˆ ì¬ì‹¤í–‰í–ˆìœ¼ë©´ í”Œë˜ê·¸ ì´ˆê¸°í™”
            st.session_state.language_changed = False

    L = LANG[st.session_state.language]

    st.title(L["sidebar_title"])
    st.markdown("---")

    # â­ API Key ì„¤ì • ì„¹ì…˜ ì¶”ê°€
    st.subheader("ğŸ”‘ API Key ì„¤ì •")
    
    # LLM ì„ íƒ
    llm_options = {
        "openai_gpt4": "OpenAI GPT-4",
        "openai_gpt35": "OpenAI GPT-3.5",
        "gemini_pro": "Google Gemini Pro",
        "gemini_flash": "Google Gemini Flash",
        "claude": "Anthropic Claude",
        "groq": "Groq",
        "nvidia": "NVIDIA NIM"
    }
    
    current_llm = st.session_state.get("selected_llm", "openai_gpt4")
    selected_llm = st.selectbox(
        "LLM ëª¨ë¸ ì„ íƒ",
        options=list(llm_options.keys()),
        format_func=lambda x: llm_options[x],
        index=list(llm_options.keys()).index(current_llm) if current_llm in llm_options else 0,
        key="sidebar_llm_select"
    )
    if selected_llm != current_llm:
        st.session_state.selected_llm = selected_llm
        # st.rerun()
    
    # API Key ë§¤í•‘
    api_key_map = {
        "openai_gpt4": "openai",
        "openai_gpt35": "openai",
        "gemini_pro": "gemini",
        "gemini_flash": "gemini",
        "claude": "claude",
        "groq": "groq",
        "nvidia": "nvidia"
    }
    
    api_name = api_key_map.get(selected_llm, "openai")
    api_config = SUPPORTED_APIS.get(api_name, {})
    
    if api_config:
        # í˜„ì¬ API Key í™•ì¸
        current_key = get_api_key(api_name)
        if not current_key:
            # ìˆ˜ë™ ì…ë ¥ í•„ë“œ
            session_key = api_config.get("session_key", "")
            manual_key = st.text_input(
                api_config.get("label", "API Key"),
                value=st.session_state.get(session_key, ""),
                type="password",
                placeholder=api_config.get("placeholder", "API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”"),
                key=f"manual_api_key_{selected_llm}"
            )
            if manual_key and manual_key != st.session_state.get(session_key, ""):
                st.session_state[session_key] = manual_key
                # st.rerun()
        else:
            st.success(f"âœ… {api_config.get('label', 'API Key')} ì„¤ì •ë¨")
    
    st.markdown("---")

    st.subheader("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœ")
    if st.session_state.llm_init_error_msg:
        st.error(st.session_state.llm_init_error_msg)
    elif st.session_state.is_llm_ready:
        st.success("âœ… LLM í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
    else:
        st.info("ğŸ’¡ API KeyëŠ” í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” Streamlit Secretsì—ì„œ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.")

    if st.session_state.openai_client:
        st.success("âœ… OpenAI TTS/Whisper í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
    else:
        st.warning(L["openai_missing"])

    st.markdown("---")

    # â­ ê¸°ëŠ¥ ì„ íƒ - ê¸°ë³¸ê°’ì„ AI ì±— ì‹œë®¬ë ˆì´í„°ë¡œ ì„¤ì •
    if "feature_selection" not in st.session_state:
        st.session_state.feature_selection = L["sim_tab_chat_email"]

    # â­ í•µì‹¬ ê¸°ëŠ¥ê³¼ ë”ë³´ê¸° ê¸°ëŠ¥ ë¶„ë¦¬ (RAGëŠ” ë”ë³´ê¸°ë¡œ ì´ë™)
    core_features = [L["sim_tab_chat_email"], L["sim_tab_phone"]]
    other_features = [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["voice_rec_header"]]
    
    # ëª¨ë“  ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•© (í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥í•˜ë„ë¡)
    all_features = core_features + other_features
    
    # í˜„ì¬ ì„ íƒëœ ê¸°ëŠ¥
    current_selection = st.session_state.get("feature_selection", L["sim_tab_chat_email"])
    
    # í˜„ì¬ ì„ íƒì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    try:
        current_index = all_features.index(current_selection) if current_selection in all_features else 0
    except (ValueError, AttributeError):
        current_index = 0
    
    # í•µì‹¬ ê¸°ëŠ¥ í‘œì‹œ (ì‹œê°ì  êµ¬ë¶„)
    st.write("**í•µì‹¬ ê¸°ëŠ¥**")
    for i, feature in enumerate(core_features):
        if i == current_index and current_selection in core_features:
            st.write(f"âœ“ {feature}")
        else:
            st.write(f"â—‹ {feature}")
    
    # ë”ë³´ê¸° ê¸°ëŠ¥
    with st.expander("ë”ë³´ê¸° ê¸°ëŠ¥", expanded=(current_selection in other_features)):
        for i, feature in enumerate(other_features, start=len(core_features)):
            if i == current_index and current_selection in other_features:
                st.write(f"âœ“ {feature}")
            else:
                st.write(f"â—‹ {feature}")
    
    # â­ í•˜ë‚˜ì˜ í†µí•©ëœ ì„ íƒ ë¡œì§ (í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥)
    selected_feature = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        all_features,
        index=current_index,
        key="unified_feature_selection",
        label_visibility="hidden"
    )
    
    # ì„ íƒëœ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸
    if selected_feature != current_selection:
        st.session_state.feature_selection = selected_feature
    
    feature_selection = st.session_state.get("feature_selection", L["sim_tab_chat_email"])

# ë©”ì¸ íƒ€ì´í‹€
# â­ L ë³€ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì‚¬ì´ë“œë°”ì—ì„œ ì´ë¯¸ ì •ì˜ë¨)
if "language" not in st.session_state:
    st.session_state.language = "ko"
L = LANG[st.session_state.language]

st.title(L["title"])

# â­ í”„ë¡œì íŠ¸ ëª©í‘œ í•œì¤„ ì •ë¦¬
st.info("ğŸ¯ **í”„ë¡œì íŠ¸ ëª©í‘œ**: CS ì„¼í„° ì§ì› êµìœ¡ìš© AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° - ê¶ê·¹ì ìœ¼ë¡œ CS ì—…ë¬´ ì‹œìŠ¤í…œ ëŒ€ì²´ì¬")

# ========================================
# 10. ê¸°ëŠ¥ë³„ í˜ì´ì§€
# ========================================

# -------------------- Voice Record Tab --------------------
if feature_selection == L["voice_rec_header"]:
    # ... (ê¸°ì¡´ ìŒì„± ê¸°ë¡ íƒ­ ë¡œì§ ìœ ì§€)
    st.header(L["voice_rec_header"])
    st.caption(L["record_help"])

    col_rec, col_list = st.columns([1, 1])

    # ë…¹ìŒ/ì—…ë¡œë“œ + ì „ì‚¬ + ì €ì¥
    with col_rec:
        st.subheader(L["rec_header"])
        audio_file = st.file_uploader(
            L["uploaded_file"],
            type=["wav", "mp3", "m4a", "webm", "ogg"],
            key="voice_rec_uploader",
        )
        audio_bytes = None
        audio_mime = "audio/webm"

        if audio_file is not None:
            audio_bytes = audio_file.getvalue()
            audio_mime = audio_file.type or "audio/webm"

        # ì¬ìƒ
        # Streamlit ë¬¸ì„œ: bytes, íŒŒì¼ ê²½ë¡œ, URL ëª¨ë‘ ì§€ì›
        if audio_bytes:
            try:
                # MIME íƒ€ì…ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê³  ê¸°ë³¸ê°’ ì„¤ì •
                valid_formats = ["audio/wav", "audio/mp3", "audio/mpeg", "audio/webm", "audio/ogg", "audio/m4a"]
                if audio_mime not in valid_formats:
                    # MIME íƒ€ì…ì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ íŒŒì¼ í™•ì¥ìë¡œ ì¶”ì •
                    audio_mime = "audio/wav"  # ê¸°ë³¸ê°’
                st.audio(audio_bytes, format=audio_mime)
            except Exception as e:
                st.error(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                # ê¸°ë³¸ í¬ë§·ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    st.audio(audio_bytes, format="audio/wav")
                except:
                    st.error("ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì¬ìƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì „ì‚¬ ë²„íŠ¼
        if audio_bytes and st.button(L["transcribe_btn"]):
            if st.session_state.openai_client is None:
                st.error(L["openai_missing"])
            else:
                with st.spinner(L["transcribing"]):
                    text = transcribe_bytes_with_whisper(
                        audio_bytes, audio_mime, lang_code=st.session_state.language
                    )
                    st.session_state.last_transcript = text
                    snippet = text[:50].replace("\n", " ")
                    if len(text) > 50:
                        snippet += "..."
                    if text.startswith("âŒ"):
                        st.error(text)
                    else:
                        st.success(f"{L['transcript_result']} **{snippet}**")

        st.text_area(
            L["transcript_text"],
            value=st.session_state.last_transcript,
            height=150,
            key="voice_rec_transcript_area",
        )

        if audio_bytes and st.button(L["save_btn"]):
            try:
                ext = audio_mime.split("/")[-1] if "/" in audio_mime else "webm"
                filename = f"record_{int(time.time())}.{ext}"
                save_audio_record_local(
                    audio_bytes,
                    filename,
                    st.session_state.last_transcript,
                    mime_type=audio_mime,
                )
                st.success(L["saved_success"])
                st.session_state.last_transcript = ""
                # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
            except Exception as e:
                st.error(f"{L['error']} {e}")

    # ì €ì¥ëœ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
    with col_list:
        st.subheader(L["rec_list_title"])
        try:
            records = load_voice_records()
        except Exception as e:
            st.error(f"read error: {e}")
            records = []

        if not records:
            st.info(L["no_records"])
        else:
            for rec in records:
                rec_id = rec["id"]
                created_at = rec.get("created_at")
                try:
                    dt = datetime.fromisoformat(created_at)
                    created_str = dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    created_str = str(created_at)

                transcript_snippet = (rec.get("transcript") or "")[:50].replace("\n", " ")
                if len(rec.get("transcript") or "") > 50:
                    transcript_snippet += "..."

                with st.expander(f"[{created_str}] {transcript_snippet}"):
                    st.write(f"**{L['transcript_text']}:** {rec.get('transcript') or 'N/A'}")
                    st.caption(
                        f"**Size:** {rec.get('size')} bytes | **File:** {rec.get('audio_filename')}"
                    )

                    col_p, col_r, col_d = st.columns([2, 1, 1])

                    if col_p.button(L["playback"], key=f"play_{rec_id}"):
                        try:
                            b, info = get_audio_bytes_local(rec_id)
                            mime = info.get("mime_type", "audio/webm")
                            # Streamlit ë¬¸ì„œ: bytes ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
                            # MIME íƒ€ì… ê²€ì¦
                            valid_formats = ["audio/wav", "audio/mp3", "audio/mpeg", "audio/webm", "audio/ogg", "audio/m4a"]
                            if mime not in valid_formats:
                                mime = "audio/wav"  # ê¸°ë³¸ê°’
                            st.audio(b, format=mime, autoplay=False)
                        except Exception as e:
                            st.error(f"{L['gcs_playback_fail']}: {e}")

                    if col_r.button(L["retranscribe"], key=f"re_{rec_id}"):
                        if st.session_state.openai_client is None:
                            st.error(L["openai_missing"])
                        else:
                            with st.spinner(L["transcribing"]):
                                try:
                                    b, info = get_audio_bytes_local(rec_id)
                                    mime = info.get("mime_type", "audio/webm")
                                    new_text = transcribe_bytes_with_whisper(
                                        b, mime, lang_code=st.session_state.language
                                    )
                                    records = load_voice_records()
                                    for r in records:
                                        if r["id"] == rec_id:
                                            r["transcript"] = new_text
                                            break
                                    save_voice_records(records)
                                    st.success(L["retranscribe"] + " " + L["saved_success"])
                                    # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
                                except Exception as e:
                                    st.error(f"{L['error']} {e}")

                    if col_d.button(L["delete"], key=f"del_{rec_id}"):
                        if st.session_state.get(f"confirm_del_{rec_id}", False):
                            ok = delete_audio_record_local(rec_id)
                            if ok:
                                st.success(L["delete_success"])
                            else:
                                st.error(L["delete_fail"])
                            st.session_state[f"confirm_del_{rec_id}"] = False
                            # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
                        else:
                            st.session_state[f"confirm_del_{rec_id}"] = True
                            st.warning(L["delete_confirm_rec"])
                            st.write("sim_stage:", st.session_state.get("sim_stage"))
                            st.write("is_llm_ready:", st.session_state.get("is_llm_ready"))

# -------------------- Simulator (Chat/Email) Tab --------------------
elif feature_selection == L["sim_tab_chat_email"]:
    # ... (ê¸°ì¡´ ì±„íŒ…/ì´ë©”ì¼ ì‹œë®¬ë ˆì´í„° ë¡œì§ ìœ ì§€)
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])

    current_lang = st.session_state.language
    L = LANG[current_lang]  # ë‹¤ì‹œ L ì—…ë°ì´íŠ¸

    # =========================
    # 0. ì „ì²´ ì´ë ¥ ì‚­ì œ
    # =========================
    col_del, _ = st.columns([1, 4])
    with col_del:
        if st.button(L["delete_history_button"], key="trigger_delete_hist"):
            st.session_state.show_delete_confirm = True

    if st.session_state.show_delete_confirm:
        with st.container():
            st.warning(L["delete_confirm_message"])
            c_yes, c_no = st.columns(2)
            if c_yes.button(L["delete_confirm_yes"], key="confirm_del_yes"):
                with st.spinner(L["deleting_history_progress"]):
                    delete_all_history_local()
                    st.session_state.simulator_messages = []
                    st.session_state.simulator_memory.clear()
                    st.session_state.show_delete_confirm = False
                    st.session_state.is_chat_ended = False
                    st.session_state.sim_stage = "WAIT_FIRST_QUERY"
                    st.session_state.customer_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
                    st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                    st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
                    st.success(L["delete_success"])
                    # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
            if c_no.button(L["delete_confirm_no"], key="confirm_del_no"):
                st.session_state.show_delete_confirm = False

    # =========================
    # 1. ì´ì „ ì´ë ¥ ë¡œë“œ (ê²€ìƒ‰/í•„í„°ë§ ê¸°ëŠ¥ ê°œì„ )
    # =========================
    with st.expander(L["history_expander_title"]):
        # Always load all available histories for the current language (sorted by recency)
        histories = load_simulation_histories_local(current_lang)

        # ì „ì²´ í†µê³„ ë° íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        cases_with_summary = [
            h for h in histories
            if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
               and not h.get("is_call", False)  # ì „í™” ì´ë ¥ ì œì™¸
        ]

        if cases_with_summary:
            st.markdown("---")
            st.subheader("ğŸ“ˆ ê³¼ê±° ì¼€ì´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ")

            # íŠ¸ë Œë“œ ì°¨íŠ¸ í‘œì‹œ
            trend_chart = visualize_case_trends(histories, current_lang)
            if trend_chart:
                st.plotly_chart(trend_chart, use_container_width=True)
            else:
                # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                avg_sentiment = np.mean(
                    [h["summary"].get("customer_sentiment_score", 50) for h in cases_with_summary if h.get("summary")])
                avg_satisfaction = np.mean(
                    [h["summary"].get("customer_satisfaction_score", 50) for h in cases_with_summary if
                     h.get("summary")])
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("í‰ê·  ê°ì • ì ìˆ˜", f"{avg_sentiment:.1f}/100", f"ì´ {len(cases_with_summary)}ê±´")
                with col2:
                    st.metric("í‰ê·  ë§Œì¡±ë„", f"{avg_satisfaction:.1f}/100", f"ì´ {len(cases_with_summary)}ê±´")

            st.markdown("---")

        # â­ ê²€ìƒ‰ í¼ ì œê±° ë° ë…ë¦½ëœ ìœ„ì ¯ ì‚¬ìš©
        col_search, col_btn = st.columns([4, 1])

        with col_search:
            # st.text_inputì€ Enter í‚¤ ì…ë ¥ ì‹œ ì•±ì„ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.
            search_query = st.text_input(L["search_history_label"], key="sim_hist_search_input_new")

        with col_btn:
            # ê²€ìƒ‰ ë²„íŠ¼: ëˆ„ë¥´ë©´ ì•±ì„ ê°•ì œ ì¬ì‹¤í–‰í•˜ì—¬ ê²€ìƒ‰/í•„í„°ë§ ë¡œì§ì„ ë‹¤ì‹œ íƒ€ë„ë¡ í•©ë‹ˆë‹¤.
            st.markdown("<br>", unsafe_allow_html=True)  # Align button vertically
            search_clicked = st.button(L["history_search_button"], key="apply_search_btn_new")

        # ë‚ ì§œ ë²”ìœ„ í•„í„°
        today = datetime.now().date()
        date_range_value = [today - timedelta(days=7), today]
        dr = st.date_input(
            L["date_range_label"],
            value=date_range_value,
            key="sim_hist_date_range_actual",
        )

        # --- Filtering Logic ---
        current_search_query = search_query.strip()

        if histories:
            start_date = min(dr)
            end_date = max(dr)

            filtered = []
            for h in histories:
                # ì „í™” ì´ë ¥ì€ ì œì™¸ (ì±„íŒ…/ì´ë©”ì¼ íƒ­ì´ë¯€ë¡œ)
                if h.get("is_call", False):
                    continue

                ok_search = True
                if current_search_query:
                    q = current_search_query.lower()
                    # ê²€ìƒ‰ ëŒ€ìƒ: ì´ˆê¸° ë¬¸ì˜, ê³ ê° ìœ í˜•, ìš”ì•½ ë°ì´í„°
                    text = (h["initial_query"] + " " + h["customer_type"]).lower()

                    # ìš”ì•½ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš”ì•½ ë‚´ìš©ë„ ê²€ìƒ‰ ëŒ€ìƒì— í¬í•¨
                    summary = h.get("summary")
                    if summary and isinstance(summary, dict):
                        summary_text = summary.get("main_inquiry", "") + " " + summary.get("summary", "")
                        text += " " + summary_text.lower()

                    # Check if query matches in initial query, customer type, or summary
                    if q not in text:
                        ok_search = False

                ok_date = True
                ts = h.get("timestamp")
                if ts:
                    try:
                        d = datetime.fromisoformat(ts).date()
                        # Apply date filtering
                        if not (start_date <= d <= end_date):
                            ok_date = False
                    except Exception:
                        pass  # Ignore histories with invalid timestamp

                if ok_search and ok_date:
                    filtered.append(h)
        else:
            filtered = []

        # Determine the list for display (â­ ìš”ì²­ ì‚¬í•­: ê²€ìƒ‰ì–´/í•„í„°ê°€ ì—†ìœ¼ë©´ ìµœê·¼ 10ê±´ë§Œ í‘œì‹œ)
        is_searching_or_filtering = bool(current_search_query) or dr != date_range_value

        if not is_searching_or_filtering:
            # ê²€ìƒ‰/í•„í„° ì¡°ê±´ì´ ì—†ìœ¼ë©´, ì „ì²´ ì´ë ¥ ì¤‘ ìµœì‹  10ê±´ë§Œ í‘œì‹œ
            filtered_for_display = filtered[:10]  # í•„í„°ë§ëœ ëª©ë¡(ì „í™” ì œì™¸) ì¤‘ 10ê°œ
        else:
            # ê²€ìƒ‰/í•„í„° ì¡°ê±´ì´ ìˆìœ¼ë©´, í•„í„°ë§ëœ ëª¨ë“  ê²°ê³¼ë¥¼ í‘œì‹œ
            filtered_for_display = filtered

        # --- Display Logic ---

        if filtered_for_display:
            def _label(h):
                try:
                    t = datetime.fromisoformat(h["timestamp"])
                    t_str = t.strftime("%m-%d %H:%M")
                except Exception:
                    t_str = h.get("timestamp", "")

                # ìš”ì•½ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš”ì•½ ì •ë³´ í‘œì‹œ, ì—†ìœ¼ë©´ ì´ˆê¸° ë¬¸ì˜ í‘œì‹œ
                summary = h.get("summary")
                if summary and isinstance(summary, dict):
                    main_inquiry = summary.get("main_inquiry", h["initial_query"][:30])
                    sentiment = summary.get("customer_sentiment_score", 50)
                    satisfaction = summary.get("customer_satisfaction_score", 50)
                    q = main_inquiry[:30].replace("\n", " ")
                    # ì²¨ë¶€ íŒŒì¼ ì—¬ë¶€ í‘œì‹œ ì¶”ê°€
                    attachment_icon = "ğŸ“" if h.get("attachment_context") else ""
                    # ìš”ì•½ ë°ì´í„° í‘œì‹œ (ê°ì •/ë§Œì¡±ë„ ì ìˆ˜ í¬í•¨)
                    return f"[{t_str}] {attachment_icon} {h['customer_type']} | ê°ì •:{sentiment} ë§Œì¡±:{satisfaction} - {q}..."
                else:
                    q = h["initial_query"][:30].replace("\n", " ")
                    attachment_icon = "ğŸ“" if h.get("attachment_context") else ""
                    return f"[{t_str}] {attachment_icon} {h['customer_type']} - {q}..."


            options_map = {_label(h): h for h in filtered_for_display}

            # Show a message indicating what is displayed if filters were applied
            if is_searching_or_filtering:
                st.caption(f"ğŸ” ì´ {len(filtered_for_display)}ê°œ ì´ë ¥ ê²€ìƒ‰ë¨ (ì „í™” ì´ë ¥ ì œì™¸)")
            else:
                st.caption(f"â­ ìµœê·¼ {len(filtered_for_display)}ê°œ ì´ë ¥ í‘œì‹œ ì¤‘ (ì „í™” ì´ë ¥ ì œì™¸)")

            sel_key = st.selectbox(L["history_selectbox_label"], options=list(options_map.keys()))

            if st.button(L["history_load_button"], key="load_hist_btn"):
                h = options_map[sel_key]
                st.session_state.customer_query_text_area = h["initial_query"]

                # ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆê³  ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì†Œí•œì˜ ë©”ì‹œì§€ ì¬êµ¬ì„±
                if not h.get("messages") and h.get("summary"):
                    summary = h["summary"]
                    # ìš”ì•½ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±
                    reconstructed_messages = [
                        {"role": "customer", "content": h["initial_query"]}
                    ]
                    # ìš”ì•½ì—ì„œ í•µì‹¬ ì‘ë‹µ ì¶”ê°€
                    if summary.get("key_responses"):
                        for response in summary.get("key_responses", [])[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                            reconstructed_messages.append({"role": "agent_response", "content": response})
                    # ìš”ì•½ ì •ë³´ë¥¼ supervisor ë©”ì‹œì§€ë¡œ ì¶”ê°€
                    summary_text = f"**ìš”ì•½ëœ ìƒë‹´ ì´ë ¥**\n\n"
                    summary_text += f"ì£¼ìš” ë¬¸ì˜: {summary.get('main_inquiry', 'N/A')}\n"
                    summary_text += f"ê³ ê° ê°ì • ì ìˆ˜: {summary.get('customer_sentiment_score', 50)}/100\n"
                    summary_text += f"ê³ ê° ë§Œì¡±ë„: {summary.get('customer_satisfaction_score', 50)}/100\n"
                    summary_text += f"\nì „ì²´ ìš”ì•½:\n{summary.get('summary', 'N/A')}"
                    reconstructed_messages.append({"role": "supervisor", "content": summary_text})
                    st.session_state.simulator_messages = reconstructed_messages

                    # ìš”ì•½ ë°ì´í„° ì‹œê°í™”
                    st.markdown("---")
                    st.subheader("ğŸ“Š ë¡œë“œëœ ì¼€ì´ìŠ¤ ë¶„ì„")

                    # ìš”ì•½ ë°ì´í„°ë¥¼ í”„ë¡œí•„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    loaded_profile = {
                        "sentiment_score": summary.get("customer_sentiment_score", 50),
                        "urgency_level": "medium",  # ê¸°ë³¸ê°’
                        "predicted_customer_type": h.get("customer_type", "normal")
                    }

                    # í”„ë¡œí•„ ì ìˆ˜ ì°¨íŠ¸
                    profile_chart = visualize_customer_profile_scores(loaded_profile, current_lang)
                    if profile_chart:
                        st.plotly_chart(profile_chart, use_container_width=True)
                    else:
                        # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric(L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
                                      f"{summary.get('customer_sentiment_score', 50)}/100")
                        with col2:
                            st.metric(L.get("urgency_score_label", "ê¸´ê¸‰ë„"), f"50/100")
                        with col3:
                            st.metric(L.get("customer_type_label", "ê³ ê° ìœ í˜•"), h.get("customer_type", "normal"))

                    # ê³ ê° íŠ¹ì„± ì‹œê°í™”
                    if summary.get("customer_characteristics") or summary.get("privacy_info"):
                        characteristics_chart = visualize_customer_characteristics(summary, current_lang)
                        if characteristics_chart:
                            st.plotly_chart(characteristics_chart, use_container_width=True)
                else:
                    # ê¸°ì¡´ ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    st.session_state.simulator_messages = h.get("messages", [])

                st.session_state.initial_advice_provided = True
                st.session_state.is_chat_ended = h.get("is_chat_ended", False)
                st.session_state.sim_attachment_context_for_llm = h.get("attachment_context", "")  # ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
                st.session_state.customer_attachment_file = []  # ë¡œë“œëœ ì´ë ¥ì—ëŠ” íŒŒì¼ ê°ì²´ ëŒ€ì‹  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë§Œ ì‚¬ìš©
                st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”

                # ìƒíƒœ ë³µì›
                if st.session_state.is_chat_ended:
                    st.session_state.sim_stage = "CLOSING"
                else:
                    messages = st.session_state.simulator_messages
                    last_role = messages[-1]["role"] if messages else None
                    if last_role == "agent_response":
                        st.session_state.sim_stage = "CUSTOMER_TURN"
                    elif last_role == "customer_rebuttal":
                        st.session_state.sim_stage = "AGENT_TURN"
                    elif last_role == "supervisor" and messages and messages[-1]["content"] == L[
                        "customer_closing_confirm"]:
                        st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"
                    else:
                        st.session_state.sim_stage = "AGENT_TURN"

                st.session_state.simulator_memory.clear()  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
                # â­ ë¡œë“œ í›„ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¬ì‹¤í–‰
                # st.rerun()
        else:
            st.info(L["no_history_found"])

    # =========================
    # AHT íƒ€ì´ë¨¸ (í™”ë©´ ìµœìƒë‹¨)
    # =========================
    if st.session_state.sim_stage not in ["WAIT_FIRST_QUERY", "CLOSING", "idle"]:
        elapsed_placeholder = st.empty()

        if st.session_state.start_time is not None:
            # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ í˜ì´ì§€ ë¡œë“œ ì‹œë§ˆë‹¤ í˜„ì¬ ì‹œê°„ ê³„ì‚°
            elapsed_time = datetime.now() - st.session_state.start_time
            total_seconds = elapsed_time.total_seconds()

            # Hold ì‹œê°„ ì œì™¸ (ì±„íŒ…/ì´ë©”ì¼ì€ Hold ì—†ìŒ, ì „í™” íƒ­ê³¼ ë¡œì§ í†µì¼ ìœ„í•´ ìœ ì§€)
            # total_seconds -= st.session_state.total_hold_duration.total_seconds()

            # ì‹œê°„ í˜•ì‹ í¬ë§·íŒ…
            minutes = int(total_seconds // 60)
            seconds = int(total_seconds % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # ê²½ê³  ê¸°ì¤€
            if total_seconds > 900:  # 15ë¶„
                delta_str = L["timer_info_risk"]
                delta_color = "inverse"
            elif total_seconds > 600:  # 10ë¶„
                delta_str = L["timer_info_warn"]
                delta_color = "off"
            else:
                delta_str = L["timer_info_ok"]
                delta_color = "normal"

            elapsed_placeholder.metric(
                L["timer_metric"],
                time_str,
                delta=delta_str,
                delta_color=delta_color
            )

            # â­ ìˆ˜ì •: 3ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰í•˜ì—¬ AHT ì‹¤ì‹œê°„ì„± í™•ë³´
            if seconds % 3 == 0 and total_seconds < 1000:
                time.sleep(1)
                # st.rerun()

        st.markdown("---")

    # =========================
    # 2. LLM ì¤€ë¹„ ì²´í¬ & ì±„íŒ… ì¢…ë£Œ ìƒíƒœ
    # =========================
    if not st.session_state.is_llm_ready:
        st.warning(L["simulation_no_key_warning"])

    if st.session_state.sim_stage == "CLOSING":
        st.success(L["survey_sent_confirm"])
        st.info(L["new_simulation_ready"])
        
        # â­ ì¶”ê°€: í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
        st.markdown("---")
        st.markdown("**ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ**")
        download_col1, download_col2, download_col3 = st.columns(3)
        
        # í˜„ì¬ ì„¸ì…˜ì˜ ì´ë ¥ì„ ìƒì„±
        current_session_history = None
        if st.session_state.simulator_messages:
            try:
                customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
                current_session_summary = generate_chat_summary(
                    st.session_state.simulator_messages,
                    st.session_state.customer_query_text_area,
                    customer_type_display,
                    st.session_state.language
                )
                current_session_history = [{
                    "id": f"session_{st.session_state.sim_instance_id}",
                    "timestamp": datetime.now().isoformat(),
                    "initial_query": st.session_state.customer_query_text_area,
                    "customer_type": customer_type_display,
                    "language_key": st.session_state.language,
                    "messages": st.session_state.simulator_messages,
                    "summary": current_session_summary,
                    "is_chat_ended": True,
                    "attachment_context": st.session_state.sim_attachment_context_for_llm
                }]
            except Exception as e:
                st.warning(f"ì´ë ¥ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤ì„ ì§ì ‘ í‘œì‹œ
        if current_session_history:
            with download_col1:
                try:
                    filepath_word = export_history_to_word(current_session_history)
                    with open(filepath_word, "rb") as f:
                        st.download_button(
                            label=L["download_history_word"],
                            data=f.read(),
                            file_name=os.path.basename(filepath_word),
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            key="download_word_file"
                        )
                except Exception as e:
                    st.error(f"Word ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col2:
                try:
                    filepath_pptx = export_history_to_pptx(current_session_history)
                    with open(filepath_pptx, "rb") as f:
                        st.download_button(
                            label=L["download_history_pptx"],
                            data=f.read(),
                            file_name=os.path.basename(filepath_pptx),
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                            key="download_pptx_file"
                        )
                except Exception as e:
                    st.error(f"PPTX ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col3:
                try:
                    filepath_pdf = export_history_to_pdf(current_session_history)
                    with open(filepath_pdf, "rb") as f:
                        st.download_button(
                            label=L["download_history_pdf"],
                            data=f.read(),
                            file_name=os.path.basename(filepath_pdf),
                            mime="application/pdf",
                            key="download_pdf_file"
                        )
                except Exception as e:
                    st.error(f"PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        else:
            st.warning("ë‹¤ìš´ë¡œë“œí•  ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        
        if st.button(L["new_simulation_button"], key="new_simulation_btn"):
            # ì´ˆê¸°í™” ë¡œì§
            st.session_state.simulator_messages = []
            st.session_state.simulator_memory.clear()
            st.session_state.initial_advice_provided = False
            st.session_state.is_chat_ended = False
            st.session_state.agent_response_area_text = ""
            st.session_state.customer_query_text_area = ""
            st.session_state.last_transcript = ""
            st.session_state.sim_audio_bytes = None
            st.session_state.sim_stage = "WAIT_FIRST_QUERY"
            st.session_state.customer_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.start_time = None
            # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""
            st.session_state.sim_call_outbound_target = None
            # â­ ì¬ì‹¤í–‰
            st.rerun()
        # st.stop()

    # =========================
    # 5-A. ì „í™” ë°œì‹  ì§„í–‰ ì¤‘ (OUTBOUND_CALL_IN_PROGRESS)
    # =========================
    elif st.session_state.sim_stage == "OUTBOUND_CALL_IN_PROGRESS":
        L = LANG[st.session_state.language]
        target = st.session_state.get("sim_call_outbound_target", "ëŒ€ìƒ")
        st.warning(L["call_outbound_loading"])

        # LLM í˜¸ì¶œ ë° ìš”ì•½ ìƒì„±
        with st.spinner(L["call_outbound_loading"]):
            # 1. LLM í˜¸ì¶œí•˜ì—¬ í†µí™” ìš”ì•½ ìƒì„±
            summary = generate_outbound_call_summary(
                st.session_state.customer_query_text_area,
                st.session_state.language,
                target
            )

            # 2. ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì „í™” ì‹œë„) ì¶”ê°€
            st.session_state.simulator_messages.append(
                {"role": "system_end", "content": L["call_outbound_system_msg"].format(target=target)}
            )

            # 3. ìš”ì•½ ë©”ì‹œì§€ (ê²°ê³¼) ì¶”ê°€
            summary_markdown = f"### {L['call_outbound_summary_header']}\n\n{summary}"
            st.session_state.simulator_messages.append(
                {"role": "supervisor", "content": summary_markdown}
            )

            # 4. Agent Turnìœ¼ë¡œ ë³µê·€
            st.session_state.sim_stage = "AGENT_TURN"
            st.session_state.sim_call_outbound_summary = summary_markdown  # Save for display/reference
            st.session_state.sim_call_outbound_target = None  # Reset target

            # 5. ì´ë ¥ ì €ì¥ (ì „í™” ë°œì‹  í›„ ìƒíƒœ ì €ì¥)
            customer_type_display = st.session_state.get("customer_type_sim_select", "")
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display + f" (Outbound Call to {target})",
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )

        st.success(f"âœ… {L['call_outbound_simulation_header']}ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ í™•ì¸í•˜ê³  ê³ ê°ì—ê²Œ íšŒì‹ í•˜ì„¸ìš”.")
        # st.rerun()

    # ========================================
    # 3. ì´ˆê¸° ë¬¸ì˜ ì…ë ¥ (WAIT_FIRST_QUERY)
    # ========================================
    if st.session_state.sim_stage == "WAIT_FIRST_QUERY":
        customer_query = st.text_area(
            L["customer_query_label"],
            key="customer_query_text_area",
            height=150,
            placeholder=L["initial_query_sample"],
        )

        # --- í•„ìˆ˜ ì…ë ¥ í•„ë“œ (ìš”ì²­ 3 ë°˜ì˜: UI í…ìŠ¤íŠ¸ ë³€ê²½) ---
        customer_email = st.text_input(
            L["customer_email_label"],
            key="customer_email_input",
            value=st.session_state.customer_email,
        )
        customer_phone = st.text_input(
            L["customer_phone_label"],
            key="customer_phone_input",
            value=st.session_state.customer_phone,
        )
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.customer_email = customer_email
        st.session_state.customer_phone = customer_phone
        # --------------------------------------------------

        customer_type_options = L["customer_type_options"]
        # st.session_state.customer_type_sim_selectëŠ” ì´ë¯¸ ì´ˆê¸°í™”ë¨
        default_idx = customer_type_options.index(
            st.session_state.customer_type_sim_select) if st.session_state.customer_type_sim_select in customer_type_options else 0

        # SelectboxëŠ” ìì²´ì ìœ¼ë¡œ ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ, ì—¬ê¸°ì— valueë¥¼ ì„¤ì •í•  í•„ìš” ì—†ìŒ
        st.session_state.customer_type_sim_select = st.selectbox(
            L["customer_type_label"],
            customer_type_options,
            index=default_idx,
            key="customer_type_sim_select_widget",
        )

        # --- ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë” ì¶”ê°€ ---
        customer_attachment_widget = st.file_uploader(
            L["attachment_label"],
            type=["png", "jpg", "jpeg", "pdf"],
            key="customer_attachment_file_uploader",
            help=L["attachment_placeholder"],
            accept_multiple_files=False  # ì±„íŒ…/ì´ë©”ì¼ì€ ë‹¨ì¼ íŒŒì¼ë§Œ í—ˆìš©
        )

        # íŒŒì¼ ì •ë³´ ì €ì¥ ë° LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        if customer_attachment_widget:
            st.session_state.customer_attachment_file = customer_attachment_widget
            st.session_state.sim_attachment_context_for_llm = L["attachment_status_llm"].format(
                filename=customer_attachment_widget.name, filetype=customer_attachment_widget.type
            )
        else:
            st.session_state.customer_attachment_file = None
            st.session_state.sim_attachment_context_for_llm = ""
        # --------------------------

        if st.button(L["button_simulate"], key=f"btn_simulate_initial_{st.session_state.sim_instance_id}"):  # ê³ ìœ  í‚¤ ì‚¬ìš©
            if not customer_query.strip():
                st.warning(L["simulation_warning_query"])
                # st.stop()

            # --- í•„ìˆ˜ ì…ë ¥ í•„ë“œ ê²€ì¦ (ìš”ì²­ 3 ë°˜ì˜: ê²€ì¦ ë¡œì§ ì¶”ê°€) ---
            if not st.session_state.customer_email.strip() or not st.session_state.customer_phone.strip():
                st.error(L["error_mandatory_contact"])
                # st.stop()
            # ------------------------------------------

            # ì´ˆê¸° ìƒíƒœ ë¦¬ì…‹
            st.session_state.simulator_messages = []
            st.session_state.simulator_memory.clear()
            st.session_state.is_chat_ended = False
            st.session_state.initial_advice_provided = False
            st.session_state.is_solution_provided = False  # ì†”ë£¨ì…˜ í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state.language_transfer_requested = False  # ì–¸ì–´ ìš”ì²­ í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state.transfer_summary_text = ""  # ì´ê´€ ìš”ì•½ ë¦¬ì…‹
            st.session_state.start_time = None  # AHT íƒ€ì´ë¨¸ ì´ˆê¸°í™” (ì²« ê³ ê° ë°˜ì‘ í›„ ì‹œì‘)
            st.session_state.sim_instance_id = str(uuid.uuid4())  # ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ID í• ë‹¹
            # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""
            st.session_state.sim_call_outbound_target = None

            # 1) ê³ ê° ì²« ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.simulator_messages.append(
                {"role": "customer", "content": customer_query}
            )

            # 2) Supervisor ê°€ì´ë“œ + ì´ˆì•ˆ ìƒì„±
            # ê³ ê° í”„ë¡œí•„ ë¶„ì„ (ì‹œê°í™”ë¥¼ ìœ„í•´ ë¨¼ì € ìˆ˜í–‰)
            customer_profile = analyze_customer_profile(customer_query, current_lang)
            similar_cases = find_similar_cases(customer_query, customer_profile, current_lang, limit=5)

            # ì‹œê°í™” ì°¨íŠ¸ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“Š ê³ ê° í”„ë¡œí•„ ë¶„ì„")

            # ê³ ê° í”„ë¡œí•„ ì ìˆ˜ ì°¨íŠ¸
            profile_chart = visualize_customer_profile_scores(customer_profile, current_lang)
            if profile_chart:
                st.plotly_chart(profile_chart, use_container_width=True)
            else:
                # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
                        f"{customer_profile.get('sentiment_score', 50)}/100"
                    )
                with col2:
                    urgency_map = {"low": 25, "medium": 50, "high": 75}
                    urgency_score = urgency_map.get(customer_profile.get("urgency_level", "medium").lower(), 50)
                    st.metric(
                        L.get("urgency_score_label", "ê¸´ê¸‰ë„"),
                        f"{urgency_score}/100"
                    )
                with col3:
                    st.metric(
                        L.get("customer_type_label", "ê³ ê° ìœ í˜•"),
                        customer_profile.get("predicted_customer_type", "normal")
                    )

            # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì‹œê°í™”
            if similar_cases:
                st.markdown("---")
                st.subheader("ğŸ” ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ì²œ")
                similarity_chart = visualize_similarity_cases(similar_cases, current_lang)
                if similarity_chart:
                    st.plotly_chart(similarity_chart, use_container_width=True)

                # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìš”ì•½ í‘œì‹œ
                with st.expander(f"ğŸ’¡ {len(similar_cases)}ê°œ ìœ ì‚¬ ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´"):
                    for idx, similar_case in enumerate(similar_cases, 1):
                        case = similar_case["case"]
                        summary = similar_case["summary"]
                        similarity = similar_case["similarity_score"]
                        st.markdown(f"### ì¼€ì´ìŠ¤ {idx} (ìœ ì‚¬ë„: {similarity:.1f}%)")
                        st.markdown(f"**ë¬¸ì˜ ë‚´ìš©:** {summary.get('main_inquiry', 'N/A')}")
                        st.markdown(f"**ê°ì • ì ìˆ˜:** {summary.get('customer_sentiment_score', 50)}/100")
                        st.markdown(f"**ë§Œì¡±ë„ ì ìˆ˜:** {summary.get('customer_satisfaction_score', 50)}/100")
                        if summary.get("key_responses"):
                            st.markdown("**í•µì‹¬ ì‘ë‹µ:**")
                            for response in summary.get("key_responses", [])[:3]:
                                st.markdown(f"- {response[:100]}...")
                        st.markdown("---")

            # ì´ˆê¸° ì¡°ì–¸ ìƒì„±
            text = _generate_initial_advice(
                customer_query,
                st.session_state.customer_type_sim_select,
                st.session_state.customer_email,
                st.session_state.customer_phone,
                current_lang,
                st.session_state.customer_attachment_file
            )
            st.session_state.simulator_messages.append({"role": "supervisor", "content": text})

            st.session_state.initial_advice_provided = True
            save_simulation_history_local(
                customer_query,
                st.session_state.customer_type_sim_select,
                st.session_state.simulator_messages,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
                is_chat_ended=False,
            )
            st.session_state.sim_stage = "AGENT_TURN"
            # â­ ì¬ì‹¤í–‰
            # st.rerun()

    # =========================
    # 4. ëŒ€í™” ë¡œê·¸ í‘œì‹œ (ê³µí†µ)
    # =========================
    for idx, msg in enumerate(st.session_state.simulator_messages):
        role = msg["role"]
        content = msg["content"]
        avatar = {"customer": "ğŸ™‹", "supervisor": "ğŸ¤–", "agent_response": "ğŸ§‘â€ğŸ’»", "customer_rebuttal": "âœ¨",
                  "system_end": "ğŸ“Œ", "system_transfer": "ğŸ“Œ"}.get(role, "ğŸ’¬")
        tts_role = "customer" if role.startswith("customer") or role == "customer_rebuttal" else (
            "agent" if role == "agent_response" else "supervisor")

        with st.chat_message(role, avatar=avatar):
            st.markdown(content)
            # ì¸ë±ìŠ¤ë¥¼ render_tts_buttonì— ì „ë‹¬í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±ì— ì‚¬ìš©
            render_tts_button(content, st.session_state.language, role=tts_role, prefix=f"{role}_", index=idx)

            # â­ [ìƒˆë¡œìš´ ë¡œì§] ê³ ê° ì²¨ë¶€ íŒŒì¼ ë Œë”ë§ (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ ê²½ìš°)
            if idx == 0 and role == "customer" and st.session_state.customer_attachment_b64:
                mime = st.session_state.customer_attachment_mime or "image/png"
                data_url = f"data:{mime};base64,{st.session_state.customer_attachment_b64}"

                # ì´ë¯¸ì§€ íŒŒì¼ë§Œ í‘œì‹œ (PDF ë“±ì€ ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
                if mime.startswith("image/"):
                    st.image(data_url, caption=f"ì²¨ë¶€ëœ ì¦ê±°ë¬¼ ({st.session_state.customer_attachment_file.name})",
                             use_column_width=True)
                elif mime == "application/pdf":
                    # PDF íŒŒì¼ì¼ ê²½ìš°, íŒŒì¼ ì´ë¦„ê³¼ í•¨ê»˜ ë‹¤ìš´ë¡œë“œ ë§í¬ ë˜ëŠ” ê²½ê³  í‘œì‹œ
                    st.warning(
                        f"ì²¨ë¶€ëœ PDF íŒŒì¼ ({st.session_state.customer_attachment_file.name})ì€ í˜„ì¬ ì¸ë¼ì¸ ë¯¸ë¦¬ë³´ê¸°ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ì´ê´€ ìš”ì•½ í‘œì‹œ (ì´ê´€ í›„ì—ë§Œ) - ë£¨í”„ ë°–ìœ¼ë¡œ ì´ë™í•˜ì—¬ í•œ ë²ˆë§Œ í‘œì‹œ
    if st.session_state.transfer_summary_text or (st.session_state.language != st.session_state.language_at_transfer_start and st.session_state.language_at_transfer_start):
                st.markdown("---")
                st.markdown(f"**{L['transfer_summary_header']}**")
                st.info(L["transfer_summary_intro"])

                # ë²ˆì—­ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° (ë¹ˆ ë¬¸ìì—´)
                # â­ ìˆ˜ì •ëœ ë¶€ë¶„ 1: DuplicateWidgetID ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ ê³ ìœ  í‚¤ì— UUID ì¶”ê°€
                is_translation_failed = not st.session_state.transfer_summary_text or st.session_state.transfer_summary_text.startswith(
                    "âŒ Translation Error")

                if is_translation_failed:
                    st.error(f"âŒ LLM_TRANSLATION_ERROR (ë²ˆì—­ ì‹¤íŒ¨). ìƒì„¸ ì •ë³´ëŠ” ì•„ë˜ ìš”ì•½ ë°•ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    st.info(st.session_state.transfer_summary_text)
                    # ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ ì¶”ê°€
                    if st.button(L["button_retry_translation"],
                                 key=f"btn_retry_translation_{st.session_state.sim_instance_id}"):  # ê³ ìœ  í‚¤ ì‚¬ìš©
                        # ì¬ì‹œë„ ë¡œì§ ì‹¤í–‰
                        with st.spinner(L["transfer_loading"]):
                            source_lang = st.session_state.language_at_transfer_start
                            target_lang = st.session_state.language

                            # ì´ì „ ëŒ€í™” ë‚´ìš© ì¬ê°€ê³µ
                            history_text = get_chat_history_for_prompt(include_attachment=False)
                            for msg in st.session_state.simulator_messages:
                                role = "Customer" if msg["role"].startswith("customer") or msg[
                                    "role"] == "initial_query" else "Agent"
                                if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                                   "customer_closing_response"]:
                                    history_text += f"{role}: {msg['content']}\n"

                            translated_summary = translate_text_with_llm(history_text, target_lang, source_lang)
                            st.session_state.transfer_summary_text = translated_summary
                            st.session_state.transfer_retry_count += 1

                            # â­ ì¬ì‹¤í–‰
                            # st.rerun()


                else:
                    # [ìˆ˜ì • 2] ë²ˆì—­ ì„±ê³µ ì‹œ ë‚´ìš© í‘œì‹œ ë° TTS ë²„íŠ¼ ì¶”ê°€
                    st.markdown(st.session_state.transfer_summary_text)
            # â­ ìˆ˜ì •: ì´ê´€ ìš”ì•½ì˜ ê²½ìš° ì•ˆì •ì ì¸ í‚¤ë¥¼ ìƒì„±í•˜ë„ë¡ ìˆ˜ì • (ì„¸ì…˜ IDì™€ ì–¸ì–´ ì½”ë“œ ì¡°í•©)
                    render_tts_button(
                        st.session_state.transfer_summary_text,
                        st.session_state.language,
                        role="agent",
                        prefix="trans_summary_tts",
                        index=-1  # ê³ ìœ  ì„¸ì…˜ ID ê¸°ë°˜ì˜ í‚¤ë¥¼ ìƒì„±í•˜ë„ë¡ ì§€ì‹œ
                    )
                st.markdown("---")

    # =========================
    # 5. ì—ì´ì „íŠ¸ ì…ë ¥ ë‹¨ê³„ (AGENT_TURN)
    # =========================
    if st.session_state.sim_stage == "AGENT_TURN":
        st.markdown(f"### {L['agent_response_header']}")

        # --- ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸ ì˜ì—­ ---
        hint_cols = st.columns([4, 1])
        with hint_cols[0]:
            st.info(L["hint_placeholder"] + st.session_state.realtime_hint_text)

        with hint_cols[1]:
            # íŒíŠ¸ ìš”ì²­ ë²„íŠ¼
            if st.button(L["button_request_hint"], key=f"btn_request_hint_{st.session_state.sim_instance_id}"):
                with st.spinner(L["response_generating"]):
                    # ì±„íŒ…/ì´ë©”ì¼ íƒ­ì´ë¯€ë¡œ is_call=False
                    hint = generate_realtime_hint(current_lang, is_call=False)
                    st.session_state.realtime_hint_text = hint
                    # â­ ì¬ì‹¤í–‰
                    # st.rerun()

        # --- ì–¸ì–´ ì´ê´€ ìš”ì²­ ê°•ì¡° í‘œì‹œ ---
        if st.session_state.language_transfer_requested:
            st.error("ğŸš¨ ê³ ê°ì´ ì–¸ì–´ ì „í™˜(ì´ê´€)ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì‘ëŒ€í•˜ê±°ë‚˜ ì´ê´€ì„ ì§„í–‰í•˜ì„¸ìš”ã€‚")

        # --- ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì¬í‘œì‹œ ---
        if st.session_state.sim_attachment_context_for_llm:
            st.info(
                f"ğŸ“ ìµœì´ˆ ë¬¸ì˜ ì‹œ ì²¨ë¶€ëœ íŒŒì¼ ì •ë³´:\n\n{st.session_state.sim_attachment_context_for_llm.replace('[ATTACHMENT STATUS]', '').strip()}")

        # --- AI ì‘ë‹µ ì´ˆì•ˆ ìƒì„± ë²„íŠ¼ (ìš”ì²­ 1 ë°˜ì˜) ---
        if st.button(L["button_generate_draft"], key=f"btn_generate_ai_draft_{st.session_state.sim_instance_id}"):
            if not st.session_state.is_llm_ready:
                st.warning(L["simulation_no_key_warning"])
            else:
                with st.spinner(L["draft_generating"]):
                    # ì´ˆì•ˆ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
                    ai_draft = generate_agent_response_draft(current_lang)
                    if ai_draft and not ai_draft.startswith("âŒ"):
                        st.session_state.agent_response_area_text = ai_draft
                        st.success(L["draft_success"])
                        # â­ ì¬ì‹¤í–‰í•˜ì—¬ í…ìŠ¤íŠ¸ ì˜ì—­ ì—…ë°ì´íŠ¸
                        st.rerun()
                    else:
                        st.error(ai_draft if ai_draft else L.get("draft_error", "ì‘ë‹µ ì´ˆì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."))

        # --- ì „í™” ë°œì‹  ë²„íŠ¼ ì¶”ê°€ (ìš”ì²­ 2 ë°˜ì˜) ---
        st.markdown("---")
        st.subheader(L["button_call_outbound"])
        call_cols = st.columns(3)

        with call_cols[0]:
            if st.button(L["button_call_outbound"].replace("ì „í™” ë°œì‹ ", "í˜„ì§€ ì—…ì²´ ì „í™” ë°œì‹ "), key="btn_call_outbound_partner"):
                # ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜: í˜„ì§€ ì—…ì²´
                st.session_state.sim_call_outbound_target = "í˜„ì§€ ì—…ì²´/íŒŒíŠ¸ë„ˆ"
                st.session_state.sim_stage = "OUTBOUND_CALL_IN_PROGRESS"
                # st.rerun()

        with call_cols[1]:
            if st.button(L["button_call_outbound"].replace("ì „í™” ë°œì‹ ", "ê³ ê° ì „í™” ë°œì‹ "), key="btn_call_outbound_customer"):
                # ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜: ê³ ê°
                st.session_state.sim_call_outbound_target = "ê³ ê°"
                st.session_state.sim_stage = "OUTBOUND_CALL_IN_PROGRESS"
                # st.rerun()

        st.markdown("---")
        # --- ì „í™” ë°œì‹  ë²„íŠ¼ ì¶”ê°€ ë ---

        st.markdown("### ğŸš¨ Supervisor ì •ì±…/ì§€ì‹œ ì‚¬í•­ ì—…ë¡œë“œ (ì˜ˆì™¸ ì²˜ë¦¬ ë°©ì¹¨)")

        # --- Supervisor ì •ì±… ì—…ë¡œë” ì¶”ê°€ ---
        supervisor_attachment_widget = st.file_uploader(
            "Supervisor ì§€ì‹œ ì‚¬í•­/ìŠ¤í¬ë¦°ìƒ· ì—…ë¡œë“œ (ì˜ˆì™¸ ì •ì±… í¬í•¨)",
            type=["png", "jpg", "jpeg", "pdf", "txt"],
            key="supervisor_policy_uploader",
            help="ë¹„í–‰ê¸° ì§€ì—°, ì§ˆë³‘ ë“± ì˜ˆì™¸ì  ìƒí™©ì— ëŒ€í•œ Supervisorì˜ ìµœì‹  ì§€ì‹œ ì‚¬í•­ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
            accept_multiple_files=False
        )

        # íŒŒì¼ ì •ë³´ ì €ì¥ ë° LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        if supervisor_attachment_widget:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ë˜ëŠ” PDF/ì´ë¯¸ì§€ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì»¨í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì—¬ policy_contextì— ì €ì¥í•´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” íŒŒì¼ ì´ë¦„ê³¼ íƒ€ì…ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ê³ , LLMì´ ì´ê²ƒì´ 'ì˜ˆì™¸ ì •ì±…'ì„ì„ ì•Œë„ë¡ ìœ ë„
            file_name = supervisor_attachment_widget.name
            st.session_state.supervisor_policy_context = f"[Supervisor Policy Attached] Filename: {file_name}, Filetype: {supervisor_attachment_widget.type}. This file contains a CRITICAL, temporary policy update regarding exceptions (e.g., flight delays, illness, natural disasters). Analyze and prioritize this policy in the response."
            st.success(f"âœ… Supervisor ì •ì±… íŒŒì¼: **{file_name}**ì´(ê°€) ì‘ëŒ€ ê°€ì´ë“œì— ë°˜ì˜ë©ë‹ˆë‹¤.")
        elif st.session_state.supervisor_policy_context:
            st.info("â­ í˜„ì¬ ì ìš© ì¤‘ì¸ Supervisor ì •ì±…ì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.session_state.supervisor_policy_context = ""

        # --- ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë” (ë‹¤ì¤‘ íŒŒì¼ í—ˆìš©) ---
        agent_attachment_files = st.file_uploader(
            L["agent_attachment_label"],
            type=["png", "jpg", "jpeg", "pdf"],
            key="agent_attachment_file_uploader",
            help=L["agent_attachment_placeholder"],
            accept_multiple_files=True
        )

        if agent_attachment_files:
            st.session_state.agent_attachment_file = [
                {"name": f.name, "type": f.type, "size": f.size} for f in agent_attachment_files
            ]
            file_names = ", ".join([f["name"] for f in
                                    st.session_state.agent_attachment_file])  # ìˆ˜ì •: file_infos ëŒ€ì‹  st.session_state.agent_attachment_file ì‚¬ìš©
            st.info(f"âœ… {len(agent_attachment_files)}ê°œ ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ: {file_names}")
        else:
            st.session_state.agent_attachment_file = []

        # --- ì…ë ¥ í•„ë“œ ë° ë²„íŠ¼ ---
        col_mic, col_text = st.columns([1, 2])

        # --- ë§ˆì´í¬ ë…¹ìŒ ---
        with col_mic:
            mic_audio = mic_recorder(
                start_prompt=L["button_mic_input"],
                stop_prompt=L["button_mic_stop"],
                just_once=False,
                format="wav",
                use_container_width=True,
                key="sim_mic_recorder",
            )

        if mic_audio and mic_audio.get("bytes"):
            st.session_state.sim_audio_bytes = mic_audio["bytes"]
            st.info("âœ… ë…¹ìŒ ì™„ë£Œ! ì•„ë˜ ì „ì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì„¸ìš”.")

        if st.session_state.sim_audio_bytes:
            col_audio, col_transcribe, col_del = st.columns([3, 1, 1])

            # 1. ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´
            # Streamlit ë¬¸ì„œ: bytes ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
            with col_audio:
                try:
                    st.audio(st.session_state.sim_audio_bytes, format="audio/wav", autoplay=False)
                except Exception as e:
                    st.error(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")

            # 2. ë…¹ìŒ ì‚­ì œ ë²„íŠ¼ (ì¶”ê°€ ìš”ì²­ ë°˜ì˜)
            with col_del:
                st.markdown("<br>", unsafe_allow_html=True)  # ë²„íŠ¼ ìˆ˜ì§ ì •ë ¬
                if st.button(L["delete_mic_record"], key="btn_delete_sim_audio_call"):
                    # ì˜¤ë””ì˜¤ ë° ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
                    st.session_state.sim_audio_bytes = None
                    st.session_state.last_transcript = ""
                    # â­ ìˆ˜ì •: ìœ„ì ¯ì´ ìƒì„±ëœ í›„ì—ëŠ” session_stateë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í”Œë˜ê·¸ ì‚¬ìš©
                    st.session_state.reset_agent_response_area = True
                    st.success("ë…¹ìŒì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë…¹ìŒí•´ ì£¼ì„¸ìš”.")
                    st.rerun()

            # 3. ì „ì‚¬(Whisper) ë²„íŠ¼ (ê¸°ì¡´ ë¡œì§ ëŒ€ì²´)
            col_tr, _ = st.columns([1, 2])
            if col_tr.button(L["transcribe_btn"], key="sim_transcribe_btn"):
                if st.session_state.sim_audio_bytes is None:
                    st.warning("ë¨¼ì € ë§ˆì´í¬ë¡œ ë…¹ìŒì„ ì™„ë£Œí•˜ì„¸ìš”.")
                elif st.session_state.openai_client is None:
                    st.error(L["whisper_client_error"])
                else:
                    with st.spinner(L["whisper_processing"]):
                        # transcribe_bytes_with_whisper í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
                        transcribed_text = transcribe_bytes_with_whisper(
                            st.session_state.sim_audio_bytes,
                            "audio/wav",
                            lang_code=st.session_state.language,
                        )
                        if transcribed_text.startswith("âŒ"):
                            st.error(transcribed_text)
                            st.session_state.last_transcript = ""
                        else:
                            st.session_state.last_transcript = transcribed_text.strip()
                            # â­ ìˆ˜ì •: ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì°½ì˜ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ì— ë°˜ì˜
                            st.session_state.agent_response_area_text = transcribed_text.strip()
                            st.session_state.agent_response_input_box_widget = transcribed_text.strip()

                            snippet = transcribed_text[:50].replace("\n", " ")
                            if len(transcribed_text) > 50:
                                snippet += "..."
                            st.success(L["whisper_success"] + f"\n\n**ì¸ì‹ ë‚´ìš©:** *{snippet}*")
                            # â­ ì£¼ì„ ì²˜ë¦¬: ì „ì‚¬ ê²°ê³¼ê°€ ì´ë¯¸ ì„¸ì…˜ ìƒíƒœì— ë°˜ì˜ë˜ì—ˆìœ¼ë¯€ë¡œ ìë™ìœ¼ë¡œ ì…ë ¥ì°½ì— í‘œì‹œë¨
                            # st.rerun()  # UI ì—…ë°ì´íŠ¸

        col_text, col_button = st.columns([4, 1])

        # --- ì…ë ¥ í•„ë“œ ë° ë²„íŠ¼ ---
        with col_text:
            # â­ ìˆ˜ì •: ìœ„ì ¯ ìƒì„± ì „ì— ì´ˆê¸°í™” í”Œë˜ê·¸ë¥¼ í™•ì¸í•˜ì—¬ ê°’ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
            if st.session_state.get("reset_agent_response_area", False):
                st.session_state.agent_response_area_text = ""
                st.session_state.reset_agent_response_area = False
            
            # st.text_areaì˜ ê°’ì„ ì½ì–´ ì„¸ì…˜ ìƒíƒœë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ëŠ” on_changeë¥¼ ì œê±°í•˜ê³ 
            # st.text_area ìœ„ì ¯ ìì²´ì˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ send_clicked ì‹œ ìµœì‹  ê°’ì„ ì½ë„ë¡ í•©ë‹ˆë‹¤.
            # (Streamlit ê¸°ë³¸ ë™ì‘: ë²„íŠ¼ í´ë¦­ ì‹œ ìœ„ì ¯ì˜ ìµœì¢… ê°’ì´ ì„¸ì…˜ ìƒíƒœì— ë°˜ì˜ë¨)
            # â­ ìˆ˜ì •: keyë¥¼ agent_response_area_textë¡œ í†µì¼í•˜ì—¬ ì„¸ì…˜ ìƒíƒœì™€ ë™ê¸°í™”
            agent_response_input = st.text_area(
                L["agent_response_placeholder"],
                value=st.session_state.agent_response_area_text,
                key="agent_response_area_text",  # ì„¸ì…˜ ìƒíƒœ í‚¤ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •í•˜ì—¬ ë™ê¸°í™” ë³´ì¥
                height=150,
            )

            # ì†”ë£¨ì…˜ ì œê³µ ì²´í¬ë°•ìŠ¤
            st.session_state.is_solution_provided = st.checkbox(
                L["solution_check_label"],
                value=st.session_state.is_solution_provided,
                key="solution_checkbox_widget",
            )

        with col_button:
            send_clicked = st.button(L["send_response_button"], key="send_agent_response_btn")

        if send_clicked:
            # â­ ìˆ˜ì •: st.session_state.agent_response_area_textì—ì„œ ìµœì‹  ì…ë ¥ê°’ì„ ê°€ì ¸ì˜´ (keyì™€ ë™ì¼)
            agent_response = st.session_state.agent_response_area_text.strip()

            if not agent_response:
                st.warning(L["empty_response_warning"])
                # st.stop()

            # AHT íƒ€ì´ë¨¸ ì‹œì‘
            if st.session_state.start_time is None and len(st.session_state.simulator_messages) >= 1:
                st.session_state.start_time = datetime.now()

            # --- ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬ (ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬) ---
            final_response_content = agent_response
            if st.session_state.agent_attachment_file:
                file_infos = st.session_state.agent_attachment_file
                file_names = ", ".join([f["name"] for f in file_infos])
                attachment_msg = L["agent_attachment_status"].format(
                    filename=file_names, filetype=f"ì´ {len(file_infos)}ê°œ íŒŒì¼"
                )
                final_response_content = f"{agent_response}\n\n---\n{attachment_msg}"

            # ë¡œê·¸ ì—…ë°ì´íŠ¸
            st.session_state.simulator_messages.append(
                {"role": "agent_response", "content": final_response_content}
            )

            # ì…ë ¥ì°½/ì˜¤ë””ì˜¤/ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            # â­ ìˆ˜ì •: ìœ„ì ¯ì´ ìƒì„±ëœ í›„ì—ëŠ” session_stateë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ,
            # rerun í›„ ìœ„ì ¯ì´ ë‹¤ì‹œ ìƒì„±ë  ë•Œ ì´ˆê¸°ê°’ì´ ì ìš©ë˜ë„ë¡ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            st.session_state.sim_audio_bytes = None
            st.session_state.agent_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.language_transfer_requested = False
            st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""  # ì „í™” ë°œì‹  ìš”ì•½ ì´ˆê¸°í™”

            # â­ ìˆ˜ì •: ê³ ê° ë°˜ì‘ ìƒì„± ë¡œì§ì„ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ sim_stage ë³€ê²½
            st.session_state.sim_stage = "CUSTOMER_TURN"
            
            # â­ ìˆ˜ì •: agent_response_area_textëŠ” rerun í›„ ìœ„ì ¯ì´ ë‹¤ì‹œ ìƒì„±ë  ë•Œ ì´ˆê¸°í™”ë˜ë„ë¡
            # í”Œë˜ê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ìœ„ì ¯ ìƒì„± ì „ì— ì´ í”Œë˜ê·¸ë¥¼ í™•ì¸í•˜ì—¬ ê°’ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
            st.session_state.reset_agent_response_area = True
            
            # â­ ì¬ì‹¤í–‰: ì´ ë¶€ë¶„ì´ ì¦‰ì‹œ ê³ ê° ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
            st.rerun()

        # --- ì–¸ì–´ ì´ê´€ ë²„íŠ¼ ---
        st.markdown("---")
        st.markdown(f"**{L['transfer_header']}**")
        transfer_cols = st.columns(len(LANG) - 1)

        languages = list(LANG.keys())
        languages.remove(current_lang)


        def transfer_session(target_lang: str, current_messages: List[Dict[str, str]]):
            """ì–¸ì–´ ì´ê´€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  ì„¸ì…˜ ì–¸ì–´ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""

            # API í‚¤ ì²´í¬ëŠ” run_llm ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ Gemini í‚¤ë¥¼ ìš”êµ¬í•¨
            if not get_api_key("gemini"):
                st.error(LANG[current_lang]["simulation_no_key_warning"].replace('API Key', 'Gemini API Key'))
                # st.stop()
                return

            current_lang_at_start = st.session_state.language  # Source language

            # AHT íƒ€ì´ë¨¸ ì¤‘ì§€
            st.session_state.start_time = None

            # 1. ë¡œë”© ì‹œì‘ (ì‹œê°„ ì–‘í•´ ë©”ì‹œì§€ ì‹œë®¬ë ˆì´ì…˜)
            with st.spinner(L["transfer_loading"]):
                # ì‹¤ì œ ëŒ€ê¸° ì‹œê°„ 5~10ì´ˆ (3~10ë¶„ ì‹œë®¬ë ˆì´ì…˜)
                time.sleep(np.random.uniform(5, 10))

                # 2. ëŒ€í™” ê¸°ë¡ì„ ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¡œ ê°€ê³µ
                history_text = ""
                for msg in current_messages:
                    role = "Customer" if msg["role"].startswith("customer") or msg[
                        "role"] == "initial_query" else "Agent"
                    if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                       "customer_closing_response"]:
                        history_text += f"{role}: {msg['content']}\n"

                # 3. LLM ë²ˆì—­ ì‹¤í–‰ (ìˆ˜ì •ëœ ë²ˆì—­ í•¨ìˆ˜ ì‚¬ìš©)
                translated_summary = translate_text_with_llm(history_text, target_lang,
                                                             current_lang_at_start)  # Use current_lang_at_start as source

                # 4. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.transfer_summary_text = translated_summary
                st.session_state.language_at_transfer = target_lang  # Save destination language
                st.session_state.language_at_transfer_start = current_lang_at_start  # Save source language for retry
                st.session_state.language = target_lang  # Language switch

                # --- ê¸°ì¡´ ê°€ì´ë“œë¼ì¸ ì‚­ì œ ë° ìƒˆ ê°€ì´ë“œë¼ì¸ ìƒì„± (ì–¸ì–´ í†µì¼ì„± í™•ë³´) ---
                # 1. ê¸°ì¡´ Supervisor Advice ë©”ì‹œì§€ ì‚­ì œ
                st.session_state.simulator_messages = [
                    msg for msg in st.session_state.simulator_messages
                    if msg['role'] != 'supervisor'
                ]

                # 2. ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ê°€ì´ë“œë¼ì¸/ì´ˆì•ˆ ì¬ìƒì„±
                new_advice = _generate_initial_advice(
                    st.session_state.customer_query_text_area,
                    st.session_state.customer_type_sim_select,
                    st.session_state.customer_email,
                    st.session_state.customer_phone,
                    target_lang,  # ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ìƒì„±
                    st.session_state.customer_attachment_file
                )
                st.session_state.simulator_messages.append({"role": "supervisor", "content": new_advice})
                # -------------------------------------------------------------------

                st.session_state.is_solution_provided = False  # ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ìœ„í•´ í”Œë˜ê·¸ ë¦¬ì…‹
                st.session_state.language_transfer_requested = False  # í”Œë˜ê·¸ ë¦¬ì…‹
                st.session_state.sim_stage = "AGENT_TURN"

                # 5. ì´ë ¥ ì €ì¥
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                save_simulation_history_local(
                    st.session_state.customer_query_text_area,
                    customer_type_display + f" (Transferred from {current_lang_at_start} to {target_lang})",
                    st.session_state.simulator_messages,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                    is_chat_ended=False,
                )

            # 6. UI ì¬ì‹¤í–‰ (ì–¸ì–´ ë³€ê²½ ì ìš©)
            st.success(f"âœ… {LANG[target_lang]['transfer_summary_header']}ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
            st.rerun()


        for i, target_lang in enumerate(languages):
            button_label_key = f"transfer_to_{target_lang}"
            button_label = L.get(button_label_key, f"Transfer to {target_lang.capitalize()} Team")

            if transfer_cols[i].button(button_label, key=f"btn_transfer_{target_lang}"):
                transfer_session(target_lang, st.session_state.simulator_messages)

        st.markdown("---")

    # --- Language Transfer Buttons End ---

    # =========================
    # 6. ê³ ê° ë°˜ì‘ ìƒì„± ë‹¨ê³„ (CUSTOMER_TURN)
    # =========================
    elif st.session_state.sim_stage == "CUSTOMER_TURN":
        L = LANG[st.session_state.language]
        customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
        st.info(L["customer_turn_info"])

        # 1. ê³ ê° ë°˜ì‘ ìƒì„±
        with st.spinner(L["generating_customer_response"]):
            customer_response = generate_customer_reaction(st.session_state.language, is_call=False)

        # 2. ëŒ€í™” ë¡œê·¸ ì—…ë°ì´íŠ¸
        st.session_state.simulator_messages.append(
            {"role": "customer", "content": customer_response}
        )

        # 3. ì¢…ë£Œ ì¡°ê±´ ê²€í† 
        positive_closing_phrases = [L["customer_positive_response"], L["customer_no_more_inquiries"]]
        is_positive_closing = any(phrase in customer_response for phrase in positive_closing_phrases)

        # â­ ìˆ˜ì •: ê³ ê°ì´ "ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤"ë¼ê³  ë‹µë³€í–ˆì„ ë•Œ, ì†”ë£¨ì…˜ì´ ì œê³µëœ ê²½ìš°ì—ë§Œ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ í™•ì¸ ë‹¨ê³„ë¡œ ì´ë™
        # ì •í™•í•œ ë¬¸ìì—´ ë¹„êµê°€ ì•„ë‹Œ í¬í•¨ ì—¬ë¶€ë¡œ í™•ì¸ (LLM ì‘ë‹µì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        if L["customer_positive_response"] in customer_response:
            # ì†”ë£¨ì…˜ì´ ì œê³µëœ ê²½ìš°ì—ë§Œ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ í™•ì¸ ë‹¨ê³„ë¡œ ì´ë™
            if st.session_state.is_solution_provided:
                st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
            else:
                # ì†”ë£¨ì…˜ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ì§€
                st.session_state.sim_stage = "AGENT_TURN"
        elif is_positive_closing:
            # ê¸ì • ì¢…ë£Œ ì‘ë‹µ ì²˜ë¦¬
            if L['customer_no_more_inquiries'] in customer_response:
                # â­ ìˆ˜ì •: "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ë‹µë³€ ì‹œ ìë™ìœ¼ë¡œ ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë° ì‘ëŒ€ ì¢…ë£Œ
                # AHT íƒ€ì´ë¨¸ ì •ì§€
                st.session_state.start_time = None
                
                # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë©”ì‹œì§€ ì¶”ê°€
                end_msg = L["prompt_survey"]
                st.session_state.simulator_messages.append(
                    {"role": "system_end", "content": end_msg}
                )
                
                # ì±„íŒ… ì¢…ë£Œ ì²˜ë¦¬
                st.session_state.is_chat_ended = True
                st.session_state.sim_stage = "CLOSING"
                
                # ì´ë ¥ ì €ì¥ (ì¢…ë£Œ ìƒíƒœë¡œ ì €ì¥)
                save_simulation_history_local(
                    st.session_state.customer_query_text_area, customer_type_display,
                    st.session_state.simulator_messages, is_chat_ended=True,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                )
            else:
                # "ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤"ì™€ ìœ ì‚¬í•œ ê¸ì • ì‘ë‹µì¸ ê²½ìš°, ì†”ë£¨ì…˜ ì œê³µ ì—¬ë¶€ í™•ì¸
                if st.session_state.is_solution_provided:
                    st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"
                else:
                    st.session_state.sim_stage = "AGENT_TURN"


        # â­ ìˆ˜ì •: ê³ ê°ì´ ì•„ì§ ì†”ë£¨ì…˜ì— ë§Œì¡±í•˜ì§€ ì•Šê±°ë‚˜ ì¶”ê°€ ì§ˆë¬¸ì„ í•œ ê²½ìš° (ì¼ë°˜ì ì¸ í„´)
        elif customer_response.startswith(L["customer_escalation_start"]):
            st.session_state.sim_stage = "ESCALATION_REQUIRED"  # ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”
        else:
            # ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ì§€ (ê³ ê°ì´ ì¶”ê°€ ì§ˆë¬¸í•˜ê±°ë‚˜ ì •ë³´ ì œê³µ)
            st.session_state.sim_stage = "AGENT_TURN"

            # 4. ì¬ì‹¤í–‰
            # st.rerun()

            st.session_state.is_solution_provided = False  # ì¢…ë£Œ ë‹¨ê³„ ì§„ì… í›„ í”Œë˜ê·¸ ë¦¬ì…‹

            # ì´ë ¥ ì €ì¥ (ì¢…ë£Œë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì €ì¥)
        # â­ ìˆ˜ì •: "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ë‹µë³€ ì‹œì—ëŠ” ì´ë¯¸ ì´ë ¥ ì €ì¥ì„ í–ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì €ì¥ ë°©ì§€
        if st.session_state.sim_stage != "CLOSING":
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )

        st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
        # â­ ì¬ì‹¤í–‰: ê³ ê° ë°˜ì‘ì´ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ ìƒíƒœ ë³€ê²½ ë°˜ì˜
        st.rerun()


    # =========================
    # 7. ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ëŒ€ê¸° (WAIT_CLOSING_CONFIRMATION_FROM_AGENT)
    # =========================
    elif st.session_state.sim_stage == "WAIT_CLOSING_CONFIRMATION_FROM_AGENT":
        st.success("ê³ ê°ì´ ì†”ë£¨ì…˜ì— ê¸ì •ì ìœ¼ë¡œ ë°˜ì‘í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")

        col_chat_end, col_email_end = st.columns(2)  # ë²„íŠ¼ì„ ë‚˜ë€íˆ ë°°ì¹˜

        # [1] ì±„íŒ… - ì¶”ê°€ ë¬¸ì˜ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸° ë²„íŠ¼
        with col_chat_end:
            # [ìˆ˜ì • 1] ë‹¤êµ­ì–´ ë ˆì´ë¸” ì‚¬ìš©
            if st.button(L["send_closing_confirm_button"],
                         key=f"btn_send_closing_confirm_{st.session_state.sim_instance_id}"):
                # [ìˆ˜ì • 1] ë‹¤êµ­ì–´ ë ˆì´ë¸” ì‚¬ìš©
                closing_msg = L["customer_closing_confirm"]

                # ì—ì´ì „íŠ¸ ì‘ë‹µìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡
                st.session_state.simulator_messages.append(
                    {"role": "agent_response", "content": closing_msg}
                )

                # [ì¶”ê°€] TTS ë²„íŠ¼ ë Œë”ë§ì„ ìœ„í•´ sleep/rerun ê°•ì œ
                time.sleep(0.1)
                st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"
                st.rerun()

        # [2] ì´ë©”ì¼ - ìƒë‹´ ì¢…ë£Œ ë²„íŠ¼ (ì¦‰ì‹œ ì¢…ë£Œ)
        with col_email_end:
            # [ìˆ˜ì • 1] ë‹¤êµ­ì–´ ë ˆì´ë¸” ì‚¬ìš©
            if st.button(L["button_email_end_chat"], key=f"btn_email_end_chat_{st.session_state.sim_instance_id}"):
                # AHT íƒ€ì´ë¨¸ ì •ì§€
                st.session_state.start_time = None

                # [ìˆ˜ì • 1] ë‹¤êµ­ì–´ ë ˆì´ë¸” ì‚¬ìš©
                end_msg = L["prompt_survey"]
                st.session_state.simulator_messages.append(
                    {"role": "system_end", "content": "(ì‹œìŠ¤í…œ: ì´ë©”ì¼ ìƒë‹´ ì¢…ë£Œ) " + end_msg}
                )

                # [ì¶”ê°€] TTS ë²„íŠ¼ ë Œë”ë§ì„ ìœ„í•´ sleep/rerun ê°•ì œ
                time.sleep(0.1)
                st.session_state.is_chat_ended = True
                st.session_state.sim_stage = "CLOSING"  # ë°”ë¡œ CLOSINGìœ¼ë¡œ ì „í™˜
                st.rerun()

    # =========================
    # 8. ê³ ê° ìµœì¢… ì‘ë‹µ ìƒì„± ë° ì²˜ë¦¬ (WAIT_CUSTOMER_CLOSING_RESPONSE)
    # =========================
    elif st.session_state.sim_stage == "WAIT_CUSTOMER_CLOSING_RESPONSE":
        L = LANG[st.session_state.language]
        customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
        
        # â­ ìˆ˜ì •: ì´ë¯¸ ê³ ê° ì‘ë‹µì´ ìƒì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        last_customer_message = None
        for msg in reversed(st.session_state.simulator_messages):
            if msg.get("role") == "customer_rebuttal":
                last_customer_message = msg.get("content", "")
                break
        
        # ê³ ê° ì‘ë‹µì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ìƒì„±
        if last_customer_message is None:
            st.info("ì—ì´ì „íŠ¸ê°€ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê³ ê°ì˜ ìµœì¢… ë‹µë³€ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

            # ê³ ê° ë‹µë³€ ìë™ ìƒì„± (LLM Key ê²€ì¦ í¬í•¨)
            if not st.session_state.is_llm_ready:
                st.warning("LLM Keyê°€ ì—†ì–´ ê³ ê° ë°˜ì‘ ìë™ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ 'ê³ ê° ë°˜ì‘ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜ AGENT_TURNìœ¼ë¡œ ëŒì•„ê°€ì„¸ìš”ã€‚")
                if st.button(L["customer_generate_response_button"], key="btn_generate_final_response"):
                    st.session_state.sim_stage = "AGENT_TURN"
                    st.rerun()
                st.stop()
            
            # LLMì´ ì¤€ë¹„ëœ ê²½ìš° ê³ ê° ì‘ë‹µ ìƒì„±
            with st.spinner(L["generating_customer_response"]):
                final_customer_reaction = generate_customer_closing_response(st.session_state.language)

            # ë¡œê·¸ ê¸°ë¡
            st.session_state.simulator_messages.append(
                {"role": "customer_rebuttal", "content": final_customer_reaction}
            )
            last_customer_message = final_customer_reaction
        
        # ê³ ê° ì‘ë‹µì— ë”°ë¼ ì²˜ë¦¬ (ìƒì„± ì§í›„ ë˜ëŠ” ì´ë¯¸ ìˆëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬)
        if last_customer_message is None:
            # ê³ ê° ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° (ì´ë¯¸ ìƒì„±í–ˆëŠ”ë°ë„ Noneì¸ ê²½ìš°ëŠ” ì—ëŸ¬)
            st.warning("ê³ ê° ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        else:
            final_customer_reaction = last_customer_message
            
            # (A) "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ê²½ë¡œ -> FINAL_CLOSING_ACTION ë‹¨ê³„ë¡œ ì´ë™í•˜ì—¬ ë²„íŠ¼ í‘œì‹œ
            if L['customer_no_more_inquiries'] in final_customer_reaction:
                # FINAL_CLOSING_ACTION ë‹¨ê³„ë¡œ ì´ë™
                st.session_state.sim_stage = "FINAL_CLOSING_ACTION"
                st.session_state.realtime_hint_text = ""
                st.rerun()
            # (B) "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤" ê²½ë¡œ -> AGENT_TURNìœ¼ë¡œ ë³µê·€
            elif L['customer_has_additional_inquiries'] in final_customer_reaction:
                st.session_state.sim_stage = "AGENT_TURN"
                save_simulation_history_local(
                    st.session_state.customer_query_text_area, customer_type_display,
                    st.session_state.simulator_messages, is_chat_ended=False,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                )
                st.session_state.realtime_hint_text = ""
                st.rerun()

    # =========================
    # 9. ìµœì¢… ì¢…ë£Œ í–‰ë™ (FINAL_CLOSING_ACTION)
    # =========================
    elif st.session_state.sim_stage == "FINAL_CLOSING_ACTION":
        L = LANG[st.session_state.language]
        st.success("ê³ ê°ì´ ë” ì´ìƒ ë¬¸ì˜í•  ì‚¬í•­ì´ ì—†ë‹¤ê³  í™•ì¸í–ˆìŠµë‹ˆë‹¤ã€‚")

        # â­ ìˆ˜ì •: "ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë° ì‘ëŒ€ ì¢…ë£Œ" ë²„íŠ¼ í‘œì‹œ
        if st.button(L["sim_end_chat_button"], key="btn_final_end_chat"):
            # AHT íƒ€ì´ë¨¸ ì •ì§€
            st.session_state.start_time = None

            # ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë©”ì‹œì§€ ì¶”ê°€
            end_msg = L["prompt_survey"]
            st.session_state.simulator_messages.append(
                {"role": "system_end", "content": end_msg}
            )

            # ì±„íŒ… ì¢…ë£Œ ì²˜ë¦¬
            st.session_state.is_chat_ended = True
            st.session_state.sim_stage = "CLOSING"
            
            # ì´ë ¥ ì €ì¥
            customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=True,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )
            
            st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
            st.rerun()

# ========================================
# ì „í™” ì‹œë®¬ë ˆì´í„° ë¡œì§
# ========================================

elif feature_selection == L["sim_tab_phone"]:
    st.header(L["phone_header"])
    st.markdown(L["simulator_desc"])

    current_lang = st.session_state.language
    L = LANG[current_lang]



    # ========================================
    # AHT íƒ€ì´ë¨¸ (IN_CALL ìƒíƒœì—ì„œë§Œ ë™ì‘)
    # ========================================
    if st.session_state.call_sim_stage == "IN_CALL":
        # AHT íƒ€ì´ë¨¸ ê³„ì‚° ë¡œì§
        col_timer, col_duration = st.columns([1, 4])

        if st.session_state.start_time is not None:
            now = datetime.now()

            # Hold ì¤‘ì´ë¼ë©´, Hold ìƒíƒœê°€ ëœ ì´í›„ì˜ ì‹œê°„ì„ í˜„ì¬ total_hold_durationì— ë”í•˜ì§€ ì•ŠìŒ (Resume ì‹œ ì •ì‚°)
            if st.session_state.is_on_hold and st.session_state.hold_start_time:
                # Hold ì¤‘ì´ì§€ë§Œ AHT íƒ€ì´ë¨¸ëŠ” ê³„ì† í˜ëŸ¬ê°€ì•¼ í•˜ë¯€ë¡œ, Hold ì‹œê°„ì€ ì œì™¸í•˜ì§€ ì•Šê³  ìµœì¢… AHT ê³„ì‚°ì—ë§Œ ì‚¬ìš©
                elapsed_time_total = now - st.session_state.start_time
            else:
                elapsed_time_total = now - st.session_state.start_time

            # â­ AHTëŠ” í†µí™” ì‹œì‘ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ì´ ê²½ê³¼ ì‹œê°„ì…ë‹ˆë‹¤.
            total_seconds = elapsed_time_total.total_seconds()
            total_seconds = max(0, total_seconds)  # ìŒìˆ˜ ë°©ì§€

            # ì‹œê°„ í˜•ì‹ í¬ë§·íŒ…
            minutes = int(total_seconds // 60)
            seconds = int(total_seconds % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # ê²½ê³  ê¸°ì¤€
            if total_seconds > 900:  # 15ë¶„
                delta_str = L["timer_info_risk"]
                delta_color = "inverse"
            elif total_seconds > 600:  # 10ë¶„
                delta_str = L["timer_info_warn"]
                delta_color = "off"
            else:
                delta_str = L["timer_info_ok"]
                delta_color = "normal"

                with col_timer:
                    # AHT íƒ€ì´ë¨¸ í‘œì‹œ
                    st.metric(L["timer_metric"], time_str, delta=delta_str, delta_color=delta_color)

                # â­ ìˆ˜ì •: AHT íƒ€ì´ë¨¸ ì‹¤ì‹œê°„ ê°±ì‹ ì„ ìœ„í•œ ê°•ì œ ì¬ì‹¤í–‰ ë¡œì§ ì¶”ê°€
                # í†µí™” ì¤‘ì´ê³ , Hold ìƒíƒœê°€ ì•„ë‹ ë•Œë§Œ 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸í•˜ì—¬ ì‹¤ì‹œê°„ì„±ì„ í™•ë³´
                if not st.session_state.is_on_hold and total_seconds < 1000:
                    time.sleep(1)
                    # st.rerun()  # ë§¤ ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰í•˜ì—¬ AHT ê°±ì‹ 

        # ========================================
        # í™”ë©´ êµ¬ë¶„ (ì• ë‹ˆë©”ì´ì…˜ / CC)
        # ========================================
    col_video, col_cc = st.columns([1, 2])

    with col_video:
        st.subheader("ğŸ“º ê³ ê° ì˜ìƒ ì‹œë®¬ë ˆì´ì…˜")

        if st.session_state.call_sim_stage == "WAITING_CALL":
            st.info("í†µí™” ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")

        elif st.session_state.call_sim_stage == "CALL_ENDED":
            st.info("í†µí™” ì¢…ë£Œ")

        else:
            # â­ ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì˜µì…˜ ì¶”ê°€ (ë¡œì»¬ ê²½ë¡œ ì§€ì›)
            with st.expander("ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ/ë¡œë“œ", expanded=False):
                # ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ
                uploaded_video = st.file_uploader(
                    "ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ (MP4, WebM, OGG)",
                    type=["mp4", "webm", "ogg"],
                    key="customer_video_uploader"
                )
                
                # ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì…ë ¥
                video_path_input = st.text_input(
                    "ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì…ë ¥",
                    placeholder="ì˜ˆ: C:\\Users\\Admin\\Downloads\\video.mp4 ë˜ëŠ” video.mp4",
                    key="video_path_input"
                )
                
                # ë¹„ë””ì˜¤ ì¬ìƒ
                video_to_play = None
                video_format = "video/mp4"
                
                if uploaded_video:
                    # ì—…ë¡œë“œëœ íŒŒì¼ ì‚¬ìš©
                    video_to_play = uploaded_video.read()
                    # íŒŒì¼ í™•ì¥ìë¡œ í¬ë§· ê²°ì •
                    file_ext = uploaded_video.name.split('.')[-1].lower()
                    video_format = {
                        'mp4': 'video/mp4',
                        'webm': 'video/webm',
                        'ogg': 'video/ogg'
                    }.get(file_ext, 'video/mp4')
                elif video_path_input:
                    # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©
                    try:
                        # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                        if os.path.isabs(video_path_input):
                            video_path = video_path_input
                        else:
                            # ìƒëŒ€ ê²½ë¡œëŠ” ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€
                            video_path = os.path.join(os.getcwd(), video_path_input)
                        
                        if os.path.exists(video_path):
                            with open(video_path, "rb") as f:
                                video_to_play = f.read()
                            # íŒŒì¼ í™•ì¥ìë¡œ í¬ë§· ê²°ì •
                            file_ext = os.path.splitext(video_path)[1].lower().lstrip('.')
                            video_format = {
                                'mp4': 'video/mp4',
                                'webm': 'video/webm',
                                'ogg': 'video/ogg'
                            }.get(file_ext, 'video/mp4')
                        else:
                            st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
                    except Exception as e:
                        st.error(f"ë¹„ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
                
                # ë¹„ë””ì˜¤ ì¬ìƒ
                if video_to_play:
                    try:
                        # Streamlit ë¬¸ì„œ: bytes ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
                        st.video(video_to_play, format=video_format, autoplay=False, loop=False, muted=False)
                        st.success("âœ… ë¹„ë””ì˜¤ ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        st.error(f"ë¹„ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                        st.info("ğŸ’¡ ë¹„ë””ì˜¤ê°€ H.264 ì½”ë±ìœ¼ë¡œ ì¸ì½”ë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. MP4V ì½”ë±ì€ ë¸Œë¼ìš°ì €ì—ì„œ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ìƒíƒœ ì„ íƒ
            if st.session_state.is_on_hold:
                avatar_state = "HOLD"
            else:
                avatar_state = st.session_state.customer_avatar.get("state", "NEUTRAL")

            # â­ Lottie ì œê±°: ë¡œë”© ë¬¸ì œë¡œ ì¸í•´ ì™„ì „íˆ ì œê±°í•˜ê³  ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
            avatar_emoji = {
                "NEUTRAL": "ğŸ˜",
                "HAPPY": "ğŸ˜Š",
                "ANGRY": "ğŸ˜ ",
                "ASKING": "ğŸ¤”",
                "HOLD": "â¸ï¸"
            }.get(avatar_state, "ğŸ˜")
            
            st.markdown(f"### {avatar_emoji} ê³ ê° ì•„ë°”íƒ€")
            st.info(f"ìƒíƒœ: {avatar_state}")

    with col_cc:
        st.markdown(
            f"## {L['call_status_ringing'].format(number=st.session_state.incoming_phone_number)}"
        )
        st.markdown("---")

    # ========================================
    # WAITING / RINGING ìƒíƒœ
    # ========================================
    if st.session_state.call_sim_stage in ["WAITING_CALL", "RINGING"]:

        if "call_sim_mode" not in st.session_state:
            st.session_state.call_sim_mode = "INBOUND"  # INBOUND or OUTBOUND

        if st.session_state.call_sim_mode == "INBOUND":
            st.subheader(L["call_status_waiting"])
        else:
            st.subheader(L["button_call_outbound"])

        # ì´ˆê¸° ë¬¸ì˜ ì…ë ¥ (ê³ ê°ì´ ì „í™”ë¡œ ë§í•  ë‚´ìš©)
        st.session_state.call_initial_query = st.text_area(
            L["customer_query_label"],
            key="call_initial_query_text_area",
            height=100,
            placeholder=L["call_query_placeholder"],
        )

        # ê°€ìƒ ì „í™”ë²ˆí˜¸ í‘œì‹œ
        st.session_state.incoming_phone_number = st.text_input(
            "Incoming/Outgoing Phone Number",
            key="incoming_phone_number_input",
            value=st.session_state.incoming_phone_number,
            placeholder=L["call_number_placeholder"],
        )

        # ê³ ê° ìœ í˜• ì„ íƒ
        customer_type_options = L["customer_type_options"]
        default_idx = customer_type_options.index(
            st.session_state.customer_type_sim_select) if st.session_state.customer_type_sim_select in customer_type_options else 0

        st.session_state.customer_type_sim_select = st.selectbox(
            L["customer_type_label"],
            customer_type_options,
            index=default_idx,
            key="call_customer_type_sim_select_widget",
        )

        st.markdown("---")

        col_in, col_out = st.columns(2)

        # ì „í™” ì‘ë‹µ (ìˆ˜ì‹ )
        with col_in:
            if st.button(L["button_answer"], key=f"answer_call_btn_{st.session_state.sim_instance_id}"):
                # ì…ë ¥ ê²€ì¦
                if not st.session_state.call_initial_query.strip():
                    st.warning(L["simulation_warning_query"])
                    # st.stop()

                if not st.session_state.is_llm_ready or st.session_state.openai_client is None:
                    st.error(L["simulation_no_key_warning"] + " " + L["openai_missing"])
                    # st.stop()

                # INBOUND ëª¨ë“œ ì„¤ì •
                st.session_state.call_sim_mode = "INBOUND"

                # ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ë° ì‹œì‘
                st.session_state.call_sim_stage = "IN_CALL"
                st.session_state.is_call_ended = False
                st.session_state.is_on_hold = False
                st.session_state.total_hold_duration = timedelta(0)
                st.session_state.hold_start_time = None
                st.session_state.start_time = datetime.now()  # í†µí™” ì‹œì‘ ì‹œê°„ (AHT ì‹œì‘)
                st.session_state.simulator_messages = []
                st.session_state.current_customer_audio_text = ""
                st.session_state.current_agent_audio_text = ""
                st.session_state.agent_response_input_box_widget_call = ""
                st.session_state.sim_instance_id = str(uuid.uuid4())
                st.session_state.call_summary_text = ""  # ìš”ì•½ ì´ˆê¸°í™”
                st.session_state.customer_initial_audio_bytes = None  # ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
                st.session_state.customer_history_summary = ""  # AI ìš”ì•½ ì´ˆê¸°í™” (ì¶”ê°€)
                st.session_state.sim_audio_bytes = None  # ë…¹ìŒ íŒŒì¼ ì´ˆê¸°í™” (ì¶”ê°€)

                # â­ [ìˆ˜ì • 1-1] IN_CALL ì§„ì… ì‹œ ì¸ì‚¬ë§ ìƒì„± í”Œë˜ê·¸ í™œì„±í™”
                st.session_state.just_entered_call = True
                st.session_state.customer_turn_start = False

                # ê³ ê°ì˜ ì²« ë²ˆì§¸ ìŒì„± ë©”ì‹œì§€ (ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ë©”ì‹œì§€)
                initial_query_text = st.session_state.call_initial_query.strip()
                st.session_state.current_customer_audio_text = initial_query_text

                # â­ ê³ ê°ì˜ ì²« ë¬¸ì˜ TTS ìŒì„± ìƒì„± ë° ì €ì¥
                with st.spinner(L["tts_status_generating"] + " (Initial Customer Query)"):
                    audio_bytes, msg = synthesize_tts(initial_query_text, st.session_state.language, role="customer")
                    if audio_bytes:
                        st.session_state.customer_initial_audio_bytes = audio_bytes
                    else:
                        st.error(f"âŒ {msg}")
                        st.session_state.customer_initial_audio_bytes = None

                # âœ… ìƒíƒœ ë³€ê²½ í›„ ì¬ì‹¤í–‰í•˜ì—¬ IN_CALL ìƒíƒœë¡œ ì „í™˜
                st.rerun()

        # ì „í™” ë°œì‹  (ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘)
        with col_out:
            st.markdown(f"### {L['button_call_outbound']}")
            call_targets = [
                L["call_target_customer"],
                L["call_target_partner"]
            ]

            call_target_selection = st.radio(
                "ë°œì‹  ëŒ€ìƒ ì„ íƒ",
                call_targets,
                key="outbound_call_target_radio",
                horizontal=True
            )

            if st.button(L["button_call_outbound"], key=f"outbound_call_start_btn_{st.session_state.sim_instance_id}", type="secondary"):
                # ì…ë ¥ ê²€ì¦
                if not st.session_state.call_initial_query.strip():
                    st.warning("ì „í™” ë°œì‹  ëª©í‘œ (ê³ ê° ë¬¸ì˜ ë‚´ìš©)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚")
                    # st.stop()

                if not st.session_state.is_llm_ready or st.session_state.openai_client is None:
                    st.error(L["simulation_no_key_warning"] + " " + L["openai_missing"])
                    # st.stop()

                # OUTBOUND ëª¨ë“œ ì„¤ì • ë° ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘
                st.session_state.call_sim_mode = "OUTBOUND"

                # ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ë° ì‹œì‘
                st.session_state.call_sim_stage = "IN_CALL"
                st.session_state.is_call_ended = False
                st.session_state.is_on_hold = False
                st.session_state.total_hold_duration = timedelta(0)
                st.session_state.hold_start_time = None
                st.session_state.start_time = datetime.now()  # í†µí™” ì‹œì‘ ì‹œê°„ (AHT ì‹œì‘)
                st.session_state.simulator_messages = []

                # â­ [ìˆ˜ì • 1-1] IN_CALL ì§„ì… ì‹œ ì¸ì‚¬ë§ ìƒì„± í”Œë˜ê·¸ í™œì„±í™”
                st.session_state.just_entered_call = True
                st.session_state.customer_turn_start = False

                initial_query_text = st.session_state.call_initial_query.strip()

                # ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ì—ì´ì „íŠ¸ê°€ ë¨¼ì € ë§í•´ì•¼ í•˜ë¯€ë¡œ, ê³ ê° CC í…ìŠ¤íŠ¸ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ë¡œ ì„¤ì •
                st.session_state.current_customer_audio_text = f"ğŸ“ {L['button_call_outbound']} ì„±ê³µ! {call_target_selection}ì´(ê°€) ë°›ì•˜ìŠµë‹ˆë‹¤ã€‚ ì ì‹œ í›„ ì‘ë‹µì´ ì‹œì‘ë©ë‹ˆë‹¤ã€‚ (ë¬¸ì˜ ëª©í‘œ: {initial_query_text[:50]}...)"
                st.session_state.current_agent_audio_text = ""  # Agent speaks first
                st.session_state.agent_response_input_box_widget_call = ""
                st.session_state.sim_instance_id = str(uuid.uuid4())
                st.session_state.call_summary_text = ""
                st.session_state.customer_initial_audio_bytes = None
                st.session_state.customer_history_summary = ""
                st.session_state.sim_audio_bytes = None

                st.success(f"'{call_target_selection}'ì—ê²Œ ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ì˜ ì²« ì‘ë‹µì„ ë…¹ìŒí•˜ì„¸ìš”ã€‚")
                st.rerun()

        # ------------------
        # IN_CALL ìƒíƒœ (í†µí™” ì¤‘)
        # ------------------
    elif st.session_state.call_sim_stage == "IN_CALL":
        # --------------------------------------------------------------------
        # â­ 1ë‹¨ê³„: ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ ì¬ìƒ ë° ë¡œê·¸ ê¸°ë¡ (just_entered_call=True)
        # --------------------------------------------------------------------
        if st.session_state.just_entered_call:
            initial_query = st.session_state.call_initial_query.strip()

            agent_greeting = generate_agent_first_greeting(
                st.session_state.language,
                initial_query
            )

            # ì—ì´ì „íŠ¸ ì²« ì¸ì‚¬ë§ TTS (ìë™ ì¬ìƒ)
            if st.session_state.openai_client and agent_greeting:
                with st.spinner(L["tts_status_generating"] + " (Agent Greeting)"):
                    audio_bytes, msg = synthesize_tts(
                        agent_greeting, st.session_state.language, role="agent"
                    )
                    if audio_bytes:
                        # Streamlit ë¬¸ì„œ: autoplayëŠ” ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì œí•œë  ìˆ˜ ìˆìŒ
                        try:
                            st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                            st.success("âœ… ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ ìë™ ì¬ìƒ ì™„ë£Œ. ê³ ê° ë¬¸ì˜ ì¬ìƒì„ ì¤€ë¹„í•©ë‹ˆë‹¤.")
                        # â­ ìˆ˜ì •: TTS ë™ê¸°í™” ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ ì§§ì€ ëŒ€ê¸° í›„ rerun
                            time.sleep(1)
                        except Exception as e:
                            st.warning(f"ìë™ ì¬ìƒ ì‹¤íŒ¨ (ë¸Œë¼ìš°ì € ì •ì±…): {e}. ìˆ˜ë™ìœ¼ë¡œ ì¬ìƒí•´ì£¼ì„¸ìš”.")
                            st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                            st.success("âœ… ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ ìƒì„± ì™„ë£Œ. ì¬ìƒ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                    else:
                        st.error(f"âŒ TTS ì˜¤ë¥˜: {msg}")

            # 1. CCì— ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ ë°˜ì˜ ë° ë¡œê·¸ ê¸°ë¡
            st.session_state.current_agent_audio_text = agent_greeting
            st.session_state.simulator_messages.append(
                {"role": "agent", "content": agent_greeting}
            )

            # ì•„ë°”íƒ€ í‘œì • ì´ˆê¸°í™”
            st.session_state.customer_avatar["state"] = "NEUTRAL"
            st.session_state.just_entered_call = False

            # ë‹¤ìŒ ë‹¨ê³„(ê³ ê° ë¬¸ì˜ ì¬ìƒ)ë¡œ ì „í™˜
            st.session_state.customer_turn_start = True
            st.rerun()  # ë‹¤ìŒ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ê³ ê° ë¬¸ì˜ê°€ ì¬ìƒë˜ë„ë¡ ìœ ë„

        # --------------------------------------------------------------------
        # â­ 2ë‹¨ê³„: ê³ ê° ë¬¸ì˜ ì¬ìƒ ë° CC ì—…ë°ì´íŠ¸ (customer_turn_start=True)
        # --------------------------------------------------------------------
        elif st.session_state.customer_turn_start:
            customer_first_utterance = st.session_state.call_initial_query.strip()

            # 1. ê³ ê°ì˜ ì²« ë¬¸ì˜ TTS ìŒì„± ì¬ìƒ
            # INBOUND ì‹œì‘ ì‹œ ì €ì¥ëœ ì˜¤ë””ì˜¤ë¥¼ ì‚¬ìš©í•˜ë©°, key ì¸ìˆ˜ë¥¼ ì œê±°í•˜ì—¬ TypeError ë°©ì§€
            if st.session_state.customer_initial_audio_bytes:
                # Streamlit ë¬¸ì„œ: autoplayëŠ” ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì œí•œë  ìˆ˜ ìˆìŒ
                try:
                    st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                except Exception as e:
                    st.warning(f"ìë™ ì¬ìƒ ì‹¤íŒ¨: {e}. ìˆ˜ë™ìœ¼ë¡œ ì¬ìƒí•´ì£¼ì„¸ìš”.")
                    st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=False)
                st.success("âœ… ê³ ê°ì˜ ìµœì´ˆ ë¬¸ì˜ ì¬ìƒ ì‹œì‘")
            else:
                st.error("âŒ ê³ ê° ìµœì´ˆ ë¬¸ì˜ ì˜¤ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")

            # 2. ê³ ê°ì˜ ì²« ë¬¸ì˜ CCì— ë°˜ì˜ ë° ë¡œê·¸ ê¸°ë¡
            st.session_state.current_customer_audio_text = customer_first_utterance
            st.session_state.current_agent_audio_text = ""  # ì—ì´ì „íŠ¸ CCë¥¼ ë¹„ì›Œ ë…¹ìŒ ëŒ€ê¸° ìƒíƒœë¡œ ë§Œë“¦

            # 3. ë¡œê·¸ì— êµí™˜ ê¸°ë¡ (ì—ì´ì „íŠ¸ ì¸ì‚¬ë§ + ê³ ê° ë¬¸ì˜)
            agent_greeting = st.session_state.simulator_messages[-1]['content']
            log_entry = f"Agent (Greeting): {agent_greeting} | Customer (Initial Query): {customer_first_utterance}"
            # ê¸°ì¡´ì˜ "agent" ì—­í•  ë©”ì‹œì§€ë¥¼ ì‚­ì œí•˜ê³ , êµí™˜ ë¡œê·¸ë¡œ ëŒ€ì²´
            st.session_state.simulator_messages = [
                msg for msg in st.session_state.simulator_messages if msg.get("role") != "agent"
            ]
            st.session_state.simulator_messages.append({"role": "phone_exchange", "content": log_entry})

            # 4. ë‹¨ê³„ ì¢…ë£Œ
            st.session_state.customer_turn_start = False
            st.rerun()  # CC ë°˜ì˜ ë° ë…¹ìŒ ëŒ€ê¸° ìƒíƒœë¡œ ìµœì¢… ì „í™˜
        # ------------------------------
        # ì „í™” í†µí™” ì œëª©
        # ------------------------------
        if st.session_state.call_sim_mode == "INBOUND":
            title = L['call_status_ringing'].format(number=st.session_state.incoming_phone_number)
        else:
            title = L['button_call_outbound'] + f" ({st.session_state.incoming_phone_number})"

        st.markdown(f"## {title}")
        st.markdown("---")

        # ------------------------------
        # Hangup / Hold ë²„íŠ¼
        # ------------------------------
        col_hangup, col_hold = st.columns(2)

        with col_hangup:
            if st.button(L["button_hangup"], key="hangup_call_btn"):

                # Hold ì •ì‚°
                if st.session_state.is_on_hold and st.session_state.hold_start_time:
                    st.session_state.total_hold_duration += datetime.now() - st.session_state.hold_start_time

                # ìš”ì•½ ìƒì„±
                with st.spinner("AI ìš”ì•½ ìƒì„± ì¤‘..."):
                    # â­ [ìˆ˜ì • 9] í•¨ìˆ˜ëª… í†µì¼: summarize_history_for_callë¡œ ë³€ê²½ ë° í˜¸ì¶œ
                    summary = summarize_history_for_call(
                        st.session_state.simulator_messages,
                        st.session_state.call_initial_query,
                        st.session_state.language
                    )
                    st.session_state.call_summary_text = summary

                # ì¢…ë£Œ
                st.session_state.call_sim_stage = "CALL_ENDED"
                st.session_state.is_call_ended = True

                # â­ [ìˆ˜ì • 10] Hangup í›„ UI ê°±ì‹ ì„ ìœ„í•´ rerun ì¶”ê°€
                st.rerun()

        # ------------------------------
        # Hold / Resume
        # ------------------------------
        with col_hold:
            if st.session_state.is_on_hold:
                if st.button(L["button_resume"], key="resume_call_btn"):
                    # Hold ìƒíƒœ í•´ì œ ë° ì‹œê°„ ì •ì‚°
                    st.session_state.is_on_hold = False
                    if st.session_state.hold_start_time:
                        st.session_state.total_hold_duration += datetime.now() - st.session_state.hold_start_time
                        st.session_state.hold_start_time = None
                    st.rerun()  # â­ [ìˆ˜ì • 10] Resume í›„ UI ê°±ì‹ ì„ ìœ„í•´ rerun ì¶”ê°€
            else:
                if st.button(L["button_hold"], key="hold_call_btn"):
                    st.session_state.is_on_hold = True
                    st.session_state.hold_start_time = datetime.now()
                    st.rerun()  # â­ [ìˆ˜ì • 10] Hold í›„ UI ê°±ì‹ ì„ ìœ„í•´ rerun ì¶”ê°€

        # ------------------------------
        # Hold í‘œì‹œ
        # ------------------------------
        if st.session_state.is_on_hold:
            if st.session_state.hold_start_time:
                current_hold = datetime.now() - st.session_state.hold_start_time
            else:
                current_hold = timedelta(0)

            total_hold = st.session_state.total_hold_duration + current_hold
            hold_str = str(total_hold).split('.')[0]

            st.warning(L["hold_status"].format(duration=hold_str))
            time.sleep(1)

        # ------------------------------
        # (ì¤‘ëµ) - **ì´ê´€, íŒíŠ¸, ìš”ì•½, CC, Whisper ì „ì‚¬, ê³ ê° ë°˜ì‘ ìƒì„±**
        # ------------------------------
        def transfer_session(target_lang: str, current_messages: List[Dict[str, str]]):
            """ì–¸ì–´ ì´ê´€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  ì„¸ì…˜ ì–¸ì–´ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""

            current_lang = st.session_state.language  # í˜„ì¬ ì–¸ì–´ í™•ì¸ (Source language)
            L = LANG[current_lang]

            # API í‚¤ ì²´í¬
            if not st.session_state.is_llm_ready:
                st.error(L["simulation_no_key_warning"].replace('API Key', 'LLM API Key'))
                return

            current_lang_at_start = st.session_state.language  # Source language

            # AHT íƒ€ì´ë¨¸ ì •ì§€ (ì‹¤ì œë¡œ í†µí™”ê°€ ì¢…ë£Œë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ, AHTëŠ” ê³„ì† íë¦„)
            # st.session_state.start_time = None

            # 1. ë¡œë”© ì‹œì‘ (ì‹œê°„ ì–‘í•´ ë©”ì‹œì§€ ì‹œë®¬ë ˆì´ì…˜)
            with st.spinner(L["transfer_loading"]):
                time.sleep(np.random.uniform(5, 10))

                # 2. ëŒ€í™” ê¸°ë¡ì„ ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¡œ ê°€ê³µ
                history_text = ""
                for msg in current_messages:
                    role = "Customer" if msg["role"].startswith("customer") or msg[
                        "role"] == "initial_query" else "Agent"
                    if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                       "customer_closing_response", "phone_exchange"]:  # phone_exchange ì¶”ê°€
                        history_text += f"{role}: {msg['content']}\n"

                # 3. LLM ë²ˆì—­ ì‹¤í–‰ (ìˆ˜ì •ëœ ë²ˆì—­ í•¨ìˆ˜ ì‚¬ìš©)
                translated_summary = translate_text_with_llm(history_text, target_lang,
                                                             current_lang_at_start)  # Use current_lang_at_start as source

                # 4. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.transfer_summary_text = translated_summary
                st.session_state.language_at_transfer = target_lang  # Save destination language
                st.session_state.language_at_transfer_start = current_lang_at_start  # Save source language for retry
                st.session_state.language = target_lang  # Language switch

                # --- ì‹œìŠ¤í…œ ì´ê´€ ë©”ì‹œì§€ ì¶”ê°€ ---
                # ì „í™”ì—ì„œëŠ” ë³„ë„ì˜ Supervisor ë©”ì‹œì§€ ì—†ì´ ë¡œê·¸ì—ë§Œ ë‚¨ê¹€
                st.session_state.simulator_messages.append(
                    {"role": "system_transfer",
                     "content": LANG[target_lang]['transfer_system_msg'].format(target_lang=target_lang)})

                st.session_state.is_solution_provided = False
                st.session_state.language_transfer_requested = False

                # ì´ê´€ í›„ ìƒíƒœ ì „í™˜: í†µí™” ì¤‘ì¸ ìƒíƒœëŠ” ìœ ì§€
                st.session_state.call_sim_stage = "IN_CALL"

                # 5. ì´ë ¥ ì €ì¥
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                save_simulation_history_local(
                    st.session_state.call_initial_query,
                    customer_type_display + f" (Transferred from {current_lang_at_start} to {target_lang})",
                    st.session_state.simulator_messages,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                    is_chat_ended=False,
                    is_call=(st.session_state.call_sim_stage == "IN_CALL")  # ì „í™” ì´ë ¥ì„ì„ í‘œì‹œ
                )

            # 6. UI ì¬ì‹¤í–‰ (ì–¸ì–´ ë³€ê²½ ì ìš©)
            st.success(f"âœ… {LANG[target_lang]['transfer_summary_header']}ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
            st.rerun()


        st.markdown("---")
        st.markdown(f"**{L['transfer_header']}**")
        transfer_cols = st.columns(len(LANG) - 1)

        languages = list(LANG.keys())
        languages.remove(current_lang)

        # transfer_session í•¨ìˆ˜ë¥¼ ì¬ì •ì˜í•˜ì§€ ì•Šê³ , ê¸°ì¡´ì˜ transfer_session í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        for i, target_lang in enumerate(languages):
            button_label_key = f"transfer_to_{target_lang}"
            button_label = L.get(button_label_key, f"Transfer to {target_lang.capitalize()} Team")

            # â­ [ìˆ˜ì • FIX] í‚¤ ì¤‘ë³µ ì˜¤ë¥˜ í•´ê²°: ì„¸ì…˜ IDì™€ ëŒ€ìƒ ì–¸ì–´ë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
            if transfer_cols[i].button(button_label, key=f"btn_transfer_phone_{target_lang}_{st.session_state.sim_instance_id}"):
                # transfer_session í˜¸ì¶œ ì‹œ, í˜„ì¬ í†µí™” ë©”ì‹œì§€(simulator_messages)ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.
                transfer_session(target_lang, st.session_state.simulator_messages)

        # =========================
        # AI ìš”ì•½ ë²„íŠ¼ ë° í‘œì‹œ ë¡œì§ (ì¶”ê°€ëœ ê¸°ëŠ¥)
        # =========================
        st.markdown("---")
        # â­ history_expander_titleì—ì„œ ê´„í˜¸ ì•ˆ ë‚´ìš©ë§Œ ì œê±° (ì˜ˆ: (ìµœê·¼ 10ê±´))
        summary_title = L['history_expander_title'].split('(')[0].strip()
        st.markdown(f"### ğŸ“‘ {summary_title} ìš”ì•½")

        # 1. ìš”ì•½/ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ ì˜ì—­
        col_sum_btn, col_trans_btn = st.columns(2)

        with col_sum_btn:
            # â­ [ìˆ˜ì • FIX] í‚¤ ì¤‘ë³µ ì˜¤ë¥˜ í•´ê²°: ì„¸ì…˜ IDë¥¼ í‚¤ì— ì¶”ê°€
            if st.button(L["btn_request_phone_summary"], key=f"btn_request_phone_summary_{st.session_state.sim_instance_id}"):
                # ìš”ì•½ í•¨ìˆ˜ í˜¸ì¶œ
                st.session_state.customer_history_summary = summarize_history_with_ai(st.session_state.language)
                st.rerun()

        # 2. ì´ê´€ ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ (ì´ê´€ í›„ ë²ˆì—­ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš°)
        if st.session_state.language != st.session_state.language_at_transfer_start and not st.session_state.transfer_summary_text:
            with col_trans_btn:
                # â­ [ìˆ˜ì • FIX] í‚¤ ì¤‘ë³µ ì˜¤ë¥˜ í•´ê²°: ì„¸ì…˜ IDì™€ ì–¸ì–´ ì½”ë“œë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±
                retry_key = f"btn_retry_translation_{st.session_state.language_at_transfer_start}_{st.session_state.language}_{st.session_state.sim_instance_id}"
                if st.button(L["button_retry_translation"], key=retry_key):
                    with st.spinner(L["transfer_loading"]):
                        # ì´ê´€ ë²ˆì—­ ë¡œì§ ì¬ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                        translated_summary = translate_text_with_llm(
                            get_chat_history_for_prompt(include_attachment=False),
                            st.session_state.language,
                            st.session_state.language_at_transfer_start
                        )
                        st.session_state.transfer_summary_text = translated_summary
                        st.rerun()

        # 3. ìš”ì•½ ë‚´ìš© í‘œì‹œ
        if st.session_state.transfer_summary_text:
            st.subheader(f"ğŸ” {L['transfer_summary_header']}")
            st.info(st.session_state.transfer_summary_text)
            # â­ ì´ê´€ ìš”ì•½ì— TTS ë²„íŠ¼ ì¶”ê°€
            render_tts_button(
                st.session_state.transfer_summary_text,
                st.session_state.language,
                role="agent",
                prefix="trans_summary_tts_call",
                index=-1  # ê³ ìœ  ì„¸ì…˜ ID ê¸°ë°˜ì˜ í‚¤ë¥¼ ìƒì„±í•˜ë„ë¡ ì§€ì‹œ
            )
        elif st.session_state.customer_history_summary:
            st.subheader("ğŸ’¡ AI ìš”ì•½")
            st.info(st.session_state.customer_history_summary)

        st.markdown("---")

        # --- ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸ ì˜ì—­ ---
        hint_cols = st.columns([4, 1])
        with hint_cols[0]:
            st.info(L["hint_placeholder"] + st.session_state.realtime_hint_text)

        with hint_cols[1]:
            # íŒíŠ¸ ìš”ì²­ ë²„íŠ¼
            if st.button(L["button_request_hint"], key=f"btn_request_hint_call_{st.session_state.sim_instance_id}"):
                with st.spinner(L["response_generating"]):
                    # ì „í™” íƒ­ì´ë¯€ë¡œ is_call=True
                    hint = generate_realtime_hint(current_lang, is_call=True)
                    st.session_state.realtime_hint_text = hint
                    st.rerun()

        # =========================
        # CC ìë§‰ / ìŒì„± ì…ë ¥ ë° ì œì–´ ë¡œì§ (ê¸°ì¡´ ë¡œì§)
        # =========================================

        # --- ì‹¤ì‹œê°„ CC ìë§‰ / ì „ì‚¬ ì˜ì—­ ---
        st.subheader(L["cc_live_transcript"])

        if st.session_state.is_on_hold:
            st.text_area("Customer", value="[ê³ ê°: ì ì‹œ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤...]", height=50, disabled=True, key="customer_live_cc_area")
            st.text_area("Agent", value="[ì—ì´ì „íŠ¸: Hold ì¤‘ì…ë‹ˆë‹¤. í†µí™” ì¬ê°œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.]", height=50, disabled=True,
                         key="agent_live_cc_area")
        else:
            # ê³ ê° CC (LLM ìƒì„± í…ìŠ¤íŠ¸)
            st.text_area(
                "Customer",
                value=st.session_state.current_customer_audio_text,
                height=50,
                disabled=True,
                key="customer_live_cc_area",
            )

            # ì—ì´ì „íŠ¸ CC (ë§ˆì´í¬ ì „ì‚¬)
            st.text_area(
                "Agent",
                value=st.session_state.current_agent_audio_text,
                height=50,
                disabled=True,
                key="agent_live_cc_area",
            )

        st.markdown("---")

        # â­ ìˆ˜ì •: ì „ì‚¬ í›„ ê³ ê° ë°˜ì‘ ìƒì„± ì²˜ë¦¬ (ë‹¤ìŒ ì‹¤í–‰ ì£¼ê¸°)
        # ì „ì‚¬ ê²°ê³¼ê°€ CCì— ë¨¼ì € í‘œì‹œëœ í›„ ê³ ê° ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ ë¶„ë¦¬
        if st.session_state.get("process_customer_reaction") and st.session_state.get("pending_agent_transcript"):
            pending_transcript = st.session_state.pending_agent_transcript
            # í”Œë˜ê·¸ ì´ˆê¸°í™”
            st.session_state.process_customer_reaction = False
            del st.session_state.pending_agent_transcript

            # ê³ ê° ë°˜ì‘ ìƒì„±
            with st.spinner("ê³ ê° ë°˜ì‘ ìƒì„± ì¤‘..."):
                customer_reaction = generate_customer_reaction_for_call(
                    st.session_state.language,
                    pending_transcript
                )

                # ê³ ê° ë°˜ì‘ì„ TTSë¡œ ì¬ìƒ ë° CCì— ë°˜ì˜
                if not customer_reaction.startswith("âŒ"):
                    audio_bytes, msg = synthesize_tts(customer_reaction, st.session_state.language, role="customer")
                    if audio_bytes:
                        # Streamlit ë¬¸ì„œ: autoplayëŠ” ë¸Œë¼ìš°ì € ì •ì±…ìƒ ì œí•œë  ìˆ˜ ìˆìŒ
                        try:
                            st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                            st.success(f"ğŸ—£ï¸ ê³ ê°ì´ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤: {customer_reaction.strip()[:50]}...")
                        except Exception as e:
                            st.warning(f"ìë™ ì¬ìƒ ì‹¤íŒ¨: {e}. ìˆ˜ë™ìœ¼ë¡œ ì¬ìƒí•´ì£¼ì„¸ìš”.")
                            st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                            st.success(f"ğŸ—£ï¸ ê³ ê°ì´ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤: {customer_reaction.strip()[:50]}...")
                    else:
                        st.error(f"âŒ ê³ ê° ìŒì„± ìƒì„± ì˜¤ë¥˜: {msg}")

                    # ê³ ê° ë°˜ì‘ í…ìŠ¤íŠ¸ë¥¼ CC ì˜ì—­ì— ë°˜ì˜
                    st.session_state.current_customer_audio_text = customer_reaction.strip()

                    # ì´ë ¥ ì €ì¥
                    log_entry = f"Agent: {st.session_state.current_agent_audio_text} | Customer: {st.session_state.current_customer_audio_text}"
                    st.session_state.simulator_messages.append(
                        {"role": "phone_exchange", "content": log_entry})

                    # ì—ì´ì „íŠ¸ ì…ë ¥ ì˜ì—­ ì´ˆê¸°í™” (ë‹¤ìŒ ë…¹ìŒì„ ìœ„í•´)
                    st.session_state.current_agent_audio_text = ""
                    st.session_state.realtime_hint_text = ""

                    # ê³ ê° ë°˜ì‘ í›„ ì¬ì‹¤í–‰
                    st.rerun()

        # --- ì—ì´ì „íŠ¸ ìŒì„± ì…ë ¥ / ë…¹ìŒ ---
        st.subheader(L["mic_input_status"])

        # ìŒì„± ì…ë ¥: ì§§ì€ ì²­í¬ë¡œ ëŠì–´ì„œ ì „ì‚¬í•´ì•¼ ì‹¤ì‹œê°„ CC ëª¨ë°© ê°€ëŠ¥
        if st.session_state.is_on_hold:
            st.info("í†µí™”ê°€ Hold ì¤‘ì…ë‹ˆë‹¤. í†µí™” ì¬ê°œ í›„ ë…¹ìŒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            mic_audio = None
        else:
            # âœ… ë§ˆì´í¬ ìœ„ì ¯ì„ í•­ìƒ ë Œë”ë§í•˜ì—¬ í™œì„±í™” ìƒíƒœë¥¼ ìœ ì§€
            mic_audio = mic_recorder(
                start_prompt=L["agent_response_prompt"],
                stop_prompt=L["agent_response_stop_and_send"],
                just_once=True,
                format="wav",
                use_container_width=True,
                key="call_sim_mic_recorder",
            )

            # ë…¹ìŒ ì™„ë£Œ (mic_audio.get("bytes")ê°€ ì±„ì›Œì§) ì‹œ, ë°”ì´íŠ¸ë¥¼ ì €ì¥í•˜ê³  ì¬ì‹¤í–‰
            if mic_audio and mic_audio.get("bytes") and "bytes_to_process" not in st.session_state:
                st.session_state.bytes_to_process = mic_audio["bytes"]
                st.session_state.current_agent_audio_text = "ğŸ™ï¸ ë…¹ìŒ ì™„ë£Œ. ì „ì‚¬ ì²˜ë¦¬ ì¤‘..."  # ì²˜ë¦¬ ì¤‘ ë©”ì‹œì§€
                # âœ… ì¬ì‹¤í–‰í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ì „ì‚¬ ë¡œì§ì„ ì²˜ë¦¬
                st.rerun()

            # â­ ì „ì‚¬ ë¡œì§: bytes_to_processì— ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰
            if "bytes_to_process" in st.session_state and st.session_state.bytes_to_process:
                if not st.session_state.openai_client:
                    st.error(L["openai_missing"])
                    st.session_state.bytes_to_process = None
                    # âœ… ì¬ì‹¤í–‰
                    # st.rerun()

                if st.session_state.get("bytes_to_process"):
                    # â­ ì „ì‚¬ ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”
                    agent_response_transcript = None

                    # â­ [ìˆ˜ì • 12]: Whisper ì „ì‚¬ ë¡œì§ì— ìŠ¤í”¼ë„ˆ ì¶”ê°€
                    with st.spinner(L["whisper_processing"]):
                        # 1) Whisper ì „ì‚¬
                        agent_response_transcript = transcribe_bytes_with_whisper(
                            st.session_state.bytes_to_process,
                            "audio/wav",
                            lang_code=st.session_state.language
                        )

                        # ì „ì‚¬ í›„ ë°”ì´íŠ¸ ë°ì´í„° ì‚­ì œ
                        del st.session_state.bytes_to_process

                        # 2) ì „ì‚¬ ì‹¤íŒ¨ ì²˜ë¦¬
                    if agent_response_transcript and agent_response_transcript.startswith("âŒ"):
                        st.error(agent_response_transcript)
                        st.session_state.current_agent_audio_text = f"[ERROR: {L['error']} Whisper failed]"
                        # ì „ì‚¬ ì‹¤íŒ¨ ì‹œì—ë„ CCì— ë°˜ì˜ë˜ë„ë¡ ì¬ì‹¤í–‰
                        st.rerun()
                    elif agent_response_transcript:
                        # 3) CCì— ë°˜ì˜ (ì „ì‚¬ ê²°ê³¼ë¥¼ ë¨¼ì € CC ì˜ì—­ì— í‘œì‹œ)
                        st.session_state.current_agent_audio_text = agent_response_transcript.strip()

                            # â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­
                            # ğŸ¯ ì•„ë°”íƒ€ í‘œì • ì—…ë°ì´íŠ¸ (ìµœì¢… ì •ë¦¬ë³¸)
                        response_text = agent_response_transcript.lower()
                            # ... (ì•„ë°”íƒ€ í‘œì • ì—…ë°ì´íŠ¸ ë¡œì§) ...
                        if "refund" in response_text or "í™˜ë¶ˆ" in response_text:
                            st.session_state.customer_avatar["state"] = "HAPPY"
                        elif ("wait" in response_text or "ê¸°ë‹¤ë ¤" in response_text or "ì ì‹œë§Œ" in response_text):
                                st.session_state.customer_avatar["state"] = "ASKING"
                        elif ("no" in response_text or "ë¶ˆê°€" in response_text or "ì•ˆ ë©ë‹ˆë‹¤" in response_text or "cannot" in response_text):
                                st.session_state.customer_avatar["state"] = "ANGRY"
                        else:
                            st.session_state.customer_avatar["state"] = "NEUTRAL"
                            # â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­â­

                        # â­ ìˆ˜ì •: ì „ì‚¬ ê²°ê³¼ê°€ CCì— ë°˜ì˜ë˜ë„ë¡ ë¨¼ì € ì¬ì‹¤í–‰
                        # ì±„íŒ…ê³¼ ë™ì¼í•˜ê²Œ ì „ì‚¬ ê²°ê³¼ë¥¼ ë¨¼ì € í™”ë©´ì— í‘œì‹œí•œ í›„ ê³ ê° ë°˜ì‘ ìƒì„±
                        # ë‹¤ìŒ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ê³ ê° ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
                        st.session_state.process_customer_reaction = True
                        st.session_state.pending_agent_transcript = agent_response_transcript.strip()
                        st.rerun()


    # ========================================
    # CALL_ENDED ìƒíƒœ
    # ========================================
    elif st.session_state.call_sim_stage == "CALL_ENDED":
        st.success(L["call_end_message"])

        # AHT
        if st.session_state.start_time is not None:
            final_aht_seconds = max(0, (datetime.now() - st.session_state.start_time).total_seconds())
            final_aht_str = str(timedelta(seconds=final_aht_seconds)).split('.')[0]
            st.metric("Final AHT", final_aht_str)

            hold_str = str(st.session_state.total_hold_duration).split('.')[0]
            st.metric("Total Hold Time", hold_str)
        else:
            st.warning(L["aht_not_recorded"])

        st.markdown("---")

        # â­ ì¶”ê°€: í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ (ì±„íŒ…/ì´ë©”ì¼ê³¼ ë™ì¼)
        st.markdown("**ğŸ“¥ í˜„ì¬ ì„¸ì…˜ ì´ë ¥ ë‹¤ìš´ë¡œë“œ**")
        download_col1, download_col2, download_col3 = st.columns(3)
        
        # í˜„ì¬ ì„¸ì…˜ì˜ ì´ë ¥ì„ ìƒì„±
        current_session_history = None
        if st.session_state.simulator_messages:
            try:
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                # ì „í™” ìš”ì•½ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒì„±
                if st.session_state.call_summary_text:
                    # call_summary_textë¥¼ summary í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    summary_data = {
                        "main_inquiry": st.session_state.call_initial_query,
                        "key_responses": [],
                        "customer_sentiment_score": 50,  # ê¸°ë³¸ê°’
                        "customer_satisfaction_score": 50,  # ê¸°ë³¸ê°’
                        "customer_characteristics": {},
                        "privacy_info": {},
                        "summary": st.session_state.call_summary_text
                    }
                else:
                    # ìš”ì•½ ìƒì„±
                    summary_data = generate_chat_summary(
                        st.session_state.simulator_messages,
                        st.session_state.call_initial_query,
                        customer_type_display,
                        st.session_state.language
                    )
                
                current_session_history = [{
                    "id": f"call_session_{st.session_state.sim_instance_id}",
                    "timestamp": datetime.now().isoformat(),
                    "initial_query": st.session_state.call_initial_query,
                    "customer_type": customer_type_display,
                    "language_key": st.session_state.language,
                    "messages": st.session_state.simulator_messages,
                    "summary": summary_data,
                    "is_chat_ended": True,
                    "attachment_context": st.session_state.get("sim_attachment_context_for_llm", ""),
                    "is_call": True
                }]
            except Exception as e:
                st.warning(f"ì´ë ¥ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤ì„ ì§ì ‘ í‘œì‹œ
        if current_session_history:
            with download_col1:
                try:
                    filepath_word = export_history_to_word(current_session_history)
                    with open(filepath_word, "rb") as f:
                        st.download_button(
                            label=L["download_history_word"],
                            data=f.read(),
                            file_name=os.path.basename(filepath_word),
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            key="download_call_word_file"
                        )
                except Exception as e:
                    st.error(f"Word ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col2:
                try:
                    filepath_pptx = export_history_to_pptx(current_session_history)
                    with open(filepath_pptx, "rb") as f:
                        st.download_button(
                            label=L["download_history_pptx"],
                            data=f.read(),
                            file_name=os.path.basename(filepath_pptx),
                            mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                            key="download_call_pptx_file"
                        )
                except Exception as e:
                    st.error(f"PPTX ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            
            with download_col3:
                try:
                    filepath_pdf = export_history_to_pdf(current_session_history)
                    with open(filepath_pdf, "rb") as f:
                        st.download_button(
                            label=L["download_history_pdf"],
                            data=f.read(),
                            file_name=os.path.basename(filepath_pdf),
                            mime="application/pdf",
                            key="download_call_pdf_file"
                        )
                except Exception as e:
                    st.error(f"PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        else:
            st.warning("ë‹¤ìš´ë¡œë“œí•  ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")

        st.markdown("---")

        with st.expander("í†µí™” ê¸°ë¡ ìš”ì•½"):
            st.subheader("AI í†µí™” ìš”ì•½")

            if st.session_state.call_summary_text:
                st.info(st.session_state.call_summary_text)
            else:
                st.error("âŒ í†µí™” ìš”ì•½ ìƒì„± ì‹¤íŒ¨")

            st.markdown("---")

            st.subheader("ê³ ê° ìµœì´ˆ ë¬¸ì˜ (ìŒì„±)")
            if st.session_state.customer_initial_audio_bytes:
                # Streamlit ë¬¸ì„œ: bytes ë°ì´í„°ë¥¼ ì§ì ‘ ì „ë‹¬ ê°€ëŠ¥
                try:
                    st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3", autoplay=False)
                except Exception as e:
                    st.error(f"ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜: {e}")
                st.caption(f"ì „ì‚¬: {st.session_state.call_initial_query}")
            else:
                st.info("ê³ ê° ìµœì´ˆ ìŒì„± ì—†ìŒ")

            st.markdown("---")
            st.subheader("ì „ì²´ êµí™˜ ë¡œê·¸")
            for log in st.session_state.simulator_messages:
                st.write(log["content"])

        # ìƒˆ ì‹œë®¬ë ˆì´ì…˜
        if st.button(L["new_simulation_button"]):
            st.session_state.call_sim_stage = "WAITING_CALL"
            st.session_state.call_sim_mode = "INBOUND"
            st.session_state.is_on_hold = False
            st.session_state.total_hold_duration = timedelta(0)
            st.session_state.hold_start_time = None
            st.session_state.start_time = None
            st.session_state.current_customer_audio_text = ""
            st.session_state.current_agent_audio_text = ""
            st.session_state.agent_response_input_box_widget_call = ""
            st.session_state.call_initial_query = ""
            st.session_state.simulator_messages = []
            st.session_state.call_summary_text = ""
            st.session_state.customer_initial_audio_bytes = None
            st.session_state.customer_history_summary = ""
            st.session_state.sim_audio_bytes = None
            st.rerun() # ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ì‹œ rerun


# -------------------- RAG Tab --------------------
elif feature_selection == L["rag_tab"]:
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    st.markdown("---")

    # â­ RAG ë°ì´í„° í•™ìŠµ ê¸°ëŠ¥ ì¶”ê°€ - AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° ë°ì´í„°ë¥¼ ì¼ì¼ íŒŒì¼ë¡œ í•™ìŠµ
    st.subheader("ğŸ“š ê³ ê° ê°€ì´ë“œ ìë™ ìƒì„± (ì¼ì¼ í•™ìŠµ)")
    
    if st.button("ì˜¤ëŠ˜ ë‚ ì§œ ê³ ê° ê°€ì´ë“œ ìƒì„±", key="generate_daily_guide"):
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: 251130_ê³ ê°ê°€ì´ë“œ.TXT)
        today_str = datetime.now().strftime("%y%m%d")
        guide_filename = f"{today_str}_ê³ ê°ê°€ì´ë“œ.TXT"
        guide_filepath = os.path.join(DATA_DIR, guide_filename)
        
        # ìµœê·¼ ì´ë ¥ ë¡œë“œ
        all_histories = load_simulation_histories_local(st.session_state.language)
        recent_histories = all_histories[:50]  # ìµœê·¼ 50ê°œ ì´ë ¥ ì‚¬ìš©
        
        if recent_histories:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ ê³ ê° ê°€ì´ë“œ ìƒì„±
            guide_prompt = f"""
ë‹¹ì‹ ì€ CS ì„¼í„° êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê³ ê° ì‘ëŒ€ ì´ë ¥ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ê³ ê° ì‘ëŒ€ ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•˜ì„¸ìš”.

ë¶„ì„í•  ì´ë ¥ ë°ì´í„°:
{json.dumps([h.get('summary', {}) for h in recent_histories if h.get('summary')], ensure_ascii=False, indent=2)}

ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•˜ì„¸ìš”:
1. ê³ ê° ìœ í˜•ë³„ ì‘ëŒ€ ì „ëµ (ì¼ë°˜/ê¹Œë‹¤ë¡œìš´/ë§¤ìš° ë¶ˆë§Œì¡±)
2. ë¬¸í™”ê¶Œë³„ ì‘ëŒ€ ê°€ì´ë“œ (ì–¸ì–´, ë¬¸í™”ì  ë°°ê²½ ê³ ë ¤)
3. ì£¼ìš” ë¬¸ì˜ ìœ í˜•ë³„ í•´ê²° ë°©ë²•
4. ê³ ê° ê°ì • ì ìˆ˜ì— ë”°ë¥¸ ì‘ëŒ€ ì „ëµ
5. ê°œì¸ì •ë³´ ì²˜ë¦¬ ê°€ì´ë“œ
6. íš¨ê³¼ì ì¸ ì†Œí†µ ìŠ¤íƒ€ì¼ ê¶Œì¥ì‚¬í•­

ê°€ì´ë“œë¼ì¸ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
            
            if st.session_state.is_llm_ready:
                with st.spinner("ê³ ê° ê°€ì´ë“œ ìƒì„± ì¤‘..."):
                    guide_content = run_llm(guide_prompt)
                    
                    # íŒŒì¼ ì €ì¥
                    with open(guide_filepath, "w", encoding="utf-8") as f:
                        f.write(f"ê³ ê° ì‘ëŒ€ ê°€ì´ë“œë¼ì¸\n")
                        f.write(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"ë¶„ì„ ì´ë ¥ ìˆ˜: {len(recent_histories)}\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(guide_content)
                    
                    st.success(f"âœ… ê³ ê° ê°€ì´ë“œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {guide_filename}")
                    st.info(f"íŒŒì¼ ìœ„ì¹˜: {guide_filepath}")
                    
                    # ìƒì„±ëœ íŒŒì¼ì„ ìë™ìœ¼ë¡œ RAGì— ì¶”ê°€í• ì§€ ì„ íƒ
                    if st.button("ìƒì„±ëœ ê°€ì´ë“œë¥¼ RAGì— ì¶”ê°€", key="add_guide_to_rag"):
                        # íŒŒì¼ì„ ì—…ë¡œë“œëœ íŒŒì¼ì²˜ëŸ¼ ì²˜ë¦¬í•˜ì—¬ RAGì— ì¶”ê°€
                        st.info("RAG ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
                        # ì‹¤ì œë¡œëŠ” íŒŒì¼ì„ ì½ì–´ì„œ RAG ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ëŠ” ë¡œì§ í•„ìš”
            else:
                st.error("LLMì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API Keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        else:
            st.warning("ë¶„ì„í•  ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    st.markdown("---")

    # --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
    # â­ ìˆ˜ì •ëœ ë¶€ë¶„: RAG íƒ­ ì „ìš© í‚¤ ì‚¬ìš©
    uploaded_files = st.file_uploader(
        L["file_uploader"],
        type=["pdf", "txt", "html"],
        key="rag_file_uploader", # RAG ì „ìš© í‚¤
        accept_multiple_files=True
    )

    if uploaded_files:
        if uploaded_files != st.session_state.uploaded_files_state:
            # íŒŒì¼ì´ ë³€ê²½ë˜ë©´ RAG ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.is_rag_ready = False
            st.session_state.rag_vectorstore = None
            st.session_state.uploaded_files_state = uploaded_files

        if not st.session_state.is_rag_ready:
            if st.button(L["button_start_analysis"]):
                if not st.session_state.is_llm_ready:
                    st.error(L["simulation_no_key_warning"])
                    # st.stop()

                with st.spinner(L["data_analysis_progress"]):
                    vectorstore, count = build_rag_index(uploaded_files)

                if vectorstore:
                    st.session_state.rag_vectorstore = vectorstore
                    st.session_state.is_rag_ready = True
                    st.success(L["embed_success"].format(count=count))
                    st.session_state.rag_messages = [
                        {"role": "assistant", "content": f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}
                    ]
                else:
                    st.error(L["embed_fail"])
                    st.session_state.is_rag_ready = False
    else:
        st.info(L["warning_no_files"])
        st.session_state.is_rag_ready = False
        st.session_state.rag_vectorstore = None
        st.session_state.rag_messages = []

    st.markdown("---")

    # --- ì±—ë´‡ ì„¹ì…˜ ---
    if st.session_state.is_rag_ready and st.session_state.rag_vectorstore:
        if "rag_messages" not in st.session_state:
            st.session_state.rag_messages = [{"role": "assistant", "content": "ë¶„ì„ëœ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}]

        for message in st.session_state.rag_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.rag_messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner(L["response_generating"]):
                    response = rag_answer(
                        prompt,
                        st.session_state.rag_vectorstore,
                        st.session_state.language
                    )
                    st.markdown(response)

            st.session_state.rag_messages.append({"role": "assistant", "content": response})
    else:
        st.warning(L["warning_rag_not_ready"])

# -------------------- Content Tab --------------------
elif feature_selection == L["content_tab"]:
    st.header(L["content_header"])
    st.markdown(L["content_desc"])
    st.markdown("---")

    if not st.session_state.is_llm_ready:
        st.warning(L["simulation_no_key_warning"])
        st.info("ğŸ’¡ API Keyë¥¼ ì„¤ì •í•˜ë©´ ì½˜í…ì¸  ìƒì„± ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # st.stop() ì œê±°: UIëŠ” í‘œì‹œí•˜ë˜ ê¸°ëŠ¥ë§Œ ë¹„í™œì„±í™”

    # ë‹¤êµ­ì–´ ë§µí•‘ ë³€ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
    level_map = {
        "ì´ˆê¸‰": "Beginner",
        "ì¤‘ê¸‰": "Intermediate",
        "ê³ ê¸‰": "Advanced",
        "Beginner": "Beginner",
        "Intermediate": "Intermediate",
        "Advanced": "Advanced",
        "åˆç´š": "Beginner",
        "ä¸­ç´š": "Intermediate",
        "ä¸Šç´š": "Advanced",
    }
    content_map = {
        "í•µì‹¬ ìš”ì•½ ë…¸íŠ¸": "summary",
        "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­": "quiz",
        "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´": "example",
        "Key Summary Note": "summary",
        "10 MCQ Questions": "quiz",
        "Practical Example Idea": "example",
        "æ ¸å¿ƒè¦ç´„ãƒãƒ¼ãƒˆ": "summary",
        "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•": "quiz",
        "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢": "example",
    }

    topic = st.text_input(L["topic_label"])
    level_display = st.selectbox(L["level_label"], L["level_options"])
    content_display = st.selectbox(L["content_type_label"], L["content_options"])

    level = level_map.get(level_display, "Beginner")
    content_type = content_map.get(content_display, "summary")

    if st.button(L["button_generate"]):
        if not topic.strip():
            st.warning(L["warning_topic"])
            # st.stop() ì œê±°: ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ê³„ì† ì§„í–‰
        elif not st.session_state.is_llm_ready:
            st.error("âŒ LLMì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API Keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            # st.stop() ì œê±°: ì—ëŸ¬ë§Œ í‘œì‹œí•˜ê³  ê³„ì† ì§„í–‰
        else:
            target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]

            # ê³µí†µ í”„ë¡¬í”„íŠ¸ ì„¤ì • (í€´ì¦ˆ í˜•ì‹ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ê¸°ë³¸ í…œí”Œë¦¿)
            system_prompt = f"""
            You are a professional AI coach. Generate learning content in {target_lang} for the topic '{topic}' at the '{level}' difficulty.
            The content format requested is: {content_display}.
            Output ONLY the raw content.
            """

            if content_type == "quiz":
                # í€´ì¦ˆ ì „ìš© í”„ë¡¬í”„íŠ¸ ë° JSON êµ¬ì¡° ê°•ì œ (ë¡œì§ ìœ ì§€)
                quiz_prompt = f"""
                You are an expert quiz generator. Based on the topic '{topic}' and difficulty '{level}', generate 10 multiple-choice questions.
                Your output MUST be a **raw JSON object** containing a single key "quiz_questions" which holds an array of 10 questions.
                Each object in the array must strictly follow the required keys: "question", "options" (array of 4 strings), and "answer" (an integer index starting from 1).
                DO NOT include any explanation, introductory text, or markdown code blocks (e.g., ```json).
                Output ONLY the raw JSON object, starting with '{{' and ending with '}}'.
                """

            generated_json_text = None
            llm_attempts = []

            # 1ìˆœìœ„: OpenAI (JSON modeê°€ ê°€ì¥ ì•ˆì •ì )
            if get_api_key("openai"):
                llm_attempts.append(("openai", get_api_key("openai"), "gpt-4o"))
            # 2ìˆœìœ„: Gemini (Fallback)
            if get_api_key("gemini"):
                llm_attempts.append(("gemini", get_api_key("gemini"), "gemini-2.5-flash"))

            with st.spinner(L["response_generating"]):
                for provider, api_key, model_name in llm_attempts:
                    try:
                        if provider == "openai":
                            client = OpenAI(api_key=api_key)
                            response = client.chat.completions.create(
                                model=model_name,
                                messages=[{"role": "user", "content": quiz_prompt}],
                                # JSON Mode ê°•ì œ
                                response_format={"type": "json_object"},
                            )
                            # OpenAIëŠ” JSON ê°ì²´ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, íœìŠ¤ ì œê±° ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•¨
                            generated_json_text = response.choices[0].message.content.strip()
                            break

                        elif provider == "gemini":
                            # GeminiëŠ” response_formatì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, run_llmì„ í†µí•´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ í˜¸ì¶œ
                            generated_json_text = run_llm(quiz_prompt)
                            # Markdown íœìŠ¤ ì œê±° ì‹œë„
                            raw_text = generated_json_text.strip()
                            if raw_text.startswith("```json"):
                                generated_json_text = raw_text.split("```json")[1].split("```")[0].strip()
                            elif raw_text.startswith("```"):
                                generated_json_text = raw_text.split("```")[1].split("```")[0].strip()

                            # Geminiì˜ ì‘ë‹µì´ JSONì²˜ëŸ¼ ë³´ì´ë©´ ì‹œë„ë¥¼ ë©ˆì¶¤
                            if generated_json_text.startswith('{'):
                                break

                    except Exception as e:
                        print(f"JSON generation failed with {provider}: {e}")
                        continue

            # --- START: JSON Parsing and Error Handling Logic ---
            if generated_json_text and generated_json_text.startswith('{'):
                try:
                    # JSON ê°ì²´ íŒŒì‹± ì‹œë„ (ìµœìƒìœ„ëŠ” ê°ì²´ì—¬ì•¼ í•¨)
                    parsed_obj = json.loads(generated_json_text)

                    # 'quiz_questions' í‚¤ì—ì„œ ë°°ì—´ ì¶”ì¶œ
                    quiz_data = parsed_obj.get("quiz_questions")

                    if not isinstance(quiz_data, list) or len(quiz_data) < 1:
                        raise ValueError("Missing 'quiz_questions' key or empty array.")

                    # 3. íŒŒì‹± ì„±ê³µ ë° ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ í›„ ìƒíƒœ ì €ì¥
                    st.session_state.quiz_data = quiz_data
                    st.session_state.current_question_index = 0
                    st.session_state.quiz_score = 0
                    st.session_state.quiz_answers = [1] * len(quiz_data)
                    st.session_state.show_explanation = False
                    st.session_state.is_quiz_active = True
                    st.session_state.quiz_type_key = str(uuid.uuid4())

                    st.success(f"**{topic}** - {content_display} ìƒì„± ì™„ë£Œ")
                    # st.rerun()  # í€´ì¦ˆ UIë¡œ ì „í™˜

                except (json.JSONDecodeError, ValueError) as e:
                    # 4. íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° êµ¬ì¡° ë¬¸ì œ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
                    st.error(L["quiz_error_llm"])
                    st.caption(f"Error Details: {type(e).__name__} - {e}")
                    st.subheader(L["quiz_original_response"])
                    st.code(generated_json_text, language="json")
                    # st.stop() ì œê±°: ì—ëŸ¬ í‘œì‹œ í›„ ê³„ì† ì§„í–‰
            else:
                st.error(L["quiz_error_llm"])
                if generated_json_text:
                    st.text_area(L["quiz_original_response"], generated_json_text, height=200)
                # st.stop() ì œê±°: ì—ëŸ¬ í‘œì‹œ í›„ ê³„ì† ì§„í–‰
                # --- END: JSON Parsing and Error Handling Logic ---

                else:  # ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±
                    st.session_state.is_quiz_active = False
                with st.spinner(L["response_generating"]):
                    content = run_llm(system_prompt)
                st.session_state.generated_content = content

                st.markdown("---")
                st.markdown(f"### {content_display}")
                st.markdown(st.session_state.generated_content)

    # --- í€´ì¦ˆ/ì¼ë°˜ ì½˜í…ì¸  ì¶œë ¥ ë¡œì§ ---
    if st.session_state.get("is_quiz_active", False) and st.session_state.get("quiz_data"):
        # í€´ì¦ˆ ì§„í–‰ ë¡œì§ (ìƒëµ - ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        quiz_data = st.session_state.quiz_data
        idx = st.session_state.current_question_index

        # â­ í€´ì¦ˆ ì™„ë£Œ ì‹œ IndexError ë°©ì§€ ë¡œì§ (idx >= len(quiz_data))
        if idx >= len(quiz_data):
            # í€´ì¦ˆ ì™„ë£Œ ì‹œ ìµœì¢… ì ìˆ˜ í‘œì‹œ
            st.success(L["quiz_complete"])
            total_questions = len(quiz_data)
            score = st.session_state.quiz_score
            st.subheader(f"{L['score']}: {score} / {total_questions} ({(score / total_questions) * 100:.1f}%)")

            if st.button(L["retake_quiz"], key="retake_quiz_btn"):
                # í€´ì¦ˆ ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.is_quiz_active = False
                st.session_state.quiz_data = None
                st.session_state.current_question_index = 0
                st.session_state.quiz_score = 0
                st.session_state.quiz_answers = []
                st.session_state.show_explanation = False
                # st.rerun()  # ìƒíƒœ ì´ˆê¸°í™” í›„ ì¦‰ì‹œ ì¬ì‹¤í–‰
            # st.stop() ì œê±°: í€´ì¦ˆ ì™„ë£Œ í›„ì—ë„ UIëŠ” ê³„ì† í‘œì‹œ

        # í€´ì¦ˆ ì§„í–‰ (í˜„ì¬ ë¬¸í•­)
        question_data = quiz_data[idx]
        st.subheader(f"Question {idx + 1}/{len(quiz_data)}")
        st.markdown(f"**{question_data['question']}**")

        # ê¸°ì¡´ í€´ì¦ˆ ì§„í–‰ ë° ì±„ì  ë¡œì§ (ë³€í™” ì—†ìŒ)
        current_selection_index = st.session_state.quiz_answers[idx]

        options = question_data['options']
        current_answer = st.session_state.quiz_answers[idx]

        if current_answer is None or not isinstance(current_answer, int) or current_answer <= 0:
            radio_index = 0
        else:
            radio_index = min(current_answer - 1, len(options) - 1)

        selected_option = st.radio(
            L["select_answer"],
            options,
            index=radio_index,
            key=f"quiz_radio_{st.session_state.quiz_type_key}_{idx}"
        )

        selected_option_index = options.index(selected_option) + 1 if selected_option in options else None

        check_col, next_col = st.columns([1, 1])

        if check_col.button(L["check_answer"], key=f"check_answer_btn_{idx}"):
            if selected_option_index is None:
                st.warning("ì„ íƒì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
            else:
                # ì ìˆ˜ ê³„ì‚° ë¡œì§
                if st.session_state.quiz_answers[idx] != 'Correctly Scored':
                    correct_answer = question_data.get('answer')  # answer í‚¤ê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„
                    if selected_option_index == correct_answer:
                        st.session_state.quiz_score += 1
                        st.session_state.quiz_answers[idx] = 'Correctly Scored'
                        st.success(L["correct_answer"])
                    else:
                        st.session_state.quiz_answers[idx] = selected_option_index  # ì˜¤ë‹µì€ ì„ íƒì§€ ì¸ë±ìŠ¤ ì €ì¥
                        st.error(L["incorrect_answer"])

                st.session_state.show_explanation = True
                # st.rerun()

        # ì •ë‹µ ë° í•´ì„¤ í‘œì‹œ
        if st.session_state.show_explanation:
            correct_index = question_data.get('answer', 1)
            correct_answer_text = question_data['options'][correct_index - 1] if 0 < correct_index <= len(
                question_data['options']) else "N/A"

            st.markdown("---")
            st.markdown(f"**{L['correct_is']}:** {correct_answer_text}")
            with st.expander(f"**{L['explanation']}**", expanded=True):
                st.info(question_data.get('explanation', 'í•´ì„¤ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'))

            # ë‹¤ìŒ ë¬¸í•­ ë²„íŠ¼
            if next_col.button(L["next_question"], key=f"next_question_btn_{idx}"):
                st.session_state.current_question_index += 1
                st.session_state.show_explanation = False
                # st.rerun()

        else:
            # ì‚¬ìš©ìê°€ ì´ë¯¸ ì •ë‹µì„ ì²´í¬í–ˆê³  (ë‹¤ì‹œ ë¡œë“œëœ ê²½ìš°), ë‹¤ìŒ ë²„íŠ¼ì„ ë°”ë¡œ í‘œì‹œ
            if st.session_state.quiz_answers[idx] == 'Correctly Scored' or (
                    isinstance(st.session_state.quiz_answers[idx], int) and st.session_state.quiz_answers[idx] > 0):
                if next_col.button(L["next_question"], key=f"next_question_btn_after_check_{idx}"):
                    st.session_state.current_question_index += 1
                    st.session_state.show_explanation = False
                    # st.rerun()

    else:
        # ì¼ë°˜ ì½˜í…ì¸  (í•µì‹¬ ìš”ì•½ ë…¸íŠ¸, ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´) ì¶œë ¥
        if st.session_state.get("generated_content"):
            content = st.session_state.generated_content  # Contentë¥¼ ë‹¤ì‹œ ê°€ì ¸ì˜´
            content_lines = content.split('\n')

            st.markdown("---")
            st.markdown(f"### {content_display}")

            # --- START: íš¨ìœ¨ì„± ê°œì„  (ìƒë‹¨ ë¶„ì„/í•˜ë‹¨ ë³¸ë¬¸) ---

            st.subheader("ğŸ’¡ ì½˜í…ì¸  ë¶„ì„ (Plotly ì‹œê°í™”)")

            if IS_PLOTLY_AVAILABLE:
                # 1. í‚¤ì›Œë“œ ë¹ˆë„ ì‹œê°í™” (ëª¨ì˜ ë°ì´í„°)

                # ì½˜í…ì¸ ë¥¼ í…ìŠ¤íŠ¸ ì¤„ë¡œ ë¶„í• í•˜ì—¬ ëª¨ì˜ í‚¤ì›Œë“œ ë° ì£¼ìš” ë¬¸ì¥ ìƒì„±
                content = st.session_state.generated_content
                content_lines = content.split('\n')
                all_words = ' '.join(content_lines).replace('.', '').replace(',', '').split()

                # ëª¨ì˜ í‚¤ì›Œë“œ ë¹ˆë„ ë°ì´í„° ìƒì„±
                words = ['AI', 'ê¸°ìˆ í˜ì‹ ', 'ê³ ê°ê²½í—˜', 'ë°ì´í„°ë¶„ì„', 'íš¨ìœ¨ì„±', 'ì—¬í–‰ì‚°ì—…']
                np.random.seed(42)
                counts = np.random.randint(5, 30, size=len(words))

                # ë‚œì´ë„ì— ë”°ë¼ ì ìˆ˜ ê°€ì¤‘ì¹˜ (ëª¨ì˜ ê°ì„± ì ìˆ˜ ë³€í™”)
                difficulty_score = {'Beginner': 60, 'Intermediate': 75, 'Advanced': 90}.get(level, 70)

                # --- ì°¨íŠ¸ 1: í‚¤ì›Œë“œ ë¹ˆë„ (Plotly Bar Chart) ---
                fig_bar = go.Figure(data=[
                    go.Bar(
                        x=words,
                        y=counts,
                        marker_color=px.colors.sequential.Plotly3,
                        name="í‚¤ì›Œë“œ ë¹ˆë„"
                    )
                ])
                fig_bar.update_layout(
                    title_text=f"ì£¼ìš” í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„",
                    height=300,
                    margin=dict(l=20, r=20, t=50, b=20)
                )
                st.plotly_chart(fig_bar, use_container_width=True)

                # --- ì°¨íŠ¸ 2: ì½˜í…ì¸  ê°ì„± ë° ë³µì¡ë„ ì¶”ì´ (Plotly Line Chart) ---
                # ëª¨ì˜ ê°ì„±/ë³µì¡ë„ ì ìˆ˜ ì¶”ì´ (5ê°œ ë¬¸ë‹¨ ëª¨ì˜)
                sections = ['ë„ì…ë¶€', 'í•µì‹¬1', 'í•µì‹¬2', 'í•´ê²°ì±…', 'ê²°ë¡ ']
                sentiment_scores = [difficulty_score - 10, difficulty_score + 5, difficulty_score,
                                    difficulty_score + 10, difficulty_score + 2]

                fig_line = go.Figure()
                fig_line.add_trace(go.Scatter(
                    x=sections,
                    y=sentiment_scores,
                    mode='lines+markers',
                    name='ê°ì„±/ë³µì¡ë„ ì ìˆ˜',
                    line=dict(color='orange', width=2),
                    marker=dict(size=8)
                ))
                fig_line.update_layout(
                    title_text="ì½˜í…ì¸  ì„¹ì…˜ë³„ ê°ì„± ë° ë³µì¡ë„ ì¶”ì´ (ëª¨ì˜)",
                    yaxis_range=[50, 100],
                    height=300,
                    margin=dict(l=20, r=20, t=50, b=20)
                )
                st.plotly_chart(fig_line, use_container_width=True)

            else:  # Plotlyê°€ ì—†ì„ ê²½ìš° ê¸°ì¡´ í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ì˜ ìœ ì§€
                st.info("Plotly ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ì˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                all_words = ' '.join(content_lines).replace('.', '').replace(',', '').split()
                unique_words = sorted(set(all_words), key=len, reverse=True)[:5] if all_words else ["N/A"]
                key_sentences = [
                    content_lines[0].strip() if content_lines else "N/A",
                    content_lines[len(content_lines) // 2].strip() if len(content_lines) > 1 else "",
                    content_lines[-1].strip() if len(content_lines) > 1 else ""
                ]
                key_sentences = [s for s in key_sentences if s and s != "N/A"]

                col_keyword, col_sentences = st.columns([1, 1])

                with col_keyword:
                    st.markdown("**í•µì‹¬ í‚¤ì›Œë“œ/ê°œë… (ëª¨ì˜)**")
                    st.info(f"[{', '.join(unique_words)}...]")

                with col_sentences:
                    st.markdown("**ì£¼ìš” ë¬¸ì¥ ìš”ì•½ (ëª¨ì˜)**")
                    for sentence in key_sentences[:2]:
                        st.write(f"â€¢ {sentence[:50]}...")

            st.markdown("---")

            # 2. í•˜ë‹¨ ë³¸ë¬¸ ì¶œë ¥
            st.markdown(f"### ğŸ“ ì›ë³¸ ì½˜í…ì¸ ")
            st.markdown(content)

            # --- END: íš¨ìœ¨ì„± ê°œì„  ---

            # --- START: ì•„ì´ì½˜ ë²„íŠ¼ í™œì„±í™” ---
            st.markdown("---")

            # 1. ë³µì‚¬í•  ë‚´ìš© ì •ë¦¬ ë° ì´ìŠ¤ì¼€ì´í”„
            content_for_js = json.dumps(content)

            # JavaScript ì½”ë“œëŠ” ì´ìŠ¤ì¼€ì´í”„ëœ ì¤‘ê´„í˜¸ {{}}ë¥¼ ì‚¬ìš©
            js_copy_script = """
               function copyToClipboard(text) {{
                   navigator.clipboard.writeText(text).then(function() {{
                       // Streamlit toast í˜¸ì¶œ (ëª¨ì˜)
                       const elements = window.parent.document.querySelectorAll('[data-testid="stToast"]');
                       if (elements.length === 0) {{
                           // Fallback UI update (use Streamlit's native mechanism if possible, or simple alert)
                           console.log("ë³µì‚¬ ì™„ë£Œ: " + text.substring(0, 50) + "...");
                           }}
                       }}, function(err) {{
                           // Fallback: Copy via execCommand (deprecated but often works in Streamlit's iframe)
                           const textarea = document.createElement('textarea');
                           textarea.value = text;
                           document.body.appendChild(textarea);
                           textarea.select();
                           document.execCommand('copy');
                           document.body.removeChild(textarea);
                           alert("ë³µì‚¬ ì™„ë£Œ!"); 
                       }});
                   }}
                   // f-string ëŒ€ì‹  .formatì„ ì‚¬ìš©í•˜ì—¬ JavaScript ì½”ë“œì— ì£¼ì…
                   // content_for_jsëŠ” ì´ë¯¸ Pythonì—ì„œ JSON ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ì´ìŠ¤ì¼€ì´í”„ë¨
                   copyToClipboard(JSON.parse('{content_json_safe}'));
               """.format(content_json_safe=content_for_js)

            # --- JavaScript for SHARE Menu (Messenger Mock) ---
            # Streamlitì€ í˜„ì¬ ì†Œì…œ ë¯¸ë””ì–´ APIë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, URL ë³µì‚¬ë¥¼ ì‚¬ìš©í•˜ê³  UIì— ë©”ì‹œì§€ ì˜µì…˜ì„ ëª¨ì˜í•©ë‹ˆë‹¤.
            js_share_url_copy = """
               function copyShareUrl() {{
                   const url = window.location.href;
                   navigator.clipboard.writeText(url).then(function() {{
                       console.log('App URL copied');
                   }}, function(err) {{
                       // Fallback
                       const textarea = document.createElement('textarea');
                       textarea.value = url;
                       document.body.appendChild(textarea);
                       textarea.select();
                       document.execCommand('copy');
                       document.body.removeChild(textarea);
                   }});
               }}
            """

            # --- JavaScript for SHARE Menu (Messenger Mock) ---
            # Streamlitì€ í˜„ì¬ ì†Œì…œ ë¯¸ë””ì–´ APIë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, URL ë³µì‚¬ë¥¼ ì‚¬ìš©í•˜ê³  UIì— ë©”ì‹œì§€ ì˜µì…˜ì„ ëª¨ì˜í•©ë‹ˆë‹¤.
            js_native_share = """
               function triggerNativeShare(title, text, url) {{
                   if (navigator.share) {{
                       // 1. ë„¤ì´í‹°ë¸Œ ê³µìœ  API ì§€ì› ì‹œ ì‚¬ìš©
                       navigator.share({{
                           title: title,
                           text: text,
                           url: url,
                       }}).then(() => {{
                           console.log('Successful share');
                       }}).catch((error) => {{
                           console.log('Error sharing', error);
                       }});
                       return true;
                   }} else {{
                      // 2. ë„¤ì´í‹°ë¸Œ ê³µìœ  API ë¯¸ì§€ì› ì‹œ (PC í™˜ê²½ ë“±)
                      return false;
                   }}
               }}
            """


            # --- ë” ë³´ê¸° ë©”ë‰´ (íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì—´ê¸° ëª¨ì˜) ---

            def mock_download(file_type: str, file_name: str):
                """ëª¨ì˜ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥: íŒŒì¼ëª…ê³¼ í•¨ê»˜ ì„±ê³µ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
                st.toast(f"ğŸ“¥ {file_type} íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤: {file_name}")
                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë¡œì§ì€ Streamlit ì»´í¬ë„ŒíŠ¸ í™˜ê²½ì—ì„œëŠ” ë³µì¡í•˜ì—¬ ìƒëµí•©ë‹ˆë‹¤.


            col_like, col_dislike, col_share, col_copy, col_more = st.columns([1, 1, 1, 1, 6])
            current_content_id = str(uuid.uuid4())  # ë™ì  ID ìƒì„±

            # 1. ì¢‹ì•„ìš” ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™”)
            if col_like.button("ğŸ‘", key=f"content_like_{current_content_id}"):
                st.toast(L["toast_like"])

            # 2. ì‹«ì–´ìš” ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™”)
            if col_dislike.button("ğŸ‘", key=f"content_dislike_{current_content_id}"):
                st.toast(L["toast_dislike"])

            # 3. ê³µìœ  ë²„íŠ¼ (Web Share API í˜¸ì¶œ í†µí•©)
            with col_share:
                share_clicked = st.button("ğŸ”—", key=f"content_share_{current_content_id}")

            if share_clicked:
                # 1ë‹¨ê³„: ë„¤ì´í‹°ë¸Œ ê³µìœ  API í˜¸ì¶œ ì‹œë„ (ëª¨ë°”ì¼ í™˜ê²½ ëŒ€ìƒ)
                share_title = f"{content_display} ({topic})"
                share_text = content[:150] + "..."
                share_url = "https://utility-convenience-salmonyeonwoo.streamlit.app/"  # ì‹¤ì œ ë°°í¬ URLë¡œ ê°€ì •

                # JavaScript ì‹¤í–‰: ë„¤ì´í‹°ë¸Œ ê³µìœ  í˜¸ì¶œ
                st.components.v1.html(
                    f"""
                    <script>{js_native_share}
                        const shared = triggerNativeShare('{share_title}', '{share_text}', '{share_url}');
                        if (shared) {{
                           // ë„¤ì´í‹°ë¸Œ ê³µìœ  ì„±ê³µ ì‹œ (í† ìŠ¤íŠ¸ ë©”ì‹œì§€ëŠ” ë¸Œë¼ìš°ì €ê°€ ê´€ë¦¬)
                            console.log("Native Share Attempted.");
                        }} else {{
                           // ë„¤ì´í‹°ë¸Œ ê³µìœ  ë¯¸ì§€ì› ì‹œ, ëŒ€ì‹  URL ë³µì‚¬
                           const url = window.location.href;
                           const textarea = document.createElement('textarea');
                           textarea.value = url;
                           document.body.appendChild(textarea);
                           textarea.select();
                           document.execCommand('copy');
                           document.body.removeChild(textarea);
                           // PC í™˜ê²½ì—ì„œ URL ë³µì‚¬ ì™„ë£Œ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ì¶œë ¥
                           const toastElement = window.parent.document.querySelector('[data-testid="stToast"]');
                           if (toastElement) {{
                               // ì´ë¯¸ í† ìŠ¤íŠ¸ ë©”ì‹œì§€ê°€ ì—´ë ¤ ìˆë‹¤ë©´ ê°±ì‹  (Streamlitì˜ toast ê¸°ëŠ¥ì„ ê°€ì •)
                           }} else {{
                              alert('URLì´ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.');
                           }}
                        }}
                    </script>
                    """,
                    height=0,
                )

                # Streamlitì˜ toast ë©”ì‹œì§€ëŠ” ë„¤ì´í‹°ë¸Œ ê³µìœ  ì„±ê³µ ì—¬ë¶€ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ URL ë³µì‚¬ ì™„ë£Œë¥¼ ì•Œë¦¼
                st.toast(L["toast_share"])


            # 4. ë³µì‚¬ ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™” - ì½˜í…ì¸  í…ìŠ¤íŠ¸ ë³µì‚¬)
            if col_copy.button("ğŸ“‹", key=f"content_copy_{current_content_id}"):
                # JavaScriptë¥¼ ì‹¤í–‰í•˜ì—¬ ë³µì‚¬ (execCommand ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •í™”)
                st.components.v1.html(
                    f"""<script>{js_copy_script}</script>""",
                    height=0,
                )
                st.toast(L["toast_copy"])

            # 5. ë”ë³´ê¸° ë²„íŠ¼ (ê¸°ëŠ¥ í™œì„±í™” - íŒŒì¼ ì˜µì…˜ ëª¨ì˜)
            with col_more:
                more_clicked = st.button("â€¢â€¢â€¢", key=f"content_more_{current_content_id}")

            if more_clicked:
                st.toast(L["toast_more"])

                # íŒŒì¼ ì˜µì…˜ ëª¨ì˜ ì¶œë ¥ (ë²„íŠ¼ ë°°ì¹˜)
                st.markdown("**ë¬¸ì„œ ì˜µì…˜ (ëª¨ì˜):**")
                col_doc1, col_doc2, col_doc3 = st.columns(3)

                # ë‹¤êµ­ì–´ ë ˆì´ë¸” ì ìš©
                if col_doc1.button(L["mock_pdf_save"], key=f"mock_pdf_save_{current_content_id}"):  # ë™ì  ID ì ìš©
                    mock_download("PDF", f"{topic}_summary.pdf")
                if col_doc2.button(L["mock_word_open"], key=f"mock_word_open_{current_content_id}"):  # ë™ì  ID ì ìš©
                    mock_download("Word", f"{topic}_summary.docx")
                if col_doc3.button(L["mock_print"], key=f"mock_print_{current_content_id}"):  # ë™ì  ID ì ìš©
                    st.toast("ğŸ–¨ ë¸Œë¼ìš°ì € ì¸ì‡„ ì°½ì´ ì—´ë¦½ë‹ˆë‹¤.")

            # --- END: íš¨ìœ¨ì„± ê°œì„  ---

            # --- END: ì•„ì´ì½˜ ë²„íŠ¼ ì¶”ê°€ ---

# -------------------- LSTM Tab --------------------
elif feature_selection == L["lstm_tab"]:
    # ... (ê¸°ì¡´ LSTM íƒ­ ë¡œì§ ìœ ì§€)
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])

    # â­ ìµœì í™”: ë²„íŠ¼ ìì²´ê°€ rerunì„ ìœ ë„í•˜ë¯€ë¡œ ëª…ì‹œì  rerun ì œê±° (ë²„íŠ¼ í´ë¦­ ì‹œ ìë™ ì¬ì‹¤í–‰)
    if st.button(L["lstm_rerun_button"]):
        # ë²„íŠ¼ í´ë¦­ ì‹œ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰
        pass

    try:
        data = load_or_train_lstm()
        predicted_score = float(np.clip(data[-1] + np.random.uniform(-3, 5), 50, 100))

        st.markdown("---")
        st.subheader(L["lstm_result_header"])

        col_score, col_chart = st.columns([1, 2])

        with col_score:
            suffix = "ì " if st.session_state.language == "ko" else ""
            st.metric(L["lstm_score_metric"], f"{predicted_score:.1f}{suffix}")
            st.info(L["lstm_score_info"].format(predicted_score=predicted_score))
        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label="Past Scores", marker="o")
            ax.plot(len(data), predicted_score, marker="*", markersize=10)
            ax.set_title(L["lstm_header"])
            ax.set_xlabel("Time (attempts)")
            ax.set_ylabel("Score (0-100)")
            ax.legend()
            st.pyplot(fig)
    except Exception as e:
        st.info(f"LSTM ê¸°ëŠ¥ ì—ëŸ¬: {e}")
