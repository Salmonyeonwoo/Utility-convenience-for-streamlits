# ========================================
# streamlit_app.py (ì „ì²´ ìˆ˜ì •ëœ ì½”ë“œ)
#
# ì£¼ìš” ê°œì„  ì‚¬í•­:
# 1. ì±„íŒ…/ì´ë©”ì¼ íƒ­ì— 'ì „í™” ë°œì‹  (í˜„ì§€ ì—…ì²´/ê³ ê°)' ë²„íŠ¼ ë° ê¸°ëŠ¥ ì¶”ê°€ (ì˜ˆì™¸ ì²˜ë¦¬ ëŒ€ì‘)
# 2. ì „í™” íƒ­ì— 'ì „í™” ë°œì‹ ' ë²„íŠ¼ ì¶”ê°€ ë° ë°œì‹  í†µí™” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì§€ì›
# 3. ê´€ë ¨ ì–¸ì–´ íŒ© ì¶”ê°€ ë° ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
# 4. í€´ì¦ˆ ê¸°ëŠ¥ì˜ ì •ë‹µ í™•ì¸, í•´ì„¤, ì ìˆ˜ í‘œì‹œ ë¡œì§ ì™„ì„±
# 5. [BUG FIX] ì–¸ì–´ ì´ê´€ ì‹œ 'ë²ˆì—­ ë‹¤ì‹œ ì‹œë„' ë²„íŠ¼ì˜ DuplicateWidgetID ì˜¤ë¥˜ í•´ê²°
# 6. [BUG FIX] ì½˜í…ì¸  ìƒì„± íƒ­ì˜ LLM ì‘ë‹µ ë° ë¼ë””ì˜¤ ë²„íŠ¼ ì´ˆê¸°í™” ì˜¤ë¥˜ í•´ê²°
# ========================================

import os
import io
import json
import time
import uuid
import base64
import tempfile
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Any
import google.generativeai as genai
import numpy as np
import streamlit as st
from matplotlib import pyplot as plt

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots

    IS_PLOTLY_AVAILABLE = True
except ImportError:
    IS_PLOTLY_AVAILABLE = False

from openai import OpenAI
from anthropic import Anthropic

# mic_recorder (0.0.8) - returns dict with key "bytes"
from streamlit_mic_recorder import mic_recorder

# LangChain / RAG ê´€ë ¨
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

try:
    from langchain_google_genai import GoogleGenerativeAIEmbeddings

    IS_GEMINI_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_GEMINI_EMBEDDING_AVAILABLE = False

try:
    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

    IS_NVIDIA_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_NVIDIA_EMBEDDING_AVAILABLE = False

# ========================================
# 0. ê¸°ë³¸ ê²½ë¡œ/ë¡œì»¬ DB ì„¤ì •
# ========================================

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "local_db")
AUDIO_DIR = os.path.join(DATA_DIR, "audio")
RAG_INDEX_DIR = os.path.join(DATA_DIR, "rag_index")

VOICE_META_FILE = os.path.join(DATA_DIR, "voice_records.json")
SIM_META_FILE = os.path.join(DATA_DIR, "simulation_histories.json")

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(AUDIO_DIR, exist_ok=True)
os.makedirs(RAG_INDEX_DIR, exist_ok=True)


# ----------------------------------------
# JSON Helper
# ----------------------------------------
def _load_json(path: str, default: Any):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return default


def _save_json(path: str, data: Any):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ========================================
# 1. ë‹¤êµ­ì–´ ì„¤ì • (ì „í™” ë°œì‹  ê´€ë ¨ í…ìŠ¤íŠ¸ ì¶”ê°€)
# ========================================
DEFAULT_LANG = "ko"

LANG: Dict[str, Dict[str, str]] = {
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜ (ìŒì„± ë° DB í†µí•©)",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "sim_tab_chat_email": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì±„íŒ…/ì´ë©”ì¼)",
        "sim_tab_phone": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì „í™”)",
        "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
        "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
        "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
        "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
        "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
        "check_answer": "ì •ë‹µ í™•ì¸",
        "next_question": "ë‹¤ìŒ ë¬¸í•­",
        "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
        "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤ã€‚ğŸ˜",
        "correct_is": "ì •ë‹µ",
        "explanation": "í•´ì„¤",
        "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
        "score": "ì ìˆ˜",
        "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
        "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
        "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚",
        "db_save_complete": "(DB ì €ì¥ ì™„ë£Œ)",
        "data_analysis_progress": "ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘...",
        "response_generating": "ë‹µë³€ ìƒì„± ì¤‘...",
        "lstm_result_header": "í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼",
        "lstm_score_metric": "í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„",
        "lstm_score_info": "ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!",
        "lstm_rerun_button": "ìƒˆë¡œìš´ ê°€ìƒ ë°ì´í„°ë¡œ ì˜ˆì¸¡",

        # --- ì‹œë®¬ë ˆì´í„° ---
        "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤ã€‚",
        "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
        "customer_type_label": "ê³ ê° ì„±í–¥",
        "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
        "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
        "customer_generate_response_button": "ê³ ê° ë°˜ì‘ ìƒì„±",
        "send_closing_confirm_button": "ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸°",
        "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ê¸° ë•Œë¬¸ì— ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
        "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
        "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
        "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
        "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
        "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!",
        "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
        "history_expander_title": "ğŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê±´)",
        "initial_query_sample": "í”„ë‘ìŠ¤ íŒŒë¦¬ì— ë„ì°©í–ˆëŠ”ë°, í´ë£©ì—ì„œ êµ¬ë§¤í•œ eSIMì´ í™œì„±í™”ê°€ ì•ˆ ë©ë‹ˆë‹¤...",
        "button_mic_input": "ğŸ™ ìŒì„± ì…ë ¥",
        "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤ã€‚",
        "prompt_survey": "ì§€ê¸ˆê¹Œì§€ ìƒë‹´ì› 000ì˜€ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ í•˜ë£¨ ë˜ì‹œê¸° ë°”ëë‹ˆë‹¤. [ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬]",
        "customer_closing_confirm": "ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹­ë‹ˆê¹Œ?",
        "customer_positive_response": "ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤ã€‚",
        "button_email_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ìš”ì²­)",
        "error_mandatory_contact": "ì´ë©”ì¼ê³¼ ì „í™”ë²ˆí˜¸ ì…ë ¥ì€ í•„ìˆ˜ì…ë‹ˆë‹¤ã€‚",
        "customer_attachment_label": "ğŸ“ ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ",
        "attachment_info_llm": "[ê³ ê° ì²¨ë¶€ íŒŒì¼: {filename}ì´(ê°€) í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”.]",
        "button_retry_translation": "ë²ˆì—­ ë‹¤ì‹œ ì‹œë„",
        "button_request_hint": "ğŸ’¡ ì‘ëŒ€ íŒíŠ¸ ìš”ì²­ (AHT ëª¨ë‹ˆí„°ë§ ì¤‘)",
        "hint_placeholder": "ë¬¸ì˜ ì‘ëŒ€ì— ëŒ€í•œ íŒíŠ¸:",
        "survey_sent_confirm": "ğŸ“¨ ì„¤ë¬¸ì¡°ì‚¬ ë§í¬ê°€ ì „ì†¡ë˜ì—ˆìœ¼ë©°, ì´ ìƒë‹´ì€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "new_simulation_ready": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ã€‚",
        "agent_response_header": "âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ",
        "agent_response_placeholder": "ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš”...",
        "send_response_button": "ì‘ë‹µ ì „ì†¡",
        "customer_turn_info": "ì—ì´ì „íŠ¸ ì‘ë‹µ ì „ì†¡ ì™„ë£Œ. ê³ ê° ë°˜ì‘ì„ ìë™ìœ¼ë¡œ ìƒì„± ì¤‘ì…ë‹ˆë‹¤ã€‚",
        "generating_customer_response": "ê³ ê° ë°˜ì‘ ìƒì„± ì¤‘...",
        "customer_escalation_start": "ìƒê¸‰ìì™€ ì´ì•¼ê¸°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
        "request_rebuttal_button": "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­",
        "new_simulation_button": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘",
        "history_selectbox_label": "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
        "history_load_button": "ì„ íƒëœ ì´ë ¥ ë¡œë“œ",
        "delete_history_button": "âŒ ëª¨ë“  ì´ë ¥ ì‚­ì œ",
        "delete_confirm_message": "ì •ë§ë¡œ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        "delete_confirm_yes": "ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤",
        "delete_confirm_no": "ì•„ë‹ˆì˜¤, ìœ ì§€í•©ë‹ˆë‹¤",
        "delete_success": "âœ… ì‚­ì œ ì™„ë£Œ!",
        "deleting_history_progress": "ì´ë ¥ ì‚­ì œ ì¤‘...",
        "search_history_label": "ì´ë ¥ ê²€ìƒ‰",
        "date_range_label": "ë‚ ì§œ ë²”ìœ„ í•„í„°",
        "history_search_button": "ğŸ” ê²€ìƒ‰",
        "no_history_found": "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "customer_email_label": "ê³ ê° ì´ë©”ì¼ (í•„ìˆ˜)",
        "customer_phone_label": "ê³ ê° ì—°ë½ì²˜ / ì „í™”ë²ˆí˜¸ (í•„ìˆ˜)",
        "transfer_header": "ì–¸ì–´ ì´ê´€ ìš”ì²­ (ë‹¤ë¥¸ íŒ€)",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ ì˜ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ ì¼ë³¸ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŒ€ìœ¼ë¡œ ì´ê´€",
        "transfer_system_msg": "ğŸ“Œ ì‹œìŠ¤í…œ ë©”ì‹œì§€: ê³ ê° ìš”ì²­ì— ë”°ë¼ ìƒë‹´ ì–¸ì–´ê°€ {target_lang} íŒ€ìœ¼ë¡œ ì´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìƒë‹´ì›(AI)ì´ ì‘ëŒ€í•©ë‹ˆë‹¤ã€‚",
        "transfer_loading": "ì´ê´€ ì²˜ë¦¬ ì¤‘: ì´ì „ ëŒ€í™” ì´ë ¥ ë²ˆì—­ ë° ê²€í†  (ê³ ê°ë‹˜ê»˜ 3~10ë¶„ ì–‘í•´ ìš”ì²­)",
        "transfer_summary_header": "ğŸ” ì´ê´€ëœ ìƒë‹´ì›ì„ ìœ„í•œ ìš”ì•½ (ë²ˆì—­ë¨)",
        "transfer_summary_intro": "ê³ ê°ë‹˜ê³¼ì˜ ì´ì „ ëŒ€í™” ì´ë ¥ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ëŒ€ë¥¼ ì´ì–´ë‚˜ê°€ì„¸ìš”ã€‚",
        "llm_translation_error": "âŒ ë²ˆì—­ ì‹¤íŒ¨: LLM ì‘ë‹µ ì˜¤ë¥˜",
        "timer_metric": "ìƒë‹´ ê²½ê³¼ ì‹œê°„ (AHT)",
        "timer_info_ok": "AHT (15ë¶„ ê¸°ì¤€)",
        "timer_info_warn": "AHT (10ë¶„ ì´ˆê³¼)",
        "timer_info_risk": "ğŸš¨ 15ë¶„ ì´ˆê³¼: ë†’ì€ ë¦¬ìŠ¤í¬",
        "solution_check_label": "âœ… ì´ ì‘ë‹µì— ì†”ë£¨ì…˜/í•´ê²°ì±…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ã€‚",
        "sentiment_score_label": "ê³ ê° ê°ì • ì ìˆ˜",
        "urgency_score_label": "ê¸´ê¸‰ë„ ì ìˆ˜",
        "similarity_chart_title": "ìœ ì‚¬ ì¼€ì´ìŠ¤ ìœ ì‚¬ë„",
        "scores_comparison_title": "ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ",
        "similarity_score_label": "ìœ ì‚¬ë„",
        "satisfaction_score_label": "ë§Œì¡±ë„",
        "sentiment_trend_label": "ê°ì • ì ìˆ˜ ì¶”ì´",
        "satisfaction_trend_label": "ë§Œì¡±ë„ ì ìˆ˜ ì¶”ì´",
        "case_trends_title": "ê³¼ê±° ì¼€ì´ìŠ¤ ì ìˆ˜ ì¶”ì´",
        "date_label": "ë‚ ì§œ",
        "score_label": "ì ìˆ˜ (0-100)",
        "customer_characteristics_title": "ê³ ê° íŠ¹ì„± ë¶„í¬",
        "language_label": "ì–¸ì–´",
        "email_provided_label": "ì´ë©”ì¼ ì œê³µ",
        "phone_provided_label": "ì „í™”ë²ˆí˜¸ ì œê³µ",
        "region_label": "ì§€ì—­",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "ì „í™” ë°œì‹ ",
        "call_outbound_system_msg": "ğŸ“Œ ì‹œìŠ¤í…œ ë©”ì‹œì§€: ì—ì´ì „íŠ¸ê°€ {target}ì—ê²Œ ì „í™” ë°œì‹ ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤.",
        "call_outbound_simulation_header": "ğŸ“ ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼",
        "call_outbound_summary_header": "ğŸ“ í˜„ì§€ ì—…ì²´/ê³ ê°ê³¼ì˜ í†µí™” ìš”ì•½",
        "call_outbound_loading": "ì „í™” ì—°ê²° ë° í†µí™” ê²°ê³¼ ì •ë¦¬ ì¤‘... (LLM í˜¸ì¶œ)",
        "call_target_customer": "ê³ ê°ì—ê²Œ ë°œì‹ ",
        "call_target_partner": "í˜„ì§€ ì—…ì²´ ë°œì‹ ",

        # --- ìŒì„± ê¸°ë¡ ---
        "voice_rec_header": "ìŒì„± ê¸°ë¡ & ê´€ë¦¬",
        "record_help": "ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "uploaded_file": "ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ",
        "rec_list_title": "ì €ì¥ëœ ìŒì„± ê¸°ë¡",
        "transcribe_btn": "ì „ì‚¬(Whisper)",
        "save_btn": "ìŒì„± ê¸°ë¡ ì €ì¥",
        "transcribing": "ìŒì„± ì „ì‚¬ ì¤‘...",
        "transcript_result": "ì „ì‚¬ ê²°ê³¼:",
        "transcript_text": "ì „ì‚¬ í…ìŠ¤íŠ¸",
        "openai_missing": "OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "whisper_client_error": "âŒ Whisper API Client ì´ˆê¸°í™” ì‹¤íŒ¨",
        "whisper_auth_error": "âŒ Whisper API ì¸ì¦ ì‹¤íŒ¨",
        "whisper_format_error": "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤ã€‚",
        "whisper_success": "âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ!",
        "playback": "ë…¹ìŒ ì¬ìƒ",
        "retranscribe": "ì¬ì „ì‚¬",
        "delete": "ì‚­ì œ",
        "no_records": "ì €ì¥ëœ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "saved_success": "ì €ì¥ ì™„ë£Œ!",
        "delete_confirm_rec": "ì •ë§ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
        "gcs_not_conf": "GCS ë¯¸ì„¤ì •",
        "gcs_playback_fail": "ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨",
        "gcs_no_audio": "ì˜¤ë””ì˜¤ ì—†ìŒ",
        "error": "ì˜¤ë¥˜:",
        "firestore_no_db_connect": "DB ì—°ê²° ì‹¤íŒ¨",
        "save_history_success": "ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "save_history_fail": "ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨",
        "delete_fail": "ì‚­ì œ ì‹¤íŒ¨",
        "rec_header": "ìŒì„± ì…ë ¥ ë° ì „ì‚¬",
        "whisper_processing": "ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘",
        "empty_response_warning": "ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”ã€‚",
        "customer_no_more_inquiries": "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤ã€‚",
        "customer_has_additional_inquiries": "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤ã€‚",
        "sim_end_chat_button": "ì„¤ë¬¸ ì¡°ì‚¬ ë§í¬ ì „ì†¡ ë° ì‘ëŒ€ ì¢…ë£Œ",
        "delete_mic_record": "âŒ ë…¹ìŒ ì‚­ì œ",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "ê³ ê° ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë“œ (ìŠ¤í¬ë¦°ìƒ· ë“±)",
        "attachment_placeholder": "íŒŒì¼ì„ ì²¨ë¶€í•˜ì—¬ ìƒí™©ì„ ì„¤ëª…í•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­)",
        "attachment_info_llm": "[ê³ ê° ì²¨ë¶€ íŒŒì¼: {filename}ì´(ê°€) í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‘ëŒ€í•˜ì„¸ìš”.]",
        "agent_attachment_label": "ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ (ìŠ¤í¬ë¦°ìƒ· ë“±)",
        "agent_attachment_placeholder": "ì‘ë‹µì— ì²¨ë¶€í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì„ íƒ ì‚¬í•­)",
        "agent_attachment_status": "ğŸ“ ì—ì´ì „íŠ¸ê°€ **{filename}** íŒŒì¼ì„ ì‘ë‹µì— ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. (íŒŒì¼ íƒ€ì…: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG ì„ë² ë”© ì‹¤íŒ¨: OpenAI API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_gemini": "RAG ì„ë² ë”© ì‹¤íŒ¨: Gemini API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_nvidia": "RAG ì„ë² ë”© ì‹¤íŒ¨: NVIDIA API Keyê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "rag_embed_error_none": "RAG ì„ë² ë”©ì— í•„ìš”í•œ ëª¨ë“  í‚¤(OpenAI, Gemini, NVIDIA)ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”ã€‚",

        # --- ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ì¶”ê°€ ---
        "phone_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„° (ì „í™”)",
        "call_status_waiting": "ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...",
        "call_status_ringing": "ì „í™” ìˆ˜ì‹  ì¤‘: {number}",
        "button_answer": "ğŸ“ ì „í™” ì‘ë‹µ",
        "button_hangup": "ğŸ”´ ì „í™” ëŠê¸°",
        "button_hold": "â¸ï¸ Hold (ì†ŒìŒ ì°¨ë‹¨)",
        "button_resume": "â–¶ï¸ í†µí™” ì¬ê°œ",
        "hold_status": "í†µí™” Hold ì¤‘ (ëˆ„ì  Hold ì‹œê°„: {duration})",
        "cc_live_transcript": "ğŸ¤ ì‹¤ì‹œê°„ CC ìë§‰ / ì „ì‚¬",
        "mic_input_status": "ğŸ™ï¸ ì—ì´ì „íŠ¸ ìŒì„± ì…ë ¥",
        "customer_audio_playback": "ğŸ—£ï¸ ê³ ê° ìŒì„± ì¬ìƒ",
        "agent_response_prompt": "ê³ ê°ì—ê²Œ ë§í•  ì‘ë‹µì„ ë…¹ìŒí•˜ì„¸ìš”ã€‚",
        "call_end_message": "í†µí™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. AHT ë° ì´ë ¥ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "call_query_placeholder": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”ã€‚",
        "call_number_placeholder": "+82 10-xxxx-xxxx (ê°€ìƒ ë²ˆí˜¸)",
        "call_summary_header": "AI í†µí™” ìš”ì•½",
        "customer_audio_header": "ê³ ê° ìµœì´ˆ ë¬¸ì˜ (ìŒì„±)",
        "aht_not_recorded": "âš ï¸ í†µí™” ì‹œì‘ ì‹œê°„ì´ ê¸°ë¡ë˜ì§€ ì•Šì•„ AHTë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "no_audio_record": "ê³ ê°ì˜ ìµœì´ˆ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",
    },

    # --- â­ ì˜ì–´ ë²„ì „ (í•œêµ­ì–´ 100% ë§¤ì¹­) ---
    "en": {
        "title": "Personalized AI Study Coach (Voice & Local DB)",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction Dashboard",
        "sim_tab_chat_email": "AI Customer Support Simulator (Chat / Email)",
        "sim_tab_phone": "AI Customer Support Simulator (Phone)",
        "simulator_tab": "AI Customer Support Simulator",
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answer questions based on uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "llm_error_key": "âš ï¸ Warning: GEMINI_API_KEY is not set.",
        "llm_error_init": "LLM initialization error. Please check your API key.",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content based on the topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "10 MCQ Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Achievement Prediction Dashboard",
        "lstm_desc": "Train an LSTM model on hypothetical quiz scores and predict performance.",
        "lang_select": "Select Language",
        "embed_success": "Learning DB built with {count} chunks!",
        "embed_fail": "Embedding failed: quota exceeded or network issue.",
        "warning_no_files": "Please upload study materials first.",
        "warning_rag_not_ready": "RAG is not ready. Upload materials and analyze.",
        "quiz_fail_structure": "Quiz data structure is invalid.",
        "select_answer": "Select answer",
        "check_answer": "Check answer",
        "next_question": "Next question",
        "correct_answer": "Correct! ğŸ‰",
        "incorrect_answer": "Incorrect ğŸ˜",
        "correct_is": "Correct answer",
        "explanation": "Explanation",
        "quiz_complete": "Quiz Complete!",
        "score": "Score",
        "retake_quiz": "Retake Quiz",
        "quiz_error_llm": "Quiz generation failed: invalid JSON.",
        "quiz_original_response": "Original LLM Response",
        "firestore_loading": "Loading RAG index...",
        "firestore_no_index": "No existing RAG index found.",
        "db_save_complete": "(DB Save Complete)",
        "data_analysis_progress": "Analyzing materials and building DB...",
        "response_generating": "Generating response...",
        "lstm_result_header": "Achievement Prediction",
        "lstm_score_metric": "Predicted Achievement",
        "lstm_score_info": "Estimated next quiz score: **{predicted_score:.1f}**.",
        "lstm_rerun_button": "Predict with New Data",

        # Simulator
        "simulator_header": "AI Customer Response Simulator",
        "simulator_desc": "AI generates draft responses and guidelines for customer inquiries.",
        "customer_query_label": "Customer Message (links allowed)",
        "customer_type_label": "Customer Type",
        "customer_type_options": ["General Inquiry", "Difficult Customer", "Highly Dissatisfied Customer"],
        "button_simulate": "Generate Response",
        "customer_generate_response_button": "Generate Customer Response",
        "send_closing_confirm_button": "Send Closing Confirmation",
        "simulation_warning_query": "Please enter the customerâ€™s message.",
        "simulation_no_key_warning": "âš ï¸ API Key missing. Simulation cannot proceed.",
        "simulation_advice_header": "AI Response Guidelines",
        "simulation_draft_header": "Recommended Response Draft",
        "button_listen_audio": "Play as Audio",
        "tts_status_ready": "Ready to generate audio",
        "tts_status_generating": "Generating audio...",
        "tts_status_success": "Audio ready!",
        "tts_status_error": "TTS error occurred",
        "history_expander_title": "ğŸ“ Load Previous Sessions (Last 10)",
        "initial_query_sample": "I arrived in Paris but my Klook eSIM won't activateâ€¦",
        "button_mic_input": "ğŸ™ Voice Input",
        "prompt_customer_end": "No further inquiries. Ending chat.",
        "prompt_survey": "This was Agent 000. Have a nice day. [Survey Link]",
        "customer_closing_confirm": "Is there anything else we can assist you with?",
        "customer_positive_response": "I understand. Thank you.",
        "button_email_end_chat": "End supports (Survey Request)",
        "error_mandatory_contact": "Email and Phone number input are mandatory.",
        "customer_attachment_label": "ğŸ“ Customer Attachment Upload",
        "attachment_info_llm": "[Customer Attachment: {filename} is confirmed. Reference this file in your response.]",
        "button_retry_translation": "Retry Translation",
        "button_request_hint": "ğŸ’¡ Request Response Hint (AHT Monitored)",
        "hint_placeholder": "Hints for responses",
        "survey_sent_confirm": "ğŸ“¨ The survey link has been sent. This chat session is now closed.",
        "new_simulation_ready": "You can now start a new simulation.",
        "agent_response_header": "âœï¸ Agent Response",
        "agent_response_placeholder": "Write a response...",
        "send_response_button": "Send Response",
        "customer_turn_info": "Agent response sent. Generating customer reaction automatically.",
        "generating_customer_response": "Generating customer response...",
        "customer_escalation_start": "I want to speak to a supervisor",
        "request_rebuttal_button": "Request Customer Reaction",
        "new_simulation_button": "Start New Simulation",
        "history_selectbox_label": "Choose a record to load:",
        "history_load_button": "Load Selected Record",
        "delete_history_button": "âŒ Delete All History",
        "delete_confirm_message": "Are you sure you want to delete all records?",
        "delete_confirm_yes": "Yes, Delete",
        "delete_confirm_no": "Cancel",
        "delete_success": "Deleted successfully!",
        "deleting_history_progress": "Deleting history...",
        "search_history_label": "Search History",
        "date_range_label": "Date Filter",
        "history_search_button": "ğŸ” Search",
        "no_history_found": "No matching history found.",
        "customer_email_label": "Customer Email (Mandatory)",
        "customer_phone_label": "Customer Phone / WhatsApp (Mandatory)",
        "transfer_header": "Language Transfer Request (To Other Teams)",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ English Team Transfer",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ Japanese Team Transfer",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· Korean Team Transfer",
        "transfer_system_msg": "ğŸ“Œ System Message: The session language has been transferred to the {target_lang} team per customer request. A new agent (AI) will now respond.",
        "transfer_loading": "Transferring: Translating and reviewing chat history (3-10 minute wait requested from customer)",
        "transfer_summary_header": "ğŸ” Summary for Transferred Agent (Translated)",
        "transfer_summary_intro": "This is the previous chat history. Please continue the support based on this summary.",
        "llm_translation_error": "âŒ Translation failed: LLM response error",
        "timer_metric": "Elapsed Time",
        "timer_info_ok": "AHT (15 min standard)",
        "timer_info_warn": "AHT (Over 10 min)",
        "timer_info_risk": "ğŸš¨ Over 15 min: High Risk",
        "solution_check_label": "âœ… This response includes a solution/fix.",
        "sentiment_score_label": "Customer Sentiment Score",
        "urgency_score_label": "Urgency Score",
        "similarity_chart_title": "Case Similarity",
        "scores_comparison_title": "Sentiment & Satisfaction Scores",
        "similarity_score_label": "Similarity",
        "satisfaction_score_label": "Satisfaction",
        "sentiment_trend_label": "Sentiment Trend",
        "satisfaction_trend_label": "Satisfaction Trend",
        "case_trends_title": "Case Score Trends",
        "date_label": "Date",
        "score_label": "Score (0-100)",
        "customer_characteristics_title": "Customer Characteristics",
        "language_label": "Language",
        "email_provided_label": "Email Provided",
        "phone_provided_label": "Phone Provided",
        "region_label": "Region",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "Call Outbound",
        "call_outbound_system_msg": "ğŸ“Œ System Message: Agent attempted an outbound call to {target}.",
        "call_outbound_simulation_header": "ğŸ“ Outbound Call Simulation Result",
        "call_outbound_summary_header": "ğŸ“ Summary of Call with Local Partner/Customer",
        "call_outbound_loading": "Connecting call and summarizing outcome... (LLM Call)",
        "call_target_customer": "Call Customer",
        "call_target_partner": "Call Local Partner",

        # --- ìŒì„± ê¸°ë¡ ---
        "voice_rec_header": "Voice Record & Management",
        "record_help": "Record using the microphone or upload a file.",
        "uploaded_file": "Upload Audio File",
        "rec_list_title": "Saved Voice Records",
        "transcribe_btn": "Transcribe (Whisper)",
        "save_btn": "Save Record",
        "transcribing": "Transcribing...",
        "transcript_result": "Transcription:",
        "transcript_text": "Transcribed Text",
        "openai_missing": "OpenAI API Key is missing. Please set OPENAI_API_KEY.",
        "whisper_client_error": "âŒ Error: Whisper API client not initialized.",
        "whisper_auth_error": "âŒ Whisper API authentication failed. Check your API Key.",
        "whisper_format_error": "âŒ Error: Unsupported audio format.",
        "playback": "Playback Recording",
        "retranscribe": "Re-transcribe",
        "delete": "Delete",
        "no_records": "No saved voice records.",
        "saved_success": "Saved successfully!",
        "delete_confirm_rec": "Are you sure you want to delete this voice record?",
        "gcs_not_conf": "GCS not configured or no audio available",
        "gcs_playback_fail": "Failed to play audio",
        "gcs_no_audio": "No audio file found",
        "error": "Error:",
        "firestore_no_db_connect": "DB connection failed",
        "save_history_success": "Saved successfully.",
        "save_history_fail": "Save failed.",
        "delete_fail": "Delete failed",
        "rec_header": "Voice Input & Transcription",
        "whisper_processing": "Processing...",
        "empty_response_warning": "Please enter a response.",
        "customer_no_more_inquiries": "No, that will be all, thank you.",
        "customer_has_additional_inquiries": "Yes, I have an additional question.",
        "sim_end_chat_button": "Send Survey Link and End Consultations",
        "delete_mic_record": "âŒ Delete recordings",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "Customer Attachment Upload (Screenshot, etc.)",
        "attachment_placeholder": "Attach a file to explain the situation (optional)",
        "attachment_status_llm": "ê³ ê°ì´ **{filename}** íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ìŠ¤í¬ë¦°ìƒ·ì´ë¼ê³  ê°€ì •í•˜ê³  ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì— ë°˜ì˜í•˜ì„¸ìš”. (íŒŒì¼ íƒ€ì…: {filetype})",
        "agent_attachment_label": "Agent Attachment (Screenshot, etc.)",
        "agent_attachment_placeholder": "Select a file to attach to the response (optional)",
        "agent_attachment_status": "ğŸ“ ì—ì´ì „íŠ¸ê°€ **{filename}** íŒŒì¼ì„ ì‘ë‹µì— ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. (íŒŒì¼ íƒ€ì…: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG embedding failed: OpenAI API Key is invalid or not set.",
        "rag_embed_error_gemini": "RAG embedding failed: Gemini API Key is invalid or not set.",
        "rag_embed_error_nvidia": "RAG embedding failed: NVIDIA API Key is invalid or not set.",
        "rag_embed_error_none": "RAG embedding failed: All required keys (OpenAI, Gemini, NVIDIA) are invalid or not set. Please configure a key.",

        # --- ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ì¶”ê°€ ---
        "phone_header": "AI Customer Support Simulator (Phone)",
        "call_status_waiting": "Waiting for incoming call...",
        "call_status_ringing": "Incoming Call from: {number}",
        "button_answer": "ğŸ“ Answer Call",
        "button_hangup": "ğŸ”´ Hang Up",
        "button_hold": "â¸ï¸ Hold (Mute)",
        "button_resume": "â–¶ï¸ Resume Call",
        "hold_status": "On Hold (Total Hold Time: {duration})",
        "cc_live_transcript": "ğŸ¤ Live CC Transcript",
        "mic_input_status": "ğŸ™ï¸ Agent Voice Input",
        "customer_audio_playback": "ğŸ—£ï¸ Customer Audio Playback",
        "agent_response_prompt": "Record your response to the customer.",
        "call_end_message": "Call ended. Check AHT and history.",
        "call_query_placeholder": "Enter customer's initial query.",
        "call_number_placeholder": "+1 (555) 123-4567 (Mock Number)",
        "call_summary_header": "AI Call Summary",
        "customer_audio_header": "Customer Initial Query (Voice)",
        "aht_not_recorded": "âš ï¸ Call start time not recorded. Cannot calculate AHT.",
        "no_audio_record": "No initial customer voice record.",

    },

    # --- â­ ì¼ë³¸ì–´ ë²„ì „ (í•œêµ­ì–´ 100% ë§¤ì¹­) ---
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«DB)",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
        "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
        "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "content_tab": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "sim_tab_chat_email": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(ãƒãƒ£ãƒƒãƒˆãƒ»ãƒ¡ãƒ¼ãƒ«)",
        "sim_tab_phone": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(é›»è©±)",
        "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
        "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸè³‡æ–™ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
        "rag_input_placeholder": "è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
        "llm_error_key": "âš ï¸ æ³¨æ„: GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "llm_error_init": "LLM åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«å¿œã˜ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
        "level_label": "é›£æ˜“åº¦",
        "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¨®é¡",
        "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
        "content_options": ["è¦ç‚¹ã‚µãƒãƒªãƒ¼", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã‚¢ã‚¤ãƒ‡ã‚¢"],
        "button_generate": "ç”Ÿæˆã™ã‚‹",
        "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "lstm_header": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "lstm_desc": "ä»®æƒ³ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨ã—ã¦é”æˆåº¦ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚",
        "lang_select": "è¨€èªé¸æŠ",
        "embed_success": "{count}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã§DBæ§‹ç¯‰å®Œäº†!",
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—ï¼šã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œã€‚",
        "warning_no_files": "è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "warning_rag_not_ready": "RAGãŒæº–å‚™ã§ãã¦ã„ã¾ã›ã‚“ã€‚",
        "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "select_answer": "å›ç­”ã‚’é¸æŠã—ã¦ãã ã•ã„",
        "check_answer": "å›ç­”ã‚’ç¢ºèª",
        "next_question": "æ¬¡ã®è³ªå•",
        "correct_answer": "æ­£è§£ï¼ ğŸ‰",
        "incorrect_answer": "ä¸æ­£è§£ ğŸ˜",
        "correct_is": "æ­£è§£",
        "explanation": "è§£èª¬",
        "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
        "score": "ã‚¹ã‚³ã‚¢",
        "retake_quiz": "å†æŒ‘æˆ¦",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨ï¼šJSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "quiz_original_response": "LLM åŸæœ¬å›ç­”",
        "firestore_loading": "RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ä¸­...",
        "firestore_no_index": "ä¿å­˜ã•ã‚ŒãŸRAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
        "db_save_complete": "(DBä¿å­˜å®Œäº†)",
        "data_analysis_progress": "è³‡æ–™åˆ†æä¸­...",
        "response_generating": "å¿œç­”ç”Ÿæˆä¸­...",
        "lstm_result_header": "é”æˆåº¦äºˆæ¸¬çµæœ",
        "lstm_score_metric": "äºˆæ¸¬é”æˆåº¦",
        "lstm_score_info": "æ¬¡ã®ã‚¹ã‚³ã‚¢äºˆæ¸¬: **{predicted_score:.1f}ì **",
        "lstm_rerun_button": "æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§å†äºˆæ¸¬",

        # --- Simulator ---
        "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "simulator_desc": "é›£ã—ã„é¡§å®¢å•ã„åˆã‚ã›ã«å¯¾ã™ã‚‹AIã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã¨è‰æ¡ˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "customer_query_label": "é¡§å®¢ã‹ã‚‰ã®å•ã„åˆã‚ã›å†…å®¹ (ãƒªãƒ³ã‚¯å¯)",
        "customer_type_label": "é¡§å®¢ã‚¿ã‚¤ãƒ—",
        "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "é›£ã—ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"],
        "button_simulate": "å¿œå¯¾ã‚¬ã‚¤ãƒ‰ç”Ÿæˆ",
        "customer_generate_response_button": "é¡§å®¢ã®è¿”ä¿¡ã‚’ç”Ÿæˆ",
        "send_closing_confirm_button": "è¿½åŠ ã®ã”è³ªå•æœ‰ç„¡ã‚’ç¢ºèªã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡",
        "simulation_warning_query": "ãŠå•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "simulation_no_key_warning": "âš ï¸ APIã‚­ãƒ¼ä¸è¶³ã®ãŸã‚å¿œå¯¾ç”Ÿæˆä¸å¯ã€‚",
        "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
        "simulation_draft_header": "æ¨å¥¨å¿œå¯¾è‰æ¡ˆ",
        "button_listen_audio": "éŸ³å£°ã§èã",
        "tts_status_ready": "éŸ³å£°ç”Ÿæˆæº–å‚™å®Œäº†",
        "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
        "tts_status_success": "éŸ³å£°æº–å‚™å®Œäº†ï¼",
        "tts_status_error": "TTS ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "history_expander_title": "ğŸ“ éå»ã®å¯¾å¿œå±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ (æœ€æ–°10ä»¶)",
        "initial_query_sample": "ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã®eSIMãŒä½¿ãˆã¾ã›ã‚“â€¦",
        "button_mic_input": "ğŸ™ éŸ³å£°å…¥åŠ›",
        "prompt_customer_end": "è¿½åŠ ã®è³ªå•ãŒãªã„ãŸã‚ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚",
        "prompt_survey": "æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ000ã§ã—ãŸã€‚è‰¯ã„ä¸€æ—¥ã‚’ãŠéã”ã—ãã ã•ã„ã€‚ [ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯]",
        "customer_closing_confirm": "ä»–ã®ãŠå•åˆã›ã¯ã”ã–ã„ã¾ã›ã‚“ã§ã—ã‚‡ã†ã‹ã€‚",
        "customer_positive_response": "æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚",
        "button_email_end_chat": "å¿œå¯¾çµ‚äº†ï¼ˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆï¼‰",
        "error_mandatory_contact": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨é›»è©±ç•ªå·ã®å…¥åŠ›ã¯å¿…é ˆã§ã™ã€‚",
        "customer_attachment_label": "ğŸ“ é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "attachment_info_llm": "[é¡§å®¢æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«: {filename}ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦å¯¾å¿œã—ã¦ãã ã•ã„ã€‚]",
        "button_retry_translation": "ç¿»è¨³ã‚’å†è©¦è¡Œ",
        "button_request_hint": "ğŸ’¡ å¿œå¯¾ãƒ’ãƒ³ãƒˆã‚’è¦è«‹ (AHT ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ä¸­)",
        "hint_placeholder": "ãŠå•åˆã›ã®å¿œå¯¾ã«å¯¾ã™ã‚‹ãƒ’ãƒ³ãƒˆï¼š",
        "new_simulation_ready": "æ–°ã—ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã§ãã¾ã™ã€‚",
        "survey_sent_confirm": "ğŸ“¨ ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚ã“ã®ãƒãƒ£ãƒƒãƒˆã¯çµ‚äº†ã—ã¾ã—ãŸã€‚",
        "agent_response_header": "âœï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”",
        "agent_response_placeholder": "é¡§å®¢ã¸è¿”ä¿¡å†…å®¹ã‚’å…¥åŠ›â€¦",
        "send_response_button": "è¿”ä¿¡é€ä¿¡",
        "customer_turn_info": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”é€ä¿¡å®Œäº†ã€‚é¡§å®¢ã®åå¿œã‚’è‡ªå‹•ç”Ÿæˆä¸­ã§ã™ã€‚",
        "generating_customer_response": "é¡§å®¢ã®åå¿œã‚’ç”Ÿæˆä¸­...",
        "customer_escalation_start": "ä¸Šç´šã®æ‹…å½“è€…ã¨è©±ã—ãŸã„",
        "request_rebuttal_button": "é¡§å®¢ã®åå¿œã‚’ç”Ÿæˆ",
        "new_simulation_button": "æ–°è¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
        "history_selectbox_label": "å±¥æ­´ã‚’é¸æŠ:",
        "history_load_button": "å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€",
        "delete_history_button": "âŒ å…¨å±¥æ­´å‰Šé™¤",
        "delete_confirm_message": "ã™ã¹ã¦ã®å±¥æ­´ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ",
        "delete_confirm_yes": "ã¯ã„ã€å‰Šé™¤ã—ã¾ã™ã€‚",
        "delete_confirm_no": "ã„ã„ãˆã€ç¶­æŒã—ã¾ã™ã€‚",
        "delete_success": "å‰Šé™¤å®Œäº†ï¼",
        "deleting_history_progress": "å‰Šé™¤ä¸­...",
        "search_history_label": "å±¥æ­´æ¤œç´¢",
        "date_range_label": "æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼",
        "history_search_button": "ğŸ” æ¤œç´¢",
        "no_history_found": "è©²å½“ã™ã‚‹å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "customer_email_label": "é¡§å®¢ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆå¿…ä¿®ï¼‰",
        "customer_phone_label": "é¡§å®¢é€£çµ¡å…ˆ / é›»è©±ç•ªå·ï¼ˆå¿…ä¿®ï¼‰",
        "transfer_header": "è¨€èªåˆ‡ã‚Šæ›¿ãˆè¦è«‹ï¼ˆä»–ãƒãƒ¼ãƒ ã¸ï¼‰",
        "transfer_to_en": "ğŸ‡ºğŸ‡¸ è‹±èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_to_ja": "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_to_ko": "ğŸ‡°ğŸ‡· éŸ“å›½èªãƒãƒ¼ãƒ ã¸è»¢é€",
        "transfer_system_msg": "ğŸ“Œ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: é¡§å®¢ã®è¦è«‹ã«ã‚ˆã‚Šã€å¯¾å¿œè¨€èªãŒ {target_lang} ãƒãƒ¼ãƒ ã¸åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã—ãŸã€‚æ–°ã—ã„æ‹…å½“è€…(AI)ãŒå¯¾å¿œã—ã¾ã™ã€‚",
        "transfer_loading": "è»¢é€ä¸­: éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ç¿»è¨³ãŠã‚ˆã³ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ã„ã¾ã™ (ãŠå®¢æ§˜ã«ã¯3ã€œ10åˆ†ã®ãŠæ™‚é–“ã‚’ã„ãŸã ã„ã¦ã„ã¾ã™)",
        "transfer_summary_header": "ğŸ” è»¢é€ã•ã‚ŒãŸæ‹…å½“è€…å‘ã‘ã®è¦ç´„ (ç¿»è¨³æ¸ˆã¿)",
        "transfer_summary_intro": "ã“ã‚ŒãŒé¡§å®¢ã¨ã®éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã§ã™ã€‚ã“ã®è¦ç´„ã«åŸºã¥ã„ã¦ã‚µãƒãƒ¼ãƒˆã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚",
        "llm_translation_error": "âŒ ç¿»è¨³å¤±æ•—: LLMå¿œç­”ã‚¨ãƒ©ãƒ¼",
        "timer_metric": "çµŒéæ™‚é–“",
        "timer_info_ok": "AHT (15ë¶„ ê¸°ì¤€)",
        "timer_info_warn": "AHT (10ë¶„ ì´ˆê³¼)",
        "timer_info_risk": "ğŸš¨ 15åˆ†è¶…: é«˜ã„ãƒªã‚¹ã‚¯",
        "solution_check_label": "âœ… ã“ã®å¿œç­”ã«è§£æ±ºç­–/å¯¾å¿œç­–ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚",

        # --- ì¶”ê°€ëœ ì „í™” ë°œì‹  ê¸°ëŠ¥ ê´€ë ¨ ---
        "button_call_outbound": "é›»è©±ç™ºä¿¡",
        "call_outbound_system_msg": "ğŸ“Œ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ{target}ã¸é›»è©±ç™ºä¿¡ã‚’è©¦ã¿ã¾ã—ãŸã€‚",
        "call_outbound_simulation_header": "ğŸ“ é›»è©±ç™ºä¿¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ",
        "call_outbound_summary_header": "ğŸ“ ç¾åœ°æ¥­è€…/é¡§å®¢ã¨ã®é€šè©±è¦ç´„",
        "call_outbound_loading": "é›»è©±æ¥ç¶šã¨é€šè©±çµæœã®æ•´ç†ä¸­... (LLMã‚³ãƒ¼ãƒ«)",
        "call_target_customer": "é¡§å®¢ã¸é›»è©±ç™ºä¿¡",
        "call_target_partner": "ç¾åœ°æ¥­è€…ã¸é›»è©±ç™ºä¿¡",

        # --- Voice ---
        "voice_rec_header": "éŸ³å£°è¨˜éŒ²ï¼†ç®¡ç†",
        "record_help": "éŒ²éŸ³ã™ã‚‹ã‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚",
        "uploaded_file": "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "rec_list_title": "ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ²",
        "transcribe_btn": "è»¢å†™ (Whisper)",
        "save_btn": "éŸ³å£°è¨˜éŒ²ã‚’ä¿å­˜",
        "transcribing": "éŸ³å£°ã‚’è»¢å†™ä¸­...",
        "transcript_result": "è»¢å†™çµæœ:",
        "transcript_text": "è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆ",
        "openai_missing": "OpenAI APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚",
        "whisper_client_error": "âŒ ã‚¨ãƒ©ãƒ¼: Whisper APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "whisper_auth_error": "âŒ Whisper APIèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ã”ç¢ºèªãã ã•ã„ã€‚",
        "whisper_format_error": "âŒ ã‚¨ãƒ©ãƒ¼: ã“ã®éŸ³å£°å½¢å¼ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
        "playback": "éŒ²éŸ³å†ç”Ÿ",
        "retranscribe": "å†è»¢å†™",
        "delete": "å‰Šé™¤",
        "no_records": "ä¿å­˜ã•ã‚ŒãŸéŸ³å£°è¨˜éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "saved_success": "ä¿å­˜ã—ã¾ã—ãŸï¼",
        "delete_confirm_rec": "ã“ã®éŸ³å£°è¨˜éŒ²ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ",
        "gcs_not_conf": "GCSãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€éŸ³å£°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "gcs_playback_fail": "éŸ³å£°ã®å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
        "gcs_no_audio": "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚",
        "error": "ã‚¨ãƒ©ãƒ¼:",
        "firestore_no_db_connect": "DBæ¥ç¶šå¤±æ•—",
        "save_history_success": "ä¿å­˜å®Œäº†ã€‚",
        "save_history_fail": "ä¿å­˜å¤±æ•—ã€‚",
        "delete_fail": "å‰Šé™¤å¤±æ•—",
        "rec_header": "éŸ³å£°å…¥åŠ›ï¼†è»¢å†™",
        "whisper_processing": "å‡¦ç†ä¸­...",
        "empty_response_warning": "å¿œç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "customer_no_more_inquiries": "ã„ã„ãˆã€çµæ§‹ã§ã™ã€‚å¤§ä¸ˆå¤«ã§ã™ã€‚æœ‰é›£ã†å¾¡åº§ã„ã¾ã—ãŸã€‚",
        "customer_has_additional_inquiries": "ã¯ã„ã€è¿½åŠ ã®å•ã„åˆã‚ã›ãŒã‚ã‚Šã¾ã™ã€‚",
        "sim_end_chat_button": "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’é€ä¿¡ã—ã¦å¿œå¯¾çµ‚äº†",
        "delete_mic_record": "éŒ²éŸ³ã‚’å‰Šé™¤ã™ã‚‹",

        # --- ì²¨ë¶€ íŒŒì¼ ê¸°ëŠ¥ ì¶”ê°€ ---
        "attachment_label": "é¡§å®¢ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãªã©)",
        "attachment_placeholder": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜ã—ã¦çŠ¶æ³ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
        "attachment_status_llm": "é¡§å®¢ãŒ **{filename}** íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒì¼ì„ ìŠ¤í¬ë¦°ìƒ·ì´ë¼ê³  ê°€ì •í•˜ê³  ì‘ëŒ€ ì´ˆì•ˆê³¼ ê°€ì´ë“œë¼ì¸ì— ë°˜ì˜í•´ì£¼ì„¸ìš”. (ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {filetype})",
        "agent_attachment_label": "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ« (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãªã©)",
        "agent_attachment_placeholder": "å¿œç­”ã«æ·»ä»˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
        "agent_attachment_status": "ğŸ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ **{filename}** ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¿œç­”ã«æ·»ä»˜ã—ã¾ã—ãŸã€‚(ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {filetype})",

        # --- RAG ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶”ê°€ ---
        "rag_embed_error_openai": "RAG embedding failed: OpenAI API Key is invalid or not set.",
        "rag_embed_error_gemini": "RAG embedding failed: Gemini API Key is invalid or not set.",
        "rag_embed_error_nvidia": "RAG embedding failed: NVIDIA API Key is invalid or not set.",
        "rag_embed_error_none": "RAG embedding failed: All required keys (OpenAI, Gemini, NVIDIA) are invalid or not set. Please configure a keyã€‚",

        # --- é›»è©±æ©Ÿèƒ½é–¢é€£è¿½åŠ  ---
        "phone_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼(é›»è©±)",
        "call_status_waiting": "ç€ä¿¡å¾…ã¡...",
        "call_status_ringing": "ç€ä¿¡ä¸­: {number}",
        "button_answer": "ğŸ“ é›»è©±ã«å‡ºã‚‹",
        "button_hangup": "ğŸ”´ é›»è©±ã‚’åˆ‡ã‚‹",
        "button_hold": "â¸ï¸ ä¿ç•™ (ãƒã‚¤ã‚ºé®æ–­)",
        "button_resume": "â–¶ï¸ é€šè©±å†é–‹",
        "hold_status": "ä¿ç•™ä¸­ (ç´¯è¨ˆä¿ç•™æ™‚é–“: {duration})",
        "cc_live_transcript": "ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ CCå­—å¹• / è»¢å†™",
        "mic_input_status": "ğŸ™ï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®éŸ³å£°å…¥åŠ›",
        "customer_audio_playback": "ğŸ—£ï¸ é¡§å®¢ã®éŸ³å£°å†ç”Ÿ",
        "agent_response_prompt": "é¡§å®¢ã¸ã®å¿œç­”ã‚’éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚",
        "call_end_message": "é€šè©±ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚AHTã¨å±¥æ­´ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "call_query_placeholder": "é¡§å®¢ã‹ã‚‰ã®æœ€åˆã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "call_number_placeholder": "+81 90-xxxx-xxxx (ä»®æƒ³ç•ªå·)",
        "call_summary_header": "AI é€šè©±è¦ç´„",
        "customer_audio_header": "é¡§å®¢ã®æœ€åˆã®å•ã„åˆã‚ã› (éŸ³å£°)",
        "aht_not_recorded": "âš ï¸ é€šè©±é–‹å§‹æ™‚é–“ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€AHTã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã€‚",
        "no_audio_record": "é¡§å®¢ã®æœ€åˆã®éŸ³å£°è¨˜éŒ²ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
    }
}

# ========================================
# 1-1. Session State ì´ˆê¸°í™” (ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì¶”ê°€)
# ========================================
if st.sidebar.button("ğŸ’£ Reset Simulator State"):
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    # st.experimental_rerun()

if "language" not in st.session_state:
    st.session_state.language = DEFAULT_LANG
if "is_llm_ready" not in st.session_state:
    st.session_state.is_llm_ready = False
if "llm_init_error_msg" not in st.session_state:
    st.session_state.llm_init_error_msg = ""
if "uploaded_files_state" not in st.session_state:
    st.session_state.uploaded_files_state = None
if "is_rag_ready" not in st.session_state:
    st.session_state.is_rag_ready = False
if "rag_vectorstore" not in st.session_state:
    st.session_state.rag_vectorstore = None
if "rag_messages" not in st.session_state:
    st.session_state.rag_messages = []
if "agent_input" not in st.session_state:
    st.session_state.agent_input = ""
if "last_audio" not in st.session_state:
    st.session_state.last_audio = None
if "simulator_messages" not in st.session_state:
    st.session_state.simulator_messages = []
if "simulator_memory" not in st.session_state:
    st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history")
if "simulator_chain" not in st.session_state:
    st.session_state.simulator_chain = None
if "initial_advice_provided" not in st.session_state:
    st.session_state.initial_advice_provided = False
if "is_chat_ended" not in st.session_state:
    st.session_state.is_chat_ended = False
if "show_delete_confirm" not in st.session_state:
    st.session_state.show_delete_confirm = False
if "customer_query_text_area" not in st.session_state:
    st.session_state.customer_query_text_area = ""
if "agent_response_area_text" not in st.session_state:
    st.session_state.agent_response_area_text = ""
if "last_transcript" not in st.session_state:
    st.session_state.last_transcript = ""
if "sim_audio_bytes" not in st.session_state:
    st.session_state.sim_audio_bytes = None
if "chat_state" not in st.session_state:
    st.session_state.chat_state = "idle"
    # idle â†’ initial_customer â†’ supervisor_advice â†’ agent_turn â†’ customer_turn â†’ closing
if "openai_client" not in st.session_state:
    st.session_state.openai_client = None
if "openai_init_msg" not in st.session_state:
    st.session_state.openai_init_msg = ""
if "sim_stage" not in st.session_state:
    st.session_state.sim_stage = "WAIT_FIRST_QUERY"
    # WAIT_FIRST_QUERY (ì´ˆê¸° ë¬¸ì˜ ì…ë ¥)
    # AGENT_TURN (ì—ì´ì „íŠ¸ ì‘ë‹µ ì…ë ¥)
    # CUSTOMER_TURN (ê³ ê° ë°˜ì‘ ìƒì„± ìš”ì²­)
    # WAIT_CLOSING_CONFIRMATION_FROM_AGENT (ê³ ê°ì´ ê°ì‚¬, ì—ì´ì „íŠ¸ê°€ ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸° ëŒ€ê¸°)
    # WAIT_CUSTOMER_CLOSING_RESPONSE (ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ë³´ëƒ„, ê³ ê°ì˜ ë§ˆì§€ë§‰ ì‘ë‹µ ëŒ€ê¸°)
    # FINAL_CLOSING_ACTION (ìµœì¢… ì¢…ë£Œ ë²„íŠ¼ ëŒ€ê¸°)
    # CLOSING (ì±„íŒ… ì¢…ë£Œ)
    # â­ ì¶”ê°€: OUTBOUND_CALL_IN_PROGRESS (ì „í™” ë°œì‹  ì§„í–‰ ì¤‘)
if "start_time" not in st.session_state:  # AHT íƒ€ì´ë¨¸ ì‹œì‘ ì‹œê°„
    st.session_state.start_time = None
if "is_solution_provided" not in st.session_state:  # ì†”ë£¨ì…˜ ì œê³µ ì—¬ë¶€ í”Œë˜ê·¸
    st.session_state.is_solution_provided = False
if "transfer_summary_text" not in st.session_state:  # ì´ê´€ ì‹œ ë²ˆì—­ëœ ìš”ì•½
    st.session_state.transfer_summary_text = ""
if "language_transfer_requested" not in st.session_state:  # ê³ ê°ì˜ ì–¸ì–´ ì´ê´€ ìš”ì²­ ì—¬ë¶€
    st.session_state.language_transfer_requested = False
if "customer_attachment_file" not in st.session_state:  # ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´
    st.session_state.customer_attachment_file = None
if "language_at_transfer" not in st.session_state:  # í˜„ì¬ ì–¸ì–´ì™€ ë¹„êµë¥¼ ìœ„í•œ ë³€ìˆ˜
    st.session_state.language_at_transfer = st.session_state.language
if "language_at_transfer_start" not in st.session_state:  # ë²ˆì—­ ì¬ì‹œë„ë¥¼ ìœ„í•œ ì›ë³¸ ì–¸ì–´
    st.session_state.language_at_transfer_start = st.session_state.language
if "customer_type_sim_select" not in st.session_state:  # FIX: Attribute Error í•´ê²°
    st.session_state.customer_type_sim_select = LANG[st.session_state.language]["customer_type_options"][1]  # ê¸°ë³¸ê°’ ì„¤ì •
if "customer_email" not in st.session_state:  # FIX: customer_email ì´ˆê¸°í™”
    st.session_state.customer_email = ""
if "customer_phone" not in st.session_state:  # FIX: customer_phone ì´ˆê¸°í™”
    st.session_state.customer_phone = ""
if "agent_response_input_box_widget" not in st.session_state:  # FIX: customer_phone ì´ˆê¸°í™”
    st.session_state.agent_response_input_box_widget = ""
if "sim_instance_id" not in st.session_state:  # FIX: DuplicateWidgetID ë°©ì§€ìš© ì¸ìŠ¤í„´ìŠ¤ ID ì´ˆê¸°í™”
    st.session_state.sim_instance_id = str(uuid.uuid4())
if "sim_attachment_context_for_llm" not in st.session_state:
    st.session_state.sim_attachment_context_for_llm = ""
if "realtime_hint_text" not in st.session_state:
    st.session_state.realtime_hint_text = ""
# â­ ì¶”ê°€: ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ
if "sim_call_outbound_summary" not in st.session_state:
    st.session_state.sim_call_outbound_summary = ""
if "sim_call_outbound_target" not in st.session_state:
    st.session_state.sim_call_outbound_target = None
# ----------------------------------------------------------------------
# â­ ì „í™” ê¸°ëŠ¥ ê´€ë ¨ ìƒíƒœ ì¶”ê°€
if "call_sim_stage" not in st.session_state:
    st.session_state.call_sim_stage = "WAITING_CALL"  # WAITING_CALL, RINGING, IN_CALL, CALL_ENDED
if "call_sim_mode" not in st.session_state:
    st.session_state.call_sim_mode = "INBOUND"  # INBOUND or OUTBOUND
if "incoming_phone_number" not in st.session_state:
    st.session_state.incoming_phone_number = "+82 10-1234-5678"
if "is_on_hold" not in st.session_state:
    st.session_state.is_on_hold = False
if "hold_start_time" not in st.session_state:
    st.session_state.hold_start_time = None
if "total_hold_duration" not in st.session_state:
    st.session_state.total_hold_duration = timedelta(0)
if "current_customer_audio_text" not in st.session_state:
    st.session_state.current_customer_audio_text = ""
if "current_agent_audio_text" not in st.session_state:
    st.session_state.current_agent_audio_text = ""
if "agent_response_input_box_widget_call" not in st.session_state:  # ì „í™” íƒ­ ì „ìš© ì…ë ¥ì°½
    st.session_state.agent_response_input_box_widget_call = ""
if "call_initial_query" not in st.session_state:  # ì „í™” íƒ­ ì „ìš© ì´ˆê¸° ë¬¸ì˜
    st.session_state.call_initial_query = ""
# â­ ì¶”ê°€: í†µí™” ìš”ì•½ ë° ì´ˆê¸° ê³ ê° ìŒì„± ì €ì¥ì†Œ
if "call_summary_text" not in st.session_state:
    st.session_state.call_summary_text = ""
if "customer_initial_audio_bytes" not in st.session_state:  # ê³ ê°ì˜ ì²« ìŒì„± (TTS ê²°ê³¼) ì €ì¥
    st.session_state.customer_initial_audio_bytes = None
if "supervisor_policy_context" not in st.session_state:
    # Supervisorê°€ ì—…ë¡œë“œí•œ ì˜ˆì™¸ ì •ì±… í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    st.session_state.supervisor_policy_context = ""
if "agent_policy_attachment_content" not in st.session_state:
    # ì—ì´ì „íŠ¸ê°€ ì—…ë¡œë“œí•œ ì •ì±… íŒŒì¼ ê°ì²´(ë˜ëŠ” ë‚´ìš©)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    st.session_state.agent_policy_attachment_content = ""
if "customer_attachment_b64" not in st.session_state:
    st.session_state.customer_attachment_b64 = ""
if "customer_history_summary" not in st.session_state:
    st.session_state.customer_history_summary = ""

L = LANG[st.session_state.language]

# â­ 2-A. Gemini í‚¤ ì´ˆê¸°í™” (ì˜ëª»ëœ í‚¤ ì”ì¡´ ë°©ì§€)
if "user_gemini_key" in st.session_state and st.session_state["user_gemini_key"].startswith("AIza"):
    pass

# ========================================
# 0. ë©€í‹° ëª¨ë¸ API Key ì•ˆì „ êµ¬ì¡° (Secrets + Env Varë§Œ ì‚¬ìš©)
# ========================================

# 1) ì§€ì›í•˜ëŠ” API ëª©ë¡ ì •ì˜
SUPPORTED_APIS = {
    "openai": {
        "label": "OpenAI API Key",
        "secret_key": "OPENAI_API_KEY",
        "session_key": "user_openai_key",
        "placeholder": "sk-proj-**************************",
    },
    "gemini": {
        "label": "Google Gemini API Key",
        "secret_key": "GEMINI_API_KEY",
        "session_key": "user_gemini_key",
        "placeholder": "AIza***********************************",
    },
    "nvidia": {
        "label": "NVIDIA NIM API Key",
        "secret_key": "NVIDIA_API_KEY",
        "session_key": "user_nvidia_key",
        "placeholder": "nvapi-**************************",
    },
    "claude": {
        "label": "Anthropic Claude API Key",
        "secret_key": "CLAUDE_API_KEY",
        "session_key": "user_claude_key",
        "placeholder": "sk-ant-api-**************************",
    },
    "groq": {
        "label": "Groq API Key",
        "secret_key": "GROQ_API_KEY",
        "session_key": "user_groq_key",
        "placeholder": "gsk_**************************",
    },
}

# 2) ì„¸ì…˜ ì´ˆê¸°í™”
for api, cfg in SUPPORTED_APIS.items():
    if cfg["session_key"] not in st.session_state:
        st.session_state[cfg["session_key"]] = ""

if "selected_llm" not in st.session_state:
    st.session_state.selected_llm = "openai_gpt4"


def get_api_key(api):
    cfg = SUPPORTED_APIS[api]

    # â­ 1. Streamlit Secrets (.streamlit/secrets.toml) - ìµœìš°ì„ 
    try:
        if hasattr(st, "secrets") and cfg["secret_key"] in st.secrets:
            return st.secrets[cfg["secret_key"]]
    except Exception:
        pass

    # 2. Environment Variable (os.environ)
    env_key = os.environ.get(cfg["secret_key"])
    if env_key:
        return env_key

    # 3. User Input (Session State - ì œê±°ë¨)
    user_key = st.session_state.get(cfg["session_key"], "")
    if user_key:
        return user_key

    return ""


# ========================================
# 1. Sidebar UI: API Key ì…ë ¥ ì œê±°
# ========================================
# API Key ì…ë ¥ UIëŠ” ì œê±°í•˜ê³ , í™˜ê²½ë³€ìˆ˜ì™€ Streamlit Secretsë§Œ ì‚¬ìš©í•˜ë„ë¡ í•¨.


# ========================================
# 2. LLM í´ë¼ì´ì–¸íŠ¸ ë¼ìš°íŒ… & ì‹¤í–‰
# ========================================
def get_llm_client():
    """ì„ íƒëœ ëª¨ë¸ì— ë§ëŠ” í´ë¼ì´ì–¸íŠ¸ + ëª¨ë¸ì½”ë“œ ë°˜í™˜"""
    model_key = st.session_state.get("selected_llm", "openai_gpt4")

    # --- OpenAI ---
    if model_key.startswith("openai"):
        key = get_api_key("openai")
        if not key: return None, None
        try:
            client = OpenAI(api_key=key)
            model_name = "gpt-4o" if model_key == "openai_gpt4" else "gpt-3.5-turbo"
            return client, ("openai", model_name)
        except Exception:
            return None, None

    # --- Gemini ---
    if model_key.startswith("gemini"):
        key = get_api_key("gemini")
        if not key: return None, None
        try:
            genai.configure(api_key=key)
            model_name = "gemini-2.5-pro" if model_key == "gemini_pro" else "gemini-2.5-flash"
            return genai, ("gemini", model_name)
        except Exception:
            return None, None

    # --- Claude ---
    if model_key.startswith("claude"):
        key = get_api_key("claude")
        if not key: return None, None
        try:
            client = Anthropic(api_key=key)
            model_name = "claude-3-5-sonnet-latest"
            return client, ("claude", model_name)
        except Exception:
            return None, None

    # --- Groq ---
    if model_key.startswith("groq"):
        from groq import Groq
        key = get_api_key("groq")
        if not key: return None, None
        try:
            client = Groq(api_key=key)
            model_name = (
                "llama3-70b-8192"
                if "llama3" in model_key
                else "mixtral-8x7b-32768"
            )
            return client, ("groq", model_name)
        except Exception:
            return None, None

    return None, None


def run_llm(prompt: str) -> str:
    """ì„ íƒëœ LLMìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (Gemini ìš°ì„ ìˆœìœ„ ë³€ê²½ ì ìš©)"""
    client, info = get_llm_client()

    # Note: infoëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒëœ ì£¼ë ¥ ëª¨ë¸ì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    provider, model_name = info if info else (None, None)

    # Fallback ìˆœì„œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. (Gemini ìš°ì„ )
    llm_attempts = []

    # 1. Geminië¥¼ ìµœìš°ì„  Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    gemini_key = get_api_key("gemini")
    if gemini_key:
        llm_attempts.append(("gemini", gemini_key, "gemini-2.5-pro" if "pro" in model_name else "gemini-2.5-flash"))

    # 2. OpenAIë¥¼ 2ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    openai_key = get_api_key("openai")
    if openai_key:
        llm_attempts.append(("openai", openai_key, "gpt-4o" if "4" in model_name else "gpt-3.5-turbo"))

    # 3. Claudeë¥¼ 3ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    claude_key = get_api_key("claude")
    if claude_key:
        llm_attempts.append(("claude", claude_key, "claude-3-5-sonnet-latest"))

    # 4. Groqë¥¼ 4ìˆœìœ„ Fallbackìœ¼ë¡œ ì‹œë„ (Keys í™•ì¸)
    groq_key = get_api_key("groq")
    if groq_key:
        groq_model = "llama3-70b-8192" if "llama3" in model_name else "mixtral-8x7b-32768"
        llm_attempts.append(("groq", groq_key, groq_model))

    # â­ ìˆœì„œ ì¡°ì •: ì£¼ë ¥ ëª¨ë¸(ì‚¬ìš©ìê°€ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•œ ëª¨ë¸)ì„ ê°€ì¥ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.
    # ë§Œì•½ ì£¼ë ¥ ëª¨ë¸ì´ Fallback ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, ê·¸ ëª¨ë¸ì„ ì²« ìˆœì„œë¡œ ì˜¬ë¦½ë‹ˆë‹¤.
    if provider and provider in [attempt[0] for attempt in llm_attempts]:
        # ì£¼ë ¥ ëª¨ë¸ì„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ì•„ ì œê±°
        primary_attempt = next((attempt for attempt in llm_attempts if attempt[0] == provider), None)
        if primary_attempt:
            llm_attempts.remove(primary_attempt)
            # ì£¼ë ¥ ëª¨ë¸ì´ Geminië‚˜ OpenAIê°€ ì•„ë‹ˆë¼ë©´, Fallback ìˆœì„œì™€ ê´€ê³„ì—†ì´ ê°€ì¥ ë¨¼ì € ì‹œë„í•˜ë„ë¡ ì‚½ì…
            llm_attempts.insert(0, primary_attempt)

    # LLM ìˆœì°¨ ì‹¤í–‰
    for provider, key, model in llm_attempts:
        if not key: continue

        try:
            if provider == "gemini":
                genai.configure(api_key=key)
                gen_model = genai.GenerativeModel(model)
                resp = gen_model.generate_content(prompt)
                return resp.text

            elif provider == "openai":
                o_client = OpenAI(api_key=key)
                resp = o_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.choices[0].message.content

            elif provider == "claude":
                c_client = Anthropic(api_key=key)
                resp = c_client.messages.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.content[0].text

            elif provider == "groq":
                from groq import Groq
                g_client = Groq(api_key=key)
                resp = g_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                )
                return resp.choices[0].message.content

        except Exception as e:
            # í•´ë‹¹ APIê°€ ì‹¤íŒ¨í•˜ë©´ ë‹¤ìŒ APIë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
            print(f"LLM {provider} ({model}) failed: {e}")
            continue

    # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ
    return "âŒ ëª¨ë“  LLM API í‚¤ê°€ ì‘ë™í•˜ì§€ ì•Šê±°ë‚˜ í• ë‹¹ëŸ‰ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤."


# ========================================
# 2-A. Whisper / TTS ìš© OpenAI Client ë³„ë„ë¡œ ì´ˆê¸°í™”
# ========================================

def init_openai_audio_client():
    key = get_api_key("openai")
    if not key:
        return None
    try:
        return OpenAI(api_key=key)
    except:
        return None


# â­ ìµœì í™”: LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìºì‹± (ë§¤ë²ˆ ì¬ìƒì„±í•˜ì§€ ì•Šë„ë¡)
# OpenAI í´ë¼ì´ì–¸íŠ¸ ìºì‹±
if "openai_client" not in st.session_state or st.session_state.openai_client is None:
    st.session_state.openai_client = init_openai_audio_client()

# LLM ì¤€ë¹„ ìƒíƒœ ìºì‹± (API í‚¤ ë³€ê²½ ì‹œì—ë§Œ ì¬í™•ì¸)
if "is_llm_ready" not in st.session_state or "llm_ready_checked" not in st.session_state:
    probe_client, _ = get_llm_client()
    st.session_state.is_llm_ready = probe_client is not None
    st.session_state.llm_ready_checked = True

# API í‚¤ ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•œ í•´ì‹œ ì²´í¬
current_api_keys_hash = hashlib.md5(
    f"{get_api_key('openai')}{get_api_key('gemini')}{get_api_key('claude')}{get_api_key('groq')}".encode()
).hexdigest()

if "api_keys_hash" not in st.session_state:
    st.session_state.api_keys_hash = current_api_keys_hash
elif st.session_state.api_keys_hash != current_api_keys_hash:
    # API í‚¤ê°€ ë³€ê²½ëœ ê²½ìš°ë§Œ ì¬í™•ì¸
    probe_client, _ = get_llm_client()
    st.session_state.is_llm_ready = probe_client is not None
    st.session_state.api_keys_hash = current_api_keys_hash
    # OpenAI í´ë¼ì´ì–¸íŠ¸ë„ ì¬ì´ˆê¸°í™”
    st.session_state.openai_client = init_openai_audio_client()

if st.session_state.openai_client:
    # í‚¤ë¥¼ ì°¾ì•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ëŠ” ìƒì„±ë˜ì—ˆìœ¼ë‚˜, ì‹¤ì œ ì¸ì¦ì€ API í˜¸ì¶œ ì‹œ ì´ë£¨ì–´ì§ (401 ì˜¤ë¥˜ëŠ” ì—¬ê¸°ì„œ ë°œìƒ)
    st.session_state.openai_init_msg = "âœ… OpenAI TTS/Whisper í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ (Key í™•ì¸ë¨)"
else:
    # í‚¤ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
    st.session_state.openai_init_msg = L["openai_missing"]

if not st.session_state.is_llm_ready:
    st.session_state.llm_init_error_msg = L["simulation_no_key_warning"]
else:
    st.session_state.llm_init_error_msg = ""


# ----------------------------------------
# LLM ë²ˆì—­ í•¨ìˆ˜ (Gemini í´ë¼ì´ì–¸íŠ¸ ì˜ì¡´ì„± ì œê±° ë° ê°•í™”)
# ----------------------------------------
def translate_text_with_llm(text_content: str, target_lang_code: str, source_lang_code: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. (ì•ˆì •í™”ëœ í…ìŠ¤íŠ¸ ì¶œë ¥)
    """
    target_lang = LANG.get(target_lang_code, {})
    target_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(target_lang_code, "English")
    source_lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(source_lang_code, "English")

    # ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ ë²ˆì—­ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì œ
    system_prompt = (
        f"You are a professional translation AI. Translate the entire following customer support chat history "
        f"from '{source_lang_name}' to '{target_lang_name}'. "
        f"You MUST translate the content to {target_lang_name} ONLY. "
        f"Do not include any mixed languages, the source text, or any introductory/concluding remarks. "
        f"Output ONLY the translated chat history text. "
    )
    prompt = f"Original Chat History:\n\n{text_content}"

    # LLM Fallback ìˆœì„œ: OpenAI (ê°€ì¥ ì•ˆì •ì ) -> Gemini -> Claude
    llm_attempts = [
        ("openai", get_api_key("openai"), "gpt-4o"),
        ("gemini", get_api_key("gemini"), "gemini-2.5-flash"),
        ("claude", get_api_key("claude"), "claude-3-5-sonnet-latest"),
    ]

    for provider, key, model_name in llm_attempts:
        if key:
            try:
                # 1. LLM í˜¸ì¶œ
                if provider == "gemini":
                    genai.configure(api_key=key)
                    gen_model = genai.GenerativeModel(model_name)
                    response = gen_model.generate_content(
                        contents=system_prompt,  # system_promptë¥¼ user contentë¡œ ì‚¬ìš©
                    )
                    return response.text.strip()

                if provider == "openai":
                    o_client = OpenAI(api_key=key)
                    resp = o_client.chat.completions.create(
                        model=model_name,
                        messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}],
                        temperature=0.1
                    )
                    return resp.choices[0].message.content.strip()

                elif provider == "claude":
                    from anthropic import Anthropic
                    c_client = Anthropic(api_key=key)
                    resp = c_client.messages.create(
                        model=model_name,
                        messages=[{"role": "user", "content": prompt}],
                        system=system_prompt
                    )
                    return resp.content[0].text.strip()

            except Exception as e:
                print(f"Translation API call failed with {provider}: {e}")
                continue

                # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (UI ì˜¤ë¥˜ ë°©ì§€)
    return ""


# ----------------------------------------
# Realtime Hint Generation (ìš”ì²­ 2 ë°˜ì˜)
# ----------------------------------------
def generate_realtime_hint(current_lang_key: str, is_call: bool = False):
    """í˜„ì¬ ëŒ€í™” ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ì—ê²Œ ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸(í‚¤ì›Œë“œ/ì •ì±…/ì•¡ì…˜)ë¥¼ ì œê³µ"""
    L = LANG[current_lang_key]
    # ì±„íŒ…/ì „í™” êµ¬ë¶„í•˜ì—¬ ì´ë ¥ ì‚¬ìš©
    if is_call:
        # ì „í™” ì‹œë®¬ë ˆì´í„°ì—ì„œëŠ” í˜„ì¬ CC ì˜ì—­ì— í‘œì‹œëœ í…ìŠ¤íŠ¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ í•¨ê»˜ ì‚¬ìš©
        history_text = (
            f"Initial Query: {st.session_state.call_initial_query}\n"
            f"Previous Customer Utterance: {st.session_state.current_customer_audio_text}\n"
            f"Previous Agent Utterance: {st.session_state.current_agent_audio_text}"
        )
    else:
        history_text = get_chat_history_for_prompt(include_attachment=True)

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    hint_prompt = f"""
You are an AI Supervisor providing an **urgent, internal hint** to a human agent whose AHT is being monitored.
Analyze the conversation history, especially the customer's last message, which might be about complex issues like JR Pass, Universal Studio Japan (USJ), or a complex refund policy.

Provide ONE concise, actionable hint for the agent. The purpose is to save AHT time.

Output MUST be a single paragraph/sentence in {lang_name} containing actionable advice.
DO NOT use markdown headers or titles.
Do NOT direct the agent to check the general website.
Provide an actionable fact or the next specific step (e.g., check policy section, confirm coverage).

Examples of good hints (based on the content):
- Check the official JR Pass site for current exchange rates.
- The 'Universal Express Pass' is non-refundable; clearly cite policy section 3.2.
- Ask for the order confirmation number before proceeding with any action.
- The solution lies in the section of the Klook site titled '~'.

Conversation History:
{history_text}

HINT:
"""
    if not st.session_state.is_llm_ready:
        return "(Mock Hint: LLM Key is missing. Ask the customer for the booking number.)"

    with st.spinner(f"ğŸ’¡ {L['button_request_hint']}..."):
        try:
            return run_llm(hint_prompt).strip()
        except Exception as e:
            return f"âŒ Hint Generation Error. (Try again or check API Key: {e})"


def generate_agent_response_draft(current_lang_key: str) -> str:
    """ê³ ê° ì‘ë‹µì„ ê¸°ë°˜ìœ¼ë¡œ AIê°€ ì—ì´ì „íŠ¸ ì‘ë‹µ ì´ˆì•ˆì„ ìƒì„± (ìš”ì²­ 1 ë°˜ì˜)"""
    L = LANG[current_lang_key]
    history_text = get_chat_history_for_prompt(include_attachment=True)
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"\n[ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´: {attachment_context}]\n"
    else:
        attachment_context = ""

    # ê³ ê° ìœ í˜• ë° ë°˜ë³µ ë¶ˆë§Œ íŒ¨í„´ ë¶„ì„
    customer_type = st.session_state.get('customer_type_sim_select', 'ì¼ë°˜ì ì¸ ë¬¸ì˜')
    is_difficult_customer = customer_type in ["ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°", "Difficult Customer",
                                              "Highly Dissatisfied Customer", "é›£ã—ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"]

    # ê³ ê° ë©”ì‹œì§€ ìˆ˜ ë° ê°ì • ë¶„ì„
    customer_message_count = sum(
        1 for msg in st.session_state.simulator_messages if msg.get("role") in ["customer", "customer_rebuttal"])
    agent_message_count = sum(1 for msg in st.session_state.simulator_messages if msg.get("role") == "agent_response")

    # ê³ ê°ì´ ê³„ì† ë”°ì§€ê±°ë‚˜ í™”ë‚´ëŠ” íŒ¨í„´ ê°ì§€ (ê³ ê° ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ë³´ë‹¤ ë§ê±°ë‚˜, ë°˜ë³µì ì¸ ë¶ˆë§Œ í‘œí˜„)
    is_repeating_complaints = False
    if customer_message_count > agent_message_count and customer_message_count >= 2:
        # ë§ˆì§€ë§‰ 2ê°œ ê³ ê° ë©”ì‹œì§€ ë¶„ì„
        recent_customer_messages = [msg["content"].lower() for msg in st.session_state.simulator_messages if
                                    msg.get("role") in ["customer", "customer_rebuttal"]][-2:]
        complaint_keywords = ["ì™œ", "ì´ìœ ", "ì„¤ëª…", "ë§ì´ ì•ˆ", "ì´í•´ê°€ ì•ˆ", "í™”ë‚˜", "ì§œì¦", "ë¶ˆë§Œ", "ì™œ", "why", "reason", "explain",
                              "angry", "frustrated", "complaint", "ãªãœ", "ç†ç”±", "èª¬æ˜", "æ€’ã‚Š", "ä¸æº€"]
        if any(any(keyword in msg for keyword in complaint_keywords) for msg in recent_customer_messages):
            is_repeating_complaints = True

    # ëŒ€ì²˜ë²• í¬ë©”ì´ì…˜ ì¶”ê°€ ì—¬ë¶€ ê²°ì •
    needs_coping_strategy = is_difficult_customer or (is_repeating_complaints and customer_message_count >= 2)

    # ëŒ€ì²˜ë²• ê°€ì´ë“œë¼ì¸ ìƒì„±
    coping_guidance = ""
    if needs_coping_strategy:
        coping_guidance = f"""

[CRITICAL: Handling Difficult Customer Situation]
The customer type is "{customer_type}" and the customer has sent {customer_message_count} messages while the agent has sent {agent_message_count} messages.
The customer may be showing signs of continued frustration or dissatisfaction.

**INCLUDE THE FOLLOWING COPING STRATEGY FORMAT IN YOUR RESPONSE:**

1. **Immediate Acknowledgment** (1-2 sentences):
   - Acknowledge their frustration/specific concern explicitly
   - Show deep empathy and understanding
   - Example formats:
     * "{'ì£„ì†¡í•©ë‹ˆë‹¤. ë¶ˆí¸ì„ ë“œë ¤ ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I sincerely apologize for the inconvenience. I fully understand your situation and frustration.' if current_lang_key == 'en' else 'å¤§å¤‰ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãŠå®¢æ§˜ã®çŠ¶æ³ã¨ã”ä¸ä¾¿ã‚’ååˆ†ã«ç†è§£ã—ã¦ãŠã‚Šã¾ã™ã€‚')}"
     * "{'ê³ ê°ë‹˜ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ì˜ ë“£ê³  ìˆìŠµë‹ˆë‹¤. ì •ë§ ë‹µë‹µí•˜ì…¨ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I hear your concerns clearly. This must have been very frustrating for you.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã”æ„è¦‹ã‚’ã—ã£ã‹ã‚Šã¨å—ã‘æ­¢ã‚ã¦ã„ã¾ã™ã€‚æœ¬å½“ã«ãŠå›°ã‚Šã ã£ãŸã¨æ€ã„ã¾ã™ã€‚')}"

2. **Specific Solution Recap** (2-3 sentences):
   - Clearly restate the solution/step provided previously (if any)
   - Offer a NEW concrete action or alternative solution
   - Be specific and actionable
   - Example formats:
     * "{'ì•ì„œ ì•ˆë‚´ë“œë¦° [êµ¬ì²´ì  í•´ê²°ì±…] ì™¸ì—ë„, [ìƒˆë¡œìš´ ëŒ€ì•ˆ/ì¶”ê°€ ì¡°ì¹˜]ë¥¼ ì§„í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('In addition to the [specific solution] I mentioned earlier, I can also [new alternative/additional action] for you.' if current_lang_key == 'en' else 'å…ˆã»ã©ã”æ¡ˆå†…ã—ãŸ[å…·ä½“çš„è§£æ±ºç­–]ã«åŠ ãˆã¦ã€[æ–°ã—ã„ä»£æ›¿æ¡ˆ/è¿½åŠ æªç½®]ã‚‚é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚')}"
     * "{'í˜¹ì‹œ [êµ¬ì²´ì  ë¬¸ì œì ] ë•Œë¬¸ì— ë¶ˆí¸í•˜ì…¨ë‹¤ë©´, [êµ¬ì²´ì  í•´ê²° ë°©ë²•]ì„ ë°”ë¡œ ì§„í–‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If you are experiencing [specific issue], I can immediately proceed with [specific solution].' if current_lang_key == 'en' else 'ã‚‚ã—[å…·ä½“çš„å•é¡Œ]ã§ã”ä¸ä¾¿ã§ã—ãŸã‚‰ã€[å…·ä½“çš„è§£æ±ºæ–¹æ³•]ã‚’ã™ãã«é€²ã‚ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"

3. **Escalation or Follow-up Offer** (1-2 sentences):
   - Offer to escalate to supervisor/higher level support
   - Promise immediate follow-up within specific time
   - Example formats:
     * "{'ë§Œì•½ ì—¬ì „íˆ ë¶ˆë§Œì´ í•´ì†Œë˜ì§€ ì•Šìœ¼ì‹ ë‹¤ë©´, ì¦‰ì‹œ ìƒê¸‰ ê´€ë¦¬ìì—ê²Œ ì´ê´€í•˜ì—¬ ë” ë‚˜ì€ í•´ê²°ì±…ì„ ì°¾ì•„ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('If your concern is still not resolved, I can immediately escalate this to a supervisor to find a better solution.' if current_lang_key == 'en' else 'ã‚‚ã—ã”ä¸æº€ãŒè§£æ¶ˆã•ã‚Œãªã„å ´åˆã¯ã€ã™ãã«ä¸Šç´šç®¡ç†è€…ã«ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ãƒˆã—ã¦ã€ã‚ˆã‚Šè‰¯ã„è§£æ±ºç­–ã‚’è¦‹ã¤ã‘ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚')}"
     * "{'24ì‹œê°„ ì´ë‚´ì— [êµ¬ì²´ì  ì¡°ì¹˜/ê²°ê³¼]ë¥¼ í™•ì¸í•˜ì—¬ ê³ ê°ë‹˜ê»˜ ë‹¤ì‹œ ì—°ë½ë“œë¦¬ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will follow up with you within 24 hours regarding [specific action/result].' if current_lang_key == 'en' else '24æ™‚é–“ä»¥å†…ã«[å…·ä½“çš„æªç½®/çµæœ]ã‚’ç¢ºèªã—ã€ãŠå®¢æ§˜ã«å†åº¦ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚')}"

4. **Closing with Assurance** (1 sentence):
   - Reassure that their concern is being taken seriously
   - Example formats:
     * "{'ê³ ê°ë‹˜ì˜ ëª¨ë“  ë¬¸ì˜ì‚¬í•­ì„ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.' if current_lang_key == 'ko' else ('I will prioritize resolving all of your concerns.' if current_lang_key == 'en' else 'ãŠå®¢æ§˜ã®ã™ã¹ã¦ã®ã”è³ªå•ã‚’æœ€å„ªå…ˆã§å‡¦ç†ã„ãŸã—ã¾ã™ã€‚')}"

**IMPORTANT NOTES:**
- DO NOT repeat the exact same solution that was already provided
- DO NOT sound dismissive or automated
- DO sound genuinely concerned and willing to go the extra mile
- If policy restrictions exist, acknowledge them but still offer alternatives
- Use warm, respectful tone while being firm about what can/cannot be done

**RESPONSE STRUCTURE:**
[Immediate Acknowledgment]
[Specific Solution Recap + New Action]
[Escalation/Follow-up Offer]
[Closing with Assurance]

Now generate the agent's response draft following this structure:
"""

    draft_prompt = f"""
You are an AI assistant helping a customer support agent write a professional response.

Based on the conversation history below, generate a draft response that the agent can review and modify before sending.

Requirements:
1. The response MUST be in {lang_name}
2. Be professional, empathetic, and solution-oriented
3. Address the customer's latest inquiry or concern
4. If the customer asked a question, provide a clear answer
5. If the customer expressed dissatisfaction, show empathy and offer solutions
6. Keep the tone appropriate for the customer type: {customer_type}
7. Do NOT include any markdown formatting, just plain text
8. {f'**FOLLOW THE COPING STRATEGY FORMAT BELOW**' if needs_coping_strategy else 'Use natural, conversational flow'}

Conversation History:
{history_text}
{attachment_context}
{coping_guidance if needs_coping_strategy else ''}

Generate the agent's response draft:
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        draft = run_llm(draft_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if draft.startswith("```"):
            lines = draft.split("\n")
            draft = "\n".join(lines[1:-1]) if len(lines) > 2 else draft
        return draft
    except Exception as e:
        return f"âŒ ì‘ë‹µ ì´ˆì•ˆ ìƒì„± ì˜¤ë¥˜: {e}"


# â­ ìƒˆë¡œìš´ í•¨ìˆ˜: ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ ìš”ì•½ ìƒì„±
def generate_outbound_call_summary(customer_query: str, current_lang_key: str, target: str) -> str:
    """
    Simulates an outbound call to a local partner or customer and generates a summary of the outcome.
    """
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # Get the current chat history for context
    history_text = get_chat_history_for_prompt(include_attachment=True)
    if not history_text:
        history_text = f"Initial Customer Query: {customer_query}"

    # Policy context (from supervisor) should be included to guide the outcome
    policy_context = st.session_state.supervisor_policy_context or ""

    summary_prompt = f"""
You are an AI simulating a quick, high-stakes phone call placed by the customer support agent to a '{target}' (either a local partner/vendor or the customer).

The purpose of the call is to resolve a complex, policy-restricted issue (like an exceptional refund for a non-refundable item, or urgent confirmation of an airport transfer change).

Analyze the conversation history, the initial query, and any provided supervisor policy.
Generate a concise summary of the OUTCOME of this simulated phone call.
The summary MUST be professional and strictly in {lang_name}.

[CRITICAL RULE]: For non-refundable items (e.g., Universal Studio Express Pass, non-refundable hotel/transfer), the local partner should only grant an exception IF the customer has provided strong, unavoidable proof (like a flight cancellation notice, doctor's note, or natural disaster notice). If no such proof is evident in the chat history, the outcome should usually be a denial or a request for more proof, but keep the tone professional.
If the customer's query is about Airport Transfer change, the outcome should be: 'Confirmation complete. Change is approved/denied based on partner policy.'

Conversation History:
{history_text}

Supervisor Policy Context (If any):
{policy_context}

Target of Call: {target}

Generate the phone call summary (Outcome ONLY):
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key missing. (Simulated Outcome: The {target} requested the agent to send proof via email.)"

    try:
        summary = run_llm(summary_prompt).strip()
        # ë§ˆí¬ë‹¤ìš´ ì œê±° (``` ë“±)
        if summary.startswith("```"):
            lines = summary.split("\n")
            summary = "\n".join(lines[1:-1]) if len(lines) > 2 else summary
        return summary
    except Exception as e:
        return f"âŒ Phone call simulation error: {e}"


# ========================================
# 3. Whisper / TTS Helper
# ========================================

def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = "audio/webm", lang_code: str = "ko") -> str:
    """
    OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•©ë‹ˆë‹¤.
    """
    L = LANG[st.session_state.language]
    client = st.session_state.openai_client
    if client is None:
        return f"âŒ {L['openai_missing']}"

    whisper_lang = {"ko": "ko", "en": "en", "ja": "ja"}.get(lang_code, "en")

    # ì„ì‹œ íŒŒì¼ ì €ì¥ (Whisper API í˜¸í™˜ì„±)
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp.write(audio_bytes)
    tmp.flush()
    tmp.close()

    try:
        with open(tmp.name, "rb") as f:
            res = client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                response_format="text",
                language=whisper_lang,
            )
        # res.text ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ res ìì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        return res.text.strip() if hasattr(res, 'text') else str(res).strip()
    except Exception as e:
        # íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜ ë“± ìƒì„¸ ì˜¤ë¥˜ ì²˜ë¦¬
        return f"âŒ {L['error']} Whisper: {e}"
    finally:
        try:
            os.remove(tmp.name)
        except OSError:
            pass


def transcribe_audio(audio_bytes, filename="audio.wav"):
    client = st.session_state.openai_client

    # 1ï¸âƒ£ OpenAI Whisper ì‹œë„
    if client:
        try:
            import io
            bio = io.BytesIO(audio_bytes)
            bio.name = filename
            resp = client.audio.transcriptions.create(
                model="whisper-1",
                file=bio,
            )
            return resp.text
        except Exception as e:
            print("Whisper OpenAI failed:", e)

    # 2ï¸âƒ£ Gemini STT fallback
    try:
        genai.configure(api_key=get_api_key("gemini"))
        model = genai.GenerativeModel("gemini-2.5-flash")
        text = model.generate_content("Transcribe this audio:").text
        return text or ""
    except Exception as e:
        print("Gemini STT failed:", e)

    return "âŒ STT not available"


# ì—­í• ë³„ TTS ìŒì„± ìŠ¤íƒ€ì¼ ì„¤ì •
TTS_VOICES = {
    "customer": {
        "gender": "male",
        "voice": "alloy"  # Distinct Male, Generic/Customer
    },
    "agent": {
        "gender": "female",
        "voice": "shimmer"  # Distinct Female, Professional/Agent
    },
    "supervisor": {
        "gender": "female",
        "voice": "nova"  # Another Distinct Female, Informative/Supervisor
    }
}


def synthesize_tts(text: str, lang_key: str, role: str = "agent"):
    L = LANG[lang_key]
    client = st.session_state.openai_client
    if client is None:
        return None, L["openai_missing"]

    if role not in TTS_VOICES:
        role = "agent"

    voice_name = TTS_VOICES[role]["voice"]

    try:
        # tts-1 ëª¨ë¸ ì‚¬ìš© (ì•ˆì •ì„±)
        resp = client.audio.speech.create(
            model="tts-1",
            voice=voice_name,
            input=text
            # format="mp3"ì€ ê¸°ë³¸ê°’ì…ë‹ˆë‹¤.
        )
        return resp.read(), L["tts_status_success"]

    except Exception as e:
        return None, f"{L['tts_status_error']}: {e}"


def render_tts_button(text, lang_key, role="customer", prefix="", index: int = -1):
    """
    TTS ì¬ìƒ ë²„íŠ¼ì„ ë Œë”ë§í•˜ê³ , ê³ ìœ í•œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    index: ëŒ€í™” ë‚´ì—­ì—ì„œì˜ ê³ ìœ  ì¸ë±ìŠ¤ (DuplicateWidgetID ë°©ì§€ìš©)
    """
    L = LANG[lang_key]

    # í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ê³¼ ê³ ìœ  ì¸ë±ìŠ¤ë¥¼ ê²°í•©í•˜ì—¬ í‚¤ ìƒì„±
    # ì¸ë±ìŠ¤(-1ì€ í‚¤ê°€ ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš°, ì˜ˆ: ìŒì„± ê¸°ë¡ ëª©ë¡)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
    content_hash = hashlib.md5(text[:100].encode()).hexdigest()
    safe_key = f"{prefix}_{index}_{content_hash}"

    # ì¬ìƒ ë²„íŠ¼ì„ ëˆ„ë¥¼ ë•Œë§Œ TTS ìš”ì²­
    if st.button(L["button_listen_audio"], key=safe_key):
        with st.spinner(L["tts_status_generating"]):
            # ê°ì • ë¶„ì„ (í˜„ì¬ ë¯¸ì‚¬ìš©) ëŒ€ì‹  ë‹¨ìˆœ í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
            audio_bytes, msg = synthesize_tts(text, lang_key, role=role)
            if audio_bytes:
                st.audio(audio_bytes, format="audio/mp3")
                st.success(msg)
            else:
                st.error(msg)


# ========================================
# 4. ë¡œì»¬ ìŒì„± ê¸°ë¡ Helper
# ========================================

def load_voice_records() -> List[Dict[str, Any]]:
    return _load_json(VOICE_META_FILE, [])


def save_voice_records(records: List[Dict[str, Any]]):
    _save_json(VOICE_META_FILE, records)


def save_audio_record_local(
        audio_bytes: bytes,
        filename: str,
        transcript_text: str,
        mime_type: str = "audio/webm",
        meta: Dict[str, Any] = None,
) -> str:
    records = load_voice_records()
    rec_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    ext = filename.split(".")[-1] if "." in filename else "webm"
    audio_filename = f"{rec_id}.{ext}"
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "wb") as f:
        f.write(audio_bytes)

    rec = {
        "id": rec_id,
        "created_at": ts,
        "filename": filename,
        "audio_filename": audio_filename,
        "size": len(audio_bytes),
        "transcript": transcript_text,
        "mime_type": mime_type,
        "language": st.session_state.language,
        "meta": meta or {},
    }
    records.insert(0, rec)
    save_voice_records(records)
    return rec_id


def delete_audio_record_local(rec_id: str) -> bool:
    records = load_voice_records()
    idx = next((i for i, r in enumerate(records) if r.get("id") == rec_id), None)
    if idx is None:
        return False
    rec = records.pop(idx)
    audio_filename = rec.get("audio_filename")
    if audio_filename:
        audio_path = os.path.join(AUDIO_DIR, audio_filename)
        try:
            os.remove(audio_path)
        except FileNotFoundError:
            pass
    save_voice_records(records)
    return True


def get_audio_bytes_local(rec_id: str):
    records = load_voice_records()
    rec = next((r for r in records if r.get("id") == rec_id), None)
    if not rec:
        raise FileNotFoundError("record not found")
    audio_filename = rec["audio_filename"]
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "rb") as f:
        b = f.read()
    return b, rec


# ========================================
# 5. ë¡œì»¬ ì‹œë®¬ë ˆì´ì…˜ ì´ë ¥ Helper (ìš”ì²­ 4 ë°˜ì˜)
# ========================================

def load_simulation_histories_local(lang_key: str) -> List[Dict[str, Any]]:
    histories = _load_json(SIM_META_FILE, [])
    # í˜„ì¬ ì–¸ì–´ì™€ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ê°€ ìœ íš¨í•œ ì´ë ¥ë§Œ í•„í„°ë§
    return [
        h for h in histories
        if h.get("language_key") == lang_key and (isinstance(h.get("messages"), list) or h.get("summary"))
    ]


def generate_chat_summary(messages: List[Dict[str, Any]], initial_query: str, customer_type: str,
                          current_lang_key: str) -> Dict[str, Any]:
    """ì±„íŒ… ë‚´ìš©ì„ AIë¡œ ìš”ì•½í•˜ì—¬ ì£¼ìš” ì •ë³´ì™€ ì ìˆ˜ë¥¼ ì¶”ì¶œ (ìš”ì²­ 4)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ëŒ€í™” ë‚´ìš© ì¶”ì¶œ
    conversation_text = f"Initial Query: {initial_query}\n\n"
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role in ["customer", "customer_rebuttal"]:
            conversation_text += f"Customer: {content}\n"
        elif role == "agent_response":
            conversation_text += f"Agent: {content}\n"

    summary_prompt = f"""
You are an AI analyst summarizing a customer support conversation.

Analyze the conversation and provide a structured summary in JSON format (ONLY JSON, no markdown).

Extract and score:
1. Main inquiry topic (what the customer asked about)
2. Key responses provided by the agent (list of max 3 core actions/solutions)
3. Customer sentiment score (0-100, where 0=very negative, 50=neutral, 100=very positive)
4. Customer satisfaction score (0-100, based on final response)
5. Customer characteristics:
   - Language preference (if mentioned)
   - Cultural background hints (if any)
   - Location/region (if mentioned, but anonymize specific addresses)
   - Communication style (formal/casual, brief/detailed)
6. Privacy-sensitive information (anonymize: names, emails, phone numbers, specific addresses)
   - Extract patterns only (e.g., "email provided", "phone number provided", "resides in Asia region")

Output format (JSON only):
{{
  "main_inquiry": "brief description of main issue",
  "key_responses": ["response 1", "response 2"],
  "customer_sentiment_score": 75,
  "customer_satisfaction_score": 80,
  "customer_characteristics": {{
    "language": "ko/en/ja or unknown",
    "cultural_hints": "brief description or unknown",
    "region": "general region or unknown",
    "communication_style": "formal/casual/brief/detailed"
  }},
  "privacy_info": {{
    "has_email": true/false,
    "has_phone": true/false,
    "has_address": true/false,
    "region_hint": "general region or unknown"
  }},
  "summary": "overall conversation summary in {lang_name}"
}}

Conversation:
{conversation_text}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        # Fallback summary
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Customer inquiry about: {initial_query[:100]}"
        }

    try:
        summary_text = run_llm(summary_prompt).strip()
        # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°)
        if "```json" in summary_text:
            summary_text = summary_text.split("```json")[1].split("```")[0].strip()
        elif "```" in summary_text:
            summary_text = summary_text.split("```")[1].split("```")[0].strip()

        import json
        summary_data = json.loads(summary_text)
        return summary_data
    except Exception as e:
        # Fallback on error
        return {
            "main_inquiry": initial_query[:100],
            "key_responses": [],
            "customer_sentiment_score": 50,
            "customer_satisfaction_score": 50,
            "customer_characteristics": {
                "language": current_lang_key,
                "cultural_hints": "unknown",
                "region": "unknown",
                "communication_style": "unknown"
            },
            "privacy_info": {
                "has_email": False,
                "has_phone": False,
                "has_address": False,
                "region_hint": "unknown"
            },
            "summary": f"Error generating summary: {str(e)}"
        }


def save_simulation_history_local(initial_query: str, customer_type: str, messages: List[Dict[str, Any]],
                                  is_chat_ended: bool, attachment_context: str, is_call: bool = False):
    """AI ìš”ì•½ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì´ë ¥ì„ ì €ì¥ (ìš”ì²­ 4 ë°˜ì˜)"""
    histories = _load_json(SIM_META_FILE, [])
    doc_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    # AI ìš”ì•½ ìƒì„± (ì±„íŒ… ì¢…ë£Œ ì‹œ ë˜ëŠ” ì¶©ë¶„í•œ ëŒ€í™”ê°€ ìˆì„ ë•Œ)
    summary_data = None
    if is_chat_ended or len(messages) > 4:  # ì¶©ë¶„í•œ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ìš”ì•½ ìƒì„±
        summary_data = generate_chat_summary(messages, initial_query, customer_type, st.session_state.language)

    # ìš”ì•½ ë°ì´í„°ê°€ ìƒì„±ëœ ê²½ìš°ì—ë§Œ ì €ì¥ (ìš”ì•½ ì¤‘ì‹¬ ì €ì¥)
    if summary_data:
        # ìš”ì•½ ë°ì´í„°ì— ì´ˆê¸° ë¬¸ì˜ì™€ í•µì‹¬ ì •ë³´ í¬í•¨
        data = {
            "id": doc_id,
            "initial_query": initial_query,  # ì´ˆê¸° ë¬¸ì˜ëŠ” ìœ ì§€
            "customer_type": customer_type,
            "messages": [],  # ì „ì²´ ë©”ì‹œì§€ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ìš”ì•½ë§Œ ì €ì¥)
            "summary": summary_data,  # AI ìš”ì•½ ë°ì´í„° (ì£¼ìš” ì €ì¥ ë‚´ìš©)
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",  # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸
            "is_call": is_call,  # ì „í™” ì—¬ë¶€ í”Œë˜ê·¸
        }
    else:
        # ìš”ì•½ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° (ì§„í–‰ ì¤‘ì¸ ëŒ€í™”), ìµœì†Œí•œì˜ ì •ë³´ë§Œ ì €ì¥
        data = {
            "id": doc_id,
            "initial_query": initial_query,
            "customer_type": customer_type,
            "messages": messages[:10] if len(messages) > 10 else messages,  # ìµœê·¼ 10ê°œë§Œ ì €ì¥
            "summary": None,  # ìš”ì•½ ì—†ìŒ
            "language_key": st.session_state.language,
            "timestamp": ts,
            "is_chat_ended": is_chat_ended,
            "attachment_context": attachment_context if attachment_context else "",
            "is_call": is_call,
        }

    # ê¸°ì¡´ ì´ë ¥ì— ì¶”ê°€ (ìµœì‹ ìˆœ)
    histories.insert(0, data)
    # ë„ˆë¬´ ë§ì€ ì´ë ¥ ë°©ì§€ (ì˜ˆ: 100ê°œë¡œ ì¦ê°€ - ìš”ì•½ë§Œ ì €ì¥í•˜ë¯€ë¡œ ìš©ëŸ‰ ë¶€ë‹´ ì ìŒ)
    _save_json(SIM_META_FILE, histories[:100])
    return doc_id


def delete_all_history_local():
    _save_json(SIM_META_FILE, [])


# ========================================
# 6. RAG Helper (FAISS)
# ========================================
# RAG ê´€ë ¨ í•¨ìˆ˜ëŠ” ì‹œë®¬ë ˆì´í„°ì™€ ë¬´ê´€í•˜ë¯€ë¡œ ê¸°ì¡´ ì½”ë“œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

def load_documents(files) -> List[Document]:
    docs: List[Document] = []
    for f in files:
        name = f.name
        lower = name.lower()
        if lower.endswith(".pdf"):
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
            tmp.write(f.read())
            tmp.flush()
            tmp.close()
            loader = PyPDFLoader(tmp.name)
            file_docs = loader.load()
            for d in file_docs:
                d.metadata["source"] = name
            docs.extend(file_docs)
            try:
                os.remove(tmp.name)
            except OSError:
                pass
        elif lower.endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
        elif lower.endswith(".html") or lower.endswith(".htm"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
    return docs


def split_documents(docs: List[Document]) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " ", ""],
    )
    return splitter.split_documents(docs)


def get_embedding_model():
    if get_api_key("openai"):
        try:
            return OpenAIEmbeddings(model="text-embedding-3-small")
        except:
            pass
    if get_api_key("gemini"):
        try:
            return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        except:
            pass
    return None


def get_embedding_function():
    """
    RAG ì„ë² ë”©ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ê²°ì •í•©ë‹ˆë‹¤.
    API í‚¤ ìœ íš¨ì„± ìˆœì„œ: OpenAI (ì‚¬ìš©ì ì„¤ì • ì‹œ) -> Gemini -> NVIDIA -> HuggingFace (fallback)
    API ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì´ë™í•˜ë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    # 1. OpenAI ì„ë² ë”© ì‹œë„ (ì‚¬ìš©ìê°€ ìœ íš¨í•œ í‚¤ë¥¼ ì„¤ì •í–ˆì„ ê²½ìš°)
    openai_key = get_api_key("openai")
    if openai_key:
        try:
            st.info("ğŸ”¹ RAG: OpenAI Embedding ì‚¬ìš© ì¤‘")
            return OpenAIEmbeddings(openai_api_key=openai_key)
        except Exception as e:
            st.warning(f"OpenAI ì„ë² ë”© ì‹¤íŒ¨ â†’ Geminië¡œ Fallback: {e}")

    # 2. Gemini ì„ë² ë”© ì‹œë„
    gemini_key = get_api_key("gemini")
    if IS_GEMINI_EMBEDDING_AVAILABLE and gemini_key:
        try:
            st.info("ğŸ”¹ RAG: Gemini Embedding ì‚¬ìš© ì¤‘")
            # â­ ìˆ˜ì •: ëª¨ë¸ ì´ë¦„ í˜•ì‹ì„ 'models/model-name'ìœ¼ë¡œ ìˆ˜ì •
            return GoogleGenerativeAIEmbeddings(google_api_key=gemini_key, model="models/text-embedding-004")
        except Exception as e:
            st.warning(f"Gemini ì„ë² ë”© ì‹¤íŒ¨ â†’ NVIDIAë¡œ Fallback: {e}")

    # 3. NVIDIA ì„ë² ë”© ì‹œë„
    nvidia_key = get_api_key("nvidia")
    if IS_NVIDIA_EMBEDDING_AVAILABLE and nvidia_key:
        try:
            st.info("ğŸ”¹ RAG: NVIDIA Embedding ì‚¬ìš© ì¤‘")
            # NIM ëª¨ë¸ ì‚¬ìš© (ì‹¤ì œ í‚¤ê°€ ìœ íš¨í•´ì•¼ í•¨)
            return NVIDIAEmbeddings(api_key=nvidia_key, model="ai-embed-qa-4")
        except Exception as e:
            st.warning(f"NVIDIA ì„ë² ë”© ì‹¤íŒ¨ â†’ HuggingFace Fallback: {e}")

    # 4. HuggingFace Embeddings (Local Fallback)
    try:
        st.info("ğŸ”¹ RAG: Local HuggingFace Embedding ì‚¬ìš© ì¤‘")
        # ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    except Exception as e:
        st.warning(f"ìµœì¢… Fallback ì„ë² ë”© ì‹¤íŒ¨: {e}")

    st.error("âŒ RAG ì„ë² ë”© ì‹¤íŒ¨: ì‚¬ìš© ê°€ëŠ¥í•œ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    return None


def build_rag_index(files):
    L = LANG[st.session_state.language]
    if not files: return None, 0

    # ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‹œë„í•˜ëŠ” ê³¼ì •ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ try-exceptë¡œ ê°ìŒ‰ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception as e:
        st.error(f"RAG ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, 0

    if embeddings is None:
        # ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ë„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŒì„ ì•Œë¦¼
        error_msg = L["rag_embed_error_none"]

        # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ êµ¬ì„± (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°)
        if not get_api_key("openai"):
            error_msg += f"\n- {L['rag_embed_error_openai']}"
        if not get_api_key("gemini"):
            error_msg += f"\n- {L['rag_embed_error_gemini']}"
        if not get_api_key("nvidia"):
            error_msg += f"\n- {L['rag_embed_error_nvidia']}"

        st.error(error_msg)
        return None, 0

    # ì„ë² ë”© ê°ì²´ ì´ˆê¸°í™” ì„±ê³µ í›„, ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    docs = load_documents(files)
    if not docs: return None, 0

    chunks = split_documents(docs)
    if not chunks: return None, 0

    try:
        vectorstore = FAISS.from_documents(chunks, embeddings)
        # ì €ì¥
        vectorstore.save_local(RAG_INDEX_DIR)
    except Exception as e:
        # API ì¸ì¦ ì‹¤íŒ¨ ë“± ì‹¤ì œ API í˜¸ì¶œ ì˜¤ë¥˜ ì²˜ë¦¬
        st.error(f"RAG ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, 0

    return vectorstore, len(chunks)


def load_rag_index():
    # RAG ì¸ë±ìŠ¤ ë¡œë“œ ì‹œì—ë„ ìœ íš¨í•œ ì„ë² ë”© í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        embeddings = get_embedding_function()
    except Exception:
        # get_embedding_function ë‚´ì—ì„œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ìŠ¤í‚µí•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
        return None

    if embeddings is None:
        return None

    try:
        # allow_dangerous_deserialization=TrueëŠ” í•„ìˆ˜
        vs = FAISS.load_local(RAG_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception:
        return None


def rag_answer(question: str, vectorstore: FAISS, lang_key: str) -> str:
    # RAG AnswerëŠ” LLM í´ë¼ì´ì–¸íŠ¸ ë¼ìš°íŒ…ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    llm_client, info = get_llm_client()
    if llm_client is None:
        return LANG[lang_key]["simulation_no_key_warning"]

    # Langchain ChatOpenAI ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ promptë¥¼ ì§ì ‘ êµ¬ì„±
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    docs = retriever.get_relevant_documents(question)
    context = "\n\n".join(d.page_content[:1500] for d in docs)

    # â­ ìˆ˜ì •ëœ ë¶€ë¶„: ì§ˆë¬¸ ì–¸ì–´ë¥¼ ê°ì§€í•˜ì—¬ ë‹µë³€ ì–¸ì–´ë¥¼ ê°•ì œí•©ë‹ˆë‹¤.
    # LLMì—ê²Œ 'ì§ˆë¬¸ì˜ ì–¸ì–´'ë¡œ ë‹µë³€í•˜ë¼ê³  ëª…ì‹œì ìœ¼ë¡œ ì§€ì‹œí•©ë‹ˆë‹¤.
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(lang_key, "English")

    prompt = (
            "You are a helpful AI tutor. Answer the question using ONLY the provided context.\n"
            "If you cannot find the answer in the context, say you don't know.\n"
            f"Answer STRICTLY in the language of the question (Assume the question's language is {lang_name}).\n\n"
            "Question:\n" + question + "\n\n"
                                       "Context:\n" + context + "\n\n"
                                                                "Answer:"
    )
    return run_llm(prompt)


# ========================================
# 7. LSTM Helper (ê°„ë‹¨ Mock + ì‹œê°í™”)
# ========================================

def load_or_train_lstm():
    # ì‹¤ì œ LSTM ëŒ€ì‹  ëœë¤ + sin íŒŒí˜• ê¸°ë°˜ Mock
    np.random.seed(42)
    n_points = 50
    ts = 60 + 20 * np.sin(np.linspace(0, 4 * np.pi, n_points)) + np.random.normal(0, 5, n_points)
    ts = np.clip(ts, 50, 100).astype(np.float32)
    return ts


# ========================================
# 8. LLM (ChatOpenAI) for Simulator / Content
# (RAGì™€ ë™ì¼í•˜ê²Œ run_llmìœ¼ë¡œ í†µí•©)
# ========================================

# ConversationChain ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„
# st.session_state.simulator_memoryëŠ” ìœ ì§€í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.

def get_chat_history_for_prompt(include_attachment=False):
    """ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ì¶”ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ë¬¸ìì—´ í˜•íƒœë¡œ ë°˜í™˜ (ì±„íŒ…ìš©)"""
    history_str = ""
    for msg in st.session_state.simulator_messages:
        role = msg["role"]
        content = msg["content"]
        if role == "customer" or role == "customer_rebuttal":
            history_str += f"Customer: {content}\n"
        elif role == "agent_response":
            history_str += f"Agent: {content}\n"
        # supervisor ë©”ì‹œì§€ëŠ” LLMì— ì „ë‹¬í•˜ì§€ ì•Šì•„ ì—­í•  í˜¼ë™ ë°©ì§€
    return history_str


def generate_customer_reaction(current_lang_key: str, is_call: bool = False) -> str:
    """
    ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ì„ ìƒì„±í•˜ëŠ” LLM í˜¸ì¶œ (ì±„íŒ… ì „ìš©)
    **ìˆ˜ì • ì‚¬í•­:** ì—ì´ì „íŠ¸ ì •ë³´ ìš”ì²­ ì‹œ í•„ìˆ˜ ì •ë³´ (ì£¼ë¬¸ë²ˆí˜¸, eSIM, ìë…€ ë§Œ ë‚˜ì´, ì·¨ì†Œ ì‚¬ìœ ) ì œê³µ ì˜ë¬´ë¥¼ ê°•í™”í•¨.
    """
    history_text = get_chat_history_for_prompt()
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        # LLMì—ê²Œ ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ë˜, ì—ì´ì „íŠ¸ì—ê²Œ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    next_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

Read the following conversation and respond naturally in {lang_name}.

Conversation so far:
{history_text}

RULES:
1. You are only the customer. Do not write as the agent.
2. **[CRITICAL: Mandatory Information Submission for Problem Resolution]** If the agent requested any of the following critical information, you MUST provide it:
    - Order/Booking Number (e.g., ABC123, 123456)
    - eSIM related details (e.g., Host device compatibility, local status/location, time of activation)
    - Child-related product details (e.g., Child's Date of Birth or Current Age)
    - Exception/Refund Reason (e.g., flight cancellation/delay, illness, local natural disaster)
    - **If you are a difficult customer and the agent requests this information, you MUST still provide it, but you may express frustration or impatience while doing so.**
3. **[Crucial Rule for Repetition/New Inquiry]** After the agent has provided an attempt at a solution or answer:
    - If you are still confused or the problem is not fully solved, you MUST state the remaining confusion/problem clearly and briefly. DO NOT REPEAT THE INITIAL QUERY. Focus only on the unresolved aspect or the new inquiry.
4. **[CRITICAL: Solution Acknowledgment]** If the agent provided a clear and accurate solution/confirmation:
    - You MUST respond with appreciation and satisfaction, like "{L_local['customer_positive_response']}" or similar positive acknowledgment. This applies even if you are a difficult customer.
5. If the agent's LAST message was the closing confirmation: "{L_local['customer_closing_confirm']}"
    - If you have NO additional questions: You MUST reply with "{L_local['customer_no_more_inquiries']}".
    - If you DO have additional questions: You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS IMMEDIATELY. DO NOT just repeat that you have an additional question.
6. Do NOT repeat your initial message or previous responses unless necessary.
7. Output ONLY the customer's next message.
"""
    try:
        reaction = run_llm(next_prompt)
        return reaction.strip()
    except Exception as e:
        return f"âŒ ê³ ê° ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}"


def summarize_history_with_ai(current_lang_key: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ë¥¼ ì •ë¦¬í•˜ì—¬ LLMì— ì „ë‹¬í•˜ê³  ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ë°›ëŠ” í•¨ìˆ˜."""
    # ì „í™” ë¡œê·¸ëŠ” 'phone_exchange' ì—­í• ì„ ê°€ì§€ê±°ë‚˜, 'initial_query'ì— í¬í•¨ë˜ì–´ ìˆìŒ

    # 1. ë¡œê·¸ ì¶”ì¶œ
    conversation_text = ""
    initial_query = st.session_state.get("call_initial_query", "N/A")
    if initial_query and initial_query != "N/A":
        conversation_text += f"Initial Query: {initial_query}\n"

    for msg in st.session_state.simulator_messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "phone_exchange":
            # phone_exchangeëŠ” "Agent: ... | Customer: ..." í˜•íƒœë¡œ ì´ë¯¸ ì •ë¦¬ë˜ì–´ ìˆìŒ
            conversation_text += f"{content}\n"

    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    summary_prompt = f"""
You are an AI Analyst specialized in summarizing customer phone calls. 
Analyze the full conversation log below, identify the main issue, the steps taken by the agent, and the customer's sentiment.

Provide a concise, easy-to-read summary of the key exchange STRICTLY in {lang_name}.

--- Conversation Log ---
{conversation_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return "LLM Keyê°€ ì—†ì–´ ìš”ì•½ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤."

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}"


def generate_customer_reaction_for_call(current_lang_key: str, last_agent_response: str) -> str:
    """ì „í™” ì‹œë®¬ë ˆì´í„° ì „ìš© ê³ ê° ë°˜ì‘ ìƒì„± (ê°„ê²°í™”)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]  # â­ ìˆ˜ì •: í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ íŒ©

    # ì „í™” ì‹œë®¬ë ˆì´í„°ì—ì„œëŠ” ì „ì²´ simulator_messages ëŒ€ì‹ ,
    # st.session_state.call_initial_queryì™€ st.session_state.current_customer_audio_text
    # ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µ(ì „ì‚¬ í…ìŠ¤íŠ¸)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    history_text = (
        f"Initial Query: {st.session_state.call_initial_query}\n"
        f"Previous Customer Utterance: {st.session_state.current_customer_audio_text}\n"
        f"Agent's Last Response (Transcribed): {last_agent_response}"
    )

    call_prompt = f"""
You are now ROLEPLAYING as the CUSTOMER in a PHONE CALL.
Your goal is to respond naturally and briefly (like a real person on the phone) in {lang_name}.

Conversation context:
{history_text}

RULES:
1. Respond to the Agent's Last Response. Your reply MUST be short and conversational.
2. If the agent's response is satisfactory: Acknowledge and state you are fine, or ask for closing confirmation (e.g., "{L_local['customer_positive_response']}").
3. If the agent requested information or provided an unsatisfactory answer: Briefly state the remaining problem or provide the requested information.
4. **NEVER** output the agent's response, supervisor advice, or full context. Output ONLY the next customer utterance.
5. If the agent said the call is on hold, you MUST wait silently or acknowledge briefly. (Simulate this by just outputting a very short confirmation like "Okay.")
6. If the agent's last response was the closing confirmation: You MUST reply with "{L_local['customer_no_more_inquiries']}" or "{L_local['customer_has_additional_inquiries']}" (followed by the new query).

Customer's next brief spoken response:
"""
    try:
        reaction = run_llm(call_prompt)
        return reaction.strip()
    except Exception as e:
        return f"âŒ ê³ ê° ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}"


def generate_summary_for_call(current_lang_key: str, call_logs: List[Dict[str, str]], initial_query: str) -> str:
    """ì „í™” í†µí™” ë¡œê·¸ì™€ ì´ˆê¸° ë¬¸ì˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ë³¸ì„ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    # ë¡œê·¸ ì¬êµ¬ì„± (phone_exchange ì—­í• ë§Œ ì‚¬ìš©)
    full_log_text = f"--- Initial Customer Query ---\nCustomer: {initial_query}\n"
    for log in call_logs:
        if log["role"] == "phone_exchange":
            full_log_text += f"{log['content']}\n"

    summary_prompt = f"""
You are an AI Supervisor. Analyze the following telephone support conversation log.
Provide a concise, neutral summary of the key issue, the steps taken by the agent, and the final outcome.
The summary MUST be STRICTLY in {lang_name}.

--- Conversation Log ---
{full_log_text}
---

Summary:
"""
    if not st.session_state.is_llm_ready:
        return f"âŒ LLM Key is missing. Cannot generate summary. Log length: {len(full_log_text.splitlines())}"

    try:
        summary = run_llm(summary_prompt)
        return summary.strip()
    except Exception as e:
        return f"âŒ Summary Generation Error: {e}"


def generate_customer_closing_response(current_lang_key: str) -> str:
    """ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ í™•ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ê³ ê°ì˜ ìµœì¢… ë‹µë³€ ìƒì„± (ì±„íŒ…ìš©)"""
    history_text = get_chat_history_for_prompt()
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]
    L_local = LANG[current_lang_key]  # â­ ìˆ˜ì •: í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ íŒ©

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì—ì´ì „íŠ¸ì˜ ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ì¸ì§€ í™•ì¸ (í”„ë¡¬í”„íŠ¸ì— í¬í•¨)
    closing_msg = L_local['customer_closing_confirm']

    # ì²¨ë¶€ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    attachment_context = st.session_state.sim_attachment_context_for_llm
    if attachment_context:
        attachment_context = f"[INITIAL ATTACHMENT CONTEXT (for customer reference only, do not repeat to agent)]\n{attachment_context}\n\n"
    else:
        attachment_context = ""

    final_prompt = f"""
{attachment_context}
You are now ROLEPLAYING as the CUSTOMER.

The agent's final message was the closing confirmation: "{closing_msg}".
You MUST respond to this confirmation based on the overall conversation.

Conversation history:
{history_text}

RULES:
1. If the conversation seems resolved and you have NO additional questions:
    - You MUST reply with "{L_local['customer_no_more_inquiries']}".
2. If the conversation is NOT fully resolved and you DO have additional questions (or the agent provided a cancellation denial that you want to appeal):
    - You MUST reply with "{L_local['customer_has_additional_inquiries']}" AND MUST FOLLOW UP WITH THE NEW INQUIRY DETAILS. DO NOT just repeat that you have an additional question.
3. Your reply MUST be ONLY one of the two options above, in {lang_name}.
4. Output ONLY the customer's next message (must be one of the two rule options).
"""
    try:
        reaction = run_llm(final_prompt)
        # LLMì˜ ì¶œë ¥ì´ ê·œì¹™ì„ ë”°ë¥´ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê°•ì œ ì ìš©
        reaction_text = reaction.strip()
        # "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤"ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìƒì„¸ ë‚´ìš© í¬í•¨ ê°€ì •)
        if L_local['customer_no_more_inquiries'] in reaction_text:
            return L_local['customer_no_more_inquiries']
        elif L_local['customer_has_additional_inquiries'] in reaction_text:
            return reaction_text
        else:
            # LLMì´ ê·œì¹™ì„ ì–´ê²¼ì„ ê²½ìš°, "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆë‹¤"ê³  ê°€ì •í•˜ê³  ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ë„˜ê¹€
            return L_local['customer_has_additional_inquiries']
    except Exception as e:
        st.error(f"ê³ ê° ìµœì¢… ë°˜ì‘ ìƒì„± ì˜¤ë¥˜: {e}")
        return L_local['customer_has_additional_inquiries']  # ì˜¤ë¥˜ ì‹œ ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ë„


# ----------------------------------------
# Initial Advice/Draft Generation (ì´ê´€ í›„ ì¬ì‚¬ìš©) (ìš”ì²­ 4 ë°˜ì˜)
# ----------------------------------------
def analyze_customer_profile(customer_query: str, current_lang_key: str) -> Dict[str, Any]:
    """ì‹ ê·œ ê³ ê°ì˜ ë¬¸ì˜ì‚¬í•­ê³¼ ë§íˆ¬ë¥¼ ë¶„ì„í•˜ì—¬ ê³ ê°ì„±í–¥ ì ìˆ˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì‚° (ìš”ì²­ 4)"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    analysis_prompt = f"""
You are an AI analyst analyzing a customer's inquiry to determine their profile and sentiment.

Analyze the following customer inquiry and provide a structured analysis in JSON format (ONLY JSON, no markdown).

Analyze:
1. Customer sentiment score (0-100, where 0=very negative/angry, 50=neutral, 100=very positive/happy)
2. Communication style (formal/casual, brief/detailed, polite/direct)
3. Urgency level (low/medium/high)
4. Customer type prediction (normal/difficult/very_dissatisfied)
5. Language and cultural hints (if any)
6. Key concerns or pain points

Output format (JSON only):
{{
  "sentiment_score": 45,
  "communication_style": "brief, direct, slightly frustrated",
  "urgency_level": "high",
  "predicted_customer_type": "difficult",
  "cultural_hints": "unknown",
  "key_concerns": ["issue 1", "issue 2"],
  "tone_analysis": "brief description of tone"
}}

Customer Inquiry:
{customer_query}

JSON Output:
"""

    if not st.session_state.is_llm_ready:
        return {
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": "Unable to analyze"
        }

    try:
        analysis_text = run_llm(analysis_prompt).strip()
        # JSON ì¶”ì¶œ
        if "```json" in analysis_text:
            analysis_text = analysis_text.split("```json")[1].split("```")[0].strip()
        elif "```" in analysis_text:
            analysis_text = analysis_text.split("```")[1].split("```")[0].strip()

        import json
        analysis_data = json.loads(analysis_text)
        return analysis_data
    except Exception as e:
        return {
            "sentiment_score": 50,
            "communication_style": "unknown",
            "urgency_level": "medium",
            "predicted_customer_type": "normal",
            "cultural_hints": "unknown",
            "key_concerns": [],
            "tone_analysis": f"Analysis error: {str(e)}"
        }


def find_similar_cases(customer_query: str, customer_profile: Dict[str, Any], current_lang_key: str,
                       limit: int = 5) -> List[Dict[str, Any]]:
    """ì €ì¥ëœ ìš”ì•½ ë°ì´í„°ì—ì„œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì•„ ë°˜í™˜ (ìš”ì²­ 4)"""
    histories = load_simulation_histories_local(current_lang_key)

    if not histories:
        return []

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
    ]

    if not cases_with_summary:
        return []

    # ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ + ì ìˆ˜ ìœ ì‚¬ë„)
    similar_cases = []
    query_lower = customer_query.lower()
    customer_sentiment = customer_profile.get("sentiment_score", 50)
    customer_style = customer_profile.get("communication_style", "")

    for case in cases_with_summary:
        summary = case.get("summary", {})
        main_inquiry = summary.get("main_inquiry", "").lower()
        case_sentiment = summary.get("customer_sentiment_score", 50)
        case_satisfaction = summary.get("customer_satisfaction_score", 50)

        # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        similarity_score = 0

        # 1. ë¬¸ì˜ ë‚´ìš© ìœ ì‚¬ë„ (í‚¤ì›Œë“œ ë§¤ì¹­)
        query_words = set(query_lower.split())
        inquiry_words = set(main_inquiry.split())
        if query_words and inquiry_words:
            word_overlap = len(query_words & inquiry_words) / len(query_words | inquiry_words)
            similarity_score += word_overlap * 40

        # 2. ê°ì • ì ìˆ˜ ìœ ì‚¬ë„
        sentiment_diff = abs(customer_sentiment - case_sentiment)
        sentiment_similarity = max(0, 1 - (sentiment_diff / 100)) * 30
        similarity_score += sentiment_similarity

        # 3. ë§Œì¡±ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì¼€ì´ìŠ¤)
        satisfaction_bonus = (case_satisfaction / 100) * 30
        similarity_score += satisfaction_bonus

        if similarity_score > 30:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            similar_cases.append({
                "case": case,
                "similarity_score": similarity_score,
                "summary": summary
            })

    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    similar_cases.sort(key=lambda x: x["similarity_score"], reverse=True)
    return similar_cases[:limit]


def visualize_customer_profile_scores(customer_profile: Dict[str, Any], current_lang_key: str):
    """ê³ ê° í”„ë¡œí•„ ì ìˆ˜ë¥¼ ì‹œê°í™” (ê°ì • ì ìˆ˜, ê¸´ê¸‰ë„)"""
    if not IS_PLOTLY_AVAILABLE:
        return None

    L = LANG[current_lang_key]

    sentiment_score = customer_profile.get("sentiment_score", 50)
    urgency_map = {"low": 25, "medium": 50, "high": 75}
    urgency_level = customer_profile.get("urgency_level", "medium")
    urgency_score = urgency_map.get(urgency_level.lower(), 50)

    # ê²Œì´ì§€ ì°¨íŠ¸ ìƒì„±
    fig = make_subplots(
        rows=1, cols=2,
        specs=[[{"type": "indicator"}, {"type": "indicator"}]],
        subplot_titles=(
            L.get("sentiment_score_label", "ê³ ê° ê°ì • ì ìˆ˜"),
            L.get("urgency_score_label", "ê¸´ê¸‰ë„ ì ìˆ˜")
        )
    )

    # ê°ì • ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number+delta",
            value=sentiment_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("sentiment_score_label", "ê°ì • ì ìˆ˜")},
            delta={'reference': 50},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 33], 'color': "lightgray"},
                    {'range': [33, 66], 'color': "gray"},
                    {'range': [66, 100], 'color': "lightgreen"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 90
                }
            }
        ),
        row=1, col=1
    )

    # ê¸´ê¸‰ë„ ì ìˆ˜ ê²Œì´ì§€
    fig.add_trace(
        go.Indicator(
            mode="gauge+number",
            value=urgency_score,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': L.get("urgency_score_label", "ê¸´ê¸‰ë„")},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkred"},
                'steps': [
                    {'range': [0, 50], 'color': "lightgreen"},
                    {'range': [50, 75], 'color': "yellow"},
                    {'range': [75, 100], 'color': "lightcoral"}
                ],
            }
        ),
        row=1, col=2
    )

    fig.update_layout(height=300, margin=dict(l=20, r=20, t=50, b=20))
    return fig


def visualize_similarity_cases(similar_cases: List[Dict[str, Any]], current_lang_key: str):
    """ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ì²œì„ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not similar_cases:
        return None

    L = LANG[current_lang_key]

    case_labels = []
    similarity_scores = []
    sentiment_scores = []
    satisfaction_scores = []

    for idx, similar_case in enumerate(similar_cases, 1):
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]
        case_labels.append(f"Case {idx}")
        similarity_scores.append(similarity)
        sentiment_scores.append(summary.get("customer_sentiment_score", 50))
        satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))

    # ìœ ì‚¬ë„ ì°¨íŠ¸
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=(
            L.get("similarity_chart_title", "ìœ ì‚¬ ì¼€ì´ìŠ¤ ìœ ì‚¬ë„"),
            L.get("scores_comparison_title",
                  "ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ")
        ),
        vertical_spacing=0.15
    )

    # ìœ ì‚¬ë„ ë°” ì°¨íŠ¸
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=similarity_scores,
            name=L.get("similarity_score_label", "ìœ ì‚¬ë„"),
            marker_color='lightblue',
            text=[f"{s:.1f}%" for s in similarity_scores],
            textposition='outside'
        ),
        row=1, col=1
    )

    # ê°ì • ë° ë§Œì¡±ë„ ì ìˆ˜ ë¹„êµ
    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=sentiment_scores,
            name=L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
            marker_color='lightcoral'
        ),
        row=2, col=1
    )

    fig.add_trace(
        go.Bar(
            x=case_labels,
            y=satisfaction_scores,
            name=L.get("satisfaction_score_label", "ë§Œì¡±ë„"),
            marker_color='lightgreen'
        ),
        row=2, col=1
    )

    fig.update_layout(
        height=600,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20),
        barmode='group'
    )
    fig.update_yaxes(title_text="ì ìˆ˜", row=2, col=1)
    fig.update_yaxes(title_text="ìœ ì‚¬ë„ (%)", row=1, col=1)

    return fig


def visualize_case_trends(histories: List[Dict[str, Any]], current_lang_key: str):
    """ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ íŠ¸ë Œë“œë¥¼ ì‹œê°í™”"""
    if not IS_PLOTLY_AVAILABLE or not histories:
        return None

    L = LANG[current_lang_key]

    # ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
    cases_with_summary = [
        h for h in histories
        if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
    ]

    if not cases_with_summary:
        return None

    # ë‚ ì§œë³„ë¡œ ì •ë ¬
    cases_with_summary.sort(key=lambda x: x.get("timestamp", ""))

    dates = []
    sentiment_scores = []
    satisfaction_scores = []

    for case in cases_with_summary:
        summary = case.get("summary", {})
        timestamp = case.get("timestamp", "")
        try:
            dt = datetime.fromisoformat(timestamp)
            dates.append(dt)
            sentiment_scores.append(summary.get("customer_sentiment_score", 50))
            satisfaction_scores.append(summary.get("customer_satisfaction_score", 50))
        except Exception:
            continue

    if not dates:
        return None

    # íŠ¸ë Œë“œ ë¼ì¸ ì°¨íŠ¸
    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=dates,
        y=sentiment_scores,
        mode='lines+markers',
        name=L.get("sentiment_trend_label", "ê°ì • ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightcoral', width=2),
        marker=dict(size=6)
    ))

    fig.add_trace(go.Scatter(
        x=dates,
        y=satisfaction_scores,
        mode='lines+markers',
        name=L.get("satisfaction_trend_label", "ë§Œì¡±ë„ ì ìˆ˜ ì¶”ì´"),
        line=dict(color='lightgreen', width=2),
        marker=dict(size=6)
    ))

    fig.update_layout(
        title=L.get("case_trends_title", "ê³¼ê±° ì¼€ì´ìŠ¤ ì ìˆ˜ ì¶”ì´"),
        xaxis_title=L.get("date_label", "ë‚ ì§œ"),
        yaxis_title=L.get("score_label", "ì ìˆ˜ (0-100)"),
        height=400,
        hovermode='x unified',
        legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01)
    )

    return fig


def visualize_customer_characteristics(summary: Dict[str, Any], current_lang_key: str):
    """ê³ ê° íŠ¹ì„±ì„ ì‹œê°í™” (ì–¸ì–´, ë¬¸í™”ê¶Œ, ì§€ì—­ ë“±)"""
    if not IS_PLOTLY_AVAILABLE or not summary:
        return None

    L = LANG[current_lang_key]

    characteristics = summary.get("customer_characteristics", {})
    privacy_info = summary.get("privacy_info", {})

    # íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
    labels = []
    values = []

    # ì–¸ì–´ ì •ë³´
    language = characteristics.get("language", "unknown")
    if language != "unknown":
        labels.append(L.get("language_label", "ì–¸ì–´"))
        lang_map = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}
        values.append(lang_map.get(language, language))

    # ê°œì¸ì •ë³´ ì œê³µ ì—¬ë¶€
    if privacy_info.get("has_email"):
        labels.append(L.get("email_provided_label", "ì´ë©”ì¼ ì œê³µ"))
        values.append("Yes")
    if privacy_info.get("has_phone"):
        labels.append(L.get("phone_provided_label", "ì „í™”ë²ˆí˜¸ ì œê³µ"))
        values.append("Yes")

    # ì§€ì—­ ì •ë³´
    region = privacy_info.get("region_hint", characteristics.get("region", "unknown"))
    if region != "unknown":
        labels.append(L.get("region_label", "ì§€ì—­"))
        values.append(region)

    if not labels:
        return None

    # íŒŒì´ ì°¨íŠ¸
    fig = go.Figure(data=[go.Pie(
        labels=labels,
        values=[1] * len(labels),
        hole=0.4,
        marker_colors=px.colors.qualitative.Set3[:len(labels)]
    )])

    fig.update_layout(
        title=L.get("customer_characteristics_title",
                    "ê³ ê° íŠ¹ì„± ë¶„í¬"),
        height=300,
        showlegend=True,
        margin=dict(l=20, r=20, t=50, b=20)
    )

    return fig


def generate_guideline_from_past_cases(customer_query: str, customer_profile: Dict[str, Any],
                                       similar_cases: List[Dict[str, Any]], current_lang_key: str) -> str:
    """ê³¼ê±° ìœ ì‚¬ ì¼€ì´ìŠ¤ì˜ ì„±ê³µì ì¸ í•´ê²° ë°©ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¼ì¸ ìƒì„±"""
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    if not similar_cases:
        return ""

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìš”ì•½
    past_cases_text = ""
    for idx, similar_case in enumerate(similar_cases, 1):
        case = similar_case["case"]
        summary = similar_case["summary"]
        similarity = similar_case["similarity_score"]

        past_cases_text += f"""
[Case {idx}] (Similarity: {similarity:.1f}%)
- Inquiry: {summary.get('main_inquiry', 'N/A')}
- Customer Sentiment: {summary.get('customer_sentiment_score', 50)}/100
- Customer Satisfaction: {summary.get('customer_satisfaction_score', 50)}/100
- Key Responses: {', '.join(summary.get('key_responses', [])[:3])}
- Summary: {summary.get('summary', 'N/A')[:200]}
"""

    guideline_prompt = f"""
You are an AI Customer Support Supervisor analyzing past successful cases to provide guidance.

Based on the following similar past cases and their successful resolution strategies, provide actionable guidelines for handling the current customer inquiry.

Current Customer Inquiry:
{customer_query}

Current Customer Profile:
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}

Similar Past Cases (Successful Resolutions):
{past_cases_text}

Provide a concise guideline in {lang_name} that:
1. Identifies what worked well in similar past cases
2. Suggests specific approaches based on successful patterns
3. Warns about potential pitfalls based on past experiences
4. Recommends response strategies that led to high customer satisfaction

Guideline (in {lang_name}):
"""

    if not st.session_state.is_llm_ready:
        return ""

    try:
        guideline = run_llm(guideline_prompt).strip()
        return guideline
    except Exception as e:
        return f"ê°€ì´ë“œë¼ì¸ ìƒì„± ì˜¤ë¥˜: {str(e)}"


def _generate_initial_advice(customer_query, customer_type_display, customer_email, customer_phone, current_lang_key,
                             customer_attachment_file):
    """Supervisor ê°€ì´ë“œë¼ì¸ê³¼ ì´ˆì•ˆì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì €ì¥ëœ ë°ì´í„° í™œìš©)"""
    L = LANG[current_lang_key]
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}[current_lang_key]

    contact_info_block = ""
    if customer_email or customer_phone:
        contact_info_block = (
            f"\n\n[Customer contact info for reference (DO NOT use these in your reply draft!)]"
            f"\n- Email: {customer_email or 'N/A'}"
            f"\n- Phone: {customer_phone or 'N/A'}"
        )

    attachment_block = ""
    if customer_attachment_file:
        file_name = customer_attachment_file.name
        attachment_block = f"\n\n[ATTACHMENT NOTE]: {L['attachment_info_llm'].format(filename=file_name)}"

    # ê³ ê° í”„ë¡œí•„ ë¶„ì„
    customer_profile = analyze_customer_profile(customer_query, current_lang_key)

    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì°¾ê¸°
    similar_cases = find_similar_cases(customer_query, customer_profile, current_lang_key, limit=5)

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ìƒì„±
    past_cases_guideline = ""
    if similar_cases:
        past_cases_guideline = generate_guideline_from_past_cases(
            customer_query, customer_profile, similar_cases, current_lang_key
        )

    # ê³ ê° í”„ë¡œí•„ ì •ë³´
    profile_block = f"""
[Customer Profile Analysis]
- Sentiment Score: {customer_profile.get('sentiment_score', 50)}/100
- Communication Style: {customer_profile.get('communication_style', 'unknown')}
- Urgency Level: {customer_profile.get('urgency_level', 'medium')}
- Predicted Type: {customer_profile.get('predicted_customer_type', 'normal')}
- Key Concerns: {', '.join(customer_profile.get('key_concerns', []))}
- Tone: {customer_profile.get('tone_analysis', 'unknown')}
"""

    # ê³¼ê±° ì¼€ì´ìŠ¤ ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ ë¸”ë¡
    past_cases_block = ""
    if past_cases_guideline:
        past_cases_block = f"""
[Guidelines Based on {len(similar_cases)} Similar Past Cases]
{past_cases_guideline}
"""
    elif similar_cases:
        past_cases_block = f"""
[Note: Found {len(similar_cases)} similar past cases, but unable to generate detailed guidelines.
Consider reviewing past cases manually for patterns.]
"""

    # Output ALL text (guidelines and draft) STRICTLY in {lang_name}. <--- ê°•ë ¥í•œ ì–¸ì–´ ê°•ì œ ì§€ì‹œ
    initial_prompt = f"""
Output ALL text (guidelines and draft) STRICTLY in {lang_name}.

You are an AI Customer Support Supervisor. Your role is to analyze the following customer inquiry
from a **{st.session_state.customer_type_sim_select}** and provide:

1) A detailed **response guideline for the human agent** (step-by-step).
2) A **ready-to-send draft reply** in {lang_name}.

[FORMAT]
- Use the exact markdown headers:
  - "### {L['simulation_advice_header']}"
  - "### {L['simulation_draft_header']}"

[CRITICAL GUIDELINE RULES]
1. **Initial Information Collection (Req 3):** The first step in the guideline MUST be to request the necessary initial diagnostic information (e.g., device compatibility, local status/location, order number) BEFORE attempting to troubleshoot or solve the problem.
2. **Empathy for Difficult Customers (Req 5):** If the customer type is 'Difficult Customer' or 'Highly Dissatisfied Customer', the guideline MUST emphasize extreme politeness, empathy, and apologies, even if the policy (e.g., no refund) must be enforced.
3. **24-48 Hour Follow-up (Req 6):** If the issue cannot be solved immediately or requires confirmation from a local partner/supervisor, the guideline MUST state the procedure:
   - Acknowledge the issue.
   - Inform the customer they will receive a definite answer within 24 or 48 hours.
   - Request the customer's email or phone number for follow-up contact. (Use provided contact info if available)
4. **Past Cases Learning:** If past cases guidelines are provided, incorporate successful strategies from those cases into your recommendations.

Customer Inquiry:
{customer_query}
{contact_info_block}
{attachment_block}
{profile_block}
{past_cases_block}
"""
    if not st.session_state.is_llm_ready:
        mock_text = (
            f"### {L['simulation_advice_header']}\n\n"
            f"- (Mock) {st.session_state.customer_type_sim_select} ìœ í˜• ê³ ê° ì‘ëŒ€ ê°€ì´ë“œì…ë‹ˆë‹¤. (ìš”ì²­ 3, 5, 6 ë°˜ì˜)\n\n"
            f"### {L['simulation_draft_header']}\n\n"
            f"(Mock) ì—ì´ì „íŠ¸ ì‘ëŒ€ ì´ˆì•ˆì´ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤ã€‚\n\n"
        )
        return mock_text
    else:
        with st.spinner(L["response_generating"]):
            try:
                return run_llm(initial_prompt)
            except Exception as e:
                st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return f"âŒ AI Advice Generation Error: {e}"


# ========================================
# 9. ì‚¬ì´ë“œë°”
# ========================================

with st.sidebar:
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=["ko", "en", "ja"],
        index=["ko", "en", "ja"].index(st.session_state.language),
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
    )

    # ğŸ”¹ ì–¸ì–´ ë³€ê²½ ê°ì§€
    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        # ì±„íŒ…/ì „í™” ê³µí†µ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.simulator_messages = []
        st.session_state.simulator_memory.clear()
        st.session_state.initial_advice_provided = False
        st.session_state.is_chat_ended = False
        st.session_state.agent_response_area_text = ""
        st.session_state.customer_query_text_area = ""
        st.session_state.last_transcript = ""
        st.session_state.sim_audio_bytes = None
        st.session_state.sim_stage = "WAIT_FIRST_QUERY"
        st.session_state.customer_attachment_file = []  # ì–¸ì–´ ë³€ê²½ ì‹œ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
        st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
        # ì „í™” ì‹œë®¬ë ˆì´í„° ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.call_sim_stage = "WAITING_CALL"
        st.session_state.call_sim_mode = "INBOUND"
        st.session_state.is_on_hold = False
        st.session_state.total_hold_duration = timedelta(0)
        st.session_state.hold_start_time = None
        st.session_state.current_customer_audio_text = ""
        st.session_state.current_agent_audio_text = ""
        st.session_state.agent_response_input_box_widget_call = ""
        st.session_state.call_initial_query = ""
        # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.sim_call_outbound_summary = ""
        st.session_state.sim_call_outbound_target = None

        # â­ ì–¸ì–´ ë³€ê²½ ì‹œ ì¬ì‹¤í–‰
        st.rerun()

    L = LANG[st.session_state.language]

    st.title(L["sidebar_title"])
    st.markdown("---")

    st.subheader("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœ")
    if st.session_state.llm_init_error_msg:
        st.error(st.session_state.llm_init_error_msg)
    elif st.session_state.is_llm_ready:
        st.success("âœ… LLM í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
    else:
        st.info("ğŸ’¡ API KeyëŠ” í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” Streamlit Secretsì—ì„œ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.")

    if st.session_state.openai_client:
        st.success("âœ… OpenAI TTS/Whisper í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
    else:
        st.warning(L["openai_missing"])

    st.markdown("---")

    # ê¸°ëŠ¥ ì„ íƒ - ê¸°ë³¸ê°’ì„ AI ì±— ì‹œë®¬ë ˆì´í„°ë¡œ ì„¤ì •
    if "feature_selection" not in st.session_state:
        st.session_state.feature_selection = L["sim_tab_chat_email"]

    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        [L["sim_tab_chat_email"], L["sim_tab_phone"], L["rag_tab"], L["content_tab"], L["lstm_tab"],
         L["voice_rec_header"]],
        index=0 if st.session_state.feature_selection == L["sim_tab_chat_email"] else
        (1 if st.session_state.feature_selection == L["sim_tab_phone"] else
         (2 if st.session_state.feature_selection == L["rag_tab"] else
          (3 if st.session_state.feature_selection == L["content_tab"] else
           (4 if st.session_state.feature_selection == L["lstm_tab"] else 5)))),
        key="feature_selection_radio"
    )
    st.session_state.feature_selection = feature_selection

# ë©”ì¸ íƒ€ì´í‹€
st.title(L["title"])

# ========================================
# 10. ê¸°ëŠ¥ë³„ í˜ì´ì§€
# ========================================

# -------------------- Voice Record Tab --------------------
if feature_selection == L["voice_rec_header"]:
    # ... (ê¸°ì¡´ ìŒì„± ê¸°ë¡ íƒ­ ë¡œì§ ìœ ì§€)
    st.header(L["voice_rec_header"])
    st.caption(L["record_help"])

    col_rec, col_list = st.columns([1, 1])

    # ë…¹ìŒ/ì—…ë¡œë“œ + ì „ì‚¬ + ì €ì¥
    with col_rec:
        st.subheader(L["rec_header"])
        audio_file = st.file_uploader(
            L["uploaded_file"],
            type=["wav", "mp3", "m4a", "webm", "ogg"],
            key="voice_rec_uploader",
        )
        audio_bytes = None
        audio_mime = "audio/webm"

        if audio_file is not None:
            audio_bytes = audio_file.getvalue()
            audio_mime = audio_file.type or "audio/webm"

        # ì¬ìƒ
        if audio_bytes:
            st.audio(audio_bytes, format=audio_mime)

        # ì „ì‚¬ ë²„íŠ¼
        if audio_bytes and st.button(L["transcribe_btn"]):
            if st.session_state.openai_client is None:
                st.error(L["openai_missing"])
            else:
                with st.spinner(L["transcribing"]):
                    text = transcribe_bytes_with_whisper(
                        audio_bytes, audio_mime, lang_code=st.session_state.language
                    )
                    st.session_state.last_transcript = text
                    snippet = text[:50].replace("\n", " ")
                    if len(text) > 50:
                        snippet += "..."
                    if text.startswith("âŒ"):
                        st.error(text)
                    else:
                        st.success(f"{L['transcript_result']} **{snippet}**")

        st.text_area(
            L["transcript_text"],
            value=st.session_state.last_transcript,
            height=150,
            key="voice_rec_transcript_area",
        )

        if audio_bytes and st.button(L["save_btn"]):
            try:
                ext = audio_mime.split("/")[-1] if "/" in audio_mime else "webm"
                filename = f"record_{int(time.time())}.{ext}"
                save_audio_record_local(
                    audio_bytes,
                    filename,
                    st.session_state.last_transcript,
                    mime_type=audio_mime,
                )
                st.success(L["saved_success"])
                st.session_state.last_transcript = ""
                # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
            except Exception as e:
                st.error(f"{L['error']} {e}")

    # ì €ì¥ëœ ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
    with col_list:
        st.subheader(L["rec_list_title"])
        try:
            records = load_voice_records()
        except Exception as e:
            st.error(f"read error: {e}")
            records = []

        if not records:
            st.info(L["no_records"])
        else:
            for rec in records:
                rec_id = rec["id"]
                created_at = rec.get("created_at")
                try:
                    dt = datetime.fromisoformat(created_at)
                    created_str = dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    created_str = str(created_at)

                transcript_snippet = (rec.get("transcript") or "")[:50].replace("\n", " ")
                if len(rec.get("transcript") or "") > 50:
                    transcript_snippet += "..."

                with st.expander(f"[{created_str}] {transcript_snippet}"):
                    st.write(f"**{L['transcript_text']}:** {rec.get('transcript') or 'N/A'}")
                    st.caption(
                        f"**Size:** {rec.get('size')} bytes | **File:** {rec.get('audio_filename')}"
                    )

                    col_p, col_r, col_d = st.columns([2, 1, 1])

                    if col_p.button(L["playback"], key=f"play_{rec_id}"):
                        try:
                            b, info = get_audio_bytes_local(rec_id)
                            mime = info.get("mime_type", "audio/webm")
                            st.audio(b, format=mime)
                        except Exception as e:
                            st.error(f"{L['gcs_playback_fail']}: {e}")

                    if col_r.button(L["retranscribe"], key=f"re_{rec_id}"):
                        if st.session_state.openai_client is None:
                            st.error(L["openai_missing"])
                        else:
                            with st.spinner(L["transcribing"]):
                                try:
                                    b, info = get_audio_bytes_local(rec_id)
                                    mime = info.get("mime_type", "audio/webm")
                                    new_text = transcribe_bytes_with_whisper(
                                        b, mime, lang_code=st.session_state.language
                                    )
                                    records = load_voice_records()
                                    for r in records:
                                        if r["id"] == rec_id:
                                            r["transcript"] = new_text
                                            break
                                    save_voice_records(records)
                                    st.success(L["retranscribe"] + " " + L["saved_success"])
                                    # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
                                except Exception as e:
                                    st.error(f"{L['error']} {e}")

                    if col_d.button(L["delete"], key=f"del_{rec_id}"):
                        if st.session_state.get(f"confirm_del_{rec_id}", False):
                            ok = delete_audio_record_local(rec_id)
                            if ok:
                                st.success(L["delete_success"])
                            else:
                                st.error(L["delete_fail"])
                            st.session_state[f"confirm_del_{rec_id}"] = False
                            # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
                        else:
                            st.session_state[f"confirm_del_{rec_id}"] = True
                            st.warning(L["delete_confirm_rec"])
                            st.write("sim_stage:", st.session_state.get("sim_stage"))
                            st.write("is_llm_ready:", st.session_state.get("is_llm_ready"))

# -------------------- Simulator (Chat/Email) Tab --------------------
elif feature_selection == L["sim_tab_chat_email"]:
    # ... (ê¸°ì¡´ ì±„íŒ…/ì´ë©”ì¼ ì‹œë®¬ë ˆì´í„° ë¡œì§ ìœ ì§€)
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])

    current_lang = st.session_state.language
    L = LANG[current_lang]  # ë‹¤ì‹œ L ì—…ë°ì´íŠ¸

    # =========================
    # 0. ì „ì²´ ì´ë ¥ ì‚­ì œ
    # =========================
    col_del, _ = st.columns([1, 4])
    with col_del:
        if st.button(L["delete_history_button"], key="trigger_delete_hist"):
            st.session_state.show_delete_confirm = True

    if st.session_state.show_delete_confirm:
        with st.container():
            st.warning(L["delete_confirm_message"])
            c_yes, c_no = st.columns(2)
            if c_yes.button(L["delete_confirm_yes"], key="confirm_del_yes"):
                with st.spinner(L["deleting_history_progress"]):
                    delete_all_history_local()
                    st.session_state.simulator_messages = []
                    st.session_state.simulator_memory.clear()
                    st.session_state.show_delete_confirm = False
                    st.session_state.is_chat_ended = False
                    st.session_state.sim_stage = "WAIT_FIRST_QUERY"
                    st.session_state.customer_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
                    st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                    st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
                    st.success(L["delete_success"])
                    # â­ ìµœì í™”: ë²„íŠ¼ í´ë¦­ í›„ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë¯€ë¡œ rerun ì œê±°
            if c_no.button(L["delete_confirm_no"], key="confirm_del_no"):
                st.session_state.show_delete_confirm = False

    # =========================
    # 1. ì´ì „ ì´ë ¥ ë¡œë“œ (ê²€ìƒ‰/í•„í„°ë§ ê¸°ëŠ¥ ê°œì„ )
    # =========================
    with st.expander(L["history_expander_title"]):
        # Always load all available histories for the current language (sorted by recency)
        histories = load_simulation_histories_local(current_lang)

        # ì „ì²´ í†µê³„ ë° íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ (ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        cases_with_summary = [
            h for h in histories
            if h.get("summary") and isinstance(h.get("summary"), dict) and h.get("is_chat_ended", False)
               and not h.get("is_call", False)  # ì „í™” ì´ë ¥ ì œì™¸
        ]

        if cases_with_summary:
            st.markdown("---")
            st.subheader("ğŸ“ˆ ê³¼ê±° ì¼€ì´ìŠ¤ íŠ¸ë Œë“œ ëŒ€ì‹œë³´ë“œ")

            # íŠ¸ë Œë“œ ì°¨íŠ¸ í‘œì‹œ
            trend_chart = visualize_case_trends(histories, current_lang)
            if trend_chart:
                st.plotly_chart(trend_chart, use_container_width=True)
            else:
                # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                avg_sentiment = np.mean(
                    [h["summary"].get("customer_sentiment_score", 50) for h in cases_with_summary if h.get("summary")])
                avg_satisfaction = np.mean(
                    [h["summary"].get("customer_satisfaction_score", 50) for h in cases_with_summary if
                     h.get("summary")])
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("í‰ê·  ê°ì • ì ìˆ˜", f"{avg_sentiment:.1f}/100", f"ì´ {len(cases_with_summary)}ê±´")
                with col2:
                    st.metric("í‰ê·  ë§Œì¡±ë„", f"{avg_satisfaction:.1f}/100", f"ì´ {len(cases_with_summary)}ê±´")

            st.markdown("---")

        # â­ ê²€ìƒ‰ í¼ ì œê±° ë° ë…ë¦½ëœ ìœ„ì ¯ ì‚¬ìš©
        col_search, col_btn = st.columns([4, 1])

        with col_search:
            # st.text_inputì€ Enter í‚¤ ì…ë ¥ ì‹œ ì•±ì„ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.
            search_query = st.text_input(L["search_history_label"], key="sim_hist_search_input_new")

        with col_btn:
            # ê²€ìƒ‰ ë²„íŠ¼: ëˆ„ë¥´ë©´ ì•±ì„ ê°•ì œ ì¬ì‹¤í–‰í•˜ì—¬ ê²€ìƒ‰/í•„í„°ë§ ë¡œì§ì„ ë‹¤ì‹œ íƒ€ë„ë¡ í•©ë‹ˆë‹¤.
            st.markdown("<br>", unsafe_allow_html=True)  # Align button vertically
            search_clicked = st.button(L["history_search_button"], key="apply_search_btn_new")

        # ë‚ ì§œ ë²”ìœ„ í•„í„°
        today = datetime.now().date()
        date_range_value = [today - timedelta(days=7), today]
        dr = st.date_input(
            L["date_range_label"],
            value=date_range_value,
            key="sim_hist_date_range_actual",
        )

        # --- Filtering Logic ---
        current_search_query = search_query.strip()

        if histories:
            start_date = min(dr)
            end_date = max(dr)

            filtered = []
            for h in histories:
                # ì „í™” ì´ë ¥ì€ ì œì™¸ (ì±„íŒ…/ì´ë©”ì¼ íƒ­ì´ë¯€ë¡œ)
                if h.get("is_call", False):
                    continue

                ok_search = True
                if current_search_query:
                    q = current_search_query.lower()
                    # ê²€ìƒ‰ ëŒ€ìƒ: ì´ˆê¸° ë¬¸ì˜, ê³ ê° ìœ í˜•, ìš”ì•½ ë°ì´í„°
                    text = (h["initial_query"] + " " + h["customer_type"]).lower()

                    # ìš”ì•½ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš”ì•½ ë‚´ìš©ë„ ê²€ìƒ‰ ëŒ€ìƒì— í¬í•¨
                    summary = h.get("summary")
                    if summary and isinstance(summary, dict):
                        summary_text = summary.get("main_inquiry", "") + " " + summary.get("summary", "")
                        text += " " + summary_text.lower()

                    # Check if query matches in initial query, customer type, or summary
                    if q not in text:
                        ok_search = False

                ok_date = True
                ts = h.get("timestamp")
                if ts:
                    try:
                        d = datetime.fromisoformat(ts).date()
                        # Apply date filtering
                        if not (start_date <= d <= end_date):
                            ok_date = False
                    except Exception:
                        pass  # Ignore histories with invalid timestamp

                if ok_search and ok_date:
                    filtered.append(h)
        else:
            filtered = []

        # Determine the list for display (â­ ìš”ì²­ ì‚¬í•­: ê²€ìƒ‰ì–´/í•„í„°ê°€ ì—†ìœ¼ë©´ ìµœê·¼ 10ê±´ë§Œ í‘œì‹œ)
        is_searching_or_filtering = bool(current_search_query) or dr != date_range_value

        if not is_searching_or_filtering:
            # ê²€ìƒ‰/í•„í„° ì¡°ê±´ì´ ì—†ìœ¼ë©´, ì „ì²´ ì´ë ¥ ì¤‘ ìµœì‹  10ê±´ë§Œ í‘œì‹œ
            filtered_for_display = filtered[:10]  # í•„í„°ë§ëœ ëª©ë¡(ì „í™” ì œì™¸) ì¤‘ 10ê°œ
        else:
            # ê²€ìƒ‰/í•„í„° ì¡°ê±´ì´ ìˆìœ¼ë©´, í•„í„°ë§ëœ ëª¨ë“  ê²°ê³¼ë¥¼ í‘œì‹œ
            filtered_for_display = filtered

        # --- Display Logic ---

        if filtered_for_display:
            def _label(h):
                try:
                    t = datetime.fromisoformat(h["timestamp"])
                    t_str = t.strftime("%m-%d %H:%M")
                except Exception:
                    t_str = h.get("timestamp", "")

                # ìš”ì•½ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìš”ì•½ ì •ë³´ í‘œì‹œ, ì—†ìœ¼ë©´ ì´ˆê¸° ë¬¸ì˜ í‘œì‹œ
                summary = h.get("summary")
                if summary and isinstance(summary, dict):
                    main_inquiry = summary.get("main_inquiry", h["initial_query"][:30])
                    sentiment = summary.get("customer_sentiment_score", 50)
                    satisfaction = summary.get("customer_satisfaction_score", 50)
                    q = main_inquiry[:30].replace("\n", " ")
                    # ì²¨ë¶€ íŒŒì¼ ì—¬ë¶€ í‘œì‹œ ì¶”ê°€
                    attachment_icon = "ğŸ“" if h.get("attachment_context") else ""
                    # ìš”ì•½ ë°ì´í„° í‘œì‹œ (ê°ì •/ë§Œì¡±ë„ ì ìˆ˜ í¬í•¨)
                    return f"[{t_str}] {attachment_icon} {h['customer_type']} | ê°ì •:{sentiment} ë§Œì¡±:{satisfaction} - {q}..."
                else:
                    q = h["initial_query"][:30].replace("\n", " ")
                    attachment_icon = "ğŸ“" if h.get("attachment_context") else ""
                    return f"[{t_str}] {attachment_icon} {h['customer_type']} - {q}..."


            options_map = {_label(h): h for h in filtered_for_display}

            # Show a message indicating what is displayed if filters were applied
            if is_searching_or_filtering:
                st.caption(f"ğŸ” ì´ {len(filtered_for_display)}ê°œ ì´ë ¥ ê²€ìƒ‰ë¨ (ì „í™” ì´ë ¥ ì œì™¸)")
            else:
                st.caption(f"â­ ìµœê·¼ {len(filtered_for_display)}ê°œ ì´ë ¥ í‘œì‹œ ì¤‘ (ì „í™” ì´ë ¥ ì œì™¸)")

            sel_key = st.selectbox(L["history_selectbox_label"], options=list(options_map.keys()))

            if st.button(L["history_load_button"], key="load_hist_btn"):
                h = options_map[sel_key]
                st.session_state.customer_query_text_area = h["initial_query"]

                # ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆê³  ìš”ì•½ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì†Œí•œì˜ ë©”ì‹œì§€ ì¬êµ¬ì„±
                if not h.get("messages") and h.get("summary"):
                    summary = h["summary"]
                    # ìš”ì•½ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±
                    reconstructed_messages = [
                        {"role": "customer", "content": h["initial_query"]}
                    ]
                    # ìš”ì•½ì—ì„œ í•µì‹¬ ì‘ë‹µ ì¶”ê°€
                    if summary.get("key_responses"):
                        for response in summary.get("key_responses", [])[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                            reconstructed_messages.append({"role": "agent_response", "content": response})
                    # ìš”ì•½ ì •ë³´ë¥¼ supervisor ë©”ì‹œì§€ë¡œ ì¶”ê°€
                    summary_text = f"**ìš”ì•½ëœ ìƒë‹´ ì´ë ¥**\n\n"
                    summary_text += f"ì£¼ìš” ë¬¸ì˜: {summary.get('main_inquiry', 'N/A')}\n"
                    summary_text += f"ê³ ê° ê°ì • ì ìˆ˜: {summary.get('customer_sentiment_score', 50)}/100\n"
                    summary_text += f"ê³ ê° ë§Œì¡±ë„: {summary.get('customer_satisfaction_score', 50)}/100\n"
                    summary_text += f"\nì „ì²´ ìš”ì•½:\n{summary.get('summary', 'N/A')}"
                    reconstructed_messages.append({"role": "supervisor", "content": summary_text})
                    st.session_state.simulator_messages = reconstructed_messages

                    # ìš”ì•½ ë°ì´í„° ì‹œê°í™”
                    st.markdown("---")
                    st.subheader("ğŸ“Š ë¡œë“œëœ ì¼€ì´ìŠ¤ ë¶„ì„")

                    # ìš”ì•½ ë°ì´í„°ë¥¼ í”„ë¡œí•„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    loaded_profile = {
                        "sentiment_score": summary.get("customer_sentiment_score", 50),
                        "urgency_level": "medium",  # ê¸°ë³¸ê°’
                        "predicted_customer_type": h.get("customer_type", "normal")
                    }

                    # í”„ë¡œí•„ ì ìˆ˜ ì°¨íŠ¸
                    profile_chart = visualize_customer_profile_scores(loaded_profile, current_lang)
                    if profile_chart:
                        st.plotly_chart(profile_chart, use_container_width=True)
                    else:
                        # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric(L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
                                      f"{summary.get('customer_sentiment_score', 50)}/100")
                        with col2:
                            st.metric(L.get("urgency_score_label", "ê¸´ê¸‰ë„"), f"50/100")
                        with col3:
                            st.metric(L.get("customer_type_label", "ê³ ê° ìœ í˜•"), h.get("customer_type", "normal"))

                    # ê³ ê° íŠ¹ì„± ì‹œê°í™”
                    if summary.get("customer_characteristics") or summary.get("privacy_info"):
                        characteristics_chart = visualize_customer_characteristics(summary, current_lang)
                        if characteristics_chart:
                            st.plotly_chart(characteristics_chart, use_container_width=True)
                else:
                    # ê¸°ì¡´ ë©”ì‹œì§€ê°€ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    st.session_state.simulator_messages = h.get("messages", [])

                st.session_state.initial_advice_provided = True
                st.session_state.is_chat_ended = h.get("is_chat_ended", False)
                st.session_state.sim_attachment_context_for_llm = h.get("attachment_context", "")  # ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
                st.session_state.customer_attachment_file = []  # ë¡œë“œëœ ì´ë ¥ì—ëŠ” íŒŒì¼ ê°ì²´ ëŒ€ì‹  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë§Œ ì‚¬ìš©
                st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”

                # ìƒíƒœ ë³µì›
                if st.session_state.is_chat_ended:
                    st.session_state.sim_stage = "CLOSING"
                else:
                    messages = st.session_state.simulator_messages
                    last_role = messages[-1]["role"] if messages else None
                    if last_role == "agent_response":
                        st.session_state.sim_stage = "CUSTOMER_TURN"
                    elif last_role == "customer_rebuttal":
                        st.session_state.sim_stage = "AGENT_TURN"
                    elif last_role == "supervisor" and messages and messages[-1]["content"] == L[
                        "customer_closing_confirm"]:
                        st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"
                    else:
                        st.session_state.sim_stage = "AGENT_TURN"

                st.session_state.simulator_memory.clear()  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
                # â­ ë¡œë“œ í›„ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì¬ì‹¤í–‰
                # st.rerun()
        else:
            st.info(L["no_history_found"])

    # =========================
    # AHT íƒ€ì´ë¨¸ (í™”ë©´ ìµœìƒë‹¨)
    # =========================
    if st.session_state.sim_stage not in ["WAIT_FIRST_QUERY", "CLOSING", "idle"]:
        elapsed_placeholder = st.empty()

        if st.session_state.start_time is not None:
            # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ í˜ì´ì§€ ë¡œë“œ ì‹œë§ˆë‹¤ í˜„ì¬ ì‹œê°„ ê³„ì‚°
            elapsed_time = datetime.now() - st.session_state.start_time
            total_seconds = elapsed_time.total_seconds()

            # Hold ì‹œê°„ ì œì™¸ (ì±„íŒ…/ì´ë©”ì¼ì€ Hold ì—†ìŒ, ì „í™” íƒ­ê³¼ ë¡œì§ í†µì¼ ìœ„í•´ ìœ ì§€)
            # total_seconds -= st.session_state.total_hold_duration.total_seconds()

            # ì‹œê°„ í˜•ì‹ í¬ë§·íŒ…
            minutes = int(total_seconds // 60)
            seconds = int(total_seconds % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # ê²½ê³  ê¸°ì¤€
            if total_seconds > 900:  # 15ë¶„
                delta_str = L["timer_info_risk"]
                delta_color = "inverse"
            elif total_seconds > 600:  # 10ë¶„
                delta_str = L["timer_info_warn"]
                delta_color = "off"
            else:
                delta_str = L["timer_info_ok"]
                delta_color = "normal"

            elapsed_placeholder.metric(
                L["timer_metric"],
                time_str,
                delta=delta_str,
                delta_color=delta_color
            )

            # â­ ìˆ˜ì •: 3ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰í•˜ì—¬ AHT ì‹¤ì‹œê°„ì„± í™•ë³´
            if seconds % 3 == 0 and total_seconds < 1000:
                time.sleep(1)
                # st.rerun()

        st.markdown("---")

    # =========================
    # 2. LLM ì¤€ë¹„ ì²´í¬ & ì±„íŒ… ì¢…ë£Œ ìƒíƒœ
    # =========================
    if not st.session_state.is_llm_ready:
        st.warning(L["simulation_no_key_warning"])

    if st.session_state.sim_stage == "CLOSING":
        st.success(L["survey_sent_confirm"])
        st.info(L["new_simulation_ready"])
        if st.button(L["new_simulation_button"], key="new_simulation_btn"):
            # ì´ˆê¸°í™” ë¡œì§
            st.session_state.simulator_messages = []
            st.session_state.simulator_memory.clear()
            st.session_state.initial_advice_provided = False
            st.session_state.is_chat_ended = False
            st.session_state.agent_response_area_text = ""
            st.session_state.customer_query_text_area = ""
            st.session_state.last_transcript = ""
            st.session_state.sim_audio_bytes = None
            st.session_state.sim_stage = "WAIT_FIRST_QUERY"
            st.session_state.customer_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.sim_attachment_context_for_llm = ""  # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            st.session_state.agent_attachment_file = []  # ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.start_time = None
            # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""
            st.session_state.sim_call_outbound_target = None
            # â­ ì¬ì‹¤í–‰
            # st.rerun()
        st.stop()

    # =========================
    # 5-A. ì „í™” ë°œì‹  ì§„í–‰ ì¤‘ (OUTBOUND_CALL_IN_PROGRESS)
    # =========================
    elif st.session_state.sim_stage == "OUTBOUND_CALL_IN_PROGRESS":
        L = LANG[st.session_state.language]
        target = st.session_state.get("sim_call_outbound_target", "ëŒ€ìƒ")
        st.warning(L["call_outbound_loading"])

        # LLM í˜¸ì¶œ ë° ìš”ì•½ ìƒì„±
        with st.spinner(L["call_outbound_loading"]):
            # 1. LLM í˜¸ì¶œí•˜ì—¬ í†µí™” ìš”ì•½ ìƒì„±
            summary = generate_outbound_call_summary(
                st.session_state.customer_query_text_area,
                st.session_state.language,
                target
            )

            # 2. ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì „í™” ì‹œë„) ì¶”ê°€
            st.session_state.simulator_messages.append(
                {"role": "system_end", "content": L["call_outbound_system_msg"].format(target=target)}
            )

            # 3. ìš”ì•½ ë©”ì‹œì§€ (ê²°ê³¼) ì¶”ê°€
            summary_markdown = f"### {L['call_outbound_summary_header']}\n\n{summary}"
            st.session_state.simulator_messages.append(
                {"role": "supervisor", "content": summary_markdown}
            )

            # 4. Agent Turnìœ¼ë¡œ ë³µê·€
            st.session_state.sim_stage = "AGENT_TURN"
            st.session_state.sim_call_outbound_summary = summary_markdown  # Save for display/reference
            st.session_state.sim_call_outbound_target = None  # Reset target

            # 5. ì´ë ¥ ì €ì¥ (ì „í™” ë°œì‹  í›„ ìƒíƒœ ì €ì¥)
            customer_type_display = st.session_state.get("customer_type_sim_select", "")
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display + f" (Outbound Call to {target})",
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )

        st.success(f"âœ… {L['call_outbound_simulation_header']}ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìš”ì•½ì„ í™•ì¸í•˜ê³  ê³ ê°ì—ê²Œ íšŒì‹ í•˜ì„¸ìš”.")
        # st.rerun()

    # ========================================
    # 3. ì´ˆê¸° ë¬¸ì˜ ì…ë ¥ (WAIT_FIRST_QUERY)
    # ========================================
    if st.session_state.sim_stage == "WAIT_FIRST_QUERY":
        customer_query = st.text_area(
            L["customer_query_label"],
            key="customer_query_text_area",
            height=150,
            placeholder=L["initial_query_sample"],
        )

        # --- í•„ìˆ˜ ì…ë ¥ í•„ë“œ (ìš”ì²­ 3 ë°˜ì˜: UI í…ìŠ¤íŠ¸ ë³€ê²½) ---
        customer_email = st.text_input(
            L["customer_email_label"],
            key="customer_email_input",
            value=st.session_state.customer_email,
        )
        customer_phone = st.text_input(
            L["customer_phone_label"],
            key="customer_phone_input",
            value=st.session_state.customer_phone,
        )
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.customer_email = customer_email
        st.session_state.customer_phone = customer_phone
        # --------------------------------------------------

        customer_type_options = L["customer_type_options"]
        # st.session_state.customer_type_sim_selectëŠ” ì´ë¯¸ ì´ˆê¸°í™”ë¨
        default_idx = customer_type_options.index(
            st.session_state.customer_type_sim_select) if st.session_state.customer_type_sim_select in customer_type_options else 0

        # SelectboxëŠ” ìì²´ì ìœ¼ë¡œ ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ, ì—¬ê¸°ì— valueë¥¼ ì„¤ì •í•  í•„ìš” ì—†ìŒ
        st.session_state.customer_type_sim_select = st.selectbox(
            L["customer_type_label"],
            customer_type_options,
            index=default_idx,
            key="customer_type_sim_select_widget",
        )

        # --- ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë” ì¶”ê°€ ---
        customer_attachment_widget = st.file_uploader(
            L["attachment_label"],
            type=["png", "jpg", "jpeg", "pdf"],
            key="customer_attachment_file_uploader",
            help=L["attachment_placeholder"],
            accept_multiple_files=False  # ì±„íŒ…/ì´ë©”ì¼ì€ ë‹¨ì¼ íŒŒì¼ë§Œ í—ˆìš©
        )

        # íŒŒì¼ ì •ë³´ ì €ì¥ ë° LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        if customer_attachment_widget:
            st.session_state.customer_attachment_file = customer_attachment_widget
            st.session_state.sim_attachment_context_for_llm = L["attachment_status_llm"].format(
                filename=customer_attachment_widget.name, filetype=customer_attachment_widget.type
            )
        else:
            st.session_state.customer_attachment_file = None
            st.session_state.sim_attachment_context_for_llm = ""
        # --------------------------

        if st.button(L["button_simulate"], key=f"btn_simulate_initial_{st.session_state.sim_instance_id}"):  # ê³ ìœ  í‚¤ ì‚¬ìš©
            if not customer_query.strip():
                st.warning(L["simulation_warning_query"])
                st.stop()

            # --- í•„ìˆ˜ ì…ë ¥ í•„ë“œ ê²€ì¦ (ìš”ì²­ 3 ë°˜ì˜: ê²€ì¦ ë¡œì§ ì¶”ê°€) ---
            if not st.session_state.customer_email.strip() or not st.session_state.customer_phone.strip():
                st.error(L["error_mandatory_contact"])
                st.stop()
            # ------------------------------------------

            # ì´ˆê¸° ìƒíƒœ ë¦¬ì…‹
            st.session_state.simulator_messages = []
            st.session_state.simulator_memory.clear()
            st.session_state.is_chat_ended = False
            st.session_state.initial_advice_provided = False
            st.session_state.is_solution_provided = False  # ì†”ë£¨ì…˜ í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state.language_transfer_requested = False  # ì–¸ì–´ ìš”ì²­ í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state.transfer_summary_text = ""  # ì´ê´€ ìš”ì•½ ë¦¬ì…‹
            st.session_state.start_time = None  # AHT íƒ€ì´ë¨¸ ì´ˆê¸°í™” (ì²« ê³ ê° ë°˜ì‘ í›„ ì‹œì‘)
            st.session_state.sim_instance_id = str(uuid.uuid4())  # ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ID í• ë‹¹
            # ì „í™” ë°œì‹  ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""
            st.session_state.sim_call_outbound_target = None

            # 1) ê³ ê° ì²« ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.simulator_messages.append(
                {"role": "customer", "content": customer_query}
            )

            # 2) Supervisor ê°€ì´ë“œ + ì´ˆì•ˆ ìƒì„±
            # ê³ ê° í”„ë¡œí•„ ë¶„ì„ (ì‹œê°í™”ë¥¼ ìœ„í•´ ë¨¼ì € ìˆ˜í–‰)
            customer_profile = analyze_customer_profile(customer_query, current_lang)
            similar_cases = find_similar_cases(customer_query, customer_profile, current_lang, limit=5)

            # ì‹œê°í™” ì°¨íŠ¸ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“Š ê³ ê° í”„ë¡œí•„ ë¶„ì„")

            # ê³ ê° í”„ë¡œí•„ ì ìˆ˜ ì°¨íŠ¸
            profile_chart = visualize_customer_profile_scores(customer_profile, current_lang)
            if profile_chart:
                st.plotly_chart(profile_chart, use_container_width=True)
            else:
                # Plotlyê°€ ì—†ì„ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        L.get("sentiment_score_label", "ê°ì • ì ìˆ˜"),
                        f"{customer_profile.get('sentiment_score', 50)}/100"
                    )
                with col2:
                    urgency_map = {"low": 25, "medium": 50, "high": 75}
                    urgency_score = urgency_map.get(customer_profile.get("urgency_level", "medium").lower(), 50)
                    st.metric(
                        L.get("urgency_score_label", "ê¸´ê¸‰ë„"),
                        f"{urgency_score}/100"
                    )
                with col3:
                    st.metric(
                        L.get("customer_type_label", "ê³ ê° ìœ í˜•"),
                        customer_profile.get("predicted_customer_type", "normal")
                    )

            # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì‹œê°í™”
            if similar_cases:
                st.markdown("---")
                st.subheader("ğŸ” ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ì²œ")
                similarity_chart = visualize_similarity_cases(similar_cases, current_lang)
                if similarity_chart:
                    st.plotly_chart(similarity_chart, use_container_width=True)

                # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìš”ì•½ í‘œì‹œ
                with st.expander(f"ğŸ’¡ {len(similar_cases)}ê°œ ìœ ì‚¬ ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´"):
                    for idx, similar_case in enumerate(similar_cases, 1):
                        summary = similar_case["summary"]
                        similarity = similar_case["similarity_score"]
                        st.markdown(f"### ì¼€ì´ìŠ¤ {idx} (ìœ ì‚¬ë„: {similarity:.1f}%)")
                        st.markdown(f"**ë¬¸ì˜ ë‚´ìš©:** {summary.get('main_inquiry', 'N/A')}")
                        st.markdown(f"**ê°ì • ì ìˆ˜:** {summary.get('customer_sentiment_score', 50)}/100")
                        st.markdown(f"**ë§Œì¡±ë„ ì ìˆ˜:** {summary.get('customer_satisfaction_score', 50)}/100")
                        if summary.get("key_responses"):
                            st.markdown("**í•µì‹¬ ì‘ë‹µ:**")
                            for response in summary.get("key_responses", [])[:3]:
                                st.markdown(f"- {response[:100]}...")
                        st.markdown("---")

            # ì´ˆê¸° ì¡°ì–¸ ìƒì„±
            text = _generate_initial_advice(
                customer_query,
                st.session_state.customer_type_sim_select,
                st.session_state.customer_email,
                st.session_state.customer_phone,
                current_lang,
                st.session_state.customer_attachment_file
            )
            st.session_state.simulator_messages.append({"role": "supervisor", "content": text})

            st.session_state.initial_advice_provided = True
            save_simulation_history_local(
                customer_query,
                st.session_state.customer_type_sim_select,
                st.session_state.simulator_messages,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
                is_chat_ended=False,
            )
            st.session_state.sim_stage = "AGENT_TURN"
            # â­ ì¬ì‹¤í–‰
            # st.rerun()

    # =========================
    # 4. ëŒ€í™” ë¡œê·¸ í‘œì‹œ (ê³µí†µ)
    # =========================
    for idx, msg in enumerate(st.session_state.simulator_messages):
        role = msg["role"]
        content = msg["content"]
        avatar = {"customer": "ğŸ™‹", "supervisor": "ğŸ¤–", "agent_response": "ğŸ§‘â€ğŸ’»", "customer_rebuttal": "âœ¨",
                  "system_end": "ğŸ“Œ", "system_transfer": "ğŸ“Œ"}.get(role, "ğŸ’¬")
        tts_role = "customer" if role.startswith("customer") or role == "customer_rebuttal" else (
              "agent" if role == "agent_response" else "supervisor")

        with st.chat_message(role, avatar=avatar):
            st.markdown(content)
                    # ì¸ë±ìŠ¤ë¥¼ render_tts_buttonì— ì „ë‹¬í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±ì— ì‚¬ìš©
            render_tts_button(content, st.session_state.language, role=tts_role, prefix=f"{role}_", index=idx)

            # â­ [ìƒˆë¡œìš´ ë¡œì§] ê³ ê° ì²¨ë¶€ íŒŒì¼ ë Œë”ë§ (ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¸ ê²½ìš°)
            if idx == 0 and role == "customer" and st.session_state.customer_attachment_b64:
                mime = st.session_state.customer_attachment_mime or "image/png"
                data_url = f"data:{mime};base64,{st.session_state.customer_attachment_b64}"

                # ì´ë¯¸ì§€ íŒŒì¼ë§Œ í‘œì‹œ (PDF ë“±ì€ ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
                if mime.startswith("image/"):
                    st.image(data_url, caption=f"ì²¨ë¶€ëœ ì¦ê±°ë¬¼ ({st.session_state.customer_attachment_file.name})",
                             use_column_width=True)
                elif mime == "application/pdf":
                    # PDF íŒŒì¼ì¼ ê²½ìš°, íŒŒì¼ ì´ë¦„ê³¼ í•¨ê»˜ ë‹¤ìš´ë¡œë“œ ë§í¬ ë˜ëŠ” ê²½ê³  í‘œì‹œ
                    st.warning(
                        f"ì²¨ë¶€ëœ PDF íŒŒì¼ ({st.session_state.customer_attachment_file.name})ì€ í˜„ì¬ ì¸ë¼ì¸ ë¯¸ë¦¬ë³´ê¸°ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # ì´ê´€ ìš”ì•½ í‘œì‹œ (ì´ê´€ í›„ì—ë§Œ)
        if st.session_state.transfer_summary_text or st.session_state.language != st.session_state.language_at_transfer_start:
            if st.session_state.transfer_summary_text or st.session_state.language != st.session_state.language_at_transfer_start:
                st.markdown("---")
                st.markdown(f"**{L['transfer_summary_header']}**")
                st.info(L["transfer_summary_intro"])

                # ë²ˆì—­ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° (ë¹ˆ ë¬¸ìì—´)
                # â­ ìˆ˜ì •ëœ ë¶€ë¶„ 1: DuplicateWidgetID ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ ê³ ìœ  í‚¤ì— UUID ì¶”ê°€
                unique_key = f"btn_retry_translation_{st.session_state.sim_instance_id}_{uuid.uuid4()}"

                if not st.session_state.transfer_summary_text:
                    st.error("âŒ LLM_TRANSLATION_ERROR (ë²ˆì—­ ì‹¤íŒ¨). ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                    # ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ ì¶”ê°€
                    if st.button(L["button_retry_translation"], key=unique_key):  # ê³ ìœ  í‚¤ ì‚¬ìš©
                        # ì¬ì‹œë„ ë¡œì§ ì‹¤í–‰
                        with st.spinner(L["transfer_loading"]):
                            source_lang = st.session_state.language_at_transfer_start
                            target_lang = st.session_state.language

                            # ì´ì „ ëŒ€í™” ë‚´ìš© ì¬ê°€ê³µ
                            history_text = get_chat_history_for_prompt(include_attachment=False)
                            for msg in st.session_state.simulator_messages:
                                role = "Customer" if msg["role"].startswith("customer") or msg[
                                    "role"] == "initial_query" else "Agent"
                                if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                                   "customer_closing_response"]:
                                    history_text += f"{role}: {msg['content']}\n"

                            translated_summary = translate_text_with_llm(history_text, target_lang, source_lang)
                            st.session_state.transfer_summary_text = translated_summary
                            st.session_state.transfer_summary_text = translated_summary
                            # â­ ì¬ì‹¤í–‰
                            st.rerun()

                else:
                    # ë²ˆì—­ ì„±ê³µ ì‹œ ë‚´ìš© í‘œì‹œ
                    st.markdown(st.session_state.transfer_summary_text)
                st.markdown("---")

    # =========================
    # 5. ì—ì´ì „íŠ¸ ì…ë ¥ ë‹¨ê³„ (AGENT_TURN)
    # =========================
    if st.session_state.sim_stage == "AGENT_TURN":
        st.markdown(f"### {L['agent_response_header']}")

        # --- ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸ ì˜ì—­ ---
        hint_cols = st.columns([4, 1])
        with hint_cols[0]:
            st.info(L["hint_placeholder"] + st.session_state.realtime_hint_text)

        with hint_cols[1]:
            # íŒíŠ¸ ìš”ì²­ ë²„íŠ¼
            if st.button(L["button_request_hint"], key=f"btn_request_hint_{st.session_state.sim_instance_id}"):
                with st.spinner(L["response_generating"]):
                    # ì±„íŒ…/ì´ë©”ì¼ íƒ­ì´ë¯€ë¡œ is_call=False
                    hint = generate_realtime_hint(current_lang, is_call=False)
                    st.session_state.realtime_hint_text = hint
                    # â­ ì¬ì‹¤í–‰
                    # st.rerun()

        # --- ì–¸ì–´ ì´ê´€ ìš”ì²­ ê°•ì¡° í‘œì‹œ ---
        if st.session_state.language_transfer_requested:
            st.error("ğŸš¨ ê³ ê°ì´ ì–¸ì–´ ì „í™˜(ì´ê´€)ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì‘ëŒ€í•˜ê±°ë‚˜ ì´ê´€ì„ ì§„í–‰í•˜ì„¸ìš”ã€‚")

        # --- ê³ ê° ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì¬í‘œì‹œ ---
        if st.session_state.sim_attachment_context_for_llm:
            st.info(
                f"ğŸ“ ìµœì´ˆ ë¬¸ì˜ ì‹œ ì²¨ë¶€ëœ íŒŒì¼ ì •ë³´:\n\n{st.session_state.sim_attachment_context_for_llm.replace('[ATTACHMENT STATUS]', '').strip()}")

        # --- AI ì‘ë‹µ ì´ˆì•ˆ ìƒì„± ë²„íŠ¼ (ìš”ì²­ 1 ë°˜ì˜) ---
        if st.button("ğŸ¤– AI ì‘ë‹µ ì´ˆì•ˆ ìƒì„±", key=f"btn_generate_ai_draft_{st.session_state.sim_instance_id}"):
            if not st.session_state.is_llm_ready:
                st.warning(L["simulation_no_key_warning"])
            else:
                with st.spinner("AIê°€ ì‘ë‹µ ì´ˆì•ˆì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    # ì´ˆì•ˆ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
                    ai_draft = generate_agent_response_draft(current_lang)
                    if ai_draft and not ai_draft.startswith("âŒ"):
                        st.session_state.agent_response_area_text = ai_draft
                        st.success("âœ… AI ì‘ë‹µ ì´ˆì•ˆì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.")
                        # â­ ì¬ì‹¤í–‰
                        # st.rerun()
                    else:
                        st.error(ai_draft if ai_draft else "ì‘ë‹µ ì´ˆì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # --- ì „í™” ë°œì‹  ë²„íŠ¼ ì¶”ê°€ (ìš”ì²­ 2 ë°˜ì˜) ---
        st.markdown("---")
        st.subheader(L["button_call_outbound"])
        call_cols = st.columns(3)

        with call_cols[0]:
            if st.button(L["button_call_outbound"].replace("ì „í™” ë°œì‹ ", "í˜„ì§€ ì—…ì²´ ì „í™” ë°œì‹ "), key="btn_call_outbound_partner"):
                # ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜: í˜„ì§€ ì—…ì²´
                st.session_state.sim_call_outbound_target = "í˜„ì§€ ì—…ì²´/íŒŒíŠ¸ë„ˆ"
                st.session_state.sim_stage = "OUTBOUND_CALL_IN_PROGRESS"
                # st.rerun()

        with call_cols[1]:
            if st.button(L["button_call_outbound"].replace("ì „í™” ë°œì‹ ", "ê³ ê° ì „í™” ë°œì‹ "), key="btn_call_outbound_customer"):
                # ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜: ê³ ê°
                st.session_state.sim_call_outbound_target = "ê³ ê°"
                st.session_state.sim_stage = "OUTBOUND_CALL_IN_PROGRESS"
                # st.rerun()

        st.markdown("---")
        # --- ì „í™” ë°œì‹  ë²„íŠ¼ ì¶”ê°€ ë ---

        st.markdown("### ğŸš¨ Supervisor ì •ì±…/ì§€ì‹œ ì‚¬í•­ ì—…ë¡œë“œ (ì˜ˆì™¸ ì²˜ë¦¬ ë°©ì¹¨)")

        # --- Supervisor ì •ì±… ì—…ë¡œë” ì¶”ê°€ ---
        supervisor_attachment_widget = st.file_uploader(
            "Supervisor ì§€ì‹œ ì‚¬í•­/ìŠ¤í¬ë¦°ìƒ· ì—…ë¡œë“œ (ì˜ˆì™¸ ì •ì±… í¬í•¨)",
            type=["png", "jpg", "jpeg", "pdf", "txt"],
            key="supervisor_policy_uploader",
            help="ë¹„í–‰ê¸° ì§€ì—°, ì§ˆë³‘ ë“± ì˜ˆì™¸ì  ìƒí™©ì— ëŒ€í•œ Supervisorì˜ ìµœì‹  ì§€ì‹œ ì‚¬í•­ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
            accept_multiple_files=False
        )

        # íŒŒì¼ ì •ë³´ ì €ì¥ ë° LLM ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        if supervisor_attachment_widget:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ë˜ëŠ” PDF/ì´ë¯¸ì§€ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì»¨í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì—¬ policy_contextì— ì €ì¥í•´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” íŒŒì¼ ì´ë¦„ê³¼ íƒ€ì…ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ê³ , LLMì´ ì´ê²ƒì´ 'ì˜ˆì™¸ ì •ì±…'ì„ì„ ì•Œë„ë¡ ìœ ë„
            file_name = supervisor_attachment_widget.name
            st.session_state.supervisor_policy_context = f"[Supervisor Policy Attached] Filename: {file_name}, Filetype: {supervisor_attachment_widget.type}. This file contains a CRITICAL, temporary policy update regarding exceptions (e.g., flight delays, illness, natural disasters). Analyze and prioritize this policy in the response."
            st.success(f"âœ… Supervisor ì •ì±… íŒŒì¼: **{file_name}**ì´(ê°€) ì‘ëŒ€ ê°€ì´ë“œì— ë°˜ì˜ë©ë‹ˆë‹¤.")
        elif st.session_state.supervisor_policy_context:
            st.info("â­ í˜„ì¬ ì ìš© ì¤‘ì¸ Supervisor ì •ì±…ì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.session_state.supervisor_policy_context = ""

        # --- ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì—…ë¡œë” (ë‹¤ì¤‘ íŒŒì¼ í—ˆìš©) ---
        agent_attachment_files = st.file_uploader(
            L["agent_attachment_label"],
            type=["png", "jpg", "jpeg", "pdf"],
            key="agent_attachment_file_uploader",
            help=L["agent_attachment_placeholder"],
            accept_multiple_files=True
        )

        if agent_attachment_files:
            st.session_state.agent_attachment_file = [
                {"name": f.name, "type": f.type, "size": f.size} for f in agent_attachment_files
            ]
            file_names = ", ".join([f["name"] for f in
                                    st.session_state.agent_attachment_file])  # ìˆ˜ì •: file_infos ëŒ€ì‹  st.session_state.agent_attachment_file ì‚¬ìš©
            st.info(f"âœ… {len(agent_attachment_files)}ê°œ ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì¤€ë¹„ ì™„ë£Œ: {file_names}")
        else:
            st.session_state.agent_attachment_file = []

        # --- ì…ë ¥ í•„ë“œ ë° ë²„íŠ¼ ---
        col_mic, col_text = st.columns([1, 2])

        # --- ë§ˆì´í¬ ë…¹ìŒ ---
        with col_mic:
            mic_audio = mic_recorder(
                start_prompt=L["button_mic_input"],
                stop_prompt="â¹ï¸ ë…¹ìŒ ì¢…ë£Œ",
                just_once=False,
                format="wav",
                use_container_width=True,
                key="sim_mic_recorder",
            )

        if mic_audio and mic_audio.get("bytes"):
            st.session_state.sim_audio_bytes = mic_audio["bytes"]
            st.info("âœ… ë…¹ìŒ ì™„ë£Œ! ì•„ë˜ ì „ì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì„¸ìš”.")

        if st.session_state.sim_audio_bytes:
            col_audio, col_transcribe, col_del = st.columns([3, 1, 1])

            # 1. ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´
            with col_audio:
                st.audio(st.session_state.sim_audio_bytes, format="audio/wav")

            # 2. ë…¹ìŒ ì‚­ì œ ë²„íŠ¼ (ì¶”ê°€ ìš”ì²­ ë°˜ì˜)
            with col_del:
                st.markdown("<br>", unsafe_allow_html=True)  # ë²„íŠ¼ ìˆ˜ì§ ì •ë ¬
                if st.button(L["delete_mic_record"], key="btn_delete_sim_audio_call"):
                    # ì˜¤ë””ì˜¤ ë° ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
                    st.session_state.sim_audio_bytes = None
                    st.session_state.last_transcript = ""
                    st.session_state.agent_response_area_text = ""
                    st.success("ë…¹ìŒì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë…¹ìŒí•´ ì£¼ì„¸ìš”.")
                    # st.rerun()

            # 3. ì „ì‚¬(Whisper) ë²„íŠ¼ (ê¸°ì¡´ ë¡œì§ ëŒ€ì²´)
            col_tr, _ = st.columns([1, 2])
            if col_tr.button(L["transcribe_btn"], key="sim_transcribe_btn"):
                if st.session_state.sim_audio_bytes is None:
                    st.warning("ë¨¼ì € ë§ˆì´í¬ë¡œ ë…¹ìŒì„ ì™„ë£Œí•˜ì„¸ìš”.")
                elif st.session_state.openai_client is None:
                    st.error(L["whisper_client_error"])
                else:
                    with st.spinner(L["whisper_processing"]):
                        # transcribe_bytes_with_whisper í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
                        transcribed_text = transcribe_bytes_with_whisper(
                            st.session_state.sim_audio_bytes,
                            "audio/wav",
                            lang_code=st.session_state.language,
                        )
                        if transcribed_text.startswith("âŒ"):
                            st.error(transcribed_text)
                            st.session_state.last_transcript = ""
                        else:
                            st.session_state.last_transcript = transcribed_text.strip()
                            # â­ ìˆ˜ì •: ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì°½ì˜ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ì— ë°˜ì˜
                            st.session_state.agent_response_area_text = transcribed_text.strip()
                            st.session_state.agent_response_input_box_widget = transcribed_text.strip()

                            snippet = transcribed_text[:50].replace("\n", " ")
                            if len(transcribed_text) > 50:
                                snippet += "..."
                            st.success(L["whisper_success"] + f"\n\n**ì¸ì‹ ë‚´ìš©:** *{snippet}*")
                            # st.rerun()  # UI ì—…ë°ì´íŠ¸

        col_text, col_button = st.columns([4, 1])

        # --- ì…ë ¥ í•„ë“œ ë° ë²„íŠ¼ ---
        with col_text:
            # st.text_areaì˜ ê°’ì„ ì½ì–´ ì„¸ì…˜ ìƒíƒœë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•˜ëŠ” on_changeë¥¼ ì œê±°í•˜ê³ 
            # st.text_area ìœ„ì ¯ ìì²´ì˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ send_clicked ì‹œ ìµœì‹  ê°’ì„ ì½ë„ë¡ í•©ë‹ˆë‹¤.
            # (Streamlit ê¸°ë³¸ ë™ì‘: ë²„íŠ¼ í´ë¦­ ì‹œ ìœ„ì ¯ì˜ ìµœì¢… ê°’ì´ ì„¸ì…˜ ìƒíƒœì— ë°˜ì˜ë¨)
            agent_response_input = st.text_area(
                L["agent_response_placeholder"],
                value=st.session_state.agent_response_area_text,
                key="agent_response_input_box_widget",  # ì´ í‚¤ë¥¼ í†µí•´ ë²„íŠ¼ í´ë¦­ ì‹œ ìµœì‹  ê°’ì— ì ‘ê·¼
                height=150,
            )

            # ì†”ë£¨ì…˜ ì œê³µ ì²´í¬ë°•ìŠ¤
            st.session_state.is_solution_provided = st.checkbox(
                L["solution_check_label"],
                value=st.session_state.is_solution_provided,
                key="solution_checkbox_widget",
            )

        with col_button:
            send_clicked = st.button(L["send_response_button"], key="send_agent_response_btn")

        if send_clicked:
            # â­ ìˆ˜ì •: st.session_state.agent_response_input_box_widgetì—ì„œ ìµœì‹  ì…ë ¥ê°’ì„ ê°€ì ¸ì˜´
            agent_response = st.session_state.agent_response_input_box_widget.strip()

            if not agent_response:
                st.warning(L["empty_response_warning"])
                st.stop()

            # AHT íƒ€ì´ë¨¸ ì‹œì‘
            if st.session_state.start_time is None and len(st.session_state.simulator_messages) >= 1:
                st.session_state.start_time = datetime.now()

            # --- ì—ì´ì „íŠ¸ ì²¨ë¶€ íŒŒì¼ ì²˜ë¦¬ (ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬) ---
            final_response_content = agent_response
            if st.session_state.agent_attachment_file:
                file_infos = st.session_state.agent_attachment_file
                file_names = ", ".join([f["name"] for f in file_infos])
                attachment_msg = L["agent_attachment_status"].format(
                    filename=file_names, filetype=f"ì´ {len(file_infos)}ê°œ íŒŒì¼"
                )
                final_response_content = f"{agent_response}\n\n---\n{attachment_msg}"

            # ë¡œê·¸ ì—…ë°ì´íŠ¸
            st.session_state.simulator_messages.append(
                {"role": "agent_response", "content": final_response_content}
            )

            # ì…ë ¥ì°½/ì˜¤ë””ì˜¤/ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.agent_response_area_text = ""
            st.session_state.sim_audio_bytes = None
            st.session_state.agent_attachment_file = []  # ì²¨ë¶€ íŒŒì¼ ì´ˆê¸°í™”
            st.session_state.language_transfer_requested = False
            st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
            st.session_state.sim_call_outbound_summary = ""  # ì „í™” ë°œì‹  ìš”ì•½ ì´ˆê¸°í™”

            # â­ ìˆ˜ì •: ê³ ê° ë°˜ì‘ ìƒì„± ë¡œì§ì„ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ sim_stage ë³€ê²½
            st.session_state.sim_stage = "CUSTOMER_TURN"
            # â­ ì¬ì‹¤í–‰: ì´ ë¶€ë¶„ì´ ì¦‰ì‹œ ê³ ê° ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
            st.rerun()

        # --- ì–¸ì–´ ì´ê´€ ë²„íŠ¼ ---
        st.markdown("---")
        st.markdown(f"**{L['transfer_header']}**")
        transfer_cols = st.columns(len(LANG) - 1)

        languages = list(LANG.keys())
        languages.remove(current_lang)


        def transfer_session(target_lang: str, current_messages: List[Dict[str, str]]):
            """ì–¸ì–´ ì´ê´€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  ì„¸ì…˜ ì–¸ì–´ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""

            # API í‚¤ ì²´í¬ëŠ” run_llm ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ Gemini í‚¤ë¥¼ ìš”êµ¬í•¨
            if not get_api_key("gemini"):
                st.error(LANG[current_lang]["simulation_no_key_warning"].replace('API Key', 'Gemini API Key'))
                st.stop()
                return

            current_lang_at_start = st.session_state.language  # Source language

            # AHT íƒ€ì´ë¨¸ ì¤‘ì§€
            st.session_state.start_time = None

            # 1. ë¡œë”© ì‹œì‘ (ì‹œê°„ ì–‘í•´ ë©”ì‹œì§€ ì‹œë®¬ë ˆì´ì…˜)
            with st.spinner(L["transfer_loading"]):
                # ì‹¤ì œ ëŒ€ê¸° ì‹œê°„ 5~10ì´ˆ (3~10ë¶„ ì‹œë®¬ë ˆì´ì…˜)
                time.sleep(np.random.uniform(5, 10))

                # 2. ëŒ€í™” ê¸°ë¡ì„ ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¡œ ê°€ê³µ
                history_text = ""
                for msg in current_messages:
                    role = "Customer" if msg["role"].startswith("customer") or msg[
                        "role"] == "initial_query" else "Agent"
                    if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                       "customer_closing_response"]:
                        history_text += f"{role}: {msg['content']}\n"

                # 3. LLM ë²ˆì—­ ì‹¤í–‰ (ìˆ˜ì •ëœ ë²ˆì—­ í•¨ìˆ˜ ì‚¬ìš©)
                translated_summary = translate_text_with_llm(history_text, target_lang,
                                                             current_lang_at_start)  # Use current_lang_at_start as source

                # 4. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.transfer_summary_text = translated_summary
                st.session_state.language_at_transfer = target_lang  # Save destination language
                st.session_state.language_at_transfer_start = current_lang_at_start  # Save source language for retry
                st.session_state.language = target_lang  # Language switch

                # --- ê¸°ì¡´ ê°€ì´ë“œë¼ì¸ ì‚­ì œ ë° ìƒˆ ê°€ì´ë“œë¼ì¸ ìƒì„± (ì–¸ì–´ í†µì¼ì„± í™•ë³´) ---
                # 1. ê¸°ì¡´ Supervisor Advice ë©”ì‹œì§€ ì‚­ì œ
                st.session_state.simulator_messages = [
                    msg for msg in st.session_state.simulator_messages
                    if msg['role'] != 'supervisor'
                ]

                # 2. ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ê°€ì´ë“œë¼ì¸/ì´ˆì•ˆ ì¬ìƒì„±
                new_advice = _generate_initial_advice(
                    st.session_state.customer_query_text_area,
                    st.session_state.customer_type_sim_select,
                    st.session_state.customer_email,
                    st.session_state.customer_phone,
                    target_lang,  # ìƒˆë¡œìš´ ì–¸ì–´ë¡œ ìƒì„±
                    st.session_state.customer_attachment_file
                )
                st.session_state.simulator_messages.append({"role": "supervisor", "content": new_advice})
                # -------------------------------------------------------------------

                st.session_state.is_solution_provided = False  # ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ìœ„í•´ í”Œë˜ê·¸ ë¦¬ì…‹
                st.session_state.language_transfer_requested = False  # í”Œë˜ê·¸ ë¦¬ì…‹
                st.session_state.sim_stage = "AGENT_TURN"

                # 5. ì´ë ¥ ì €ì¥
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                save_simulation_history_local(
                    st.session_state.customer_query_text_area,
                    customer_type_display + f" (Transferred from {current_lang_at_start} to {target_lang})",
                    st.session_state.simulator_messages,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                    is_chat_ended=False,
                )

            # 6. UI ì¬ì‹¤í–‰ (ì–¸ì–´ ë³€ê²½ ì ìš©)
            st.success(f"âœ… {LANG[target_lang]['transfer_summary_header']}ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
            # â­ ì¬ì‹¤í–‰
            # st.rerun()


        for i, target_lang in enumerate(languages):
            button_label_key = f"transfer_to_{target_lang}"
            button_label = L.get(button_label_key, f"Transfer to {target_lang.capitalize()} Team")

            if transfer_cols[i].button(button_label, key=f"btn_transfer_{target_lang}"):
                transfer_session(target_lang, st.session_state.simulator_messages)

        st.markdown("---")

    # --- Language Transfer Buttons End ---

    # =========================
    # 6. ê³ ê° ë°˜ì‘ ìƒì„± ë‹¨ê³„ (CUSTOMER_TURN)
    # =========================
    elif st.session_state.sim_stage == "CUSTOMER_TURN":
        L = LANG[st.session_state.language]
        customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])
        st.info(L["customer_turn_info"])

        # ë§ˆì§€ë§‰ ì—ì´ì „íŠ¸ ì‘ë‹µì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        last_agent_response = st.session_state.simulator_messages[-1][
            "content"] if st.session_state.simulator_messages else ""

        # 1. ê³ ê° ë°˜ì‘ ìƒì„±
        with st.spinner(L["generating_customer_response"]):
            # â­ ìˆ˜ì •: generate_customer_response -> generate_customer_reaction ë¡œ ìˆ˜ì •
            customer_response = generate_customer_reaction(st.session_state.language, is_call=False)

        # 2. ëŒ€í™” ë¡œê·¸ ì—…ë°ì´íŠ¸
        st.session_state.simulator_messages.append(
            {"role": "customer", "content": customer_response}
        )

        # 3. ì¢…ë£Œ ì¡°ê±´ ê²€í† 

        # â­ ìˆ˜ì •: ê³ ê°ì´ ì†”ë£¨ì…˜ì„ ìˆ˜ë½í•˜ê³  ê¸ì •ì ì¸ ì¢…ë£Œ ì˜ì‚¬ë¥¼ ë°íŒ ê²½ìš°
        positive_closing_phrases = ["ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤", "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤", "ê´œì°®ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤"]
        is_positive_closing = any(phrase in customer_response for phrase in positive_closing_phrases)

        if is_positive_closing:
            # ê¸ì • ì¢…ë£Œ (FINAL_CLOSING_ACTION) ë˜ëŠ” í™•ì¸ ë‹¨ê³„ (WAIT_CLOSING_CONFIRMATION_FROM_AGENT)ë¡œ ë¶„ê¸°

            # 'ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤'ê°€ í¬í•¨ë˜ë©´ ì¦‰ì‹œ ìµœì¢… ì¢…ë£Œ ë²„íŠ¼ í™œì„±í™”
            if "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" in customer_response or "ê´œì°®ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" in customer_response:
                st.session_state.sim_stage = "FINAL_CLOSING_ACTION"
            else:
                # 'ì•Œê² ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤'ì²˜ëŸ¼ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì•¼ í•˜ëŠ” ê²½ìš°
                # ì—ì´ì „íŠ¸ì—ê²Œ ìµœì¢… ì¸ì‚¬ ë° ì¶”ê°€ ë¬¸ì˜ í™•ì¸ ì‘ë‹µì„ ê°•ì œí•©ë‹ˆë‹¤.
                st.session_state.sim_stage = "WAIT_CLOSING_CONFIRMATION_FROM_AGENT"

        # â­ ìˆ˜ì •: ê³ ê°ì´ ì•„ì§ ì†”ë£¨ì…˜ì— ë§Œì¡±í•˜ì§€ ì•Šê±°ë‚˜ ì¶”ê°€ ì§ˆë¬¸ì„ í•œ ê²½ìš° (ì¼ë°˜ì ì¸ í„´)
        elif customer_response.startswith(L["customer_escalation_start"]):
            st.session_state.sim_stage = "ESCALATION_REQUIRED"  # ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”
        else:
            # ì—ì´ì „íŠ¸ í„´ìœ¼ë¡œ ìœ ì§€ (ê³ ê°ì´ ì¶”ê°€ ì§ˆë¬¸í•˜ê±°ë‚˜ ì •ë³´ ì œê³µ)
            st.session_state.sim_stage = "AGENT_TURN"

            # 4. ì¬ì‹¤í–‰
            # st.rerun()

            st.session_state.is_solution_provided = False  # ì¢…ë£Œ ë‹¨ê³„ ì§„ì… í›„ í”Œë˜ê·¸ ë¦¬ì…‹

            # ì´ë ¥ ì €ì¥
        save_simulation_history_local(
            st.session_state.customer_query_text_area, customer_type_display,
            st.session_state.simulator_messages, is_chat_ended=False,
            attachment_context=st.session_state.sim_attachment_context_for_llm,
        )

        st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
        # â­ ì¬ì‹¤í–‰: ê³ ê° ë°˜ì‘ì´ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ AGENT_TURNìœ¼ë¡œ ì „í™˜í•˜ì—¬ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ë‹µ ê¸°íšŒ ì œê³µ
        st.rerun()

    else:
        st.warning("LLM Keyê°€ ì—†ì–´ ê³ ê° ë°˜ì‘ ìë™ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ 'ê³ ê° ë°˜ì‘ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜ AGENT_TURNìœ¼ë¡œ ëŒì•„ê°€ì„¸ìš”.")
        # ìˆ˜ë™ìœ¼ë¡œ AGENT_TURNìœ¼ë¡œ ëŒì•„ê°€ëŠ” ë²„íŠ¼ ì œê³µ (ì˜¤ë¥˜ ë³µêµ¬ìš©)
        if st.button("AGENT_TURNìœ¼ë¡œ ëŒì•„ê°€ê¸°", key="fallback_to_agent_turn"):
            st.session_state.sim_stage = "AGENT_TURN"
            # st.rerun()

    # =========================
    # 7. ì¢…ë£Œ í™•ì¸ ë©”ì‹œì§€ ëŒ€ê¸° (WAIT_CLOSING_CONFIRMATION_FROM_AGENT)
    # =========================
elif st.session_state.sim_stage == "WAIT_CLOSING_CONFIRMATION_FROM_AGENT":
    st.success("ê³ ê°ì´ ì†”ë£¨ì…˜ì— ê¸ì •ì ìœ¼ë¡œ ë°˜ì‘í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")

    col_chat_end, col_email_end = st.columns(2)  # ë²„íŠ¼ì„ ë‚˜ë€íˆ ë°°ì¹˜

    # [1] ì±„íŒ… - ì¶”ê°€ ë¬¸ì˜ í™•ì¸ ë©”ì‹œì§€ ë³´ë‚´ê¸° ë²„íŠ¼
    with col_chat_end:
        # ìƒíƒœ ì „í™˜ ëª…í™•í™”: ì´ ë²„íŠ¼ í´ë¦­ ì‹œ ë‹¤ìŒ ë‹¨ê³„ì¸ WAIT_CUSTOMER_CLOSING_RESPONSEë¡œ ë°˜ë“œì‹œ ë„˜ì–´ê°
        if st.button(L["send_closing_confirm_button"],
                     key=f"btn_send_closing_confirm_{st.session_state.sim_instance_id}"):
            closing_msg = L["customer_closing_confirm"]

            # ì—ì´ì „íŠ¸ ì‘ë‹µìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡
            st.session_state.simulator_messages.append(
                {"role": "agent_response", "content": closing_msg}
            )

            # ë‹¤ìŒ ë‹¨ê³„: ê³ ê°ì˜ ìµœì¢… ë‹µë³€ ëŒ€ê¸°
            st.session_state.sim_stage = "WAIT_CUSTOMER_CLOSING_RESPONSE"

            # ì´ë ¥ ì €ì¥
            customer_type_display = st.session_state.get("customer_type_sim_select", "")
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )
            # â­ ì¬ì‹¤í–‰
            # st.rerun()

    # [2] ì´ë©”ì¼ - ìƒë‹´ ì¢…ë£Œ ë²„íŠ¼ (ì¦‰ì‹œ ì¢…ë£Œ)
    with col_email_end:
        if st.button(L["button_email_end_chat"], key=f"btn_email_end_chat_{st.session_state.sim_instance_id}"):
            # ì´ë©”ì¼ì€ ëì¸ì‚¬ì— ë¬¸ì˜ í™•ì¸ì´ í¬í•¨ë˜ë¯€ë¡œ, ë°”ë¡œ ìµœì¢… ì¢…ë£Œ ë‹¨ê³„ë¡œ ì´ë™

            # AHT íƒ€ì´ë¨¸ ì •ì§€
            st.session_state.start_time = None

            # ìµœì¢… ì¢…ë£Œ ë©”ì‹œì§€ (ì„¤ë¬¸ ì¡°ì‚¬ í¬í•¨)
            end_msg = L["prompt_survey"]
            st.session_state.simulator_messages.append(
                {"role": "system_end", "content": "(ì‹œìŠ¤í…œ: ì´ë©”ì¼ ìƒë‹´ ì¢…ë£Œ) " + end_msg}
            )
            st.session_state.is_chat_ended = True
            st.session_state.sim_stage = "CLOSING"  # ë°”ë¡œ CLOSINGìœ¼ë¡œ ì „í™˜

            # ì´ë ¥ ì €ì¥
            customer_type_display = st.session_state.get("customer_type_sim_select", "")
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=True,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )
            # â­ ì¬ì‹¤í–‰
            # st.rerun()

# =========================
# 8. ê³ ê° ìµœì¢… ì‘ë‹µ ìƒì„± ë° ì²˜ë¦¬ (WAIT_CUSTOMER_CLOSING_RESPONSE)
# =========================
elif st.session_state.sim_stage == "WAIT_CUSTOMER_CLOSING_RESPONSE":
    L = LANG[st.session_state.language]
    st.info("ì—ì´ì „íŠ¸ê°€ ì¶”ê°€ ë¬¸ì˜ ì—¬ë¶€ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ê³ ê°ì˜ ìµœì¢… ë‹µë³€ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")

    # ê³ ê° ë‹µë³€ ìë™ ìƒì„± (LLM Key ê²€ì¦ í¬í•¨)
    if st.session_state.is_llm_ready:
        with st.spinner(L["generating_customer_response"]):
            # ê³ ê°ì˜ ìµœì¢… ë‹µë³€ ìƒì„± (ì±„íŒ…ìš©)
            final_customer_reaction = generate_customer_closing_response(st.session_state.language)

        customer_type_display = st.session_state.get("customer_type_sim_select", L["customer_type_options"][0])

        # ë¡œê·¸ ê¸°ë¡
        st.session_state.simulator_messages.append(
            {"role": "customer_rebuttal", "content": final_customer_reaction}
        )

        # (A) "ì—†ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤" ê²½ë¡œ -> FINAL_CLOSING_ACTIONìœ¼ë¡œ
        if L['customer_no_more_inquiries'] in final_customer_reaction:
            st.session_state.sim_stage = "FINAL_CLOSING_ACTION"
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )
        # (B) "ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤" ê²½ë¡œ -> AGENT_TURNìœ¼ë¡œ ë³µê·€
        elif L['customer_has_additional_inquiries'] in final_customer_reaction:
            st.session_state.sim_stage = "AGENT_TURN"  # ë‹¤ì‹œ ì—ì´ì „íŠ¸ ì‘ë‹µ ë‹¨ê³„ë¡œ
            save_simulation_history_local(
                st.session_state.customer_query_text_area, customer_type_display,
                st.session_state.simulator_messages, is_chat_ended=False,
                attachment_context=st.session_state.sim_attachment_context_for_llm,
            )

        st.session_state.realtime_hint_text = ""  # íŒíŠ¸ ì´ˆê¸°í™”
        # â­ í•„ìˆ˜ ìˆ˜ì •: ìƒíƒœ ë³€ê²½ í›„ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ st.rerun() ì¶”ê°€
        st.rerun()

    else:
        st.warning("LLM Keyê°€ ì—†ì–´ ê³ ê° ë°˜ì‘ ìë™ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ 'ê³ ê° ë°˜ì‘ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ê±°ë‚˜ AGENT_TURNìœ¼ë¡œ ëŒì•„ê°€ì„¸ìš”.")
        if st.button(L["customer_generate_response_button"], key="btn_generate_final_response"):
            # ìˆ˜ë™ ì²˜ë¦¬ ì‹œ AGENT_TURNìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡ ì²˜ë¦¬
            st.session_state.sim_stage = "AGENT_TURN"
            # st.rerun()

# =========================
# 9. ìµœì¢… ì¢…ë£Œ í–‰ë™ (FINAL_CLOSING_ACTION)
# =========================
if st.session_state.sim_stage == "FINAL_CLOSING_ACTION":
    st.success("ê³ ê°ì´ ë” ì´ìƒ ë¬¸ì˜í•  ì‚¬í•­ì´ ì—†ë‹¤ê³  í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

    if st.button(L["sim_end_chat_button"], key="btn_final_end_chat"):
        # AHT íƒ€ì´ë¨¸ ì •ì§€
        st.session_state.start_time = None

        end_msg = L["prompt_survey"]
        st.session_state.simulator_messages.append(
            {"role": "system_end", "content": end_msg}
        )
        st.session_state.is_chat_ended = True
        st.session_state.sim_stage = "CLOSING"

        customer_type_display = st.session_state.get("customer_type_sim_select", "")
        save_simulation_history_local(
            st.session_state.customer_query_text_area, customer_type_display,
            st.session_state.simulator_messages, is_chat_ended=True,
            attachment_context=st.session_state.sim_attachment_context_for_llm,
        )

        # â­ ì¬ì‹¤í–‰
        # st.rerun()

elif feature_selection == L["sim_tab_phone"]:
    st.header(L["phone_header"])
    st.markdown(L["simulator_desc"])

    current_lang = st.session_state.language
    L = LANG[current_lang]

    # ========================================
    # ì „í™” ì‹œë®¬ë ˆì´í„° ë¡œì§
    # ========================================

    # ------------------
    # AHT íƒ€ì´ë¨¸ í‘œì‹œ (ì „í™” ì‹œë®¬ë ˆì´ì…˜ì—ì„œë§Œ)
    # ------------------
    if st.session_state.call_sim_stage == "IN_CALL":
        # AHT íƒ€ì´ë¨¸ ê³„ì‚° ë¡œì§
        col_timer, col_duration = st.columns([1, 4])

        if st.session_state.start_time is not None:
            now = datetime.now()

            # Hold ì¤‘ì´ë¼ë©´, Hold ìƒíƒœê°€ ëœ ì´í›„ì˜ ì‹œê°„ì„ í˜„ì¬ total_hold_durationì— ë”í•˜ì§€ ì•ŠìŒ (Resume ì‹œ ì •ì‚°)
            if st.session_state.is_on_hold and st.session_state.hold_start_time:
                # Hold ì¤‘ì´ì§€ë§Œ AHT íƒ€ì´ë¨¸ëŠ” ê³„ì† í˜ëŸ¬ê°€ì•¼ í•˜ë¯€ë¡œ, Hold ì‹œê°„ì€ ì œì™¸í•˜ì§€ ì•Šê³  ìµœì¢… AHT ê³„ì‚°ì—ë§Œ ì‚¬ìš©
                elapsed_time_total = now - st.session_state.start_time
            else:
                elapsed_time_total = now - st.session_state.start_time

            # â­ AHTëŠ” í†µí™” ì‹œì‘ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ì´ ê²½ê³¼ ì‹œê°„ì…ë‹ˆë‹¤.
            total_seconds = elapsed_time_total.total_seconds()
            total_seconds = max(0, total_seconds)  # ìŒìˆ˜ ë°©ì§€

            # ì‹œê°„ í˜•ì‹ í¬ë§·íŒ…
            minutes = int(total_seconds // 60)
            seconds = int(total_seconds % 60)
            time_str = f"{minutes:02d}:{seconds:02d}"

            # ê²½ê³  ê¸°ì¤€
            if total_seconds > 900:  # 15ë¶„
                delta_str = L["timer_info_risk"]
                delta_color = "inverse"
            elif total_seconds > 600:  # 10ë¶„
                delta_str = L["timer_info_warn"]
                delta_color = "off"
            else:
                delta_str = L["timer_info_ok"]
                delta_color = "normal"

            with col_timer:
                # AHT íƒ€ì´ë¨¸ í‘œì‹œ
                st.metric(L["timer_metric"], time_str, delta=delta_str, delta_color=delta_color)

            # â­ ìˆ˜ì •: AHT íƒ€ì´ë¨¸ ì‹¤ì‹œê°„ ê°±ì‹ ì„ ìœ„í•œ ê°•ì œ ì¬ì‹¤í–‰ ë¡œì§ ì¶”ê°€
            # í†µí™” ì¤‘ì´ê³ , Hold ìƒíƒœê°€ ì•„ë‹ ë•Œë§Œ 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸í•˜ì—¬ ì‹¤ì‹œê°„ì„±ì„ í™•ë³´
            if not st.session_state.is_on_hold and total_seconds < 1000:
                time.sleep(1)
                # st.rerun()  # ë§¤ ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰í•˜ì—¬ AHT ê°±ì‹ 

    # ------------------
    # WAIT_FIRST_QUERY / WAITING_CALL ìƒíƒœ
    # ------------------
    if st.session_state.call_sim_stage in ["WAITING_CALL", "RINGING"]:

        if "call_sim_mode" not in st.session_state:
            st.session_state.call_sim_mode = "INBOUND" # INBOUND or OUTBOUND

        if st.session_state.call_sim_mode == "INBOUND":
            st.subheader(L["call_status_waiting"])
        else:
            st.subheader(L["button_call_outbound"])

        # ì´ˆê¸° ë¬¸ì˜ ì…ë ¥ (ê³ ê°ì´ ì „í™”ë¡œ ë§í•  ë‚´ìš©)
        st.session_state.call_initial_query = st.text_area(
            L["customer_query_label"],
            key="call_initial_query_text_area",
            height=100,
            placeholder=L["call_query_placeholder"],
        )

        # ê°€ìƒ ì „í™”ë²ˆí˜¸ í‘œì‹œ
        st.session_state.incoming_phone_number = st.text_input(
            "Incoming/Outgoing Phone Number",
            key="incoming_phone_number_input",
            value=st.session_state.incoming_phone_number,
            placeholder=L["call_number_placeholder"],
        )

        # ê³ ê° ìœ í˜• ì„ íƒ
        customer_type_options = L["customer_type_options"]
        default_idx = customer_type_options.index(
            st.session_state.customer_type_sim_select) if st.session_state.customer_type_sim_select in customer_type_options else 0

        st.session_state.customer_type_sim_select = st.selectbox(
            L["customer_type_label"],
            customer_type_options,
            index=default_idx,
            key="call_customer_type_sim_select_widget",
        )

        st.markdown("---")

        col_in, col_out = st.columns(2)

        # ì „í™” ì‘ë‹µ (ìˆ˜ì‹ )
        with col_in:
            if st.button(L["button_answer"], key="answer_call_btn"):
                # ì…ë ¥ ê²€ì¦
                if not st.session_state.call_initial_query.strip():
                    st.warning(L["simulation_warning_query"])
                    st.stop()

                if not st.session_state.is_llm_ready or st.session_state.openai_client is None:
                    st.error(L["simulation_no_key_warning"] + " " + L["openai_missing"])
                    st.stop()

                # INBOUND ëª¨ë“œ ì„¤ì •
                st.session_state.call_sim_mode = "INBOUND"

                # ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ë° ì‹œì‘
                st.session_state.call_sim_stage = "IN_CALL"
                st.session_state.is_call_ended = False
                st.session_state.is_on_hold = False
                st.session_state.total_hold_duration = timedelta(0)
                st.session_state.hold_start_time = None
                st.session_state.start_time = datetime.now()  # í†µí™” ì‹œì‘ ì‹œê°„ (AHT ì‹œì‘)
                st.session_state.simulator_messages = []
                st.session_state.current_customer_audio_text = ""
                st.session_state.current_agent_audio_text = ""
                st.session_state.agent_response_input_box_widget_call = ""
                st.session_state.sim_instance_id = str(uuid.uuid4())
                st.session_state.call_summary_text = ""  # ìš”ì•½ ì´ˆê¸°í™”
                st.session_state.customer_initial_audio_bytes = None  # ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
                st.session_state.customer_history_summary = ""  # AI ìš”ì•½ ì´ˆê¸°í™” (ì¶”ê°€)
                st.session_state.sim_audio_bytes = None  # ë…¹ìŒ íŒŒì¼ ì´ˆê¸°í™” (ì¶”ê°€)

                # ê³ ê°ì˜ ì²« ë²ˆì§¸ ìŒì„± ë©”ì‹œì§€ (ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ë©”ì‹œì§€)
                initial_query_text = st.session_state.call_initial_query.strip()
                st.session_state.current_customer_audio_text = initial_query_text

                # â­ ê³ ê°ì˜ ì²« ë¬¸ì˜ TTS ìŒì„± ìƒì„± ë° ì €ì¥
                with st.spinner(L["tts_status_generating"] + " (Initial Customer Query)"):
                    audio_bytes, msg = synthesize_tts(initial_query_text, st.session_state.language, role="customer")
                    if audio_bytes:
                        st.session_state.customer_initial_audio_bytes = audio_bytes
                        st.audio(audio_bytes, format="audio/mp3", autoplay=True)
                    else:
                        st.error(f"âŒ {msg}")
                        st.session_state.customer_initial_audio_bytes = None

                # âœ… ìƒíƒœ ë³€ê²½ í›„ ì¬ì‹¤í–‰í•˜ì—¬ IN_CALL ìƒíƒœë¡œ ì „í™˜
                st.rerun()

        # ì „í™” ë°œì‹  (ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘)
        with col_out:
            st.markdown(f"### {L['button_call_outbound']}")
            call_targets = [
                L["call_target_customer"],
                L["call_target_partner"]
            ]

            call_target_selection = st.radio(
                "ë°œì‹  ëŒ€ìƒ ì„ íƒ",
                call_targets,
                key="outbound_call_target_radio",
                horizontal=True
            )

            if st.button(L["button_call_outbound"], key="outbound_call_start_btn", type="secondary"):
                # ì…ë ¥ ê²€ì¦
                if not st.session_state.call_initial_query.strip():
                    st.warning("ì „í™” ë°œì‹  ëª©í‘œ (ê³ ê° ë¬¸ì˜ ë‚´ìš©)ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                    st.stop()

                if not st.session_state.is_llm_ready or st.session_state.openai_client is None:
                    st.error(L["simulation_no_key_warning"] + " " + L["openai_missing"])
                    st.stop()

                # OUTBOUND ëª¨ë“œ ì„¤ì • ë° ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘
                st.session_state.call_sim_mode = "OUTBOUND"

                # ì‹œë®¬ë ˆì´ì…˜ ì´ˆê¸°í™” ë° ì‹œì‘
                st.session_state.call_sim_stage = "IN_CALL"
                st.session_state.is_call_ended = False
                st.session_state.is_on_hold = False
                st.session_state.total_hold_duration = timedelta(0)
                st.session_state.hold_start_time = None
                st.session_state.start_time = datetime.now()  # í†µí™” ì‹œì‘ ì‹œê°„ (AHT ì‹œì‘)
                st.session_state.simulator_messages = []

                initial_query_text = st.session_state.call_initial_query.strip()

                # ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ì—ì´ì „íŠ¸ê°€ ë¨¼ì € ë§í•´ì•¼ í•˜ë¯€ë¡œ, ê³ ê° CC í…ìŠ¤íŠ¸ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ë¡œ ì„¤ì •
                st.session_state.current_customer_audio_text = f"ğŸ“ {L['button_call_outbound']} ì„±ê³µ! {call_target_selection}ì´(ê°€) ë°›ì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ì‘ë‹µì´ ì‹œì‘ë©ë‹ˆë‹¤. (ë¬¸ì˜ ëª©í‘œ: {initial_query_text[:50]}...)"
                st.session_state.current_agent_audio_text = ""  # Agent speaks first
                st.session_state.agent_response_input_box_widget_call = ""
                st.session_state.sim_instance_id = str(uuid.uuid4())
                st.session_state.call_summary_text = ""
                st.session_state.customer_initial_audio_bytes = None
                st.session_state.customer_history_summary = ""
                st.session_state.sim_audio_bytes = None

                st.success(f"'{call_target_selection}'ì—ê²Œ ì „í™” ë°œì‹  ì‹œë®¬ë ˆì´ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ì˜ ì²« ì‘ë‹µì„ ë…¹ìŒí•˜ì„¸ìš”.")
                # st.rerun()

   # ------------------
   # IN_CALL ìƒíƒœ (í†µí™” ì¤‘)
   # ------------------
    elif st.session_state.call_sim_stage == "IN_CALL":
        # â­ ë°œì‹ /ìˆ˜ì‹  ëª¨ë“œì— ë”°ë¼ ì œëª© ë³€ê²½
        if st.session_state.get("call_sim_mode", "INBOUND") == "INBOUND":
            title = L['call_status_ringing'].format(number=st.session_state.incoming_phone_number)
        else:
            title = L['button_call_outbound'] + f" ({st.session_state.incoming_phone_number})"

        st.markdown(f"## {title}")
        st.markdown("---")

        # --- Hold / í†µí™” ì¬ê°œ ë²„íŠ¼ ---
        col_hangup, col_hold = st.columns([1, 1])

        with col_hangup:
            if st.button(L["button_hangup"], key="hangup_call_btn", type="primary"):

                # 1. Hold ì¤‘ì´ì—ˆë‹¤ë©´, Hold ì‹œê°„ ìµœì¢… ì •ì‚°
                if st.session_state.is_on_hold and st.session_state.hold_start_time:
                    st.session_state.total_hold_duration += datetime.now() - st.session_state.hold_start_time

                # 2. ìš”ì•½ ìƒì„± (ìš”ì²­ 4 ë°˜ì˜)
                with st.spinner("AI ìš”ì•½ ìƒì„± ì¤‘..."):
                    summary = generate_summary_for_call(
                        st.session_state.language,
                        st.session_state.simulator_messages,
                        st.session_state.call_initial_query
                    )
                    st.session_state.call_summary_text = summary

                # 3. ìƒíƒœ ì „í™˜ ë° AHT ì •ì§€
                st.session_state.call_sim_stage = "CALL_ENDED"
                st.session_state.is_call_ended = True
                # AHT ìµœì¢… ì •ì§€ëŠ” CALL_ENDEDì—ì„œ ê³„ì‚° (start_timeì€ ìœ ì§€)

                # âœ… ì¬ì‹¤í–‰
                # st.rerun()

        with col_hold:
            if st.session_state.is_on_hold:
                if st.button(L["button_resume"], key="resume_call_btn", type="secondary"):
                    # Hold ìƒíƒœ í•´ì œ ë° ì‹œê°„ ì •ì‚°
                    st.session_state.is_on_hold = False
                    if st.session_state.hold_start_time:
                        st.session_state.total_hold_duration += datetime.now() - st.session_state.hold_start_time
                        st.session_state.hold_start_time = None
                    # âœ… ì¬ì‹¤í–‰
                    # st.rerun()
            else:
                if st.button(L["button_hold"], key="hold_call_btn", type="secondary"):
                    st.session_state.is_on_hold = True
                    st.session_state.hold_start_time = datetime.now()
                    # âœ… ì¬ì‹¤í–‰
                    # st.rerun()

        if st.session_state.is_on_hold:
            # â­ ìˆ˜ì •: í˜„ì¬ Hold ì‹œê°„ì„ ê³„ì‚°í•˜ì—¬ í‘œì‹œ
            if st.session_state.hold_start_time:
                # í˜„ì¬ Hold ì¤‘ì¸ ì‹œê°„
                current_hold_duration = datetime.now() - st.session_state.hold_start_time
            else:
                current_hold_duration = timedelta(0)

            # ëˆ„ì  Hold ì‹œê°„ + í˜„ì¬ Hold ì¤‘ì¸ ì‹œê°„
            display_hold_duration = st.session_state.total_hold_duration + current_hold_duration

            # str(timedelta) í˜•ì‹: D days, H:MM:SS.microseconds
            duration_str = str(display_hold_duration).split('.')[0]
            st.warning(L["hold_status"].format(duration=duration_str))

            # Hold ì¤‘ì¼ ë•Œë„ AHT íƒ€ì´ë¨¸ ê°±ì‹ ì„ ìœ„í•´ 1ì´ˆë§ˆë‹¤ ì¬ì‹¤í–‰
            time.sleep(1)
            # st.rerun()


        def transfer_session(target_lang: str, current_messages: List[Dict[str, str]]):
            """ì–¸ì–´ ì´ê´€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  ì„¸ì…˜ ì–¸ì–´ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤."""

            current_lang = st.session_state.language  # í˜„ì¬ ì–¸ì–´ í™•ì¸ (Source language)
            L = LANG[current_lang]

            # API í‚¤ ì²´í¬
            if not st.session_state.is_llm_ready:
                st.error(L["simulation_no_key_warning"].replace('API Key', 'LLM API Key'))
                return

            current_lang_at_start = st.session_state.language  # Source language

            # AHT íƒ€ì´ë¨¸ ì •ì§€ (ì‹¤ì œë¡œ í†µí™”ê°€ ì¢…ë£Œë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ, AHTëŠ” ê³„ì† íë¦„)
            # st.session_state.start_time = None

            # 1. ë¡œë”© ì‹œì‘ (ì‹œê°„ ì–‘í•´ ë©”ì‹œì§€ ì‹œë®¬ë ˆì´ì…˜)
            with st.spinner(L["transfer_loading"]):
                time.sleep(np.random.uniform(5, 10))

                # 2. ëŒ€í™” ê¸°ë¡ì„ ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¡œ ê°€ê³µ
                history_text = ""
                for msg in current_messages:
                    role = "Customer" if msg["role"].startswith("customer") or msg[
                        "role"] == "initial_query" else "Agent"
                    if msg["role"] in ["initial_query", "customer_rebuttal", "agent_response",
                                       "customer_closing_response", "phone_exchange"]:  # phone_exchange ì¶”ê°€
                        history_text += f"{role}: {msg['content']}\n"

                # 3. LLM ë²ˆì—­ ì‹¤í–‰ (ìˆ˜ì •ëœ ë²ˆì—­ í•¨ìˆ˜ ì‚¬ìš©)
                translated_summary = translate_text_with_llm(history_text, target_lang,
                                                             current_lang_at_start)  # Use current_lang_at_start as source

                # 4. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.transfer_summary_text = translated_summary
                st.session_state.language_at_transfer = target_lang  # Save destination language
                st.session_state.language_at_transfer_start = current_lang_at_start  # Save source language for retry
                st.session_state.language = target_lang  # Language switch

                # --- ì‹œìŠ¤í…œ ì´ê´€ ë©”ì‹œì§€ ì¶”ê°€ ---
                # ì „í™”ì—ì„œëŠ” ë³„ë„ì˜ Supervisor ë©”ì‹œì§€ ì—†ì´ ë¡œê·¸ì—ë§Œ ë‚¨ê¹€
                st.session_state.simulator_messages.append(
                    {"role": "system_transfer",
                     "content": LANG[target_lang]['transfer_system_msg'].format(target_lang=target_lang)})

                st.session_state.is_solution_provided = False
                st.session_state.language_transfer_requested = False

                # ì´ê´€ í›„ ìƒíƒœ ì „í™˜: í†µí™” ì¤‘ì¸ ìƒíƒœëŠ” ìœ ì§€
                st.session_state.call_sim_stage = "IN_CALL"

                # 5. ì´ë ¥ ì €ì¥
                customer_type_display = st.session_state.get("customer_type_sim_select", "")
                save_simulation_history_local(
                    st.session_state.call_initial_query,
                    customer_type_display + f" (Transferred from {current_lang_at_start} to {target_lang})",
                    st.session_state.simulator_messages,
                    attachment_context=st.session_state.sim_attachment_context_for_llm,
                    is_chat_ended=False,
                    is_call=(st.session_state.call_sim_stage == "IN_CALL")  # ì „í™” ì´ë ¥ì„ì„ í‘œì‹œ
                )

            # 6. UI ì¬ì‹¤í–‰ (ì–¸ì–´ ë³€ê²½ ì ìš©)
            st.success(f"âœ… {LANG[target_lang]['transfer_summary_header']}ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‘ëŒ€ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
            st.rerun()


        st.markdown("---")
        st.markdown(f"**{L['transfer_header']}**")
        transfer_cols = st.columns(len(LANG) - 1)

        languages = list(LANG.keys())
        languages.remove(current_lang)

        # transfer_session í•¨ìˆ˜ë¥¼ ì¬ì •ì˜í•˜ì§€ ì•Šê³ , ê¸°ì¡´ì˜ transfer_session í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        for i, target_lang in enumerate(languages):
            button_label_key = f"transfer_to_{target_lang}"
            button_label = L.get(button_label_key, f"Transfer to {target_lang.capitalize()} Team")

            if transfer_cols[i].button(button_label, key=f"btn_transfer_phone_{target_lang}"):
                # transfer_session í˜¸ì¶œ ì‹œ, í˜„ì¬ í†µí™” ë©”ì‹œì§€(simulator_messages)ë¥¼ ë„˜ê²¨ì¤ë‹ˆë‹¤.
                transfer_session(target_lang, st.session_state.simulator_messages)

        # =========================
        # AI ìš”ì•½ ë²„íŠ¼ ë° í‘œì‹œ ë¡œì§ (ì¶”ê°€ëœ ê¸°ëŠ¥)
        # =========================
        st.markdown("---")
        # â­ history_expander_titleì—ì„œ ê´„í˜¸ ì•ˆ ë‚´ìš©ë§Œ ì œê±° (ì˜ˆ: (ìµœê·¼ 10ê±´))
        summary_title = L['history_expander_title'].split('(')[0].strip()
        st.markdown(f"### ğŸ“‘ {summary_title} ìš”ì•½")

        # 1. ìš”ì•½/ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ ì˜ì—­
        col_sum_btn, col_trans_btn = st.columns(2)

        with col_sum_btn:
            if st.button("ğŸ’¡ ì´ë ¥ ìš”ì•½ ìš”ì²­", key="btn_request_phone_summary"):
                # ìš”ì•½ í•¨ìˆ˜ í˜¸ì¶œ
                st.session_state.customer_history_summary = summarize_history_with_ai(st.session_state.language)
                # st.rerun()

        # 2. ì´ê´€ ë²ˆì—­ ì¬ì‹œë„ ë²„íŠ¼ (ì´ê´€ í›„ ë²ˆì—­ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš°)
        if st.session_state.language != st.session_state.language_at_transfer_start and not st.session_state.transfer_summary_text:
            with col_trans_btn:
                if st.button(L["button_retry_translation"], key="btn_phone_retry_translation"):
                    with st.spinner(L["transfer_loading"]):
                        # ì´ê´€ ë²ˆì—­ ë¡œì§ ì¬ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                        translated_summary = translate_text_with_llm(
                            get_chat_history_for_prompt(include_attachment=False),
                            st.session_state.language,
                            st.session_state.language_at_transfer_start
                        )
                        st.session_state.transfer_summary_text = translated_summary
                        # st.rerun()

        # 3. ìš”ì•½ ë‚´ìš© í‘œì‹œ
        if st.session_state.transfer_summary_text:
            st.subheader(f"ğŸ” {L['transfer_summary_header']}")
            st.info(st.session_state.transfer_summary_text)
        elif st.session_state.customer_history_summary:
            st.subheader("ğŸ’¡ AI ìš”ì•½")
            st.info(st.session_state.customer_history_summary)

        st.markdown("---")

        # --- ì‹¤ì‹œê°„ ì‘ëŒ€ íŒíŠ¸ ì˜ì—­ ---
        hint_cols = st.columns([4, 1])
        with hint_cols[0]:
            st.info(L["hint_placeholder"] + st.session_state.realtime_hint_text)

        with hint_cols[1]:
            # íŒíŠ¸ ìš”ì²­ ë²„íŠ¼
            if st.button(L["button_request_hint"], key=f"btn_request_hint_call_{st.session_state.sim_instance_id}"):
                with st.spinner(L["response_generating"]):
                    # ì „í™” íƒ­ì´ë¯€ë¡œ is_call=True
                    hint = generate_realtime_hint(current_lang, is_call=True)
                    st.session_state.realtime_hint_text = hint
                    # st.rerun()

        # =========================
        # CC ìë§‰ / ìŒì„± ì…ë ¥ ë° ì œì–´ ë¡œì§ (ê¸°ì¡´ ë¡œì§)
        # =========================================

        # --- ì‹¤ì‹œê°„ CC ìë§‰ / ì „ì‚¬ ì˜ì—­ ---
        st.subheader(L["cc_live_transcript"])

        if st.session_state.is_on_hold:
            st.text_area("Customer", value="[ê³ ê°: ì ì‹œ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤...]", height=50, disabled=True, key="customer_live_cc_area")
            st.text_area("Agent", value="[ì—ì´ì „íŠ¸: Hold ì¤‘ì…ë‹ˆë‹¤. í†µí™” ì¬ê°œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.]", height=50, disabled=True,
                         key="agent_live_cc_area")
        else:
            # ê³ ê° CC (LLM ìƒì„± í…ìŠ¤íŠ¸)
            st.text_area(
                "Customer",
                value=st.session_state.current_customer_audio_text,
                height=50,
                disabled=True,
                key="customer_live_cc_area",
            )

            # ì—ì´ì „íŠ¸ CC (ë§ˆì´í¬ ì „ì‚¬)
            st.text_area(
                "Agent",
                value=st.session_state.current_agent_audio_text,
                height=50,
                disabled=True,
                key="agent_live_cc_area",
            )

        st.markdown("---")

        # --- ì—ì´ì „íŠ¸ ìŒì„± ì…ë ¥ / ë…¹ìŒ ---
        st.subheader(L["mic_input_status"])

        # ìŒì„± ì…ë ¥: ì§§ì€ ì²­í¬ë¡œ ëŠì–´ì„œ ì „ì‚¬í•´ì•¼ ì‹¤ì‹œê°„ CC ëª¨ë°© ê°€ëŠ¥
        if st.session_state.is_on_hold:
            st.info("í†µí™”ê°€ Hold ì¤‘ì…ë‹ˆë‹¤. í†µí™” ì¬ê°œ í›„ ë…¹ìŒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            mic_audio = None
        else:
            # âœ… ë§ˆì´í¬ ìœ„ì ¯ì„ í•­ìƒ ë Œë”ë§í•˜ì—¬ í™œì„±í™” ìƒíƒœë¥¼ ìœ ì§€
            mic_audio = mic_recorder(
                start_prompt=L["agent_response_prompt"],
                stop_prompt="â¹ï¸ ë…¹ìŒ ì¢…ë£Œ ë° ì‘ë‹µ ì „ì†¡",
                just_once=True,
                format="wav",
                use_container_width=True,
                key="call_sim_mic_recorder",
            )

            # ë…¹ìŒ ì™„ë£Œ (mic_audio.get("bytes")ê°€ ì±„ì›Œì§) ì‹œ, ë°”ì´íŠ¸ë¥¼ ì €ì¥í•˜ê³  ì¬ì‹¤í–‰
            if mic_audio and mic_audio.get("bytes") and "bytes_to_process" not in st.session_state:
                st.session_state.bytes_to_process = mic_audio["bytes"]
                st.session_state.current_agent_audio_text = "ğŸ™ï¸ ë…¹ìŒ ì™„ë£Œ. ì „ì‚¬ ì²˜ë¦¬ ì¤‘..."  # ì²˜ë¦¬ ì¤‘ ë©”ì‹œì§€
                # âœ… ì¬ì‹¤í–‰í•˜ì—¬ ë‹¤ìŒ ì‹¤í–‰ ì£¼ê¸°ì—ì„œ ì „ì‚¬ ë¡œì§ì„ ì²˜ë¦¬
                # st.rerun()

            # â­ ì „ì‚¬ ë¡œì§: bytes_to_processì— ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰
            if "bytes_to_process" in st.session_state and st.session_state.bytes_to_process:
                if not st.session_state.openai_client:
                    st.error(L["openai_missing"])
                    st.session_state.bytes_to_process = None
                    # âœ… ì¬ì‹¤í–‰
                    # st.rerun()

                with st.spinner(L["whisper_processing"]):

                    # 1. ì—ì´ì „íŠ¸ ìŒì„± ì „ì‚¬
                    agent_response_transcript = transcribe_bytes_with_whisper(
                        st.session_state.bytes_to_process, "audio/wav", lang_code=st.session_state.language
                    )

                    # ì „ì‚¬ í›„ ë°”ì´íŠ¸ ë°ì´í„° ì‚­ì œ
                    del st.session_state.bytes_to_process

                    if agent_response_transcript.startswith("âŒ"):
                        st.error(agent_response_transcript)
                        st.session_state.current_agent_audio_text = f"[ERROR: {L['error']} Whisper failed]"
                        # âœ… ì¬ì‹¤í–‰
                        # st.rerun()

                    # 2. ì „ì‚¬ ê²°ê³¼ë¥¼ CC í…ìŠ¤íŠ¸ë¡œ ë°˜ì˜
                    st.session_state.current_agent_audio_text = agent_response_transcript.strip()

                    # 3. ê³ ê°ì˜ ë‹¤ìŒ ìŒì„± ë°˜ì‘ ìƒì„±
                    customer_reaction = generate_customer_reaction_for_call(
                        st.session_state.language, agent_response_transcript.strip()
                    )

                    # 4. ê³ ê° ë°˜ì‘ì„ TTSë¡œ ì¬ìƒ
                    if not customer_reaction.startswith("âŒ"):
                        audio_bytes, msg = synthesize_tts(customer_reaction, st.session_state.language, role="customer")
                        if audio_bytes:
                            st.audio(audio_bytes, format="audio/mp3", autoplay=True)
                            st.success(f"ğŸ—£ï¸ ê³ ê°ì´ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤: {customer_reaction.strip()[:50]}...")
                        else:
                            st.error(f"âŒ ê³ ê° ìŒì„± ìƒì„± ì˜¤ë¥˜: {msg}")

                    # 5. ê³ ê° ë°˜ì‘ í…ìŠ¤íŠ¸ë¥¼ CC ì˜ì—­ì— ë°˜ì˜
                    st.session_state.current_customer_audio_text = customer_reaction.strip()

                    # 6. ì´ë ¥ ì €ì¥
                    log_entry = f"Agent: {st.session_state.current_agent_audio_text} | Customer: {st.session_state.current_customer_audio_text}"
                    st.session_state.simulator_messages.append({"role": "phone_exchange", "content": log_entry})

                    # 7. ì—ì´ì „íŠ¸ ì…ë ¥ ì˜ì—­ ì´ˆê¸°í™”
                    st.session_state.current_agent_audio_text = ""
                    st.session_state.realtime_hint_text = ""

                    # âœ… ê³ ê° ë°˜ì‘ í›„ í™•ì‹¤í•˜ê²Œ ì¬ì‹¤í–‰
                    # st.rerun()

# ------------------
# CALL_ENDED ìƒíƒœ
# ------------------
elif st.session_state.call_sim_stage == "CALL_ENDED":
    # ... (ê¸°ì¡´ CALL_ENDED ë¡œì§)
    st.success(L["call_end_message"])

    # AHT ê³„ì‚°
    if st.session_state.start_time is not None:
        # ìµœì¢… AHT ê³„ì‚° (Hold ì‹œê°„ ì •ì‚° ë¡œì§ì´ Hang Up ë²„íŠ¼ í´ë¦­ ì‹œ ì™„ë£Œë¨)
        # AHTëŠ” Hold ì‹œê°„ë„ í¬í•¨ë˜ë¯€ë¡œ, ë‹¨ìˆœ ê²½ê³¼ ì‹œê°„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        final_aht_seconds = (datetime.now() - st.session_state.start_time).total_seconds()
        final_aht_seconds = max(0, final_aht_seconds)
        final_aht_str = str(timedelta(seconds=final_aht_seconds)).split('.')[0]
        st.metric("Final AHT", final_aht_str)

        # Hold Duration í‘œì‹œ
        hold_duration_str = str(st.session_state.total_hold_duration).split('.')[0]
        st.metric("Total Hold Time", hold_duration_str)
    else:
        st.warning(L["aht_not_recorded"])

    st.markdown("---")

    with st.expander("í†µí™” ê¸°ë¡ ìš”ì•½"):
        # 1. AI ìš”ì•½ í‘œì‹œ
        st.subheader("AI í†µí™” ìš”ì•½")
        if st.session_state.call_summary_text:
            st.info(st.session_state.call_summary_text)
        else:
            st.error("âŒ í†µí™” ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”ã€‚")

        st.markdown("---")

        # 2. ê³ ê° ìŒì„± ë…¹ìŒ ì¬ìƒ
        st.subheader("ê³ ê° ìµœì´ˆ ë¬¸ì˜ (ìŒì„±)")
        if st.session_state.customer_initial_audio_bytes:
            st.audio(st.session_state.customer_initial_audio_bytes, format="audio/mp3")
            st.caption(f"**ì „ì‚¬ í…ìŠ¤íŠ¸:** {st.session_state.call_initial_query}")
        else:
            st.info("ê³ ê°ì˜ ìµœì´ˆ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

        st.markdown("---")

        # 3. ì „ì²´ ë¡œê·¸ í‘œì‹œ (ë””ë²„ê·¸ìš©)
        st.subheader("ì „ì²´ êµí™˜ ë¡œê·¸ (ë””ë²„ê·¸)")
        for log in st.session_state.simulator_messages:
            st.write(log["content"])

    # ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ë²„íŠ¼
    if st.button(L["new_simulation_button"], key="new_call_sim_btn"):
        # ... (ì´ˆê¸°í™” ë¡œì§ ìœ ì§€)
        st.session_state.call_sim_stage = "WAITING_CALL"
        st.session_state.call_sim_mode = "INBOUND"
        st.session_state.is_on_hold = False
        st.session_state.total_hold_duration = timedelta(0)
        st.session_state.hold_start_time = None
        st.session_state.start_time = None
        st.session_state.current_customer_audio_text = ""
        st.session_state.current_agent_audio_text = ""
        st.session_state.agent_response_input_box_widget_call = ""
        st.session_state.call_initial_query = ""
        st.session_state.simulator_messages = []
        st.session_state.call_summary_text = ""  # ìš”ì•½ ì´ˆê¸°í™”
        st.session_state.customer_initial_audio_bytes = None  # ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
        st.session_state.customer_history_summary = ""  # AI ìš”ì•½ ì´ˆê¸°í™” (ì¶”ê°€)
        st.session_state.sim_audio_bytes = None  # ë…¹ìŒ íŒŒì¼ ì´ˆê¸°í™” (ì¶”ê°€)
        # â­ ì¬ì‹¤í–‰
        # st.rerun()

# -------------------- RAG Tab --------------------
elif feature_selection == L["rag_tab"]:
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    st.markdown("---")

    # --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
    # â­ ìˆ˜ì •ëœ ë¶€ë¶„: RAG íƒ­ ì „ìš© í‚¤ ì‚¬ìš©
    uploaded_files = st.file_uploader(
        L["file_uploader"],
        type=["pdf", "txt", "html"],
        key="rag_file_uploader", # RAG ì „ìš© í‚¤
        accept_multiple_files=True
    )

    if uploaded_files:
        if uploaded_files != st.session_state.uploaded_files_state:
            # íŒŒì¼ì´ ë³€ê²½ë˜ë©´ RAG ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.is_rag_ready = False
            st.session_state.rag_vectorstore = None
            st.session_state.uploaded_files_state = uploaded_files

        if not st.session_state.is_rag_ready:
            if st.button(L["button_start_analysis"]):
                if not st.session_state.is_llm_ready:
                    st.error(L["simulation_no_key_warning"])
                    st.stop()

                with st.spinner(L["data_analysis_progress"]):
                    vectorstore, count = build_rag_index(uploaded_files)

                if vectorstore:
                    st.session_state.rag_vectorstore = vectorstore
                    st.session_state.is_rag_ready = True
                    st.success(L["embed_success"].format(count=count))
                    st.session_state.rag_messages = [
                        {"role": "assistant", "content": f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ. ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}
                    ]
                else:
                    st.error(L["embed_fail"])
                    st.session_state.is_rag_ready = False
    else:
        st.info(L["warning_no_files"])
        st.session_state.is_rag_ready = False
        st.session_state.rag_vectorstore = None
        st.session_state.rag_messages = []

    st.markdown("---")

    # --- ì±—ë´‡ ì„¹ì…˜ ---
    if st.session_state.is_rag_ready and st.session_state.rag_vectorstore:
        if "rag_messages" not in st.session_state:
            st.session_state.rag_messages = [{"role": "assistant", "content": "ë¶„ì„ëœ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}]

        for message in st.session_state.rag_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.rag_messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner(L["response_generating"]):
                    response = rag_answer(
                        prompt,
                        st.session_state.rag_vectorstore,
                        st.session_state.language
                    )
                    st.markdown(response)

            st.session_state.rag_messages.append({"role": "assistant", "content": response})
    else:
        st.warning(L["warning_rag_not_ready"])

# -------------------- Content Generation Tab --------------------
elif feature_selection == L["content_tab"]:
    st.header(L["content_header"])
    st.markdown(L["content_desc"])
    st.markdown("---")

    if not st.session_state.is_llm_ready:
        st.warning(L["simulation_no_key_warning"])
        st.stop()

    topic = st.text_input(L["topic_label"], key="content_topic_input")
    level = st.selectbox(L["level_label"], L["level_options"], key="content_level_select")
    content_type = st.selectbox(L["content_type_label"], L["content_options"], key="content_type_select")

    # ì½˜í…ì¸  íƒ­ ì „ìš© íŒŒì¼ ì—…ë¡œë” í‚¤ ì‚¬ìš© (RAGì™€ì˜ ì¶©ëŒ ë°©ì§€)
    content_files = st.file_uploader(
        L["file_uploader"],
        type=["pdf", "txt", "html"],
        key="content_file_uploader",  # ì½˜í…ì¸  íƒ­ ì „ìš© í‚¤
        accept_multiple_files=True
    )

    if st.button(L["button_generate"], key="btn_generate_content"):
        if not topic.strip():
            st.warning(L["warning_topic"])
            st.stop()

        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê¸°ë³¸ í…œí”Œë¦¿)
        lang_name = {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[st.session_state.language]
        system_prompt = f"""
            You are an expert content creator. Generate learning content in {lang_name} based on the user's request.

            - Topic: {topic}
            - Difficulty: {level}
            - Format: {content_type}

            If the format is '{L['content_options'][0]}', provide a structured summary with key concepts and definitions.
            If the format is '{L['content_options'][1]}', provide a JSON array of 10 questions. Each object must have keys: 'question', 'options' (array of 4 strings), 'answer' (1-4 index), and 'explanation'.
            If the format is '{L['content_options'][2]}', provide a scenario and actionable steps.

            Ensure the content complexity matches the '{level}' difficulty.
            Output ONLY the requested content, without any introductory or concluding remarks.
            """

        # â­ ìˆ˜ì •ëœ ë¶€ë¶„ 3: í€´ì¦ˆ íƒ€ì… ë¹„êµ ë¡œì§ ìˆ˜ì • ë° ì¼ë°˜ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ìƒì„± ë¡œì§ ëª…í™•í™”
        if content_type == L["content_options"][1]:  # ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­
            # í€´ì¦ˆ ìƒì„± ë¡œì§ (ì´ì „ ìˆ˜ì • ì‚¬í•­ ìœ ì§€)
            quiz_schema_str = json.dumps({
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "question": {"type": "string", "description": "The quiz question."},
                        "options": {
                            "type": "array",
                            "description": "An array of 4 options.",
                            "items": {"type": "string"},
                            "minItems": 4,
                            "maxItems": 4
                        },
                        "answer": {"type": "integer", "description": "The index of the correct option (1 to 4)."},
                        "explanation": {"type": "string", "description": "A brief explanation of the correct answer."}
                    },
                    "required": ["question", "options", "answer", "explanation"]
                }
            }, indent=2)

            system_prompt = f"""
                You are an expert quiz generator. Based on the topic '{topic}' and difficulty '{level}', generate 10 multiple-choice questions.
                Your output MUST be a **raw JSON array** that strictly follows the provided schema.
                DO NOT include any explanation, introductory text, or markdown code blocks (e.g., ```json).
                Output ONLY the raw JSON array, starting with `[` and ending with `]`.

                JSON Schema to follow:
                {quiz_schema_str}
                """

            generated_json_text = None

            llm_attempts = []
            if get_api_key("gemini"):
                llm_attempts.append(("gemini", get_api_key("gemini"), "gemini-2.5-flash"))
            if get_api_key("openai"):
                llm_attempts.append(("openai", get_api_key("openai"), "gpt-4o"))

            with st.spinner(L["response_generating"]):
                for provider, api_key, model_name in llm_attempts:
                    try:
                        if provider == "gemini":
                            client = genai
                            client.configure(api_key=api_key)
                            g_model = client.GenerativeModel(model_name)

                            response = g_model.generate_content(
                                contents=system_prompt  # system_promptë¥¼ user contentë¡œ ì‚¬ìš©
                            )
                            generated_json_text = response.text.strip()
                            break
                        elif provider == "openai":
                            client = OpenAI(api_key=api_key)
                            response = client.chat.completions.create(
                                model=model_name,
                                messages=[{"role": "user", "content": system_prompt}],
                                response_format={"type": "json_object"},
                            )
                            generated_json_text = response.choices[0].message.content.strip()

                            if not generated_json_text.startswith('['):
                                try:
                                    parsed_obj = json.loads(generated_json_text)
                                    if isinstance(parsed_obj, dict) and 'quiz' in parsed_obj and isinstance(
                                            parsed_obj['quiz'], list):
                                        generated_json_text = json.dumps(parsed_obj['quiz'])
                                    elif isinstance(parsed_obj, list):
                                        generated_json_text = json.dumps(parsed_obj)
                                except Exception:
                                    pass

                            if generated_json_text and generated_json_text.startswith('['):
                                break

                    except Exception as e:
                        print(f"JSON generation failed with {provider}: {e}")
                        continue

            if generated_json_text and generated_json_text.startswith('['):
                try:
                    quiz_data = json.loads(generated_json_text)
                    if not isinstance(quiz_data, list) or not all(
                            isinstance(q, dict) and 'answer' in q for q in quiz_data):
                        st.error(L["quiz_error_llm"] + " (Invalid data structure/Missing answer key)")
                        st.text(generated_json_text)
                        st.stop()

                    st.session_state.quiz_data = quiz_data
                    st.session_state.current_question_index = 0
                    st.session_state.quiz_score = 0
                    st.session_state.quiz_answers = [1] * len(quiz_data)
                    st.session_state.show_explanation = False
                    st.session_state.is_quiz_active = True
                    st.session_state.quiz_type_key = str(uuid.uuid4())  # Quiz ID
                except json.JSONDecodeError:
                    st.error(L["quiz_error_llm"])
                    st.text_area(L["quiz_original_response"], generated_json_text, height=200)
                    st.stop()
            else:
                st.error(L["quiz_error_llm"])
                if generated_json_text:
                    st.text_area(L["quiz_original_response"], generated_json_text, height=200)
                st.stop()

        else:  # 'í•µì‹¬ ìš”ì•½ ë…¸íŠ¸' ë˜ëŠ” 'ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´' (ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„±)
            # í€´ì¦ˆê°€ ì•„ë‹Œ ì¼ë°˜ ì½˜í…ì¸ ì˜ ê²½ìš°, is_quiz_activeë¥¼ Falseë¡œ ì„¤ì •í•˜ê³  run_llmìœ¼ë¡œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±
            st.session_state.is_quiz_active = False
            with st.spinner(L["response_generating"]):
                # system_promptëŠ” ì´ë¯¸ ê³µí†µ í…œí”Œë¦¿ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ
                content = run_llm(system_prompt)
            st.session_state.generated_content = content

            # ìƒì„±ëœ ì½˜í…ì¸ ë¥¼ ë°”ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
            st.markdown("---")
            st.markdown(f"### {content_type}")
            st.markdown(st.session_state.generated_content)

    # --- ì½˜í…ì¸  ì¶œë ¥ ---
    if st.session_state.get("is_quiz_active", False) and st.session_state.get("quiz_data"):
        quiz_data = st.session_state.quiz_data
        idx = st.session_state.current_question_index

        # â­ ìˆ˜ì •ëœ ë¶€ë¶„: í€´ì¦ˆ ì™„ë£Œ ì‹œ IndexError ë°©ì§€ ë¡œì§ (idx >= len(quiz_data))
        if idx >= len(quiz_data):
            # í€´ì¦ˆ ì™„ë£Œ ì‹œ ìµœì¢… ì ìˆ˜ í‘œì‹œ
            st.success(L["quiz_complete"])
            total_questions = len(quiz_data)
            score = st.session_state.quiz_score
            st.subheader(f"{L['score']}: {score} / {total_questions} ({(score / total_questions) * 100:.1f}%)")

            if st.button(L["retake_quiz"], key="retake_quiz_btn"):
                # í€´ì¦ˆ ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.is_quiz_active = False
                st.session_state.quiz_data = None
                st.session_state.current_question_index = 0
                st.session_state.quiz_score = 0
                st.session_state.quiz_answers = []
                st.session_state.show_explanation = False
                st.rerun()  # ìƒíƒœ ì´ˆê¸°í™” í›„ ì¦‰ì‹œ ì¬ì‹¤í–‰
            st.stop()  # í€´ì¦ˆ ì™„ë£Œ í›„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ì™„ì „íˆ ì¤‘ë‹¨

        # í€´ì¦ˆ ì§„í–‰ (í˜„ì¬ ë¬¸í•­)
        question_data = quiz_data[idx]
        st.subheader(f"Question {idx + 1}/{len(quiz_data)}")
        st.markdown(f"**{question_data['question']}**")

        current_selection_index = st.session_state.quiz_answers[idx]

        if current_selection_index is None or isinstance(current_selection_index, str):
            radio_index = -1
        else:
            radio_index = current_selection_index - 1

        # ì„ íƒì§€ í‘œì‹œ
        options = question_data['options']
        current_answer = st.session_state.quiz_answers[idx]

        # ë¼ë””ì˜¤ ì¸ë±ìŠ¤ ê³„ì‚° ì‹œ None/ë¬¸ìì—´ ìƒíƒœì— ë”°ë¼ 0ìœ¼ë¡œ fallbackí•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
        if current_answer is None or not isinstance(current_answer, int) or current_answer <= 0:
            radio_index = 0  # st.radioëŠ” index >= 0 ì´ì–´ì•¼ í•˜ë¯€ë¡œ 0ìœ¼ë¡œ fallback (ì²« ë²ˆì§¸ ì˜µì…˜)
        else:
            radio_index = min(current_answer - 1, len(options) - 1)

        selected_option = st.radio(
            L["select_answer"],
            options,
            index=radio_index,
            key=f"quiz_radio_{st.session_state.quiz_type_key}_{idx}"
        )

        # ì„ íƒëœ ì˜µì…˜ì˜ ì¸ë±ìŠ¤ (1ë¶€í„° ì‹œì‘)
        selected_option_index = options.index(selected_option) + 1 if selected_option in options else None

        # ì •ë‹µ í™•ì¸ ë²„íŠ¼ ë° ë¡œì§
        check_col, next_col = st.columns([1, 1])

        if check_col.button(L["check_answer"], key=f"check_answer_btn_{idx}"):
            if selected_option_index is None:
                st.warning("ì„ íƒì§€ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
            else:
                st.session_state.quiz_answers[idx] = selected_option_index
                correct_answer = question_data['answer']

                # ì ìˆ˜ ê³„ì‚° ë° í”¼ë“œë°±
                if selected_option_index == correct_answer:
                    if st.session_state.quiz_answers[idx] != 'Correctly Scored':
                        st.session_state.quiz_score += 1
                        st.session_state.quiz_answers[idx] = 'Correctly Scored'  # ì ìˆ˜ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
                    st.success(L["correct_answer"])
                else:
                    st.error(L["incorrect_answer"])

                st.session_state.show_explanation = True
                st.rerun()

        # ì •ë‹µ ë° í•´ì„¤ í‘œì‹œ
        if st.session_state.show_explanation:
            correct_index = question_data['answer']
            correct_answer_text = question_data['options'][correct_index - 1]

            st.markdown("---")
            st.markdown(f"**{L['correct_is']}:** {correct_answer_text}")
            with st.expander(f"**{L['explanation']}**", expanded=True):
                st.info(question_data['explanation'])

            # ë‹¤ìŒ ë¬¸í•­ ë²„íŠ¼
            if next_col.button(L["next_question"], key=f"next_question_btn_{idx}"):
                st.session_state.current_question_index += 1
                st.session_state.show_explanation = False
                st.rerun()

        else:
            # ì‚¬ìš©ìê°€ ì´ë¯¸ ì •ë‹µì„ ì²´í¬í–ˆê³  (ë‹¤ì‹œ ë¡œë“œëœ ê²½ìš°), ë‹¤ìŒ ë²„íŠ¼ì„ ë°”ë¡œ í‘œì‹œ
            if st.session_state.quiz_answers[idx] in [selected_option_index, 'Correctly Scored']:
                st.info("ë‹µë³€ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. í•´ì„¤ì„ ë³´ë ¤ë©´ ì •ë‹µ í™•ì¸ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ë‹¤ìŒ ë¬¸í•­ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
                if next_col.button(L["next_question"], key=f"next_question_btn_after_check_{idx}"):
                    st.session_state.current_question_index += 1
                    st.session_state.show_explanation = False
                    st.rerun()

    else:
        # ì¼ë°˜ ì½˜í…ì¸  (í•µì‹¬ ìš”ì•½ ë…¸íŠ¸, ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´) ì¶œë ¥
        if st.session_state.get("generated_content"):
            st.markdown("---")
            st.markdown(f"### {content_type}")
            st.markdown(st.session_state.generated_content)

    # --- í€´ì¦ˆ ì™„ë£Œ í›„ ë¡œì§ì€ ìœ„ìª½ (idx >= len(quiz_data))ì—ì„œ ì²˜ë¦¬ë¨ --

# -------------------- LSTM Tab --------------------
elif feature_selection == L["lstm_tab"]:
    # ... (ê¸°ì¡´ LSTM íƒ­ ë¡œì§ ìœ ì§€)
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])

    # â­ ìµœì í™”: ë²„íŠ¼ ìì²´ê°€ rerunì„ ìœ ë„í•˜ë¯€ë¡œ ëª…ì‹œì  rerun ì œê±° (ë²„íŠ¼ í´ë¦­ ì‹œ ìë™ ì¬ì‹¤í–‰)
    if st.button(L["lstm_rerun_button"]):
        # ë²„íŠ¼ í´ë¦­ ì‹œ Streamlitì´ ìë™ìœ¼ë¡œ ì¬ì‹¤í–‰
        pass

    try:
        data = load_or_train_lstm()
        predicted_score = float(np.clip(data[-1] + np.random.uniform(-3, 5), 50, 100))

        st.markdown("---")
        st.subheader(L["lstm_result_header"])

        col_score, col_chart = st.columns([1, 2])

        with col_score:
            suffix = "ì " if st.session_state.language == "ko" else ""
            st.metric(L["lstm_score_metric"], f"{predicted_score:.1f}{suffix}")
            st.info(L["lstm_score_info"].format(predicted_score=predicted_score))

        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label="Past Scores", marker="o")
            ax.plot(len(data), predicted_score, marker="*", markersize=10)
            ax.set_title(L["lstm_header"])
            ax.set_xlabel("Time (attempts)")
            ax.set_ylabel("Score (0-100)")
            ax.legend()
            st.pyplot(fig)
    except Exception as e:
        st.info(f"LSTM ê¸°ëŠ¥ ì—ëŸ¬: {e}")
