# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Ïò§ÎîîÏò§ Ï≤òÎ¶¨ Î™®Îìà
ÏùåÏÑ± Ï†ÑÏÇ¨(Whisper), TTS, ÏùåÏÑ± Í∏∞Î°ù Í¥ÄÎ¶¨ Îì±Ïùò Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.
"""

import os
import io
import time
import uuid
import base64
import tempfile
import hashlib
from datetime import datetime
from typing import List, Dict, Any
import streamlit as st
import google.generativeai as genai

from config import AUDIO_DIR, VOICE_META_FILE
from utils import _load_json, _save_json
from llm_client import get_api_key, init_openai_audio_client
from lang_pack import LANG

# TTS ÏùåÏÑ± ÏÑ§Ï†ï
TTS_VOICES = {
    "customer_male": {
        "gender": "male",
        "voice": "alloy"  # Male voice
    },
    "customer_female": {
        "gender": "female",
        "voice": "nova"  # Female voice
    },
    "customer": {
        "gender": "male",
        "voice": "alloy"  # Default male voice (fallback)
    },
    "agent": {
        "gender": "female",
        "voice": "shimmer"  # Distinct Female, Professional/Agent
    },
    "supervisor": {
        "gender": "female",
        "voice": "nova"  # Another Distinct Female, Informative/Supervisor
    }
}

def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = "audio/webm", lang_code: str = None, auto_detect: bool = True) -> str:
    """
    OpenAI Whisper API ÎòêÎäî Gemini APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïò§ÎîîÏò§ Î∞îÏù¥Ìä∏Î•º ÌÖçÏä§Ìä∏Î°ú Ï†ÑÏÇ¨Ìï©ÎãàÎã§.
    OpenAIÍ∞Ä Ïã§Ìå®ÌïòÎ©¥ GeminiÎ°ú ÏûêÎèô fallbackÌï©ÎãàÎã§.
    
    Args:
        audio_bytes: Ï†ÑÏÇ¨Ìï† Ïò§ÎîîÏò§ Î∞îÏù¥Ìä∏
        mime_type: Ïò§ÎîîÏò§ MIME ÌÉÄÏûÖ
        lang_code: Ïñ∏Ïñ¥ ÏΩîÎìú (ko, en, ja Îì±). NoneÏù¥Í±∞ÎÇò auto_detect=TrueÏù¥Î©¥ ÏûêÎèô Í∞êÏßÄ
        auto_detect: TrueÏù¥Î©¥ Ïñ∏Ïñ¥Î•º ÏûêÎèô Í∞êÏßÄ (lang_code Î¨¥Ïãú)
    """
    # Ïñ∏Ïñ¥ ÌÇ§ ÏïàÏ†ÑÌïòÍ≤å Í∞ÄÏ†∏Ïò§Í∏∞
    current_lang = st.session_state.get("language", "ko")
    if current_lang not in ["ko", "en", "ja"]:
        current_lang = "ko"
    L = LANG.get(current_lang, LANG["ko"])
    
    # ÏûÑÏãú ÌååÏùº Ï†ÄÏû• (API Ìò∏ÌôòÏÑ±)
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp.write(audio_bytes)
    tmp.flush()
    tmp.close()
    
    # 1Ô∏è‚É£ OpenAI Whisper API ÏãúÎèÑ
    client = st.session_state.openai_client
    if client is not None:
        try:
            with open(tmp.name, "rb") as f:
                # Ïñ∏Ïñ¥ ÏûêÎèô Í∞êÏßÄ ÎòêÎäî ÏßÄÏ†ïÎêú Ïñ∏Ïñ¥ ÏÇ¨Ïö©
                if auto_detect or lang_code is None:
                    # language ÌååÎùºÎØ∏ÌÑ∞Î•º ÏÉùÎûµÌïòÎ©¥ WhisperÍ∞Ä ÏûêÎèôÏúºÎ°ú Ïñ∏Ïñ¥Î•º Í∞êÏßÄÌï©ÎãàÎã§
                    res = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                    )
                else:
                    whisper_lang = {"ko": "ko", "en": "en", "ja": "ja"}.get(lang_code, "en")
                    res = client.audio.transcriptions.create(
                        model="whisper-1",
                        file=f,
                        response_format="text",
                        language=whisper_lang,
                    )
            # res.text ÏÜçÏÑ±Ïù¥ ÏûàÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥† ÏóÜÏúºÎ©¥ res ÏûêÏ≤¥Î•º Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
            result = res.text.strip() if hasattr(res, 'text') else str(res).strip()
            if result:
                try:
                    os.remove(tmp.name)
                except OSError:
                    pass
                return result
        except Exception as e:
            # OpenAI Ïã§Ìå® Ïãú Î°úÍ∑∏Îßå ÎÇ®Í∏∞Í≥† GeminiÎ°ú fallback
            print(f"OpenAI Whisper failed: {e}")
    
    # 2Ô∏è‚É£ Gemini API fallback
    gemini_key = get_api_key("gemini")
    if gemini_key:
        try:
            import base64
            genai.configure(api_key=gemini_key)
            
            # GeminiÎäî Ïò§ÎîîÏò§ ÌååÏùºÏùÑ base64Î°ú Ïù∏ÏΩîÎî©ÌïòÏó¨ Ï†ÑÏÜ°
            with open(tmp.name, "rb") as f:
                audio_data = f.read()
                audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # Gemini 2.0 Flash Î™®Îç∏ ÏÇ¨Ïö© (Ïò§ÎîîÏò§ ÏßÄÏõê)
            model = genai.GenerativeModel("gemini-2.0-flash-exp")
            
            # ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
            lang_prompt = ""
            if lang_code:
                lang_map = {"ko": "ÌïúÍµ≠Ïñ¥", "en": "English", "ja": "Êó•Êú¨Ë™û"}
                lang_prompt = f"Ïù¥ Ïò§ÎîîÏò§Îäî {lang_map.get(lang_code, 'English')}Î°ú ÎßêÌïòÍ≥† ÏûàÏäµÎãàÎã§. "
            
            prompt = f"{lang_prompt}Ïù¥ Ïò§ÎîîÏò§Î•º ÌÖçÏä§Ìä∏Î°ú Ï†ÑÏÇ¨Ìï¥Ï£ºÏÑ∏Ïöî. Ïò§ÏßÅ Ï†ÑÏÇ¨Îêú ÌÖçÏä§Ìä∏Îßå Î∞òÌôòÌïòÏÑ∏Ïöî."
            
            # GeminiÎäî ÌååÏùº ÏóÖÎ°úÎìú Î∞©Ïãù ÏÇ¨Ïö© (Gemini 2.0 FlashÎäî Ïò§ÎîîÏò§ ÏßÄÏõê)
            try:
                audio_file = genai.upload_file(path=tmp.name, mime_type=mime_type)
                
                # ÌååÏùº ÏóÖÎ°úÎìú ÌõÑ Ïû†Ïãú ÎåÄÍ∏∞ (ÏóÖÎ°úÎìú ÏôÑÎ£å ÎåÄÍ∏∞)
                import time
                time.sleep(1)
                
                response = model.generate_content([prompt, audio_file])
                result = response.text.strip() if response.text else ""
                
                # ÌååÏùº ÏÇ≠Ï†ú
                try:
                    genai.delete_file(audio_file.name)
                except Exception as del_err:
                    print(f"Failed to delete Gemini file: {del_err}")
            except Exception as upload_err:
                # ÌååÏùº ÏóÖÎ°úÎìú Ïã§Ìå® Ïãú Îã§Î•∏ Î∞©Î≤ï ÏãúÎèÑ
                print(f"Gemini file upload failed: {upload_err}")
                # ÎåÄÏïà: base64 Ïù∏ÏΩîÎî©Îêú Ïò§ÎîîÏò§Î•º ÏßÅÏ†ë Ï†ÑÏÜ° (Î™®Îç∏Ïù¥ ÏßÄÏõêÌïòÎäî Í≤ΩÏö∞)
                raise upload_err
            
            if result:
                try:
                    os.remove(tmp.name)
                except OSError:
                    pass
                return result
            else:
                raise Exception("Gemini returned empty result")
        except Exception as e:
            print(f"Gemini transcription failed: {e}")
            # GeminiÎèÑ Ïã§Ìå®Ìïú Í≤ΩÏö∞ ÏóêÎü¨ Î©îÏãúÏßÄ Î∞òÌôò
            try:
                os.remove(tmp.name)
            except OSError:
                pass
            return f"‚ùå {L.get('whisper_client_error', 'Ï†ÑÏÇ¨ Ïã§Ìå®')}: OpenAIÏôÄ Gemini Î™®Îëê Ïã§Ìå®ÌñàÏäµÎãàÎã§. ({str(e)[:100]})"
    else:
        # Îëê API Î™®Îëê ÏÇ¨Ïö© Î∂àÍ∞Ä
        try:
            os.remove(tmp.name)
        except OSError:
            pass
        return f"‚ùå {L.get('openai_missing', 'OpenAI API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.')} ÎòêÎäî Gemini API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§."



def transcribe_audio(audio_bytes, filename="audio.wav"):
    client = st.session_state.openai_client

    # 1Ô∏è‚É£ OpenAI Whisper ÏãúÎèÑ
    if client:
        try:
            import io
            bio = io.BytesIO(audio_bytes)
            bio.name = filename
            resp = client.audio.transcriptions.create(
                model="whisper-1",
                file=bio,
            )
            return resp.text
        except Exception as e:
            print("Whisper OpenAI failed:", e)

    # 2Ô∏è‚É£ Gemini STT fallback
    try:
        genai.configure(api_key=get_api_key("gemini"))
        model = genai.GenerativeModel("gemini-2.5-flash")
        text = model.generate_content("Transcribe this audio:").text
        return text or ""
    except Exception as e:
        print("Gemini STT failed:", e)

    return "‚ùå STT not available"


# ========================================
# ÎπÑÎîîÏò§ ÎèôÍ∏∞Ìôî Í¥ÄÎ†® Ìï®Ïàò
# ========================================

def analyze_text_for_video_selection(text: str, current_lang_key: str, 
                                     agent_last_response: str = None,
                                     conversation_context: List[Dict] = None) -> Dict[str, Any]:
    """
    LLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌÖçÏä§Ìä∏Î•º Î∂ÑÏÑùÌïòÍ≥† Ï†ÅÏ†àÌïú Í∞êÏ†ï ÏÉÅÌÉúÏôÄ Ï†úÏä§Ï≤òÎ•º ÌåêÎã®Ìï©ÎãàÎã§.
    OpenAI/Gemini APIÎ•º ÌôúÏö©Ìïú ÏòÅÏÉÅ RAGÏùò ÌïµÏã¨ Í∏∞Îä•ÏûÖÎãàÎã§.
    
    ‚≠ê Gemini Ï†úÏïà Ï†ÅÏö©: Í∏¥Í∏âÎèÑ, ÎßåÏ°±ÎèÑ Î≥ÄÌôî, ÏóêÏù¥Ï†ÑÌä∏ ÎãµÎ≥Ä Í∏∞Î∞ò ÏòàÏ∏° Ï∂îÍ∞Ä
    
    Args:
        text: Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏ (Í≥†Í∞ùÏùò ÏßàÎ¨∏/ÏùëÎãµ)
        current_lang_key: ÌòÑÏû¨ Ïñ∏Ïñ¥ ÌÇ§
        agent_last_response: ÏóêÏù¥Ï†ÑÌä∏Ïùò ÎßàÏßÄÎßâ ÎãµÎ≥Ä (ÏÑ†ÌÉùÏ†Å, ÏòàÏ∏° Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ)
        conversation_context: ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏ (ÏÑ†ÌÉùÏ†Å, ÎßåÏ°±ÎèÑ Î≥ÄÌôî Î∂ÑÏÑùÏö©)
    
    Returns:
        {
            "emotion": "NEUTRAL" | "HAPPY" | "ANGRY" | "ASKING" | "SAD",
            "gesture": "NONE" | "HAND_WAVE" | "NOD" | "SHAKE_HEAD" | "POINT",
            "urgency": "LOW" | "MEDIUM" | "HIGH",  # ‚≠ê Ï∂îÍ∞Ä: Í∏¥Í∏âÎèÑ
            "satisfaction_delta": -1.0 to 1.0,  # ‚≠ê Ï∂îÍ∞Ä: ÎßåÏ°±ÎèÑ Î≥ÄÌôî (-1: Í∞êÏÜå, 0: Ïú†ÏßÄ, 1: Ï¶ùÍ∞Ä)
            "confidence": 0.0-1.0
        }
    """
    if not text or not text.strip():
        return {
            "emotion": "NEUTRAL", 
            "gesture": "NONE", 
            "urgency": "LOW",
            "satisfaction_delta": 0.0,
            "confidence": 0.5
        }
    
    L = LANG.get(current_lang_key, LANG["ko"])
    
    # ‚≠ê Gemini Ï†úÏïà: ÏóêÏù¥Ï†ÑÌä∏ ÎãµÎ≥Ä Í∏∞Î∞ò ÏòàÏ∏° Ïª®ÌÖçÏä§Ìä∏ Íµ¨ÏÑ±
    context_info = ""
    if agent_last_response:
        context_info = f"""
ÏóêÏù¥Ï†ÑÌä∏Ïùò ÎßàÏßÄÎßâ ÎãµÎ≥Ä: "{agent_last_response}"

ÏóêÏù¥Ï†ÑÌä∏Ïùò ÎãµÎ≥ÄÏùÑ Í≥†Î†§ÌñàÏùÑ Îïå, Í≥†Í∞ùÏù¥ ÏßÄÍ∏à ÎßêÌïòÎäî ÎÇ¥Ïö©ÏùÄ Ïñ¥Îñ§ Í∞êÏ†ïÏùÑ ÏàòÎ∞òÌï† Í≤ÉÏù∏ÏßÄ ÏòàÏ∏°ÌïòÏÑ∏Ïöî.
ÏòàÎ•º Îì§Ïñ¥:
- ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏÜîÎ£®ÏÖòÏùÑ Ï†úÏãúÌñàÎã§Î©¥ ‚Üí Í≥†Í∞ùÏùÄ HAPPY ÎòêÎäî ASKING (Ï∂îÍ∞Ä ÏßàÎ¨∏)
- ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Í±∞Ï†àÌñàÎã§Î©¥ ‚Üí Í≥†Í∞ùÏùÄ ANGRY ÎòêÎäî SAD
- ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏßàÎ¨∏ÏùÑ ÌñàÎã§Î©¥ ‚Üí Í≥†Í∞ùÏùÄ ASKING (ÎãµÎ≥Ä) ÎòêÎäî NEUTRAL
"""
    
    # ‚≠ê Gemini Ï†úÏïà: ÎßåÏ°±ÎèÑ Î≥ÄÌôî Î∂ÑÏÑù Ïª®ÌÖçÏä§Ìä∏
    satisfaction_context = ""
    if conversation_context and len(conversation_context) > 1:
        # ÏµúÍ∑º ÎåÄÌôîÏùò Í∞êÏ†ï Î≥ÄÌôî Ï∂îÏ†Å
        recent_emotions = []
        for msg in conversation_context[-3:]:  # ÏµúÍ∑º 3Í∞ú Î©îÏãúÏßÄ
            if msg.get("role") == "customer_rebuttal" or msg.get("role") == "customer":
                recent_emotions.append(msg.get("content", ""))
        
        if len(recent_emotions) >= 2:
            satisfaction_context = f"""
ÏµúÍ∑º ÎåÄÌôî ÌùêÎ¶Ñ:
- Ïù¥Ï†Ñ Í≥†Í∞ù Î©îÏãúÏßÄ: "{recent_emotions[-2] if len(recent_emotions) >= 2 else ''}"
- ÌòÑÏû¨ Í≥†Í∞ù Î©îÏãúÏßÄ: "{recent_emotions[-1]}"

ÎßåÏ°±ÎèÑ Î≥ÄÌôîÎ•º Î∂ÑÏÑùÌïòÏÑ∏Ïöî:
- Ïù¥Ï†ÑÎ≥¥Îã§ Îçî Í∏çÏ†ïÏ†ÅÏù¥Î©¥ satisfaction_delta > 0
- Ïù¥Ï†ÑÎ≥¥Îã§ Îçî Î∂ÄÏ†ïÏ†ÅÏù¥Î©¥ satisfaction_delta < 0
- ÎπÑÏä∑ÌïòÎ©¥ satisfaction_delta ‚âà 0
"""
    
    # ‚≠ê Gemini Ï†úÏïà: Í∞úÏÑ†Îêú LLM ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
    prompt = f"""Îã§Ïùå Í≥†Í∞ùÏùò ÌÖçÏä§Ìä∏Î•º Î∂ÑÏÑùÌïòÏó¨ Ï†ÅÏ†àÌïú Í∞êÏ†ï ÏÉÅÌÉú, Ï†úÏä§Ï≤ò, Í∏¥Í∏âÎèÑ, ÎßåÏ°±ÎèÑ Î≥ÄÌôîÎ•º ÌåêÎã®ÌïòÏÑ∏Ïöî.

Í≥†Í∞ù ÌÖçÏä§Ìä∏: "{text}"
{context_info}
{satisfaction_context}

Îã§Ïùå JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî (Îã§Î•∏ ÏÑ§Î™Ö ÏóÜÏù¥):
{{
    "emotion": "NEUTRAL" | "HAPPY" | "ANGRY" | "ASKING" | "SAD",
    "gesture": "NONE" | "HAND_WAVE" | "NOD" | "SHAKE_HEAD" | "POINT",
    "urgency": "LOW" | "MEDIUM" | "HIGH",
    "satisfaction_delta": -1.0 to 1.0,
    "confidence": 0.0-1.0
}}

Í∞êÏ†ï ÌåêÎã® Í∏∞Ï§Ä (ÏÑ∏Î∂ÑÌôî):
- HAPPY: Í∏çÏ†ïÏ†Å ÌëúÌòÑ, Í∞êÏÇ¨, ÎßåÏ°±, Ìï¥Í≤∞Îê® ("Í∞êÏÇ¨Ìï©ÎãàÎã§", "Ï¢ãÏïÑÏöî", "ÏôÑÎ≤ΩÌï¥Ïöî", "Ïù¥Ï†ú Ïù¥Ìï¥ÌñàÏñ¥Ïöî")
- ANGRY: Î∂àÎßå, ÌôîÎÇ®, Í±∞Î∂Ä, Í∞ïÌïú Î∂ÄÏ†ï ("ÌôîÍ∞Ä ÎÇòÏöî", "Î∂àÍ∞ÄÎä•Ìï¥Ïöî", "Í±∞Ï†àÌï©ÎãàÎã§", "ÎßêÎèÑ Ïïà ÎèºÏöî")
- ASKING: ÏßàÎ¨∏, Í∂ÅÍ∏àÌï®, ÌôïÏù∏ ÏöîÏ≤≠, Ï†ïÎ≥¥ ÏöîÍµ¨ ("Ïñ¥ÎñªÍ≤å", "Ïôú", "ÏïåÎ†§Ï£ºÏÑ∏Ïöî", "Ï£ºÎ¨∏Î≤àÌò∏Í∞Ä Î≠êÏòàÏöî?")
- SAD: Ïä¨Ìîî, Ïã§Îßù, Ï¢åÏ†à ("Ïä¨ÌîÑÎÑ§Ïöî", "Ïã§ÎßùÌñàÏñ¥Ïöî", "ÏïÑÏâΩÏäµÎãàÎã§", "Í∑∏Î†áÎã§Î©¥ Ïñ¥Ï©î Ïàò ÏóÜÎÑ§Ïöî")
- NEUTRAL: Ï§ëÎ¶ΩÏ†Å ÌëúÌòÑ, Îã®Ïàú Ï†ïÎ≥¥ Ï†ÑÎã¨ (Í∏∞Î≥∏Í∞í)

Ï†úÏä§Ï≤ò ÌåêÎã® Í∏∞Ï§Ä:
- HAND_WAVE: Ïù∏ÏÇ¨, ÌôòÏòÅ ("ÏïàÎÖïÌïòÏÑ∏Ïöî", "Î∞òÍ∞ëÏäµÎãàÎã§")
- NOD: ÎèôÏùò, Í∏çÏ†ï, Ïù¥Ìï¥ ("ÎÑ§", "ÎßûÏïÑÏöî", "Í∑∏Î†áÏäµÎãàÎã§", "ÏïåÍ≤†ÏäµÎãàÎã§")
- SHAKE_HEAD: Î∂ÄÏ†ï, Í±∞Î∂Ä, Î∂àÎßåÏ°± ("ÏïÑÎãàÏöî", "Ïïà Îê©ÎãàÎã§", "Í∑∏Í±¥ ÏïÑÎãàÏóêÏöî")
- POINT: ÏÑ§Î™Ö, ÏßÄÏãú, ÌäπÏ†ï Ìï≠Î™© Ïñ∏Í∏â ("Ïó¨Í∏∞", "Ïù¥Í≤É", "Ï†ÄÍ≤É", "Ï£ºÎ¨∏Î≤àÌò∏Îäî")
- NONE: ÌäπÎ≥ÑÌïú Ï†úÏä§Ï≤ò ÏóÜÏùå (Í∏∞Î≥∏Í∞í)

Í∏¥Í∏âÎèÑ ÌåêÎã® Í∏∞Ï§Ä:
- HIGH: Ï¶âÏãú Ìï¥Í≤∞ ÌïÑÏöî, Í∏¥Í∏âÌïú Î¨∏Ï†ú ("ÏßÄÍ∏à ÎãπÏû•", "Î∞îÎ°ú", "Í∏¥Í∏â", "Ï§ëÏöîÌï¥Ïöî")
- MEDIUM: Îπ†Î•∏ Ìï¥Í≤∞ ÏÑ†Ìò∏, Ï§ëÏöîÌïòÏßÄÎßå Í∏¥Í∏âÌïòÏßÄ ÏïäÏùå
- LOW: ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏Ïùò, Í∏¥Í∏âÌïòÏßÄ ÏïäÏùå (Í∏∞Î≥∏Í∞í)

ÎßåÏ°±ÎèÑ Î≥ÄÌôî (satisfaction_delta):
- 1.0: Îß§Ïö∞ ÎßåÏ°±, Î¨∏Ï†ú Ìï¥Í≤∞Îê®, Í∞êÏÇ¨ ÌëúÌòÑ
- 0.5: ÎßåÏ°±, Í∏çÏ†ïÏ†Å Î∞òÏùë
- 0.0: Ï§ëÎ¶Ω, Î≥ÄÌôî ÏóÜÏùå
- -0.5: Î∂àÎßåÏ°±, Î∂ÄÏ†ïÏ†Å Î∞òÏùë
- -1.0: Îß§Ïö∞ Î∂àÎßåÏ°±, ÌôîÎÇ®, Í±∞Î∂Ä

JSONÎßå ÏùëÎãµÌïòÏÑ∏Ïöî:"""

    try:
        # LLM Ìò∏Ï∂ú
        if st.session_state.is_llm_ready:
            response_text = run_llm(prompt)
            
            # JSON ÌååÏã± ÏãúÎèÑ
            try:
                # JSON Î∂ÄÎ∂ÑÎßå Ï∂îÏ∂ú (ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞)
                import re
                json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    # Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
                    valid_emotions = ["NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD"]
                    valid_gestures = ["NONE", "HAND_WAVE", "NOD", "SHAKE_HEAD", "POINT"]
                    valid_urgencies = ["LOW", "MEDIUM", "HIGH"]
                    
                    emotion = result.get("emotion", "NEUTRAL")
                    gesture = result.get("gesture", "NONE")
                    urgency = result.get("urgency", "LOW")
                    satisfaction_delta = float(result.get("satisfaction_delta", 0.0))
                    confidence = float(result.get("confidence", 0.7))
                    
                    if emotion not in valid_emotions:
                        emotion = "NEUTRAL"
                    if gesture not in valid_gestures:
                        gesture = "NONE"
                    if urgency not in valid_urgencies:
                        urgency = "LOW"
                    
                    # ‚≠ê Gemini Ï†úÏïà: ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
                    context_keywords = []
                    text_lower_for_context = text.lower()
                    
                    # Ï£ºÏöî ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú Îß§Ìïë
                    if any(word in text_lower_for_context for word in ["Ï£ºÎ¨∏Î≤àÌò∏", "order number", "Ï£ºÎ¨∏ Î≤àÌò∏"]):
                        context_keywords.append("order_number")
                    if any(word in text_lower_for_context for word in ["Ìï¥Í≤∞", "ÏôÑÎ£å", "Í∞êÏÇ¨", "solution", "resolved"]):
                        if satisfaction_delta > 0.3:
                            context_keywords.append("solution_accepted")
                    if any(word in text_lower_for_context for word in ["Í±∞Ï†à", "Î∂àÍ∞Ä", "Ïïà Îê©ÎãàÎã§", "denied", "cannot"]):
                        if emotion == "ANGRY":
                            context_keywords.append("policy_denial")
                    
                    return {
                        "emotion": emotion,
                        "gesture": gesture,
                        "urgency": urgency,
                        "satisfaction_delta": max(-1.0, min(1.0, satisfaction_delta)),
                        "context_keywords": context_keywords,  # ‚≠ê Ï∂îÍ∞Ä
                        "confidence": max(0.0, min(1.0, confidence))
                    }
            except json.JSONDecodeError:
                pass
        
        # LLM Ìò∏Ï∂ú Ïã§Ìå® Ïãú ÌÇ§ÏõåÎìú Í∏∞Î∞ò Í∞ÑÎã®Ìïú Î∂ÑÏÑù
        text_lower = text.lower()
        emotion = "NEUTRAL"
        gesture = "NONE"
        urgency = "LOW"
        satisfaction_delta = 0.0
        
        # Í∞êÏ†ï ÌÇ§ÏõåÎìú Î∂ÑÏÑù
        if any(word in text_lower for word in ["Í∞êÏÇ¨", "Ï¢ãÏïÑ", "ÏôÑÎ≤Ω", "ÎßåÏ°±", "Í≥†ÎßàÏõå", "Ìï¥Í≤∞"]):
            emotion = "HAPPY"
            satisfaction_delta = 0.5
        elif any(word in text_lower for word in ["Ìôî", "Î∂àÎßå", "Í±∞Ï†à", "Î∂àÍ∞ÄÎä•", "Ïïà Îê©ÎãàÎã§", "ÎßêÎèÑ Ïïà Îèº"]):
            emotion = "ANGRY"
            satisfaction_delta = -0.5
        elif any(word in text_lower for word in ["Ïñ¥ÎñªÍ≤å", "Ïôú", "ÏïåÎ†§", "ÏßàÎ¨∏", "Í∂ÅÍ∏à", "Ï£ºÎ¨∏Î≤àÌò∏"]):
            emotion = "ASKING"
        elif any(word in text_lower for word in ["Ïä¨ÌîÑ", "Ïã§Îßù", "ÏïÑÏâΩ", "Í∑∏Î†áÎã§Î©¥"]):
            emotion = "SAD"
            satisfaction_delta = -0.3
        
        # Í∏¥Í∏âÎèÑ ÌÇ§ÏõåÎìú Î∂ÑÏÑù
        if any(word in text_lower for word in ["ÏßÄÍ∏à ÎãπÏû•", "Î∞îÎ°ú", "Í∏¥Í∏â", "Ï§ëÏöîÌï¥Ïöî", "Ï¶âÏãú"]):
            urgency = "HIGH"
        elif any(word in text_lower for word in ["Îπ®Î¶¨", "Í∞ÄÎä•Ìïú Ìïú", "ÏµúÎåÄÌïú"]):
            urgency = "MEDIUM"
        
        # Ï†úÏä§Ï≤ò ÌÇ§ÏõåÎìú Î∂ÑÏÑù
        if any(word in text_lower for word in ["ÏïàÎÖï", "Î∞òÍ∞ë", "Ïù∏ÏÇ¨"]):
            gesture = "HAND_WAVE"
        elif any(word in text_lower for word in ["ÎÑ§", "ÎßûÏïÑ", "Í∑∏Îûò", "ÎèôÏùò", "ÏïåÍ≤†ÏäµÎãàÎã§"]):
            gesture = "NOD"
            if emotion == "HAPPY":
                satisfaction_delta = 0.3
        elif any(word in text_lower for word in ["ÏïÑÎãà", "Ïïà Îê©ÎãàÎã§", "Í±∞Ï†à"]):
            gesture = "SHAKE_HEAD"
            satisfaction_delta = -0.2
        elif any(word in text_lower for word in ["Ïó¨Í∏∞", "Ïù¥Í≤É", "Ï†ÄÍ≤É", "Ïù¥Í±∞", "Ï£ºÎ¨∏Î≤àÌò∏"]):
            gesture = "POINT"
        
        # ‚≠ê Gemini Ï†úÏïà: ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÏÑù)
        context_keywords = []
        if any(word in text_lower for word in ["Ï£ºÎ¨∏Î≤àÌò∏", "order number", "Ï£ºÎ¨∏ Î≤àÌò∏"]):
            context_keywords.append("order_number")
        if any(word in text_lower for word in ["Ìï¥Í≤∞", "ÏôÑÎ£å", "Í∞êÏÇ¨", "solution"]):
            if satisfaction_delta > 0.3:
                context_keywords.append("solution_accepted")
        if any(word in text_lower for word in ["Í±∞Ï†à", "Î∂àÍ∞Ä", "Ïïà Îê©ÎãàÎã§"]):
            if emotion == "ANGRY":
                context_keywords.append("policy_denial")
        
        return {
            "emotion": emotion,
            "gesture": gesture,
            "urgency": urgency,
            "satisfaction_delta": satisfaction_delta,
            "context_keywords": context_keywords,  # ‚≠ê Ï∂îÍ∞Ä
            "confidence": 0.6  # ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î∂ÑÏÑùÏùÄ ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ
        }
    
    except Exception as e:
        print(f"ÌÖçÏä§Ìä∏ Î∂ÑÏÑù Ïò§Î•ò: {e}")
        return {
            "emotion": "NEUTRAL", 
            "gesture": "NONE", 
            "urgency": "LOW",
            "satisfaction_delta": 0.0,
            "context_keywords": [],  # ‚≠ê Ï∂îÍ∞Ä
            "confidence": 0.5
        }


def get_video_path_by_avatar(gender: str, emotion: str, is_speaking: bool = False, 
                             gesture: str = "NONE", context_keywords: List[str] = None) -> str:
    """
    Í≥†Í∞ù ÏïÑÎ∞îÌÉÄ Ï†ïÎ≥¥(ÏÑ±Î≥Ñ, Í∞êÏ†ï ÏÉÅÌÉú, Ï†úÏä§Ï≤ò, ÏÉÅÌô©)Ïóê Îî∞Îùº Ï†ÅÏ†àÌïú ÎπÑÎîîÏò§ Í≤ΩÎ°úÎ•º Î∞òÌôòÌï©ÎãàÎã§.
    OpenAI/Gemini Í∏∞Î∞ò ÏòÅÏÉÅ RAG: LLMÏù¥ Î∂ÑÏÑùÌïú Í∞êÏ†ï/Ï†úÏä§Ï≤òÏóê Îî∞Îùº ÎπÑÎîîÏò§ ÌÅ¥Î¶ΩÏùÑ ÏÑ†ÌÉùÌï©ÎãàÎã§.
    
    ‚≠ê Gemini Ï†úÏïà: ÏÉÅÌô©Î≥Ñ ÎπÑÎîîÏò§ ÌÅ¥Î¶Ω Ìå®ÌÑ¥ ÌôïÏû• (Ïòà: male_asking_order_number.mp4)
    
    Args:
        gender: "male" ÎòêÎäî "female"
        emotion: "NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD"
        is_speaking: ÎßêÌïòÎäî Ï§ëÏù∏ÏßÄ Ïó¨Î∂Ä
        gesture: "NONE", "HAND_WAVE", "NOD", "SHAKE_HEAD", "POINT"
        context_keywords: ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú Î¶¨Ïä§Ìä∏ (Ïòà: ["order_number", "solution_accepted", "policy_denial"])
    
    Returns:
        ÎπÑÎîîÏò§ ÌååÏùº Í≤ΩÎ°ú (ÏóÜÏúºÎ©¥ None)
    """
    # ÎπÑÎîîÏò§ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú (ÏÇ¨Ïö©ÏûêÍ∞Ä ÏÑ§Ï†ïÌïú ÎπÑÎîîÏò§ ÌååÏùºÎì§Ïù¥ Ï†ÄÏû•Îêú ÏúÑÏπò)
    video_base_dir = os.path.join(DATA_DIR, "videos")
    os.makedirs(video_base_dir, exist_ok=True)
    
    # ‚≠ê Gemini Ï†úÏïà: Ïö∞ÏÑ†ÏàúÏúÑ -1 - Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î∞ò Ï∂îÏ≤ú ÎπÑÎîîÏò§ (Í∞ÄÏû• Ïö∞ÏÑ†)
    if context_keywords:
        db_recommended = get_recommended_video_from_database(emotion, gesture, context_keywords)
        if db_recommended:
            return db_recommended
    else:
        db_recommended = get_recommended_video_from_database(emotion, gesture, [])
        if db_recommended:
            return db_recommended
    
    # ‚≠ê Gemini Ï†úÏïà: Ïö∞ÏÑ†ÏàúÏúÑ 0 - ÏÉÅÌô©Î≥Ñ ÎπÑÎîîÏò§ ÌÅ¥Î¶Ω (Í∞ÄÏû• Íµ¨Ï≤¥Ï†Å)
    if context_keywords:
        for keyword in context_keywords:
            # ÏÉÅÌô©Î≥Ñ ÌååÏùºÎ™Ö Ìå®ÌÑ¥ ÏãúÎèÑ (Ïòà: male_asking_order_number.mp4)
            context_filename = f"{gender}_{emotion.lower()}_{keyword}"
            if is_speaking:
                context_filename += "_speaking"
            context_filename += ".mp4"
            context_path = os.path.join(video_base_dir, context_filename)
            if os.path.exists(context_path):
                return context_path
            
            # ÏÑ∏ÏÖò ÏÉÅÌÉúÏóêÏÑúÎèÑ ÌôïÏù∏
            context_video_key = f"video_{gender}_{emotion.lower()}_{keyword}"
            if context_video_key in st.session_state and st.session_state[context_video_key]:
                video_path = st.session_state[context_video_key]
                if os.path.exists(video_path):
                    return video_path
    
    # Ïö∞ÏÑ†ÏàúÏúÑ 1: Ï†úÏä§Ï≤òÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ Ï†úÏä§Ï≤òÎ≥Ñ ÎπÑÎîîÏò§ ÏãúÎèÑ
    if gesture != "NONE" and gesture:
        gesture_video_key = f"video_{gender}_{emotion.lower()}_{gesture.lower()}"
        if gesture_video_key in st.session_state and st.session_state[gesture_video_key]:
            video_path = st.session_state[gesture_video_key]
            if os.path.exists(video_path):
                return video_path
        
        # Ï†úÏä§Ï≤òÎ≥Ñ ÌååÏùºÎ™Ö Ìå®ÌÑ¥ ÏãúÎèÑ
        gesture_filename = f"{gender}_{emotion.lower()}_{gesture.lower()}"
        if is_speaking:
            gesture_filename += "_speaking"
        gesture_filename += ".mp4"
        gesture_path = os.path.join(video_base_dir, gesture_filename)
        if os.path.exists(gesture_path):
            return gesture_path
    
    # Ïö∞ÏÑ†ÏàúÏúÑ 2: Í∞êÏ†ï ÏÉÅÌÉúÎ≥Ñ ÎπÑÎîîÏò§ (Ï†úÏä§Ï≤ò ÏóÜÏù¥)
    video_key = f"video_{gender}_{emotion.lower()}"
    if is_speaking:
        video_key += "_speaking"
    
    # ÏÑ∏ÏÖò ÏÉÅÌÉúÏóê Ï†ÄÏû•Îêú ÎπÑÎîîÏò§ Í≤ΩÎ°úÍ∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
    if video_key in st.session_state and st.session_state[video_key]:
        video_path = st.session_state[video_key]
        if os.path.exists(video_path):
            return video_path
    
    # Í∏∞Î≥∏ ÎπÑÎîîÏò§ ÌååÏùºÎ™Ö Ìå®ÌÑ¥ ÏãúÎèÑ
    video_filename = f"{gender}_{emotion.lower()}"
    if is_speaking:
        video_filename += "_speaking"
    video_filename += ".mp4"
    
    video_path = os.path.join(video_base_dir, video_filename)
    if os.path.exists(video_path):
        return video_path
    
    # Ïö∞ÏÑ†ÏàúÏúÑ 3: Í∏∞Î≥∏ ÎπÑÎîîÏò§ ÌååÏùº ÏãúÎèÑ (Ï§ëÎ¶Ω ÏÉÅÌÉú)
    default_video = os.path.join(video_base_dir, f"{gender}_neutral.mp4")
    if os.path.exists(default_video):
        return default_video
    
    # Ïö∞ÏÑ†ÏàúÏúÑ 4: ÏÑ∏ÏÖò ÏÉÅÌÉúÏóêÏÑú ÏóÖÎ°úÎìúÎêú ÎπÑÎîîÏò§ ÌôïÏù∏
    if "current_customer_video" in st.session_state and st.session_state.current_customer_video:
        return st.session_state.current_customer_video
    
    return None


# ‚≠ê Gemini Ï†úÏïà: ÎπÑÎîîÏò§ Îß§Ìïë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í¥ÄÎ¶¨ Ìï®Ïàò
def load_video_mapping_database() -> Dict[str, Any]:
    """ÎπÑÎîîÏò§ Îß§Ìïë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Î°úÎìúÌï©ÎãàÎã§."""
    if os.path.exists(VIDEO_MAPPING_DB_FILE):
        try:
            with open(VIDEO_MAPPING_DB_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"ÎπÑÎîîÏò§ Îß§Ìïë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î°úÎìú Ïò§Î•ò: {e}")
            return {"mappings": [], "feedback_history": []}
    return {"mappings": [], "feedback_history": []}


def save_video_mapping_database(db_data: Dict[str, Any]):
    """ÎπÑÎîîÏò§ Îß§Ìïë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î•º Ï†ÄÏû•Ìï©ÎãàÎã§."""
    try:
        with open(VIDEO_MAPPING_DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(db_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ÎπÑÎîîÏò§ Îß§Ìïë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû• Ïò§Î•ò: {e}")


def add_video_mapping_feedback(
    customer_text: str,
    selected_video_path: str,
    emotion: str,
    gesture: str,
    context_keywords: List[str],
    user_rating: int,  # 1-5 Ï†êÏàò
    user_comment: str = ""
) -> None:
    """
    ‚≠ê Gemini Ï†úÏïà: ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞±ÏùÑ ÎπÑÎîîÏò§ Îß§Ìïë Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï∂îÍ∞ÄÌï©ÎãàÎã§.
    
    Args:
        customer_text: Í≥†Í∞ùÏùò ÌÖçÏä§Ìä∏
        selected_video_path: ÏÑ†ÌÉùÎêú ÎπÑÎîîÏò§ Í≤ΩÎ°ú
        emotion: Î∂ÑÏÑùÎêú Í∞êÏ†ï
        gesture: Î∂ÑÏÑùÎêú Ï†úÏä§Ï≤ò
        context_keywords: ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú
        user_rating: ÏÇ¨Ïö©Ïûê ÌèâÍ∞Ä (1-5)
        user_comment: ÏÇ¨Ïö©Ïûê ÏΩîÎ©òÌä∏ (ÏÑ†ÌÉùÏ†Å)
    """
    db_data = load_video_mapping_database()
    
    feedback_entry = {
        "timestamp": datetime.now().isoformat(),
        "customer_text": customer_text[:200],  # ÏµúÎåÄ 200Ïûê
        "selected_video": os.path.basename(selected_video_path) if selected_video_path else None,
        "video_path": selected_video_path,
        "emotion": emotion,
        "gesture": gesture,
        "context_keywords": context_keywords,
        "user_rating": user_rating,
        "user_comment": user_comment[:500] if user_comment else "",  # ÏµúÎåÄ 500Ïûê
        "is_natural_match": user_rating >= 4  # 4Ï†ê Ïù¥ÏÉÅÏù¥Î©¥ ÏûêÏó∞Ïä§Îü¨Ïö¥ Îß§Ïπ≠ÏúºÎ°ú Í∞ÑÏ£º
    }
    
    db_data["feedback_history"].append(feedback_entry)
    
    # Îß§Ìïë Í∑úÏπô ÏóÖÎç∞Ïù¥Ìä∏ (ÌèâÍ∞ÄÍ∞Ä ÎÜíÏùÄ Í≤ΩÏö∞)
    if user_rating >= 4:
        mapping_key = f"{emotion}_{gesture}_{'_'.join(context_keywords) if context_keywords else 'none'}"
        
        # Í∏∞Ï°¥ Îß§Ìïë Ï∞æÍ∏∞
        existing_mapping = None
        for mapping in db_data["mappings"]:
            if mapping.get("key") == mapping_key:
                existing_mapping = mapping
                break
        
        if existing_mapping:
            # Í∏∞Ï°¥ Îß§Ìïë ÏóÖÎç∞Ïù¥Ìä∏ (ÌèâÍ∑† Ï†êÏàò Í≥ÑÏÇ∞)
            total_rating = existing_mapping.get("total_rating", 0) + user_rating
            count = existing_mapping.get("count", 0) + 1
            existing_mapping["total_rating"] = total_rating
            existing_mapping["count"] = count
            existing_mapping["avg_rating"] = total_rating / count
            existing_mapping["last_updated"] = datetime.now().isoformat()
        else:
            # ÏÉà Îß§Ìïë Ï∂îÍ∞Ä
            db_data["mappings"].append({
                "key": mapping_key,
                "emotion": emotion,
                "gesture": gesture,
                "context_keywords": context_keywords,
                "recommended_video": os.path.basename(selected_video_path) if selected_video_path else None,
                "video_path": selected_video_path,
                "total_rating": user_rating,
                "count": 1,
                "avg_rating": float(user_rating),
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat()
            })
    
    save_video_mapping_database(db_data)


def get_recommended_video_from_database(
    emotion: str,
    gesture: str,
    context_keywords: List[str]
) -> str:
    """
    ‚≠ê Gemini Ï†úÏïà: Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Ï∂îÏ≤ú ÎπÑÎîîÏò§ Í≤ΩÎ°úÎ•º Í∞ÄÏ†∏ÏòµÎãàÎã§.
    
    Args:
        emotion: Í∞êÏ†ï ÏÉÅÌÉú
        gesture: Ï†úÏä§Ï≤ò
        context_keywords: ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú
    
    Returns:
        Ï∂îÏ≤ú ÎπÑÎîîÏò§ Í≤ΩÎ°ú (ÏóÜÏúºÎ©¥ None)
    """
    db_data = load_video_mapping_database()
    
    mapping_key = f"{emotion}_{gesture}_{'_'.join(context_keywords) if context_keywords else 'none'}"
    
    # Ï†ïÌôïÌïú Îß§Ïπ≠ Ï∞æÍ∏∞
    for mapping in db_data["mappings"]:
        if mapping.get("key") == mapping_key and mapping.get("avg_rating", 0) >= 4.0:
            video_path = mapping.get("video_path")
            if video_path and os.path.exists(video_path):
                return video_path
    
    # Î∂ÄÎ∂Ñ Îß§Ïπ≠ ÏãúÎèÑ (Í∞êÏ†ïÍ≥º Ï†úÏä§Ï≤òÎßå)
    partial_key = f"{emotion}_{gesture}_none"
    for mapping in db_data["mappings"]:
        if mapping.get("key") == partial_key and mapping.get("avg_rating", 0) >= 4.0:
            video_path = mapping.get("video_path")
            if video_path and os.path.exists(video_path):
                return video_path
    
    return None


def render_synchronized_video(text: str, audio_bytes: bytes, gender: str, emotion: str, 
                               role: str = "customer", autoplay: bool = True,
                               gesture: str = "NONE", context_keywords: List[str] = None):
    """
    TTS Ïò§ÎîîÏò§ÏôÄ ÎèôÍ∏∞ÌôîÎêú ÎπÑÎîîÏò§Î•º Î†åÎçîÎßÅÌï©ÎãàÎã§.
    
    ‚≠ê Gemini Ï†úÏïà: ÌîºÎìúÎ∞± ÌèâÍ∞Ä Í∏∞Îä• Ï∂îÍ∞Ä
    
    Args:
        text: ÎßêÌïòÎäî ÌÖçÏä§Ìä∏ ÎÇ¥Ïö©
        audio_bytes: TTSÎ°ú ÏÉùÏÑ±Îêú Ïò§ÎîîÏò§ Î∞îÏù¥Ìä∏
        gender: Í≥†Í∞ù ÏÑ±Î≥Ñ ("male" ÎòêÎäî "female")
        emotion: Í∞êÏ†ï ÏÉÅÌÉú ("NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD")
        role: Ïó≠Ìï† ("customer" ÎòêÎäî "agent")
        autoplay: ÏûêÎèô Ïû¨ÏÉù Ïó¨Î∂Ä
        gesture: Ï†úÏä§Ï≤ò (ÏÑ†ÌÉùÏ†Å)
        context_keywords: ÏÉÅÌô©Î≥Ñ ÌÇ§ÏõåÎìú (ÏÑ†ÌÉùÏ†Å)
    """
    if role == "customer":
        is_speaking = True
        if context_keywords is None:
            context_keywords = []
        
        # ‚≠ê Gemini Ï†úÏïà: Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í∏∞Î∞ò Ï∂îÏ≤ú ÎπÑÎîîÏò§ Ïö∞ÏÑ† ÏÇ¨Ïö©
        video_path = get_video_path_by_avatar(gender, emotion, is_speaking, gesture, context_keywords)
        
        if video_path and os.path.exists(video_path):
            try:
                with open(video_path, "rb") as f:
                    video_bytes = f.read()
                
                # ÎπÑÎîîÏò§ÏôÄ Ïò§ÎîîÏò§Î•º Ìï®Íªò Ïû¨ÏÉù
                # StreamlitÏùò st.videoÎäî Ïò§ÎîîÏò§ Ìä∏ÎûôÏù¥ ÏûàÎäî ÎπÑÎîîÏò§Î•º ÏßÄÏõêÌï©ÎãàÎã§
                # Ïó¨Í∏∞ÏÑúÎäî ÎπÑÎîîÏò§Îßå ÌëúÏãúÌïòÍ≥†, Ïò§ÎîîÏò§Îäî Î≥ÑÎèÑÎ°ú Ïû¨ÏÉùÌï©ÎãàÎã§
                st.video(video_bytes, format="video/mp4", autoplay=autoplay, loop=False, muted=False)
                
                # Ïò§ÎîîÏò§ÎèÑ Ìï®Íªò Ïû¨ÏÉù (ÎèôÍ∏∞Ìôî)
                if audio_bytes:
                    st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
                
                # ‚≠ê Gemini Ï†úÏïà: ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± ÌèâÍ∞Ä UI Ï∂îÍ∞Ä (Ï±ÑÌåÖ/Ïù¥Î©îÏùº ÌÉ≠Ïö©)
                if not autoplay:  # ÏûêÎèô Ïû¨ÏÉùÏù¥ ÏïÑÎãå Í≤ΩÏö∞ÏóêÎßå ÌîºÎìúÎ∞± UI ÌëúÏãú
                    st.markdown("---")
                    st.markdown("**üí¨ ÎπÑÎîîÏò§ Îß§Ïπ≠ ÌèâÍ∞Ä**")
                    st.caption("Ïù¥ ÎπÑÎîîÏò§Í∞Ä Í≥†Í∞ùÏùò ÌÖçÏä§Ìä∏ÏôÄ Í∞êÏ†ïÏóê ÏûêÏó∞Ïä§ÎüΩÍ≤å Îß§Ïπ≠ÎêòÏóàÏäµÎãàÍπå?")
                    
                    feedback_key = f"video_feedback_chat_{st.session_state.get('sim_instance_id', 'default')}_{hash(text) % 10000}"
                    
                    col_rating, col_comment = st.columns([2, 3])
                    with col_rating:
                        rating = st.slider(
                            "ÌèâÍ∞Ä Ï†êÏàò (1-5Ï†ê)",
                            min_value=1,
                            max_value=5,
                            value=3,
                            key=f"{feedback_key}_rating",
                            help="1Ï†ê: Îß§Ïö∞ Î∂ÄÏûêÏó∞Ïä§Îü¨ÏõÄ, 5Ï†ê: Îß§Ïö∞ ÏûêÏó∞Ïä§Îü¨ÏõÄ"
                        )
                    
                    with col_comment:
                        comment = st.text_input(
                            "ÏùòÍ≤¨ (ÏÑ†ÌÉùÏÇ¨Ìï≠)",
                            key=f"{feedback_key}_comment",
                            placeholder="Ïòà: ÎπÑÎîîÏò§Í∞Ä ÌÖçÏä§Ìä∏ÏôÄ Ïûò ÎßûÏïòÏäµÎãàÎã§"
                        )
                    
                    if st.button("ÌîºÎìúÎ∞± Ï†úÏ∂ú", key=f"{feedback_key}_submit"):
                        # ÌîºÎìúÎ∞±ÏùÑ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ï†ÄÏû•
                        add_video_mapping_feedback(
                            customer_text=text[:200],
                            selected_video_path=video_path,
                            emotion=emotion,
                            gesture=gesture,
                            context_keywords=context_keywords,
                            user_rating=rating,
                            user_comment=comment
                        )
                        st.success(f"‚úÖ ÌîºÎìúÎ∞±Ïù¥ Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§! (Ï†êÏàò: {rating}/5)")
                        st.info("üí° Ïù¥ ÌîºÎìúÎ∞±ÏùÄ Ìñ•ÌõÑ ÎπÑÎîîÏò§ ÏÑ†ÌÉù Ï†ïÌôïÎèÑÎ•º Í∞úÏÑ†ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.")
                
                return True
            except Exception as e:
                st.warning(f"ÎπÑÎîîÏò§ Ïû¨ÏÉù Ïò§Î•ò: {e}")
                # ÎπÑÎîîÏò§ Ïû¨ÏÉù Ïã§Ìå® Ïãú Ïò§ÎîîÏò§Îßå Ïû¨ÏÉù
                if audio_bytes:
                    st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
                return False
        else:
            # ÎπÑÎîîÏò§Í∞Ä ÏóÜÏúºÎ©¥ Ïò§ÎîîÏò§Îßå Ïû¨ÏÉù
            if audio_bytes:
                st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
            return False
    else:
        # ÏóêÏù¥Ï†ÑÌä∏Îäî ÎπÑÎîîÏò§ ÏóÜÏù¥ Ïò§ÎîîÏò§Îßå Ïû¨ÏÉù
        if audio_bytes:
            st.audio(audio_bytes, format="audio/mp3", autoplay=autoplay, loop=False)
        return False


def generate_virtual_human_video(text: str, audio_bytes: bytes, gender: str, emotion: str, 
                                 provider: str = "hyperclova") -> bytes:
    """
    Í∞ÄÏÉÅ Ìú¥Î®º Í∏∞Ïà†ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌÖçÏä§Ìä∏ÏôÄ Ïò§ÎîîÏò§Ïóê ÎßûÎäî ÎπÑÎîîÏò§Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
    
    ‚ö†Ô∏è Ï£ºÏùò: OpenAI/Gemini APIÎßåÏúºÎ°úÎäî ÏûÖÎ™®Ïñë ÎèôÍ∏∞Ìôî ÎπÑÎîîÏò§ ÏÉùÏÑ±Ïù¥ Î∂àÍ∞ÄÎä•Ìï©ÎãàÎã§.
    Í∞ÄÏÉÅ Ìú¥Î®º ÎπÑÎîîÏò§ ÏÉùÏÑ±ÏùÄ Î≥ÑÎèÑÏùò Í∞ÄÏÉÅ Ìú¥Î®º API (Ïòà: Hyperclova)Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.
    
    ÌòÑÏû¨Îäî ÎØ∏Î¶¨ Ï§ÄÎπÑÎêú ÎπÑÎîîÏò§ ÌååÏùºÏùÑ ÏÇ¨Ïö©ÌïòÎäî Î∞©ÏãùÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.
    
    Args:
        text: ÎßêÌïòÎäî ÌÖçÏä§Ìä∏ ÎÇ¥Ïö©
        audio_bytes: TTSÎ°ú ÏÉùÏÑ±Îêú Ïò§ÎîîÏò§ Î∞îÏù¥Ìä∏
        gender: Í≥†Í∞ù ÏÑ±Î≥Ñ ("male" ÎòêÎäî "female")
        emotion: Í∞êÏ†ï ÏÉÅÌÉú ("NEUTRAL", "HAPPY", "ANGRY", "ASKING", "SAD", "HOLD")
        provider: Í∞ÄÏÉÅ Ìú¥Î®º Ï†úÍ≥µÏûê ("hyperclova", "other")
    
    Returns:
        ÏÉùÏÑ±Îêú ÎπÑÎîîÏò§ Î∞îÏù¥Ìä∏ (ÏóÜÏúºÎ©¥ None)
    """
    # Í∞ÄÏÉÅ Ìú¥Î®º API ÌÇ§ ÌôïÏù∏
    if provider == "hyperclova":
        api_key = get_api_key("hyperclova")
        if not api_key:
            return None
        
        # TODO: Hyperclova API Ïó∞Îèô Íµ¨ÌòÑ (Î≥ÑÎèÑ API ÌïÑÏöî)
        # OpenAI/Gemini APIÎßåÏúºÎ°úÎäî Î∂àÍ∞ÄÎä•ÌïòÎØÄÎ°ú, Ïã§Ï†ú Í∞ÄÏÉÅ Ìú¥Î®º APIÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.
        # ÏòàÏãú Íµ¨Ï°∞:
        # response = requests.post(
        #     "https://api.hyperclova.com/virtual-human/generate",
        #     headers={"Authorization": f"Bearer {api_key}"},
        #     json={
        #         "text": text,
        #         "audio": base64.b64encode(audio_bytes).decode(),
        #         "gender": gender,
        #         "emotion": emotion
        #     }
        # )
        # return response.content
    
    # Îã§Î•∏ Ï†úÍ≥µÏûêÎèÑ Ïó¨Í∏∞Ïóê Ï∂îÍ∞Ä Í∞ÄÎä•
    # elif provider == "other":
    #     ...
    
    return None


def get_virtual_human_config() -> Dict[str, Any]:
    """
    Í∞ÄÏÉÅ Ìú¥Î®º ÏÑ§Ï†ïÏùÑ Î∞òÌôòÌï©ÎãàÎã§.
    
    Returns:
        Í∞ÄÏÉÅ Ìú¥Î®º ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
    """
    return {
        "enabled": st.session_state.get("virtual_human_enabled", False),
        "provider": st.session_state.get("virtual_human_provider", "hyperclova"),
        "api_key": get_api_key("hyperclova") if st.session_state.get("virtual_human_provider", "hyperclova") == "hyperclova" else None
    }


# Ïó≠Ìï†Î≥Ñ TTS ÏùåÏÑ± Ïä§ÌÉÄÏùº ÏÑ§Ï†ï
TTS_VOICES = {
    "customer_male": {
        "gender": "male",
        "voice": "alloy"  # Male voice
    },
    "customer_female": {
        "gender": "female",
        "voice": "nova"  # Female voice
    },
    "customer": {
        "gender": "male",
        "voice": "alloy"  # Default male voice (fallback)
    },
    "agent": {
        "gender": "female",
        "voice": "shimmer"  # Distinct Female, Professional/Agent
    },
    "supervisor": {
        "gender": "female",
        "voice": "nova"  # Another Distinct Female, Informative/Supervisor
    }
}



def synthesize_tts(text: str, lang_key: str, role: str = "agent"):
    # lang_key Í≤ÄÏ¶ù Î∞è Í∏∞Î≥∏Í∞í Ï≤òÎ¶¨
    if not lang_key or lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"  # ÏµúÏ¢Ö Í∏∞Î≥∏Í∞í
    
    L = LANG.get(lang_key, LANG["ko"])  # ÏïàÏ†ÑÌïú Ï†ëÍ∑º
    client = st.session_state.openai_client
    if client is None:
        return None, L.get("openai_missing", "OpenAI API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")

    # ‚≠ê ÏàòÏ†ï: Í≥†Í∞ù Ïó≠Ìï†Ïù∏ Í≤ΩÏö∞ ÏÑ±Î≥ÑÏóê Îî∞Îùº ÏùåÏÑ± ÏÑ†ÌÉù
    if role == "customer":
        customer_gender = st.session_state.customer_avatar.get("gender", "male")
        if customer_gender == "female":
            voice_key = "customer_female"
        else:
            voice_key = "customer_male"
        
        if voice_key in TTS_VOICES:
            voice_name = TTS_VOICES[voice_key]["voice"]
        else:
            voice_name = TTS_VOICES["customer"]["voice"]  # Fallback
    elif role in TTS_VOICES:
        voice_name = TTS_VOICES[role]["voice"]
    else:
        voice_name = TTS_VOICES["agent"]["voice"]  # Default fallback

    try:
        # ‚≠ê ÏàòÏ†ï: ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Ï†úÌïúÏùÑ Ï†úÍ±∞ÌïòÏó¨ Ï†ÑÏ≤¥ Î¨∏ÏùòÍ∞Ä Ïû¨ÏÉùÎêòÎèÑÎ°ù Ìï®
        # OpenAI TTSÎäî ÏµúÎåÄ 4096ÏûêÎ•º ÏßÄÏõêÌïòÏßÄÎßå, Ïã§Ï†úÎ°úÎäî Îçî Í∏¥ ÌÖçÏä§Ìä∏ÎèÑ Ï≤òÎ¶¨ Í∞ÄÎä•
        # Í≥†Í∞ùÏùò Î¨∏ÏùòÎ•º ÎÅùÍπåÏßÄ Îã§ Îì§Ïñ¥Ïïº ÏõêÌôúÌïú ÏùëÎåÄÍ∞Ä Í∞ÄÎä•ÌïòÎØÄÎ°ú Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏Î•º Ï≤òÎ¶¨
        # ÎßåÏïΩ ÌÖçÏä§Ìä∏Í∞Ä ÎÑàÎ¨¥ Í∏∏Î©¥ (Ïòà: 10000Ïûê Ïù¥ÏÉÅ) Ïó¨Îü¨ Ï≤≠ÌÅ¨Î°ú ÎÇòÎà†ÏÑú Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏßÄÎßå,
        # ÏùºÎ∞òÏ†ÅÏù∏ Í≥†Í∞ù Î¨∏ÏùòÎäî 4096Ïûê Ïù¥ÎÇ¥Ïù¥ÎØÄÎ°ú Ï†ÑÏ≤¥Î•º Ï≤òÎ¶¨
        
        # tts-1 Î™®Îç∏ ÏÇ¨Ïö© (ÏïàÏ†ïÏÑ±)
        resp = client.audio.speech.create(
            model="tts-1",
            voice=voice_name,
            input=text
            # format="mp3"ÏùÄ Í∏∞Î≥∏Í∞íÏûÖÎãàÎã§.
        )
        return resp.read(), L["tts_status_success"]

    except Exception as e:
        return None, f"{L['tts_status_error']}: {e}"


# ----------------------------------------
# TTS Helper
# ----------------------------------------


def render_tts_button(text, lang_key, role="customer", prefix="", index: int = -1):
    # lang_key Í≤ÄÏ¶ù Î∞è Í∏∞Î≥∏Í∞í Ï≤òÎ¶¨
    if not lang_key or lang_key not in ["ko", "en", "ja"]:
        lang_key = st.session_state.get("language", "ko")
        if lang_key not in ["ko", "en", "ja"]:
            lang_key = "ko"  # ÏµúÏ¢Ö Í∏∞Î≥∏Í∞í
    
    L = LANG.get(lang_key, LANG["ko"])  # ÏïàÏ†ÑÌïú Ï†ëÍ∑º

    # ‚≠ê ÏàòÏ†ï: index=-1Ïù∏ Í≤ΩÏö∞, UUIDÎ•º ÏÇ¨Ïö©ÌïòÏó¨ safe_key ÏÉùÏÑ±
    if index == -1:
        # Ïù¥Í¥Ä ÏöîÏïΩÏ≤òÎüº Ïù∏Îç±Ïä§Í∞Ä Í≥†Ï†ïÎêòÏßÄ ÏïäÎäî Í≤ΩÏö∞, ÌÖçÏä§Ìä∏ Ìï¥ÏãúÏôÄ ÏÑ∏ÏÖò Ïù∏Ïä§ÌÑ¥Ïä§ IDÎ•º Ï°∞Ìï©
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        session_id_part = st.session_state.get('sim_instance_id', 'default_session')
        # ‚≠ê ÏàòÏ†ï: Ïù¥Í¥Ä ÏöîÏïΩÏùò Í≤ΩÏö∞ ÏïàÏ†ïÏ†ÅÏù∏ ÌÇ§Î•º ÏÉùÏÑ± (time.time_ns() Ï†úÍ±∞ÌïòÏó¨ Îß§Î≤à Í∞ôÏùÄ ÌÇ§ ÏÉùÏÑ±)
        # Ïñ∏Ïñ¥ ÏΩîÎìúÎèÑ Ï∂îÍ∞ÄÌïòÏó¨ Ïù¥Í¥Ä ÌõÑ Ïñ∏Ïñ¥ Î≥ÄÍ≤Ω ÏãúÏóêÎèÑ Í≥†Ïú†ÏÑ± Î≥¥Ïû•
        lang_code = st.session_state.get('language', lang_key)
        safe_key = f"{prefix}_SUMMARY_{session_id_part}_{lang_code}_{content_hash}"
    else:
        # ÎåÄÌôî Î°úÍ∑∏Ï≤òÎüº Ïù∏Îç±Ïä§Í∞Ä Ï°¥Ïû¨ÌïòÎäî Í≤ΩÏö∞ (Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ)
        content_hash = hashlib.md5(text[:100].encode()).hexdigest()
        safe_key = f"{prefix}_{index}_{content_hash}"

    # Ïû¨ÏÉù Î≤ÑÌäºÏùÑ ÎàÑÎ•º ÎïåÎßå TTS ÏöîÏ≤≠
    if st.button(L["button_listen_audio"], key=safe_key):
        if not st.session_state.openai_client:
            st.error(L["openai_missing"])
            return  # ÌÇ§ ÏóÜÏúºÎ©¥ Ï¢ÖÎ£å

        with st.spinner(L["tts_status_generating"]):
            try:
                audio_bytes, msg = synthesize_tts(text, lang_key, role=role)
                if audio_bytes:
                    # ‚≠ê st.audio Ìò∏Ï∂ú Ïãú ÏÑ±Í≥µÌïú Í≤ΩÏö∞ÏóêÎßå Ïû¨ÏÉù ÏãúÍ∞ÑÏùÑ ÌôïÎ≥¥
                    # Streamlit Î¨∏ÏÑú: autoplayÎäî Î∏åÎùºÏö∞Ï†Ä Ï†ïÏ±ÖÏÉÅ ÏÇ¨Ïö©Ïûê ÏÉÅÌò∏ÏûëÏö© ÏóÜÏù¥Îäî ÏûëÎèôÌïòÏßÄ ÏïäÏùÑ Ïàò ÏûàÏùå
                    try:
                        st.audio(audio_bytes, format="audio/mp3", autoplay=True, loop=False)
                        st.success(msg)
                        # ‚≠ê ÏàòÏ†ï: Ïû¨ÏÉùÏù¥ ÏãúÏûëÎê† Ï∂©Î∂ÑÌïú ÏãúÍ∞ÑÏùÑ ÌôïÎ≥¥ÌïòÍ∏∞ ÏúÑÌï¥ ÎåÄÍ∏∞ ÏãúÍ∞ÑÏùÑ 3Ï¥àÎ°ú ÎäòÎ¶º
                        time.sleep(3)
                    except Exception as e:
                        st.warning(f"Ïò§ÎîîÏò§ Ïû¨ÏÉù Ï§ë Ïò§Î•ò: {e}. Ïò§ÎîîÏò§ ÌååÏùºÏùÄ ÏÉùÏÑ±ÎêòÏóàÏßÄÎßå ÏûêÎèô Ïû¨ÏÉùÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.")
                        st.audio(audio_bytes, format="audio/mp3", autoplay=False)
                        st.success(msg)
                else:
                    st.error(msg)
                    time.sleep(1)  # ÏóêÎü¨ Î∞úÏÉù ÏãúÎèÑ Ïû†Ïãú ÎåÄÍ∏∞
            except Exception as e:
                # TTS API Ìò∏Ï∂ú ÏûêÏ≤¥ÏóêÏÑú ÏòàÏô∏ Î∞úÏÉù Ïãú (ÎÑ§Ìä∏ÏõåÌÅ¨ Îì±)
                st.error(f"‚ùå TTS ÏÉùÏÑ± Ï§ë ÏπòÎ™ÖÏ†ÅÏù∏ Ïò§Î•ò Î∞úÏÉù: {e}")
                time.sleep(1)

            # Î≤ÑÌäº ÌÅ¥Î¶≠ Ïù¥Î≤§Ìä∏ ÌõÑ, Î∂àÌïÑÏöîÌïú Ïû¨Ïã§ÌñâÏùÑ ÎßâÍ∏∞ ÏúÑÌï¥ Ïó¨Í∏∞ÏÑú Ìï®Ïàò Ï¢ÖÎ£å
            return
        # [Ï§ëÎûµ: TTS Helper ÎÅù]


# ========================================
# 4. Î°úÏª¨ ÏùåÏÑ± Í∏∞Î°ù Helper
# ========================================


def load_voice_records() -> List[Dict[str, Any]]:
    return _load_json(VOICE_META_FILE, [])



def save_voice_records(records: List[Dict[str, Any]]):
    _save_json(VOICE_META_FILE, records)



def save_audio_record_local(
        audio_bytes: bytes,
        filename: str,
        transcript_text: str,
        mime_type: str = "audio/webm",
        meta: Dict[str, Any] = None,
) -> str:
    records = load_voice_records()
    rec_id = str(uuid.uuid4())
    ts = datetime.utcnow().isoformat()

    ext = filename.split(".")[-1] if "." in filename else "webm"
    audio_filename = f"{rec_id}.{ext}"
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "wb") as f:
        f.write(audio_bytes)

    rec = {
        "id": rec_id,
        "created_at": ts,
        "filename": filename,
        "audio_filename": audio_filename,
        "size": len(audio_bytes),
        "transcript": transcript_text,
        "mime_type": mime_type,
        "language": st.session_state.language,
        "meta": meta or {},
    }
    records.insert(0, rec)
    save_voice_records(records)
    return rec_id



def delete_audio_record_local(rec_id: str) -> bool:
    records = load_voice_records()
    idx = next((i for i, r in enumerate(records) if r.get("id") == rec_id), None)
    if idx is None:
        return False
    rec = records.pop(idx)
    audio_filename = rec.get("audio_filename")
    if audio_filename:
        audio_path = os.path.join(AUDIO_DIR, audio_filename)
        try:
            os.remove(audio_path)
        except FileNotFoundError:
            pass
    save_voice_records(records)
    return True



def get_audio_bytes_local(rec_id: str):
    records = load_voice_records()
    rec = next((r for r in records if r.get("id") == rec_id), None)
    if not rec:
        raise FileNotFoundError("record not found")
    audio_filename = rec["audio_filename"]
    audio_path = os.path.join(AUDIO_DIR, audio_filename)
    with open(audio_path, "rb") as f:
        b = f.read()
    return b, rec



