# ========================================
# streamlit_app_final.py
# ì™„ì„±ë³¸: Streamlit ì•± â€” Whisper ì „ì‚¬, Firestore/GCS í†µí•©, ì‹œë®¬ë ˆì´í„°, RAG, LSTM
# ========================================
# FIX 1: ë¡œì»¬ í™˜ê²½ ë³€ìˆ˜ (.env) ë¡œë“œë¥¼ ìœ„í•´ dotenv ì‚¬ìš© (ë¡œì»¬ ì‹¤í–‰ ì‹œ í•„ìš”)
import os

try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass  # ë°°í¬ í™˜ê²½ì—ì„œëŠ” dotenvê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ

import json
import uuid
import streamlit as st
import tempfile
import time
import re
import base64
import io
import numpy as np
from matplotlib import pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from datetime import datetime, timedelta, timezone
from openai import OpenAI
from dotenv import load_dotenv

# LangChain Imports
# FIX: ConversationChainì„ ê°€ì¥ ì•ˆì •ì ì¸ ê²½ë¡œì¸ 'langchain.chains'ì—ì„œ ê°€ì ¸ì˜¤ê³ ,
# ë‚˜ë¨¸ì§€ ëª¨ë“ˆë“¤ì€ 'langchain_community'ì—ì„œ ê°€ì ¸ì™€ ê²½ë¡œ ì¶©ëŒì„ ë°©ì§€í•©ë‹ˆë‹¤.
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.memory import ConversationBufferMemory
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from streamlit_mic_recorder import mic_recorder



# ===============================
# 0. ë¡œì»¬ JSON DB í™˜ê²½ ì¤€ë¹„
# ===============================
BASE_DB_DIR = "local_db"
AUDIO_DIR = os.path.join(BASE_DB_DIR, "audio")
SIM_HISTORY_PATH = os.path.join(BASE_DB_DIR, "simulation_histories.json")
VOICE_RECORDS_PATH = os.path.join(BASE_DB_DIR, "voice_records.json")

os.makedirs(BASE_DB_DIR, exist_ok=True)
os.makedirs(AUDIO_DIR, exist_ok=True)

# JSON DB utils
def json_db_load(path):
    """ë¡œì»¬ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” ë³µì¡í•œ ë¡œì§ì€ ì œê±°í•˜ê³ , ê¸°ë³¸ JSONë§Œ ë¡œë“œ
            return json.load(f)
    except Exception:
        # íŒŒì¼ì´ ë¹„ì–´ ìˆê±°ë‚˜ ì†ìƒëœ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []

def json_db_save(path, data):
    """ë°ì´í„°ë¥¼ ë¡œì»¬ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with open(path, "w", encoding="utf-8") as f:
        # ensure_ascii=FalseëŠ” í•œê¸€ ê¹¨ì§ ë°©ì§€
        json.dump(data, f, ensure_ascii=False, indent=2)

# ===============================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ & OpenAI ì´ˆê¸°í™”
# ===============================
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY") # OpenAI LLM ë° ì„ë² ë”© ì‚¬ìš©

# -----------------------------
# 1. Config & I18N (ë‹¤êµ­ì–´ ì§€ì›)
# -----------------------------
DEFAULT_LANG = "ko"

LANG = {
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜ (ìŒì„± ë° DB í†µí•©)",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
        "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
        "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "warning_no_db_connect": "âŒ DB ì—°ê²° ì‹¤íŒ¨: Firebase Secretsì´ ì—†ê±°ë‚˜ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
        "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
        "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
        "check_answer": "ì •ë‹µ í™•ì¸",
        "next_question": "ë‹¤ìŒ ë¬¸í•­",
        "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
        "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤. ğŸ˜",
        "correct_is": "ì •ë‹µ",
        "explanation": "í•´ì„¤",
        "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
        "score": "ì ìˆ˜",
        "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ã€‚",
        "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
        "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
        "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚",
        "db_save_complete": "(DB ì €ì¥ ì™„ë£Œ)",
        "data_analysis_progress": "ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘...",
        "response_generating": "ë‹µë³€ ìƒì„± ì¤‘...",
        "lstm_result_header": "í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼",
        "lstm_score_metric": "í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„",
        "lstm_score_info": "ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!",
        "lstm_rerun_button": "ìƒˆë¡œìš´ ê°€ìƒ ë°ì´í„°ë¡œ ì˜ˆì¸¡",

        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— ëŒ€í•´ AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤ã€‚",
        "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
        "customer_type_label": "ê³ ê° ì„±í–¥",
        "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
        "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
        "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ëŠ” ê²½ìš°, ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
        "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
        "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
        "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
        "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
        "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!",
        "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
        "history_expander_title": "ğŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê°œ)",
        "initial_query_sample": "í”„ë‘ìŠ¤ íŒŒë¦¬ì— ë„ì°©í–ˆëŠ”ë°, í´ë£©ì—ì„œ êµ¬ë§¤í•œ eSIMì´ í™œì„±í™”ê°€ ì•ˆ ë©ë‹ˆë‹¤. ì—°ê²°ì´ ì•ˆ ë¼ì„œ ë„ˆë¬´ ê³¤ë€í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "button_mic_input": "ğŸ™ ìŒì„± ì…ë ¥",
        "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ ì±„íŒ…ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤ã€‚",
        "prompt_survey": "ê³ ê° ë¬¸ì˜ ì„¼í„°ì— ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹­ì‹œì˜¤ã€‚",
        "customer_closing_confirm": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?",
        "customer_positive_response": "ì¢‹ì€ ë§ì”€/ì¹œì ˆí•œ ìƒë‹´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤ã€‚",
        "button_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ì¡°ì‚¬ ìš”ì²­)",
        "agent_response_header": "âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ",
        "agent_response_placeholder": "ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš” (ê³ ê°ì˜ í•„ìˆ˜ ì •ë³´ë¥¼ ìš”ì²­/í™•ì¸í•˜ê±°ë‚˜, ë¬¸ì œ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”)",
        "send_response_button": "ì‘ë‹µ ì „ì†¡",
        "request_rebuttal_button": "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­",
        "new_simulation_button": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘",
        "history_selectbox_label": "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
        "history_load_button": "ì„ íƒëœ ì´ë ¥ ë¡œë“œ",
        "delete_history_button": "âŒ ëª¨ë“  ì´ë ¥ ì‚­ì œ",
        "delete_confirm_message": "ì •ë§ë¡œ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ã€‚",
        "delete_confirm_yes": "ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤",
        "delete_confirm_no": "ì•„ë‹ˆì˜¤, ìœ ì§€í•©ë‹ˆë‹¤",
        "delete_success": "âœ… ëª¨ë“  ìƒë‹´ ì´ë ¥ ì‚­ì œ ì™„ë£Œ!",
        "deleting_history_progress": "ì´ë ¥ ì‚­ì œ ì¤‘...",
        "search_history_label": "ì´ë ¥ í‚¤ì›Œë“œ ê²€ìƒ‰",
        "date_range_label": "ë‚ ì§œ ë²”ìœ„ í•„í„°",
        "no_history_found": "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤ã€‚",

        # â­ ìŒì„± ê¸°ë¡ í†µí•© ê´€ë ¨ í‚¤ (Voice/GCS)
        "voice_rec_header": 'ìŒì„± ê¸°ë¡ & ê´€ë¦¬',
        "record_help": 'ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚',
        "uploaded_file": 'ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ',
        "rec_list_title": 'ì €ì¥ëœ ìŒì„± ê¸°ë¡ (Whisper/GCS)',
        "transcribe_btn": 'ì „ì‚¬(Whisper)',
        "save_btn": 'ìŒì„± ê¸°ë¡ ì €ì¥',
        "transcribing": 'ìŒì„± ì „ì‚¬ ì¤‘...',
        "transcript_result": 'ì „ì‚¬ ê²°ê³¼:',
        "transcript_text": 'ì „ì‚¬ í…ìŠ¤íŠ¸',
        "openai_missing": 'OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.',
        "whisper_client_error": "âŒ ì˜¤ë¥˜: Whisper API Clientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "whisper_auth_error": "âŒ Whisper API ì¸ì¦ ì‹¤íŒ¨: API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "whisper_format_error": "âŒ ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤ã€‚",
        "whisper_success": "âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ! í…ìŠ¤íŠ¸ ì°½ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "playback": 'ë…¹ìŒ ì¬ìƒ',
        "retranscribe": 'ì¬ì „ì‚¬',
        "delete": 'ì‚­ì œ',
        "no_records": 'ì €ì¥ëœ ìŒì„± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.',
        "gcs_missing": 'GCS ë²„í‚·ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Secretsì— GCS_BUCKET_NAMEì„ ì¶”ê°€í•˜ì„¸ìš”.',
        "saved_success": 'ì €ì¥ ì™„ë£Œ!',
        "delete_confirm_rec": 'ì •ë§ë¡œ ì´ ìŒì„± ê¸°ë¡ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? GCS íŒŒì¼ë„ ì‚­ì œë©ë‹ˆë‹¤.',
        "gcs_init_fail": 'GCS ì´ˆê¸°í™” ì‹¤íŒ¨. ê¶Œí•œ ë° ë²„í‚· ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.',
        "upload_fail": 'GCS ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨',
        "gcs_not_conf": 'GCS ë¯¸ì„¤ì • ë˜ëŠ” ì˜¤ë””ì˜¤ ì—†ìŒ',
        "gcs_playback_fail": 'ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨',
        "gcs_no_audio": 'ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ (GCS ë¯¸ì„¤ì •)',
        "error": 'ì˜¤ë¥˜:',
        "firestore_no_db_connect": "âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€",
        "save_history_success": "âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "save_history_fail": "âŒ ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨",
        "delete_fail": "ì‚­ì œ ì‹¤íŒ¨",
        "rec_header": "ìŒì„± ì…ë ¥ ë° ì „ì‚¬",
        "whisper_processing": "ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘",
        "empty_response_warning": "ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”ã€‚",
    },
    "en": {
        "title": "Personalized AI Study Coach (Voice & DB Integration)",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "lang_select": "Select Language",
        "voice_rec_header": 'Voice Record & Management',
        "record_help": 'Press the microphone button to record or upload a file.',
        "gcs_missing": 'GCS bucket is not configured. Add GCS_BUCKET_NAME to Secrets.',
        "openai_missing": 'OpenAI API Key is missing. Set OPENAI_API_KEY in Secrets.',
        "delete_fail": "Deletion failed",
        "save_history_fail": "âŒ Simulation history save failed",
        "delete_success": "âœ… Successfully deleted!",
        "firestore_no_index": "Could not find existing RAG index in database. Please upload files and create a new one.",
        "embed_fail": "Embedding failed: Free tier quota exceeded or network issue.",
        "gcs_not_conf": 'GCS not configured or audio not available',
        "gcs_playback_fail": 'Audio playback failed',
        "gcs_no_audio": 'No audio file (GCS not configured)',
        "transcribing": 'Transcribing voice...',
        "playback": 'Playback Recording',
        "retranscribe": 'Re-transcribe',
        "error": 'Error:',
        "firestore_no_db_connect": "âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€",
        "save_history_success": "âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "uploaded_file": 'Upload Audio File',
        "transcript_result": 'Transcription Result:',
        "transcript_text": 'Transcribed Text',
        "llm_error_key": "âš ï¸ Warning: GEMINI API Key is not set. Please set 'GEMINI_API_KEY' in Streamlit Secrets.",
        "llm_error_init": "LLM initialization error: Please check your API key.",
        "simulation_warning_query": "Please enter the customer's query.",
        "simulation_no_key_warning": "âš ï¸ API Key is missing. Response generation cannot proceed.",
        "simulation_advice_header": "AI Response Guidelines",
        "simulation_draft_header": "Recommended Response Draft",
        "button_listen_audio": "Listen to Audio",
        "tts_status_ready": "Ready to listen",
        "tts_status_generating": "Generating audio...",
        "tts_status_success": "âœ… Audio playback complete!",
        "tts_status_error": "âŒ TTS API error occurred",
        "history_expander_title": "ğŸ“ Load Previous Simulation History (Last 10)",
        "initial_query_sample": "I arrived in Paris, France, but the eSIM I bought from Klook won't activate. I'm really struggling to get connected. What should I do?",
        "button_mic_input": "ğŸ™ Voice Input",
        "prompt_customer_end": "As there are no further inquiries, we will now end this chat session.",
        "prompt_survey": "Thank you for contacting our Customer Support Center. Please feel free to contact us anytime if you have any additional questions.",
        "customer_closing_confirm": "Is there anything else we can assist you with today?",
        "customer_positive_response": "Thank you for your kind understanding/friendly advice.",
        "button_end_chat": "End Chat (Request Survey)",
        "agent_response_header": "âœï¸ Agent Response",
        "agent_response_placeholder": "Respond to the customer (Request/confirm essential information or provide solution steps)",
        "send_response_button": "Send Response",
        "request_rebuttal_button": "Request Customer's Next Reaction",
        "new_simulation_button": "Start New Simulation",
        "history_selectbox_label": "Select history to load:",
        "history_load_button": "Load Selected History",
        "delete_history_button": "âŒ Delete All History",
        "delete_confirm_message": "Are you sure you want to delete ALL simulation history? This action cannot be undone.",
        "delete_confirm_yes": "Yes, Delete",
        "delete_confirm_no": "No, Keep",
        "deleting_history_progress": "Deleting history...",
        "search_history_label": "Search History by Keyword",
        "date_range_label": "Date Range Filter",
        "no_history_found": "No history found matching the criteria.",
        "simulator_header": "AI Customer Response Simulator",
        "simulator_desc": "Provides AI draft responses and guidelines for difficult customer inquiries.",
        "customer_query_label": "Customer Query (Links included)",
        "customer_type_label": "Customer Type",  # <-- [í•„ìˆ˜ í‚¤]
        "customer_type_options": ["General Inquiry", "Difficult Customer", "Highly Dissatisfied Customer"],
        "initial_query_sample": "I arrived in Paris, France, but the eSIM I bought from Klook won't activate. I'm really struggling to get connected. What should I do?",
        "title": "Personalized AI Study Coach (Voice & DB Integration)",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction",
        "simulator_tab": "AI Customer Response Simulator",
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answers questions based on the uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content tailored to your topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "10 Multiple-Choice Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Based Achievement Prediction",
        "lstm_desc": "Trains an LSTM model on hypothetical past quiz scores to predict future achievement.",
        "embed_success": "Learning DB built with {count} chunks!",
        "warning_no_files": "Please upload study materials first.",
        "warning_rag_not_ready": "RAG is not ready. Upload materials and click Start Analysis.",
        "quiz_fail_structure": "Quiz data structure is incorrect.",
        "select_answer": "Select answer",
        "check_answer": "Confirm answer",
        "next_question": "Next Question",
        "correct_answer": "Correct! ğŸ‰",
        "incorrect_answer": "Incorrect. ğŸ˜",
        "correct_is": "Correct answer",
        "explanation": "Explanation",
        "quiz_complete": "Quiz completed!",
        "score": "Score",
        "retake_quiz": "Retake Quiz",
        "quiz_error_llm": "Quiz generation failed: LLM did not return a valid JSON format. Check the original LLM response.",
        "quiz_original_response": "Original LLM Response",
        "firestore_loading": "Loading RAG index from database...",
        "db_save_complete": "(DB Save Complete)",
        "data_analysis_progress": "Analyzing materials and building learning DB...",
        "response_generating": "Generating response...",
        "lstm_result_header": "Prediction Results",
        "lstm_score_metric": "Current Predicted Achievement",
        "lstm_score_info": "Your next estimated quiz score is **{predicted_score:.1f}**. Maintain or improve your learning progress!",
        "lstm_rerun_button": "Predict with New Hypothetical Data",
        "rec_header": "Voice Input and Transcription",
        "whisper_processing": "Processing voice transcription",
        "empty_response_warning": "Please enter a response.",
    },
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»DBçµ±åˆ)",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "lang_select": "è¨€èªé¸æŠ",
        "voice_rec_header": 'éŸ³å£°è¨˜éŒ²ã¨ç®¡ç†',
        "record_help": 'ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦éŒ²éŸ³ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚',
        "gcs_missing": 'GCSãƒã‚±ãƒƒãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Secretsã«GCS_BUCKET_NAMEã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚',
        "openai_missing": 'OpenAI APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Secretsã«OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚',
        "delete_fail": "å‰Šé™¤å¤±æ•—",
        "save_history_fail": "âŒ å¯¾å¿œå±¥æ­´ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ",
        "delete_success": "âœ… å‰Šé™¤ãŒå®Œäº†ã•ã‚Œã¾ã—ãŸ!",
        "firestore_no_index": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ—¢å­˜ã®RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°ã—ãä½œæˆã—ã¦ãã ã•ã„ã€‚",
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—: ãƒ•ãƒªãƒ¼ãƒ†ã‚£ã‚¢ã®ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã€‚",
        "gcs_not_conf": 'GCSãŒæœªè¨­å®šã‹ã€éŸ³å£°ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“',
        "gcs_playback_fail": 'éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ',
        "gcs_no_audio": 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãªã— (GCSæœªè¨­å®š)',
        "transcribing": 'éŸ³å£°è»¢å†™ä¸­...',
        "playback": 'éŒ²éŸ³å†ç”Ÿ',
        "retranscribe": 'å†è»¢å†™',
        "error": 'ã‚¨ãƒ©ãƒ¼:',
        "firestore_no_db_connect": "âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€",
        "save_history_success": "âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ã€‚",
        "uploaded_file": 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰',
        "transcript_result": 'è»¢å†™çµæœ:',
        "transcript_text": 'è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆ',
        "llm_error_key": "âš ï¸ è­¦å‘Š: GEMINI APIí‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM åˆæœŸåŒ– ì˜¤ë¥˜ï¼šAPIí‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”ã€‚",
        "simulation_warning_query": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "simulation_no_key_warning": "âš ï¸ APIí‚¤ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ã€‚å¿œç­”ã®ç”Ÿæˆã¯ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚",
        "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
        "simulation_draft_header": "æ¨å¥¨ã•ã‚Œã‚‹å¯¾å¿œè‰æ¡ˆ",
        "button_listen_audio": "éŸ³å£°ã§èã",
        "tts_status_ready": "éŸ³å£°å†ç”Ÿã®æº–å‚™ãŒã§ãã¾ã—ãŸ",
        "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
        "tts_status_success": "âœ… éŸ³å£°å†ç”Ÿå®Œäº†!",
        "tts_status_error": "âŒ TTS APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "history_expander_title": "ğŸ“ ä»¥å‰ã®å¯¾å¿œå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰ (æœ€æ–° 10ä»¶)",
        "initial_query_sample": "ãƒ•ãƒ©ãƒ³ã‚¹ã®ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã§è³¼å…¥ã—ãŸeSIMãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚æ¥ç¶šã§ããªãã¦å›°ã£ã¦ã„ã¾ã™ã€‚ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ",
        "button_mic_input": "ğŸ™ éŸ³å£°å…¥åŠ›",
        "prompt_customer_end": "ãŠå®¢æ§˜ã‹ã‚‰ã®è¿½åŠ ã®ãŠå•ã„åˆã‚ã›ãŒãªã„ãŸã‚ã€æœ¬ãƒãƒ£ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆã‚’çµ‚äº†ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
        "prompt_survey": "ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚è¿½åŠ ã®ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã„ã¤ã§ã‚‚ã”é€£çµ¡ãã ã•ã„ã€‚",
        "customer_closing_confirm": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
        "customer_positive_response": "è¦ªåˆ‡ãªã”å¯¾å¿œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚",
        "button_end_chat": "å¯¾å¿œçµ‚äº† (ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã‚’ä¾é ¼)",
        "agent_response_header": "âœï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”",
        "agent_response_placeholder": "é¡§å®¢ã«è¿”ä¿¡ (å¿…é ˆæƒ…å ±ã®è¦æ±‚/ç¢ºèªã€ã¾ãŸã¯è§£æ±ºç­–ã®æç¤º)",
        "send_response_button": "å¿œç­”é€ä¿¡",
        "request_rebuttal_button": "é¡§å®¢ã®æ¬¡ã®åå¿œã‚’è¦æ±‚",
        "new_simulation_button": "æ–°ã—ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹",
        "history_selectbox_label": "å±¥æ­´ã‚’é¸æŠã—ã¦ãƒ­ãƒ¼ãƒ‰:",
        "history_load_button": "é¸æŠã•ã‚ŒãŸå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰",
        "delete_history_button": "âŒ å…¨å±¥æ­´ã‚’å‰Šé™¤",
        "delete_confirm_message": "æœ¬å½“ã«ã™ã¹ã¦ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’å‰Šé™¤ã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿã“ã®æ“ä½œã¯å…ƒã«æˆ»ã›ã¾ã›ã‚“ã€‚",
        "delete_confirm_yes": "ã¯ã„ã€å‰Šé™¤ã—ã¾ã™",
        "delete_confirm_no": "ã„ã„ãˆ, ìœ ì§€í•©ë‹ˆë‹¤",
        "deleting_history_progress": "å±¥æ­´å‰Šé™¤ä¸­...",
        "search_history_label": "å±¥æ­´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢",
        "date_range_label": "æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼",
        "no_history_found": "æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "simulator_desc": "å›°é›£ãªé¡§å®¢ã®å•ã„åˆã‚ã›ã«å¯¾ã—ã¦AIã®å¯¾å¿œè‰æ¡ˆã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚",
        "customer_query_label": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ (ë§í¬ã‚’å«ã‚€)",
        "customer_type_label": "é¡§å®¢ã®å‚¾å‘",  # <-- [í•„ìˆ˜ í‚¤]
        "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "å›°é›£ãªé¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"],
        "initial_query_sample": "ãƒ•ãƒ©ãƒ³ã‚¹ã®ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã§è³¼å…¥ã—ãŸeSIMãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚æ¥ç¶šã§ããªãã¦å›°ã£ã¦ã„ã¾ã™ã€‚ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ",
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»DBçµ±åˆ)",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
        "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
        "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "content_tab": "ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
        "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
        "rag_input_placeholder": "å­¦ç¿’è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
        "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ì— ë§ì¶° ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ã€‚",
        "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
        "level_label": "é›£æ˜“åº¦",
        "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼",
        "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
        "content_options": ["æ ¸å¿ƒè¦ç´„ãƒãƒ¼ãƒˆ", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢"],
        "button_generate": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "lstm_header": "LSTMãƒ™ãƒ¼ã‚¹é”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "lstm_desc": "ä»®æƒ³ã®éå»ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€LSTMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦å°†æ¥ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—è¡¨ç¤ºã—ã¾ã™ã€‚",
        "embed_success": "å…¨{count}ãƒãƒ£ãƒ³ã‚¯ã§å­¦ç¿’DBæ§‹ç¯‰å®Œäº†!",
        "warning_no_files": "ã¾ãšå­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "warning_rag_not_ready": "RAGãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€åˆ†æé–‹å§‹ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚",
        "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "select_answer": "æ­£è§£ã‚’é¸æŠã—ã¦ãã ã•ã„",
        "check_answer": "æ­£è§£ã‚’ç¢ºèª",
        "next_question": "æ¬¡ã®è³ªå•",
        "correct_answer": "æ­£è§£ã§ã™! ğŸ‰",
        "incorrect_answer": "ä¸æ­£è§£ã§ã™ã€‚ğŸ˜",
        "correct_is": "æ­£è§£",
        "explanation": "è§£èª¬",
        "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
        "score": "ã‚¹ã‚³ã‚¢",
        "retake_quiz": "ã‚¯ã‚¤ã‚ºã‚’å†æŒ‘æˆ¦",
        "quiz_error_llm": "LLMì´ ì˜¬ë°”ë¥¸ JSONì˜ í˜•ì‹ ì„ ì½ì–´ ë“¤ì´ì§€ ëª»í•¨",
        "quiz_original_response": "LLM åŸæœ¬å¿œç­”",
        "firestore_loading": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...",
        "db_save_complete": "(DBä¿å­˜å®Œäº†)",
        "data_analysis_progress": "è³‡æ–™åˆ†æ ë° å­¦ç¿’ DB êµ¬ì¶• ä¸­...",
        "response_generating": "å¿œç­”ç”Ÿæˆä¸­...",
        "lstm_result_header": "é”æˆåº¦äºˆæ¸¬çµæœ",
        "lstm_score_metric": "ç¾åœ¨ã®äºˆæ¸¬é”æˆåº¦",
        "lstm_score_info": "æ¬¡ã®ã‚¯ã‚¤ã‚ºã®æ¨å®šã‚¹ã‚³ì–´ã¯ç´„ **{predicted_score:.1f}ç‚¹**ì…ë‹ˆë‹¤ã€‚å­¦ç¿’ã®æˆæœã‚’ç¶­æŒã¾ãŸã¯å‘ä¸Šã•ã›ã¦ãã ã•ã„ï¼",
        "lstm_rerun_button": "æ–°ã—ã„ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬",
        "rec_header": "ìŒì„± ì…ë ¥ ë° ì „ì‚¬",
        "whisper_processing": "ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘",
        "empty_response_warning": "ì‘ë‹µì„ ì…ë ¥í•˜ì„¸ìš”ã€‚",
    }
}
# st.session_stateëŠ” ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
if 'language' not in st.session_state: st.session_state.language = DEFAULT_LANG
if 'is_llm_ready' not in st.session_state: st.session_state.is_llm_ready = False
if 'llm_init_error_msg' not in st.session_state: st.session_state.llm_init_error_msg = ""
if 'uploaded_files_state' not in st.session_state: st.session_state.uploaded_files_state = None
if 'is_rag_ready' not in st.session_state: st.session_state.is_rag_ready = False
if 'simulator_messages' not in st.session_state: st.session_state.simulator_messages = []
if 'simulator_memory' not in st.session_state: st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history")
if 'initial_advice_provided' not in st.session_state: st.session_state.initial_advice_provided = False
if 'is_chat_ended' not in st.session_state: st.session_state.is_chat_ended = False
if 'show_delete_confirm' not in st.session_state: st.session_state.show_delete_confirm = False
if 'last_transcript' not in st.session_state: st.session_state.last_transcript = ""
if 'sim_audio_bytes_raw' not in st.session_state: st.session_state.sim_audio_bytes_raw = None
if "agent_response_area_text" not in st.session_state: st.session_state.agent_response_area_text = ""
if "openai_client" not in st.session_state: st.session_state.openai_client = None

L = LANG[st.session_state.language]


# -----------------------------
# 1. Firebase Admin, GCS, OpenAI Initialization
# -----------------------------

def _load_service_account_from_secrets():
    """Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. (UI ì¶œë ¥ ì—†ìŒ)"""
    if "FIREBASE_SERVICE_ACCOUNT_JSON" not in st.secrets:
        # FIX: í™˜ê²½ ë³€ìˆ˜ì—ì„œë„ í‚¤ë¥¼ í™•ì¸
        if os.environ.get('FIREBASE_SERVICE_ACCOUNT_JSON'):
            service_account_data = os.environ.get('FIREBASE_SERVICE_ACCOUNT_JSON')
        else:
            return None, "FIREBASE_SERVICE_ACCOUNT_JSON Secret/Env Varê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    else:
        service_account_data = st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"]

    sa_info = None
    if isinstance(service_account_data, str):
        try:
            sa_info = json.loads(service_account_data.strip())
        except json.JSONDecodeError as e:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ JSON êµ¬ë¬¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ìƒì„¸ ì˜¤ë¥˜: {e}"
    elif hasattr(service_account_data, 'get'):
        try:
            sa_info = dict(service_account_data)
        except Exception:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì‹¤íŒ¨."
    else:
        return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."

    if not sa_info.get("project_id") or not sa_info.get("private_key"):
        return None, "JSON ë‚´ 'project_id' ë˜ëŠ” 'private_key' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    return sa_info, None


# FIX: í™˜ê²½ ë³€ìˆ˜/Secretsì—ì„œ GCS ë²„í‚· ì´ë¦„ ë¡œë“œ
def get_gcs_bucket_name():
    # GCS ë²„í‚· ì´ë¦„ì´ Secretsì´ë‚˜ í™˜ê²½ ë³€ìˆ˜ ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜
    return st.secrets.get('GCS_BUCKET_NAME') or os.environ.get('GCS_BUCKET_NAME')


@st.cache_resource(ttl=None)
def initialize_firestore_admin(L):
    """Secretsì—ì„œ ë¡œë“œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•˜ê³  DB í´ë¼ì´ì–¸íŠ¸ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    sa_info, error_message = _load_service_account_from_secrets()

    # FIX: í‚¤ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™” ìì²´ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.
    if sa_info is None:
        return None, f"âŒ {error_message} (ë¡œì»¬ í™˜ê²½ìœ¼ë¡œ ì „í™˜ë¨)"

    db_client = None
    try:
        # Check if app is already initialized (important for Streamlit rerun)
        if not firebase_admin._apps:
            # FIX: Firebase Admin SDK ì´ˆê¸°í™”
            cred = credentials.Certificate(sa_info)
            initialize_app(cred)

        # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        db_client = firestore.client()
        return db_client, "âœ… Firestore DB í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"

    except Exception as e:
        return None, f"ğŸ”¥ {L.get('firebase_init_fail', 'Firebase ì´ˆê¸°í™” ì‹¤íŒ¨')}: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¬¸ì œ. ì˜¤ë¥˜: {e}"


@st.cache_resource
def init_gcs_client(L):
    """GCS í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    gcs_bucket_name = get_gcs_bucket_name()
    sa_info, sa_error = _load_service_account_from_secrets()

    # FIX: ë²„í‚· ì´ë¦„ì´ë‚˜ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™” ê±´ë„ˆë›°ê¸°
    if not gcs_bucket_name:
        return None, L['gcs_missing']
    if sa_info is None:
        return None, sa_error

    gcs_client = None
    try:
        # Write credentials to temp file (necessary for gcs.Client() in container environments)
        # GCS í´ë¼ì´ì–¸íŠ¸ëŠ” GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ ì¸ì¦í•©ë‹ˆë‹¤.
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.json')
        # FIX: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ (sa_info)ë¥¼ ì„ì‹œ íŒŒì¼ì— ì”ë‹ˆë‹¤.
        tmp.write(json.dumps(sa_info).encode('utf-8'))
        tmp.flush()
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = tmp.name
        gcs_client = storage.Client()
        return gcs_client, "âœ… GCS í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
    except Exception as e:
        return None, f"{L['gcs_init_fail']}: {e}"
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ì„ íƒì )
        if 'GOOGLE_APPLICATION_CREDENTIALS' in os.environ:
            del os.environ['GOOGLE_APPLICATION_CREDENTIALS']
            try:
                os.remove(tmp.name)
            except Exception:
                pass


@st.cache_resource
def init_openai_client(L):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # FIX: ë¡œì»¬ í™˜ê²½ ë³€ìˆ˜ (.env)ì™€ Streamlit secretsì„ ëª¨ë‘ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
    openai_key = st.secrets.get('OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')
    if openai_key:
        try:
            # FIX: os.environ.get()ì—ì„œ ì°¾ì€ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            return OpenAI(api_key=openai_key), "âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
        except Exception as e:
            return None, f"OpenAI client init error: {e}"
    return None, L['openai_missing']


# -----------------------------
# 2. GCS, Firestore, Whisper Helpers
# -----------------------------

def upload_audio_to_gcs(bucket_name: str, blob_name: str, audio_bytes: bytes, content_type: str = 'audio/webm'):
    L = LANG[st.session_state.language]
    gcs_client = init_gcs_client(L)[0]
    if not gcs_client:
        # FIX: GCS í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ëŒ€ì‹  ì˜ˆì™¸ ë°œìƒ
        raise RuntimeError(L['gcs_not_conf'])

    # FIX: GCS í´ë¼ì´ì–¸íŠ¸ê°€ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆì„ ë•Œë§Œ ë²„í‚· ì‚¬ìš©
    bucket = gcs_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    blob.upload_from_string(audio_bytes, content_type=content_type)
    return f'gs://{bucket_name}/{blob_name}'


def download_audio_from_gcs(bucket_name: str, blob_name: str) -> bytes:
    L = LANG[st.session_state.language]
    gcs_client = init_gcs_client(L)[0]
    if not gcs_client:
        raise RuntimeError(L['gcs_not_conf'])
    try:
        bucket = gcs_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        return blob.download_as_bytes()
    except NotFound:
        raise FileNotFoundError(f"GCS Blob not found: {blob_name}")
    except Exception as e:
        raise RuntimeError(f"{L['gcs_playback_fail']}: {e}")


def save_audio_record(db, bucket_name, audio_bytes: bytes, filename:
str, transcript_text: str, meta: dict = None, mime_type: str = 'audio/webm'):
    L = LANG[st.session_state.language]

    # FIX: DB ê°ì²´ê°€ Noneì´ë©´ ì €ì¥ ë¡œì§ì„ ë°”ë¡œ ì¢…ë£Œ
    if not db:
        st.warning(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ìŒì„± ê¸°ë¡ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
        return None

    ts = datetime.now(timezone.utc)
    doc_ref = db.collection('voice_records').document()
    blob_name = f"voice_records/{doc_ref.id}/{filename}"

    gcs_path = None
    gcs_client = init_gcs_client(L)[0]

    # GCS í´ë¼ì´ì–¸íŠ¸ì™€ ë²„í‚· ì´ë¦„ì´ ëª¨ë‘ ìˆì„ ë•Œë§Œ GCS ì—…ë¡œë“œ ì‹œë„
    if bucket_name and gcs_client:
        try:
            gcs_path = upload_audio_to_gcs(bucket_name, blob_name, audio_bytes, mime_type)
        except Exception as e:
            st.warning(f"{L['upload_fail']}: {e}")
            gcs_path = None
    else:
        st.warning(L['gcs_missing'])  # GCS ë¯¸ì„¤ì • ê²½ê³ 

    data = {
        'created_at': ts,
        'filename': filename,
        'size': len(audio_bytes),
        'gcs_path': gcs_path,
        'transcript': transcript_text,
        'mime_type': mime_type,
        'language': st.session_state.language,
        'meta': meta or {}
    }

    doc_ref.set(data)
    return doc_ref.id


def delete_audio_record(db, bucket_name, doc_id: str):
    L = LANG[st.session_state.language]

    # FIX: DB ê°ì²´ê°€ Noneì´ë©´ ì‚­ì œ ë¡œì§ì„ ë°”ë¡œ ì¢…ë£Œ
    if not db:
        st.error(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
        return False

    doc_ref = db.collection('voice_records').document(doc_id)
    doc = doc_ref.get()
    if not doc.exists:
        return False
    data = doc.to_dict()

    gcs_client = init_gcs_client(L)[0]
    # delete GCS blob
    try:
        # GCS í´ë¼ì´ì–¸íŠ¸, ë²„í‚· ì´ë¦„, GCS ê²½ë¡œê°€ ëª¨ë‘ ìˆì„ ë•Œë§Œ ì‚­ì œ ì‹œë„
        if data.get('gcs_path') and gcs_client and bucket_name:
            blob_name = data['gcs_path'].split(f'gs://{bucket_name}/')[-1]
            bucket = gcs_client.bucket(bucket_name)
            blob = bucket.blob(blob_name)
            blob.delete()
    except Exception as e:
        st.warning(f"GCS delete warning: {e}")

    # delete firestore doc
    doc_ref.delete()
    return True


def transcribe_bytes_with_whisper(audio_bytes: bytes, mime_type: str = 'audio/webm', lang_code: str = 'en'):
    L = LANG[st.session_state.language]
    openai_client = st.session_state.get('openai_client')  # FIX: ì„¸ì…˜ ìƒíƒœì—ì„œ í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ
    if openai_client is None:
        # API í‚¤ê°€ ì—†ì–´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° Mock ì‘ë‹µ
        return f"âŒ {L.get('whisper_client_error', 'Whisper API Clientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')}"

    # LangChain ì–¸ì–´ ì½”ë“œ ë§µí•‘ (ko, en, ja)
    whisper_lang_map = {
        'ko': 'ko',
        'en': 'en',
        'ja': 'ja'
    }
    whisper_language = whisper_lang_map.get(lang_code, 'en')  # ê¸°ë³¸ê°’ì€ ì˜ì–´

    # Determine file extension
    ext = mime_type.split('/')[-1].lower() if '/' in mime_type else 'webm'

    # write to temp file
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=f'.{ext}')
    tmp.write(audio_bytes)
    tmp.flush()
    tmp.close()

    try:
        with open(tmp.name, 'rb') as af:
            res = openai_client.audio.transcriptions.create(
                model='whisper-1',
                file=af,
                response_format='text',
                # â­ ì–¸ì–´ ì½”ë“œ ì¶”ê°€
                language=whisper_language
            )
        return res.strip() or ''
    except Exception as e:
        # API ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´, ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì—¬ UIì— í‘œì‹œ
        return f"âŒ {L['error']} Whisper: {e}"
    finally:
        try:
            os.remove(tmp.name)
        except Exception:
            pass


# -----------------------------
# 3. Firestore/RAG/LLM Helpers
# -----------------------------

def save_simulation_history(db, initial_query, customer_type, messages):
    L = LANG[st.session_state.language]
    # FIX: DB ê°ì²´ê°€ Noneì´ë©´ ì €ì¥í•˜ì§€ ì•Šê³  ê²½ê³ ë§Œ í‘œì‹œ
    if not db:
        st.sidebar.warning(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì´ë ¥ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."));
        return False

    history_data = [{k: v for k, v in msg.items()} for msg in messages]
    data = {
        "initial_query": initial_query,
        "customer_type": customer_type,
        "messages": history_data,
        "language_key": st.session_state.language,
        "timestamp": firestore.SERVER_TIMESTAMP
    }
    try:
        db.collection("simulation_histories").add(data)
        st.sidebar.success(L.get("save_history_success"));
        return True
    except Exception as e:
        st.sidebar.error(f"âŒ {L.get('save_history_fail')}: {e}");
        return False


def load_simulation_histories(db):
    current_lang_key = st.session_state.language
    # FIX: DB ê°ì²´ê°€ Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not db:
        return []

    try:
        histories = (
            db.collection("simulation_histories").where("language_key", "==", current_lang_key).order_by("timestamp",
                                                                                                         direction=Query.DESCENDING).limit(
                10).stream())
        results = []
        for doc in histories:
            data = doc.to_dict();
            data['id'] = doc.id
            if 'messages' in data and isinstance(data['messages'], list) and data['messages']: results.append(data)
        return results
    except Exception as e:
        print(f"Error loading histories: {e}");
        return []


def delete_all_history(db):
    L = LANG[st.session_state.language]
    # FIX: DB ê°ì²´ê°€ Noneì´ë©´ ì‚­ì œí•˜ì§€ ì•Šê³  ì—ëŸ¬ í‘œì‹œ
    if not db:
        st.error(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."));
        return

    try:
        docs = db.collection("simulation_histories").stream()
        for doc in docs: doc.reference.delete()
        st.session_state.simulator_messages = [];
        st.session_state.simulator_memory.clear()
        st.session_state.show_delete_confirm = False;
        st.success(L["delete_success"]);
        st.rerun()
    except Exception as e:
        st.error(f"{L.get('delete_fail')}: {e}")


# --- Utility Placeholder Functions ---
def get_document_chunks(files):
    return []


def get_vector_store(text_chunks):
    # Mock Vector Store ë°˜í™˜ (ì„±ê³µ ê°€ì •)
    if not text_chunks:
        return None

    # Mock ê°ì²´ ë°˜í™˜ (ì‹¤ì œ FAISS ê°ì²´ê°€ ì•„ë‹˜)
    class MockVectorStore:
        def as_retriever(self, search_kwargs={}):  # search_kwargs ì¶”ê°€
            return self

        def get_relevant_documents(self, query):
            # Mock Document Source
            from langchain.schema.document import Document  # ë‚´ë¶€ ì„í¬íŠ¸
            return [Document(page_content="Mock RAG content about the uploaded files.")]

    return MockVectorStore()


def get_rag_chain(vector_store):
    # Mock RAG Chain ë°˜í™˜ (ì„±ê³µ ê°€ì •)
    if vector_store is None:
        return None

    class MockRAGChain:
        def invoke(self, inputs):
            question = inputs.get('question', 'ì§ˆë¬¸ ì—†ìŒ')
            # Mock ë‹µë³€ ìƒì„±
            return {"answer": f"ìš”ì²­í•˜ì‹  '{question}'ì— ëŒ€í•´ í•™ìŠµëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. (Mock Response)"}

    return MockRAGChain()


def save_index_to_firestore(db, vector_store, index_id="user_portfolio_rag"):
    # FIX: DBê°€ Noneì´ë©´ ì €ì¥ ê±´ë„ˆë›°ê¸°
    if not db:
        return False
    return True


def load_index_from_firestore(db, embeddings, index_id="user_portfolio_rag"):
    # FIX: DBê°€ Noneì´ë©´ ë¡œë“œ ê±´ë„ˆë›°ê¸°
    if not db:
        return None
    return None


def load_or_train_lstm():
    # Mock LSTM Model and Data Generation (using required imports)
    np.random.seed(42)
    # Generate synthetic time series data for past quiz scores (0-100)
    n_points = 50
    time_series = 60 + 20 * np.sin(np.linspace(0, 4 * np.pi, n_points)) + np.random.normal(0, 5, n_points)
    time_series = np.clip(time_series, 50, 100).astype(np.float32)

    # Simple mock model creation
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(1, 1)))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')

    # Mock training and prediction
    X = time_series[:-1].reshape(-1, 1, 1)
    Y = time_series[1:].reshape(-1, 1)
    # In a real scenario, model.fit(X, Y, epochs=10, verbose=0) would run

    # Mock prediction: Take the last known score and add a slight positive/negative trend
    last_score = time_series[-1]
    # FIX: look_back ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•˜ë„ë¡ ìˆ˜ì •
    look_back = 5
    X_pred = time_series[-look_back:].reshape(1, look_back, 1)
    predicted_raw = model.predict(X_pred, verbose=0)[0][0]
    predicted_score = np.clip(predicted_raw, 50, 95)

    return model, time_series


def force_rerun_lstm():
    st.session_state.lstm_rerun_trigger = time.time();
    st.rerun()


def render_interactive_quiz(quiz_data, current_lang):
    st.warning("Quiz UI Placeholder")


# â­ [ìˆ˜ì •] TTS API í˜¸ì¶œ ë¡œì§ì„ Python ì„œë²„ ì¸¡ìœ¼ë¡œ ì´ë™í•˜ê³  OpenAI TTS ì‚¬ìš©
def synthesize_and_play_audio(text_to_speak, current_lang_key):
    L = LANG[current_lang_key]
    openai_client = st.session_state.get('openai_client')

    if openai_client is None:
        st.error(L['openai_missing'])
        return None, f"âŒ {L['tts_status_error']} (Client Missing)"

    tts_voice = 'nova'  # ì—¬ì„± ëª©ì†Œë¦¬, ëª¨ë“  ì–¸ì–´ì— ì‚¬ìš© ê°€ëŠ¥ (í˜„ì¬ TTS-1 ëª¨ë¸ ì§€ì›)

    # ìŒì„± íŒŒì¼ ìƒì„± ë° ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ ë°˜í™˜
    try:
        response = openai_client.audio.speech.create(
            model="tts-1",
            voice=tts_voice,
            input=text_to_speak
        )
        return response.content, f"âœ… {L['tts_status_success']}"
    except Exception as e:
        return None, f"âŒ {L['tts_status_error']} (OpenAI TTS Error: {e})"


def render_tts_button(text_to_speak, current_lang_key):
    L = LANG[current_lang_key]
    # â­ [ìˆ˜ì •] ë²„íŠ¼ í‚¤ë¥¼ ê³ ìœ í•˜ê²Œ ë§Œë“¤ë˜, st.audio í‚¤ëŠ” ê³ ì •ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    button_key = f"tts_button_{hash(text_to_speak)}_{time.time()}"
    audio_player_key = f"tts_player_{hash(text_to_speak)}"

    if st.button(L.get("button_listen_audio"), key=button_key):
        with st.spinner(L['tts_status_generating']):
            audio_bytes, status_msg = synthesize_and_play_audio(text_to_speak, current_lang_key)

            # TTS ìƒíƒœë¥¼ ì•Œë¦¬ëŠ” ìŠ¤í”¼ë„ˆ ëŒ€ì‹  ì„ì‹œ ë©”ì‹œì§€ í‘œì‹œ
            if "âŒ" in status_msg:
                st.error(status_msg)
            elif audio_bytes:
                # MP3 ì˜¤ë””ì˜¤ ë°”ì´íŠ¸ë¥¼ Streamlit ì˜¤ë””ì˜¤ ìœ„ì ¯ìœ¼ë¡œ ì¬ìƒ
                st.audio(audio_bytes, format='audio/mp3', autoplay=True, key=audio_player_key)
            else:
                st.error(status_msg)


def clean_and_load_json(text):
    match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if match: text = match.group(1)
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        return None


def get_mock_response_data(lang_key, customer_type):
    L = LANG[lang_key];
    return {"advice_header": f"{L['simulation_advice_header']}", "advice": f"Mock advice for {customer_type}",
            "draft_header": f"{L['simulation_draft_header']}", "draft": f"Mock draft response in {lang_key}"}


def get_closing_messages(lang_key):
    if lang_key == 'ko':
        return {"additional_query": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?", "chat_closing": LANG['ko']['prompt_survey']}
    elif lang_key == 'en':
        return {"additional_query": "Is there anything else we can assist you with today?",
                "chat_closing": LANG['en']['prompt_survey']}
    elif lang_key == 'ja':
        return {"additional_query": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
                "chat_closing": LANG['ja']['prompt_survey']}
    return get_closing_messages('ko')


# --- End Helper Functions ---


# --- í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤í–‰ ---
firestore_db_client, db_msg = initialize_firestore_admin(L)
st.session_state.firestore_db = firestore_db_client
st.session_state.db_init_msg = db_msg

gcs_client_obj, gcs_msg = init_gcs_client(L)
gcs_client = gcs_client_obj
st.session_state.gcs_init_msg = gcs_msg

# FIX: init_openai_clientëŠ” í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ë°˜í™˜í•´ì•¼ í•˜ë¯€ë¡œ, ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.
openai_client_obj, openai_msg = init_openai_client(L)
openai_client = openai_client_obj
st.session_state.openai_client = openai_client_obj
st.session_state.openai_init_msg = openai_msg

# --- LLM ì´ˆê¸°í™” ---
# FIX: LLM_API_KEYê°€ os.environ.get()ì„ í†µí•´ ë¡œë“œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
LLM_API_KEY = st.secrets.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
LLM_MODEL = "gpt-3.5-turbo"

if 'llm' not in st.session_state and LLM_API_KEY:
    try:
        st.session_state.llm = ChatOpenAI(model=LLM_MODEL, temperature=0.7, openai_api_key=LLM_API_KEY)
        st.session_state.embeddings = OpenAIEmbeddings(openai_api_key=LLM_API_KEY)
        st.session_state.is_llm_ready = True

        SIMULATOR_PROMPT = PromptTemplate(
            template="The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n\n{chat_history}\nHuman: {input}\nAI:",
            input_variables=["input", "chat_history"]
        )
        st.session_state.simulator_chain = ConversationChain(
            llm=st.session_state.llm,
            memory=st.session_state.simulator_memory,
            prompt=SIMULATOR_PROMPT,
            input_key="input",
        )
    except Exception as e:
        st.session_state.llm_init_error_msg = f"{L['llm_error_init']} (OpenAI): {e}"
        st.session_state.is_llm_ready = False
elif not LLM_API_KEY:
    st.session_state.llm_init_error_msg = L["openai_missing"]

# RAG Index Loading
# FIX: firestore_dbê°€ Noneì¸ ê²½ìš° RAG ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„ ìì²´ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.
if st.session_state.get(
        'firestore_db') and 'conversation_chain' not in st.session_state and st.session_state.is_llm_ready:
    loaded_index = load_index_from_firestore(st.session_state.firestore_db, st.session_state.embeddings)

    if loaded_index:
        st.session_state.conversation_chain = get_rag_chain(loaded_index)
        st.session_state.is_rag_ready = True
        st.session_state.firestore_load_success = True
    else:
        st.session_state.firestore_load_success = False
        st.session_state.is_rag_ready = False
elif not st.session_state.get('firestore_db'):
    # DB ì´ˆê¸°í™”ê°€ ì•ˆ ë˜ì—ˆì„ ê²½ìš° RAGë„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
    st.session_state.firestore_load_success = False
    st.session_state.is_rag_ready = False
# --- LLM ì´ˆê¸°í™” ë ---


# -----------------------------
# 9. UI RENDERING LOGIC
# -----------------------------

# ì‚¬ì´ë“œë°” ì„¤ì • ì‹œì‘
with st.sidebar:
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=['ko', 'en', 'ja'],
        index=['ko', 'en', 'ja'].index(st.session_state.language),
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
    )

    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        L = LANG[st.session_state.language]
        if not st.session_state.is_llm_ready:
            st.session_state.llm_init_error_msg = L["llm_error_key"]
        st.rerun()

    L = LANG[st.session_state.language]
    st.title(L["sidebar_title"])

    st.markdown("---")
    st.subheader("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœ")

    # --- ì´ˆê¸°í™” ìƒíƒœ í‘œì‹œ ---
    if st.session_state.get('llm_init_error_msg'):
        st.error(st.session_state.llm_init_error_msg)
    elif st.session_state.is_llm_ready:
        st.success("âœ… LLM ë° ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

    # DB & GCS ìƒíƒœ í‘œì‹œ
    if "âŒ" in st.session_state.db_init_msg:
        st.warning(st.session_state.db_init_msg)  # DB ì´ˆê¸°í™” ì‹¤íŒ¨ ë©”ì‹œì§€ ì¶œë ¥
    elif "âœ…" in st.session_state.db_init_msg:
        st.success(st.session_state.db_init_msg)
    else:
        st.info("DB Secret ë¯¸ì„¤ì • ë˜ëŠ” ë¡œì»¬ ì „í™˜ ìƒíƒœì…ë‹ˆë‹¤.")  # í‚¤ê°€ ì—†ì–´ì„œ ì´ˆê¸°í™” ê±´ë„ˆë›´ ê²½ìš°

    if "âŒ" in st.session_state.gcs_init_msg:
        st.warning(st.session_state.gcs_init_msg)  # GCS ì´ˆê¸°í™” ì‹¤íŒ¨ ë©”ì‹œì§€ ì¶œë ¥
    elif "âœ…" in st.session_state.gcs_init_msg:
        st.success(st.session_state.gcs_init_msg)
    else:
        st.info("GCS Secret ë¯¸ì„¤ì • ë˜ëŠ” ë¡œì»¬ ì „í™˜ ìƒíƒœì…ë‹ˆë‹¤.")  # GCS ì´ˆê¸°í™” ê±´ë„ˆë›´ ê²½ìš°

    if "âœ…" in st.session_state.openai_init_msg:
        st.success(st.session_state.openai_init_msg)
    else:
        st.warning(L.get(
            "openai_missing") if "missing" in st.session_state.openai_init_msg else st.session_state.openai_init_msg)

    st.markdown("---")

    # RAG Indexing Section
    uploaded_files_widget = st.file_uploader(
        L["file_uploader"], type=["pdf", "txt", "html"], accept_multiple_files=True
    )
    if uploaded_files_widget: st.session_state.uploaded_files_state = uploaded_files_widget
    files_to_process = st.session_state.uploaded_files_state if st.session_state.uploaded_files_state else []

    # FIX: DBê°€ Noneì´ì–´ë„ RAGëŠ” Mockìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ì§€ë§Œ, DB ì €ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤.
    if files_to_process and st.session_state.is_llm_ready:
        if st.button(L["button_start_analysis"], key="start_analysis"):
            with st.spinner(L["data_analysis_progress"]):
                text_chunks = get_document_chunks(files_to_process)
                vector_store = get_vector_store(text_chunks)

                # ì˜¤ë¥˜ ìš°íšŒë¥¼ ìœ„í•´ Mock ì„±ê³µ ë¡œì§ ê°•ì œ ì‹¤í–‰
                if vector_store is None:
                    count = 1
                else:
                    count = len(text_chunks) if text_chunks else 1

                # DBê°€ ì´ˆê¸°í™” ë˜ì—ˆì„ ë•Œë§Œ ì €ì¥ ì‹œë„
                save_success_msg = ""
                if st.session_state.firestore_db:
                    save_success = save_index_to_firestore(st.session_state.firestore_db, vector_store)
                    save_success_msg = " " + L["db_save_complete"] if save_success else " (DB Save Failed)"
                else:
                    save_success_msg = " (DB ë¯¸ì—°ê²°ë¡œ ì¸ë±ìŠ¤ ì €ì¥ ê±´ë„ˆëœ€)"

                st.success(L["embed_success"].format(count=count) + save_success_msg)

                st.session_state.conversation_chain = get_rag_chain(vector_store)
                st.session_state.is_rag_ready = True

    elif not files_to_process:
        st.warning(L.get("warning_no_files"))

    st.markdown("---")

    # Feature Selection Radio
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ",
        [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["simulator_tab"], L["voice_rec_header"]]
    )

st.title(L["title"])

# ================================
# 10. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================

if feature_selection == L["voice_rec_header"]:
    st.header(L['voice_rec_header'])
    st.caption(L['record_help'])

    col_rec_ui, col_list_ui = st.columns([1, 1])

    with col_rec_ui:
        st.subheader(L['rec_header'])

        # Audio Input Widget
        audio_obj = None
        try:
            # st.file_uploaderë¥¼ ì‚¬ìš©
            audio_obj = st.file_uploader(L['uploaded_file'], type=['wav', 'mp3', 'm4a', 'webm'],
                                         key='main_file_uploader')
            st.caption(f"({L['uploaded_file']}ë§Œ ì§€ì›)")
        except Exception:
            audio_obj = None

        audio_bytes = None
        audio_mime = 'audio/webm'
        if audio_obj is not None:
            if hasattr(audio_obj, 'getvalue'):
                audio_bytes = audio_obj.getvalue()
                audio_mime = getattr(audio_obj, 'type', 'audio/webm')

        if audio_bytes:
            st.audio(audio_bytes, format=audio_mime)

            # Transcribe Action
            if st.button(L['transcribe_btn'], key='transcribe_btn_key_rec'):
                if st.session_state.openai_client is None:  # FIX: ì„¸ì…˜ ìƒíƒœ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                    st.error(L['openai_missing'])
                else:
                    with st.spinner(L['transcribing']):
                        try:
                            # â­ ì–¸ì–´ ì½”ë“œ ì „ë‹¬
                            transcript_text = transcribe_bytes_with_whisper(
                                audio_bytes,
                                audio_mime,
                                lang_code=st.session_state.language
                            )
                            st.session_state['last_transcript'] = transcript_text

                            # â­ [í”¼ë“œë°± ê°•í™”] ì¸ì‹ëœ ë‚´ìš©ì„ ì„±ê³µ ë©”ì‹œì§€ì— í¬í•¨í•˜ì—¬ ì¦‰ì‹œ í™•ì¸
                            snippet = transcript_text[:50].replace('\n', ' ') + (
                                '...' if len(transcript_text) > 50 else '')
                            st.success(f"{L['transcript_result']} - **{snippet}**")

                        except RuntimeError as e:
                            st.error(e)

            st.text_area(L['transcript_text'], value=st.session_state.get('last_transcript', ''), height=150,
                         key='transcript_area_rec')

            # Save Action
            if st.button(L['save_btn'], key='save_btn_key_rec'):
                if st.session_state.firestore_db is None:
                    # FIX: DBê°€ Noneì¼ ë•Œì˜ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ëª…í™•íˆ í•¨
                    st.error(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                else:
                    bucket_name = get_gcs_bucket_name()
                    ext = audio_mime.split('/')[-1] if '/' in audio_mime else 'webm'
                    filename = f"record_{int(time.time())}.{ext}"
                    transcript_text = st.session_state.get('last_transcript', '')

                    try:
                        save_audio_record(st.session_state.firestore_db, bucket_name, audio_bytes, filename,
                                          transcript_text, mime_type=audio_mime)
                        st.success(L['saved_success'])
                        st.session_state['last_transcript'] = ''
                        st.rerun()
                    except Exception as e:
                        st.error(f"{L['error']} {e}")

    with col_list_ui:
        st.subheader(L['rec_list_title'])
        if st.session_state.firestore_db is None:
            # FIX: DBê°€ Noneì¼ ë•Œì˜ ê²½ê³  ë©”ì‹œì§€ë¥¼ ëª…í™•íˆ í•¨
            st.warning(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì´ë ¥ ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€"))
        else:
            try:
                docs = list(st.session_state.firestore_db.collection('voice_records').order_by('created_at',
                                                                                               direction=firestore.Query.DESCENDING).limit(
                    50).stream())
            except Exception as e:
                st.error(f"Firestore read error: {e}")
                docs = []

            if not docs:
                st.info(L['no_records'])
            else:
                bucket_name = get_gcs_bucket_name()
                for d in docs:
                    data = d.to_dict()
                    doc_id = d.id
                    created_at_ts = data.get('created_at')
                    created_str = created_at_ts.astimezone(timezone.utc).strftime('%Y-%m-%d %H:%M UTC') if isinstance(
                        created_at_ts, datetime) else str(created_at_ts)
                    transcript_snippet = (data.get('transcript') or '')[:50].replace('\n', ' ') + '...'

                    with st.expander(f"[{created_str}] {transcript_snippet}"):
                        st.write(f"**{L['transcript_text']}:** {data.get('transcript') or 'N/A'}")
                        st.caption(
                            f"**Size:** {data.get('size')} bytes | **Path:** {data.get('gcs_path', L['gcs_not_conf'])}")

                        colp, colr, cold = st.columns([2, 1, 1])

                        # Playback Button
                        if colp.button(L['playback'], key=f'play_{doc_id}'):
                            if data.get('gcs_path') and gcs_client and bucket_name:
                                with st.spinner(L['playback']):
                                    try:
                                        blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(
                                            f'gs://{bucket_name}/')[-1])
                                        mime_type = data.get('mime_type', 'audio/webm')
                                        st.audio(blob_bytes, format=mime_type)
                                    except Exception as e:
                                        st.error(f"{L['gcs_playback_fail']}: {e}")
                            else:
                                st.info(L['gcs_no_audio'])

                        # Re-transcribe Button
                        if colr.button(L['retranscribe'], key=f'retx_{doc_id}'):
                            if st.session_state.openai_client is None:
                                st.error(L['openai_missing'])  # FIX: ì„¸ì…˜ ìƒíƒœ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                            elif data.get('gcs_path') and gcs_client and bucket_name:
                                with st.spinner(L['transcribing']):
                                    try:
                                        # â­ [ìˆ˜ì •] í•¨ìˆ˜ ì´ë¦„ ì˜¤íƒ€ ìˆ˜ì • ë°˜ì˜
                                        blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(
                                            f'gs://{bucket_name}/')[-1])
                                        mime_type = data.get('mime_type', 'audio/webm')
                                        new_text = transcribe_bytes_with_whisper(
                                            blob_bytes,
                                            mime_type,
                                            lang_code=st.session_state.language
                                        )
                                        # DBê°€ Noneì´ ì•„ë‹ ë•Œë§Œ ì—…ë°ì´íŠ¸ ì‹œë„
                                        if st.session_state.firestore_db:
                                            st.session_state.firestore_db.collection('voice_records').document(
                                                doc_id).update({'transcript': new_text})
                                            st.success(L['retranscribe'] + ' ' + L['saved_success'])
                                        else:
                                            st.warning("DB ë¯¸ì—°ê²° ìƒíƒœë¡œ ì „ì‚¬ë§Œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                        st.rerun()
                                    except Exception as e:
                                        st.error(f"{L['error']} {e}")
                            else:
                                st.error(L['gcs_not_conf'])

                        # Delete Button
                        if cold.button(L['delete'], key=f'del_{doc_id}'):
                            if st.session_state.get(f'confirm_del_rec_{doc_id}', False):
                                ok = delete_audio_record(st.session_state.firestore_db, bucket_name, doc_id)
                                if ok:
                                    st.success(L['delete_success'])
                                else:
                                    st.error(L['delete_fail'])
                                st.session_state[f'confirm_del_rec_{doc_id}'] = False
                                st.rerun()
                            else:
                                st.session_state[f'confirm_del_rec_{doc_id}'] = True
                                st.warning(L['delete_confirm_rec'])

elif feature_selection == L["simulator_tab"]:
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])

    # 1. TTS ìœ í‹¸ë¦¬í‹° (ìƒíƒœ í‘œì‹œê¸° ë° JS í•¨ìˆ˜)ë¥¼ í˜ì´ì§€ ìƒë‹¨ì— ì‚½ì…
    st.markdown(
        f'<div id="tts_status" style="padding: 5px; text-align: center; border-radius: 5px; background-color: #f0f0f0; margin-bottom: 10px;">{L["tts_status_ready"]}</div>',
        unsafe_allow_html=True)

    # 1.5 ì´ë ¥ ì‚­ì œ ë²„íŠ¼ ë° ëª¨ë‹¬
    db = st.session_state.get('firestore_db')
    col_delete, _ = st.columns([1, 4])
    with col_delete:
        if st.button(L["delete_history_button"], key="trigger_delete_history_sim"):
            if not db:
                st.warning(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
            else:
                st.session_state.show_delete_confirm = True

    if st.session_state.show_delete_confirm:
        with st.container(border=True):
            st.warning(L["delete_confirm_message"])
            col_yes, col_no = st.columns(2)
            if col_yes.button(L["delete_confirm_yes"], key="confirm_delete_yes", type="primary"):
                with st.spinner(L["deleting_history_progress"]):
                    delete_all_history(db)
            if col_no.button(L["delete_confirm_no"], key="confirm_delete_no"):
                st.session_state.show_delete_confirm = False
                st.rerun()

    # â­ Firebase ìƒë‹´ ì´ë ¥ ë¡œë“œ ë° ì„ íƒ ì„¹ì…˜
    if db:
        with st.expander(L["history_expander_title"]):
            histories = load_simulation_histories(db)
            search_query = st.text_input(L["search_history_label"], key="history_search_sim", value="")
            today = datetime.now().date()
            default_start_date = today - timedelta(days=7)
            date_range_input = st.date_input(L["date_range_label"], value=[default_start_date, today],
                                             key="history_date_range_sim")

            filtered_histories = []
            if histories:
                if isinstance(date_range_input, list) and len(date_range_input) == 2:
                    start_date = min(date_range_input)
                    end_date = max(date_range_input) + timedelta(days=1)
                else:
                    start_date = datetime.min.date()
                    end_date = datetime.max.date()
                for h in histories:
                    search_match = True
                    if search_query:
                        query_lower = search_query.lower()
                        searchable_text = h['initial_query'].lower() + " " + h['customer_type'].lower()
                        if query_lower not in searchable_text: search_match = False
                    date_match = True
                    if h.get('timestamp'):
                        h_date = h['timestamp'].date()
                        if not (start_date <= h_date < end_date): date_match = False
                    if search_match and date_match: filtered_histories.append(h)

            if filtered_histories:
                history_options = {
                    f"[{h['timestamp'].strftime('%m-%d %H:%M')}] {h['customer_type']} - {h['initial_query'][:30]}...": h
                    for h in filtered_histories}
                selected_key = st.selectbox(L["history_selectbox_label"], options=list(history_options.keys()))

                if st.button(L["history_load_button"], key='load_sim_history'):
                    selected_history = history_options[selected_key]
                    st.session_state.customer_query_text_area = selected_history['initial_query']
                    st.session_state.initial_advice_provided = True
                    st.session_state.simulator_messages = selected_history['messages']
                    st.session_state.is_chat_ended = selected_history.get('is_chat_ended', False)
                    st.session_state.simulator_memory.clear()
                    for msg in selected_history['messages']:
                        if msg['role'] == 'customer':
                            st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
                        elif msg['role'] == 'agent_response':
                            st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
                        elif msg['role'] in ['supervisor', 'customer_rebuttal', 'customer_end', 'system_end']:
                            st.session_state.simulator_memory.chat_memory.add_ai_message(msg['content'])
                    st.rerun()
            else:
                st.info(L.get("no_history_found"))
    else:
        # FIX: DBê°€ Noneì¼ ë•Œì˜ ê²½ê³  ë©”ì‹œì§€ë¥¼ ëª…í™•íˆ í•¨
        st.warning(L.get("warning_no_db_connect", "DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì´ë ¥ ë¡œë“œ ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€"))

    # LLM and UI logic for Simulation flow
    if st.session_state.is_llm_ready or not LLM_API_KEY:  # Use LLM_API_KEY for initial check
        if st.session_state.is_chat_ended:
            st.success(L["prompt_customer_end"] + " " + L["prompt_survey"])
            if st.button(L["new_simulation_button"], key="new_simulation"):
                st.session_state.is_chat_ended = False
                st.session_state.initial_advice_provided = False
                st.session_state.simulator_messages = []
                st.session_state.simulator_memory.clear()
                st.session_state['last_transcript'] = ''
                st.session_state['agent_response_area_text'] = ''  # FIX: ì´ˆê¸°í™” ì¶”ê°€
                st.rerun()
            st.stop()

        if 'customer_query_text_area' not in st.session_state: st.session_state.customer_query_text_area = ""

        customer_query = st.text_area(
            L["customer_query_label"], key="customer_query_text_area", height=150,
            placeholder=L["initial_query_sample"],
            value=st.session_state.customer_query_text_area,
            disabled=st.session_state.initial_advice_provided
        )
        customer_type_options_list = L["customer_type_options"]
        default_index = 1 if len(customer_type_options_list) > 1 else 0
        st.selectbox(
            L["customer_type_label"], customer_type_options_list, index=default_index,
            disabled=st.session_state.initial_advice_provided,
            key='customer_type_sim_select'
        )
        # â­ [ìˆ˜ì •] selectbox ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•˜ì—¬ ì‚¬ìš©
        customer_type_display = st.session_state.customer_type_sim_select

        current_lang_key = st.session_state.language

        # ë²„íŠ¼ì„ ëˆ„ë¥´ê¸° ì „ì— L["button_simulate"] í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ì–´ ë¡œì§ ì¶”ê°€
        simulate_button_text = L.get("button_simulate", "Simulate")

        if st.button(simulate_button_text, key="start_simulation", disabled=st.session_state.initial_advice_provided):
            if not customer_query: st.warning(L["simulation_warning_query"]); st.stop()

            st.session_state.simulator_memory.clear()
            st.session_state.simulator_messages = []
            st.session_state.is_chat_ended = False
            st.session_state.simulator_messages.append({"role": "customer", "content": customer_query})
            st.session_state.simulator_memory.chat_memory.add_user_message(customer_query)

            # í”„ë¡¬í”„íŠ¸ëŠ” LLMì´ ì¤€ë¹„ëœ ê²½ìš°ì—ë§Œ ì‚¬ìš©
            initial_prompt = f"""You are an AI Customer Support Supervisor. Your role is to analyze the following customer inquiry from a **{customer_type_display}** and provide a detailed response guideline and an initial draft response for the human agent.
[CRITICAL RULE FOR DRAFT CONTENT] The recommended draft MUST be strictly in {LANG[current_lang_key]['lang_select']}.
[CRITICAL RULE FOR SUPERVISOR TONE] Your advice and draft MUST be presented in a markdown format, using the specific headers: '### {L['simulation_advice_header']}' and '### {L['simulation_draft_header']}'.
[CRITICAL RULE FOR CUSTOMER ROLEPLAY (FUTURE MESSAGES)] When the Agent subsequently asks for information in later rounds, **Roleplay as the Customer** who is frustrated but **MUST BE HIGHLY COOPERATIVE** and provide the requested details piece by piece (not all at once). The customer MUST NOT argue or ask why the information is needed.

Customer Inquiry: {customer_query}
"""

            if not LLM_API_KEY or not st.session_state.is_llm_ready:  # Mock response for missing key
                mock_data = get_mock_response_data(current_lang_key, customer_type_display)
                ai_advice_text = f"### {mock_data['advice_header']}\n\n{mock_data['advice']}\n\n### {mock_data['draft_header']}\n\n{mock_data['draft']}"
                st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
                st.session_state.simulator_memory.chat_memory.add_ai_message(ai_advice_text)
                st.session_state.initial_advice_provided = True
                save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
                st.warning(L["simulation_no_key_warning"])
                st.rerun()

            if LLM_API_KEY and st.session_state.is_llm_ready:
                with st.spinner(L["response_generating"]):
                    try:
                        response_text = st.session_state.simulator_chain.predict(input=initial_prompt)
                        st.session_state.simulator_messages.append({"role": "supervisor", "content": response_text})
                        st.session_state.initial_advice_provided = True
                        save_simulation_history(db, customer_query, customer_type_display,
                                                st.session_state.simulator_messages)
                        st.rerun()
                    except Exception as e:
                        st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        st.markdown("---")
        for message in st.session_state.simulator_messages:
            if message["role"] == "customer":
                with st.chat_message("user", avatar="ğŸ™‹"):
                    st.markdown(message["content"])
            elif message["role"] == "supervisor":
                with st.chat_message("assistant", avatar="ğŸ¤–"):
                    st.markdown(message["content"]);
                    render_tts_button(message["content"], st.session_state.language)
            elif message["role"] == "agent_response":
                with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
                    st.markdown(message["content"])
            elif message["role"] == "customer_rebuttal":
                with st.chat_message("assistant", avatar="ğŸ˜ "):
                    st.markdown(message["content"])
            elif message["role"] == "customer_end":
                with st.chat_message("assistant", avatar="ğŸ˜Š"):
                    st.markdown(message["content"])
            elif message["role"] == "system_end":
                with st.chat_message("assistant", avatar="âœ¨"):
                    st.markdown(message["content"])

        if st.session_state.initial_advice_provided and not st.session_state.is_chat_ended:

            last_role = st.session_state.simulator_messages[-1]['role'] if st.session_state.simulator_messages else None

            # 1. ì—ì´ì „íŠ¸(ì‚¬ìš©ì)ê°€ ì‘ë‹µí•  ì°¨ë¡€ (ì´ˆê¸° ë¬¸ì˜ í›„, ì¬ë°˜ë°• í›„, ë§¤ë„ˆ ì§ˆë¬¸ í›„)
            if last_role in ["customer_rebuttal", "customer_end", "supervisor", "customer"]:

                # FIX: ìƒˆ ì‘ë‹µ í„´ì´ ì‹œì‘ë  ë•Œë§Œ agent_response_area_textë¥¼ ë¹„ì›Œì¤ë‹ˆë‹¤.
                if last_role not in ["agent_response"] and not st.session_state.get('agent_response_area_text'):
                    st.session_state.agent_response_area_text = ""

                st.markdown(f"### {L['agent_response_header']}")

                # --- â­ Whisper ì˜¤ë””ì˜¤ ì „ì‚¬ ê¸°ëŠ¥ ì¶”ê°€ ---
                col_audio, col_text_area = st.columns([1, 2])

                # ì˜¤ë””ì˜¤ íŒŒì¼ ë…¹ìŒ/ì—…ë¡œë“œ (mic_recorder component ì‚¬ìš©)
                with col_audio:
                    mic_result = mic_recorder(
                        start_prompt=L["button_mic_input"],
                        stop_prompt="âœ”ï¸ Stop Recording",
                        key="simulator_audio_input_file"
                    )

                # audio_bytes_from_micì™€ mime_typeì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ ë° ì—…ë°ì´íŠ¸
                is_audio_just_recorded = False
                if mic_result and mic_result.get('audio_bytes'):
                    current_bytes = mic_result['audio_bytes']
                    if current_bytes != st.session_state.get('sim_audio_bytes_raw'):
                        st.session_state['sim_audio_bytes'] = current_bytes
                        st.session_state['sim_audio_mime'] = mic_result.get('mime_type', 'audio/webm')
                        st.session_state['sim_audio_bytes_raw'] = current_bytes
                        is_audio_just_recorded = True

                # ë…¹ìŒì´ ë°©ê¸ˆ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì„¸ì…˜ ìƒíƒœë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ì¦‰ì‹œ RERUN
                if is_audio_just_recorded:
                    st.info("âœ… ë…¹ìŒ ì™„ë£Œ! ì•„ë˜ì˜ ì „ì‚¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
                    st.rerun()

                    # ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ í˜„ì¬ ì„¸ì…˜ì— ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ ì¬ìƒ ìœ„ì ¯ í‘œì‹œ
                if st.session_state.get('sim_audio_bytes'):
                    # Streamlitì˜ ì˜¤ë””ì˜¤ ìœ„ì ¯ì€ ì¬ìƒ/ì¼ì‹œì •ì§€ ê¸°ëŠ¥ì„ ë‚´ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.
                    st.audio(st.session_state['sim_audio_bytes'], format=st.session_state['sim_audio_mime'])

                # --- [ìˆ˜ì •] ì „ì‚¬ ë²„íŠ¼ ì‹¤í–‰ ë¡œì§ ---
                col_transcribe, _ = st.columns([1, 2])

                # â­ Key Error ë°©ì§€ë¥¼ ìœ„í•œ L.get() ì ìš©
                transcribe_button_text = L.get("transcribe_btn", "Transcribe")

                if col_transcribe.button(transcribe_button_text, key='start_whisper_transcribe'):
                    audio_bytes_to_transcribe = st.session_state.get('sim_audio_bytes')
                    audio_mime_to_transcribe = st.session_state.get('sim_audio_mime', 'audio/webm')

                    if audio_bytes_to_transcribe is None:
                        st.warning("ë¨¼ì € ë§ˆì´í¬ë¡œ ë…¹ìŒì„ ì™„ë£Œí•˜ê±°ë‚˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                    elif st.session_state.openai_client is None:  # FIX: ì„¸ì…˜ ìƒíƒœ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                        st.error(L.get("whisper_client_error", "OpenAI Keyê°€ ì—†ì–´ ìŒì„± ì¸ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                    else:
                        with st.spinner(L.get("whisper_processing", "ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")):
                            try:
                                # transcribe_bytes_with_whisper í•¨ìˆ˜ì— ì–¸ì–´ ì½”ë“œ ì „ë‹¬
                                transcribed_text = transcribe_bytes_with_whisper(
                                    audio_bytes_to_transcribe,
                                    audio_mime_to_transcribe,
                                    lang_code=current_lang_key
                                )

                                if transcribed_text.startswith("âŒ"):
                                    st.error(transcribed_text)
                                    st.session_state.agent_response_area_text = ""
                                else:
                                    st.session_state.last_transcript = transcribed_text

                                    # â­ [í•µì‹¬ ë³€ê²½] í…ìŠ¤íŠ¸ ì˜ì—­ì— ìë™ ì…ë ¥ë˜ë„ë¡ session_stateì˜ key ê°’ì„ ì—…ë°ì´íŠ¸
                                    st.session_state.agent_response_area_text = transcribed_text

                                    # â­ [í”¼ë“œë°± ê°•í™”] ì¸ì‹ëœ ë‚´ìš©ì„ ì„±ê³µ ë©”ì‹œì§€ì— í¬í•¨í•˜ì—¬ ì¦‰ì‹œ í™•ì¸ì‹œì¼œì¤ë‹ˆë‹¤.
                                    snippet = transcribed_text[:50].replace('\n', ' ') + (
                                        '...' if len(transcribed_text) > 50 else '')

                                    # API Keyê°€ Secrets ë˜ëŠ” Envì— ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ í”¼ë“œë°± ë©”ì‹œì§€ ê°•í™”
                                    if st.secrets.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY"):
                                        success_msg = f"{L.get('whisper_success', 'âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ! í…ìŠ¤íŠ¸ ì°½ì„ í™•ì¸í•˜ì„¸ìš”.')}\n\n**ì¸ì‹ ë‚´ìš© (Whisper API):** *{snippet}*"
                                    else:
                                        success_msg = f"{L.get('whisper_success', 'âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ! í…ìŠ¤íŠ¸ ì°½ì„ í™•ì¸í•˜ì„¸ìš”.')}\n\n**ì¸ì‹ ë‚´ìš©:** *{snippet}* (OpenAI Key ë¯¸ì„¤ì • ì‹œ Mock ì‘ë‹µ)"

                                    st.success(success_msg)

                                st.rerun()

                            except Exception as e:
                                st.error(f"ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                st.session_state.last_transcript = ""

                # st.text_areaëŠ” ì „ì‚¬ ê²°ê³¼ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê°’ì€ st.session_state.agent_response_area_textì— ì €ì¥ë©ë‹ˆë‹¤.
                agent_response = col_text_area.text_area(
                    L["agent_response_placeholder"],
                    value=st.session_state.agent_response_area_text,  # FIX: ì„¸ì…˜ ìƒíƒœ ê°’ ì‚¬ìš©
                    key="agent_response_area_text",
                    height=150
                )

                # --- Enter í‚¤ ì „ì†¡ ë¡œì§ ---
                js_code_for_enter = f"""
                <script>
                // st.text_areaì˜ í‚¤ê°€ 'agent_response_area_text'ì¸ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                const textarea = document.querySelector('textarea[key="agent_response_area_text"]');
                const button = document.querySelector('button[key="send_agent_response"]');

                if (textarea && button) {{
                    textarea.addEventListener('keydown', function(event) {{
                        if (event.key === 'Enter' && !event.shiftKey && !event.ctrlKey) {{
                            event.preventDefault(); 
                            button.click();
                        }}
                    }});
                }}
                </script>
                """
                st.components.v1.html(js_code_for_enter, height=0, width=0)

                if st.button(L.get("send_response_button", "ì‘ë‹µ ì „ì†¡"), key="send_agent_response"):
                    if agent_response.strip():
                        st.session_state.agent_response_area_text = ""

                        st.session_state.simulator_messages.append(
                            {"role": "agent_response", "content": agent_response}
                        )
                        st.session_state.simulator_memory.chat_memory.add_user_message(agent_response)
                        save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display,
                                                st.session_state.simulator_messages)
                        st.rerun()
                    else:
                        st.warning(L.get("empty_response_warning", "ì‘ë‹µ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”."))

            # 2. ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­ (LLM í˜¸ì¶œ) ë˜ëŠ” ì¢…ë£Œ ë²„íŠ¼ í‘œì‹œ
            if last_role == "agent_response":

                col_end, col_next = st.columns([1, 2])

                if col_end.button(L.get("button_end_chat", "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ì¡°ì‚¬ ìš”ì²­)"), key="end_chat"):
                    closing_messages = get_closing_messages(current_lang_key)

                    st.session_state.simulator_messages.append(
                        {"role": "supervisor", "content": L.get("customer_closing_confirm", "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?")})
                    st.session_state.simulator_messages.append(
                        {"role": "system_end", "content": closing_messages["chat_closing"]})
                    st.session_state.is_chat_ended = True

                    save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display,
                                            st.session_state.simulator_messages)
                    st.rerun()

                if col_next.button(L.get("request_rebuttal_button", "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­"), key="request_rebuttal"):
                    if not LLM_API_KEY:
                        st.warning("API Keyê°€ ì—†ê¸° ë•Œë¬¸ì— LLMì„ í†µí•œ ëŒ€í™”í˜• ì‹œë®¬ë ˆì´ì…˜ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        st.stop()

                    if st.session_state.simulator_chain is None:
                        st.error(L['llm_error_init'] + " (ì‹œë®¬ë ˆì´í„° ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨)")
                        st.stop()

                    next_reaction_prompt = f"""
                    Analyze the entire chat history. Roleplay as the customer ({customer_type_display}). 
                    Based on the agent's last message, generate ONE of the following responses in the customer's voice:
                    1. Provide **ONE** of the crucial, previously requested details (Model, Location, or Last Step) in a cooperative tone.
                    2. A short, positive closing remark (e.g., "{L['customer_positive_response']}").

                    The response MUST be strictly in {LANG[current_lang_key]['lang_select']}.
                    """

                    with st.spinner("ê³ ê° ë°˜ì‘ ìƒì„± ì¤‘..."):
                        customer_reaction = st.session_state.simulator_chain.predict(input=next_reaction_prompt)

                        positive_keywords = ["ê°ì‚¬", "thank you", "ã‚ã‚ŠãŒã¨ã†",
                                             L['customer_positive_response'].lower().split('/')[-1].strip()]
                        is_positive_close = any(keyword in customer_reaction.lower() for keyword in positive_keywords)

                        if is_positive_close:
                            role = "customer_end"
                            st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                            st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)

                            st.session_state.simulator_messages.append({"role": "supervisor",
                                                                        "content": L.get("customer_closing_confirm",
                                                                                         "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?")})
                            st.session_state.simulator_memory.chat_memory.add_ai_message(
                                L.get("customer_closing_confirm", "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?"))
                        else:
                            role = "customer_rebuttal"
                            st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                            st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)

                        save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display,
                                                st.session_state.simulator_messages)
                        st.rerun()

    else:
        st.error(L["llm_error_init"])

elif feature_selection == L["rag_tab"]:
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    if st.session_state.get('is_rag_ready', False) and st.session_state.get('conversation_chain'):
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner(f"ë‹µë³€ ìƒì„± ì¤‘..." if st.session_state.language == 'ko' else "Generating response..."):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question": prompt})
                        answer = response.get('answer',
                                              'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' if st.session_state.language == 'ko' else 'Could not generate response.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role": "assistant", "content": answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role": "assistant",
                                                          "content": "ì˜¤ë¥˜ ë°œìƒ" if st.session_state.language == 'ko' else "An error occurred"})
    else:
        st.warning(L["warning_rag_not_ready"])

elif feature_selection == L["content_tab"]:
    st.header(L["content_header"])
    st.markdown(L["content_desc"])

    if st.session_state.is_llm_ready:
        topic = st.text_input(L["topic_label"])

        level_map = dict(zip(L["level_options"], ["Beginner", "Intermediate", "Advanced"]))
        content_map = dict(zip(L["content_options"], ["summary", "quiz", "example"]))

        level_display = st.selectbox(L["level_label"], L["level_options"])
        content_type_display = st.selectbox(L["content_type_label"], L["content_options"])

        level = level_map[level_display]
        content_type = content_map[content_type_display]

        if st.button(L["button_generate"]):
            if topic:
                target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]

                if content_type == 'quiz':
                    full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate exactly 10 multiple-choice questions about the topic in {target_lang}.
Your entire response MUST be a valid JSON object wrapped in ```json tags.
The JSON must have a single key named 'quiz_questions', which is an array of objects.
Each question object must contain: 'question' (string), 'options' (array of objects with 'option' (A,B,C,D) and 'text' (string)), 'correct_answer' (A,B,C, or D), and 'explanation' (string).

Topic: {topic}"""
                else:
                    display_type_text = L["content_options"][L["content_options"].index(content_type_display)]
                    full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate clear and educational content in the requested {display_type_text} format based on the topic.
The response MUST be strictly in {target_lang}.

Topic: {topic}
Requested Format: {display_type_text}"""

                with st.spinner(f"Generating {content_type_display} for {topic}..."):

                    quiz_data_raw = None
                    try:
                        response = st.session_state.llm.invoke(full_prompt)
                        quiz_data_raw = response.content
                        st.session_state.quiz_data_raw = quiz_data_raw

                        if content_type == 'quiz':
                            quiz_data = clean_and_load_json(quiz_data_raw)

                            if quiz_data and 'quiz_questions' in quiz_data:
                                st.session_state.quiz_data = quiz_data
                                st.session_state.current_question = 0
                                st.session_state.quiz_submitted = False
                                st.session_state.quiz_results = [None] * len(quiz_data.get('quiz_questions', []))

                                st.success(f"**{topic}** - **{content_type_display}** Result:")
                            else:
                                st.error(L["quiz_error_llm"])
                                st.markdown(f"**{L['quiz_original_response']}**:")
                                st.code(quiz_data_raw, language="json")

                        else:
                            st.success(f"**{topic}** - **{content_type_display}** Result:")
                            st.markdown(response.content)

                    except Exception as e:
                        st.error(f"Content Generation Error: {e}")
                        if quiz_data_raw:
                            st.markdown(f"**{L['quiz_original_response']}**: {quiz_data_raw}")

            else:
                st.warning(L["warning_topic"])
    else:
        st.error(L["llm_error_init"])

    is_quiz_ready = content_type == 'quiz' and 'quiz_data' in st.session_state and st.session_state.quiz_data
    if is_quiz_ready and st.session_state.get('current_question', 0) < len(
            st.session_state.quiz_data.get('quiz_questions', [])):
        render_interactive_quiz(st.session_state.quiz_data, st.session_state.language)

elif feature_selection == L["lstm_tab"]:
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])

    try:
        model, data = load_or_train_lstm()

        look_back = 5
        last_sequence = data[-look_back:].reshape(1, look_back, 1)
        predicted_raw = model.predict(last_sequence, verbose=0)[0][0]

        predicted_score = np.clip(predicted_raw, 50, 95)

        st.markdown("---")
        st.subheader("í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼")

        col_score, col_chart = st.columns([1, 2])

        with col_score:
            suffix = "ì " if st.session_state.language == "ko" else ""
            st.metric("í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„", f"{predicted_score:.1f}{suffix}")
            st.info(f"ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!")

        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label="ê³¼ê±° ì ìˆ˜", marker="o")
            ax.plot(len(data), predicted_score, label="ì˜ˆì¸¡ ë‹¤ìŒ ì ìˆ˜", marker="*", color="red", markersize=10)

            ax.set_title("LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ë³€í™” ì¶”ì´")
            ax.set_xlabel("ì‹œì  (íšŸìˆ˜)")
            ax.set_ylabel("ì ìˆ˜ (50-95)")
            st.pyplot(fig)

    except NameError:
        st.error("TensorFlow/Matplotlib ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.info(f"LSTM ê¸°ëŠ¥ ì—ëŸ¬: {e}")