"""
RAG (Retrieval-Augmented Generation) ê´€ë ¨ í•¨ìˆ˜ ëª¨ë“ˆ
ë¬¸ì„œ ë¡œë“œ, ì„ë² ë”©, ì¸ë±ìŠ¤ êµ¬ì¶•, RAG ì§ˆì˜ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""
import os
import tempfile
import streamlit as st
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings

try:
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
    IS_GEMINI_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_GEMINI_EMBEDDING_AVAILABLE = False

try:
    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings
    IS_NVIDIA_EMBEDDING_AVAILABLE = True
except ImportError:
    IS_NVIDIA_EMBEDDING_AVAILABLE = False

from utils.config import RAG_INDEX_DIR
from utils.llm_clients import get_api_key, get_llm_client, run_llm
from utils.i18n import LANG, DEFAULT_LANG


def load_documents(files) -> List[Document]:
    """íŒŒì¼ë“¤ì„ Document ê°ì²´ë¡œ ë¡œë“œ"""
    docs: List[Document] = []
    for f in files:
        name = f.name
        lower = name.lower()
        if lower.endswith(".pdf"):
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
            tmp.write(f.read())
            tmp.flush()
            tmp.close()
            loader = PyPDFLoader(tmp.name)
            file_docs = loader.load()
            for d in file_docs:
                d.metadata["source"] = name
            docs.extend(file_docs)
            try:
                os.remove(tmp.name)
            except OSError:
                pass
        elif lower.endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
        elif lower.endswith(".html") or lower.endswith(".htm"):
            text = f.read().decode("utf-8", errors="ignore")
            docs.append(Document(page_content=text, metadata={"source": name}))
    return docs


def split_documents(docs: List[Document]) -> List[Document]:
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", ".", " ", ""],
    )
    return splitter.split_documents(docs)


def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ë ˆê±°ì‹œ í˜¸í™˜ì„±)"""
    if get_api_key("openai"):
        try:
            return OpenAIEmbeddings(model="text-embedding-3-small")
        except:
            pass
    if get_api_key("gemini") and IS_GEMINI_EMBEDDING_AVAILABLE:
        try:
            return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        except:
            pass
    return None


def get_embedding_function():
    """
    RAG ì„ë² ë”©ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ê²°ì •í•©ë‹ˆë‹¤.
    API í‚¤ ìœ íš¨ì„± ìˆœì„œ: OpenAI (ì‚¬ìš©ì ì„¤ì • ì‹œ) -> Gemini -> NVIDIA -> HuggingFace (fallback)
    """
    # 1. OpenAI ì„ë² ë”© ì‹œë„
    openai_key = get_api_key("openai")
    if openai_key:
        try:
            st.info("ğŸ”¹ RAG: OpenAI Embedding ì‚¬ìš© ì¤‘")
            return OpenAIEmbeddings(openai_api_key=openai_key)
        except Exception as e:
            st.warning(f"OpenAI ì„ë² ë”© ì‹¤íŒ¨ â†’ Geminië¡œ Fallback: {e}")

    # 2. Gemini ì„ë² ë”© ì‹œë„
    gemini_key = get_api_key("gemini")
    if IS_GEMINI_EMBEDDING_AVAILABLE and gemini_key:
        try:
            st.info("ğŸ”¹ RAG: Gemini Embedding ì‚¬ìš© ì¤‘")
            return GoogleGenerativeAIEmbeddings(google_api_key=gemini_key, model="models/text-embedding-004")
        except Exception as e:
            st.warning(f"Gemini ì„ë² ë”© ì‹¤íŒ¨ â†’ NVIDIAë¡œ Fallback: {e}")

    # 3. NVIDIA ì„ë² ë”© ì‹œë„
    nvidia_key = get_api_key("nvidia")
    if IS_NVIDIA_EMBEDDING_AVAILABLE and nvidia_key:
        try:
            st.info("ğŸ”¹ RAG: NVIDIA Embedding ì‚¬ìš© ì¤‘")
            return NVIDIAEmbeddings(api_key=nvidia_key, model="ai-embed-qa-4")
        except Exception as e:
            st.warning(f"NVIDIA ì„ë² ë”© ì‹¤íŒ¨ â†’ HuggingFace Fallback: {e}")

    # 4. HuggingFace Embeddings (Local Fallback)
    try:
        st.info("ğŸ”¹ RAG: Local HuggingFace Embedding ì‚¬ìš© ì¤‘")
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    except Exception as e:
        st.warning(f"ìµœì¢… Fallback ì„ë² ë”© ì‹¤íŒ¨: {e}")

    st.error("âŒ RAG ì„ë² ë”© ì‹¤íŒ¨: ì‚¬ìš© ê°€ëŠ¥í•œ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    return None


def build_rag_index(files):
    """RAG ì¸ë±ìŠ¤ êµ¬ì¶•"""
    L = LANG[st.session_state.language]
    if not files:
        return None, 0

    # ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
    try:
        embeddings = get_embedding_function()
    except Exception as e:
        st.error(f"RAG ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, 0

    if embeddings is None:
        error_msg = L["rag_embed_error_none"]
        if not get_api_key("openai"):
            error_msg += f"\n- {L['rag_embed_error_openai']}"
        if not get_api_key("gemini"):
            error_msg += f"\n- {L['rag_embed_error_gemini']}"
        if not get_api_key("nvidia"):
            error_msg += f"\n- {L['rag_embed_error_nvidia']}"
        st.error(error_msg)
        return None, 0

    # ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
    docs = load_documents(files)
    if not docs:
        return None, 0

    chunks = split_documents(docs)
    if not chunks:
        return None, 0

    try:
        vectorstore = FAISS.from_documents(chunks, embeddings)
        # ì €ì¥
        vectorstore.save_local(RAG_INDEX_DIR)
    except Exception as e:
        st.error(f"RAG ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, 0

    return vectorstore, len(chunks)


def load_rag_index():
    """ì €ì¥ëœ RAG ì¸ë±ìŠ¤ ë¡œë“œ"""
    try:
        embeddings = get_embedding_function()
    except Exception:
        return None

    if embeddings is None:
        return None

    try:
        vs = FAISS.load_local(RAG_INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        return vs
    except Exception:
        return None


def rag_answer(question: str, vectorstore: FAISS, lang_key: str) -> str:
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€"""
    llm_client, info = get_llm_client()
    if llm_client is None:
        return LANG[lang_key]["simulation_no_key_warning"]

    # Langchain ChatOpenAI ëŒ€ì‹  run_llmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ promptë¥¼ ì§ì ‘ êµ¬ì„±
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    # â­ ìˆ˜ì •: LangChain ë²„ì „ í˜¸í™˜ì„± - get_relevant_documents ëŒ€ì‹  invoke ì‚¬ìš©
    try:
        # ìµœì‹  LangChain ë²„ì „ (invoke ì‚¬ìš©)
        docs = retriever.invoke(question)
    except AttributeError:
        # êµ¬ë²„ì „ LangChain (get_relevant_documents ì‚¬ìš©)
        try:
            docs = retriever.get_relevant_documents(question)
        except AttributeError:
            # ëŒ€ì²´ ë°©ë²•: vectorstoreì—ì„œ ì§ì ‘ ê²€ìƒ‰
            docs = vectorstore.similarity_search(question, k=4)
    context = "\n\n".join(d.page_content[:1500] for d in docs)

    # RAG ë‹¤êµ­ì–´ ì¸ì‹ ì˜¤ë¥˜ í•´ê²°: ë‹µë³€ ìƒì„± ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ ì–¸ì–´ë¡œ ì¼ê´€ë˜ê²Œ ë‹µí•˜ë„ë¡ ê°•ë ¥íˆ ì§€ì‹œ
    lang_name = {"ko": "Korean", "en": "English", "ja": "Japanese"}.get(lang_key, "English")

    prompt = (
        f"You are a helpful AI tutor. Answer the question using ONLY the provided context.\n"
        f"The answer MUST be STRICTLY in {lang_name}, which is the language of the question.\n"
        f"If you cannot find the answer in the context, say you don't know in {lang_name}.\n"
        f"Note: The context may be in a different language, but you must still answer in {lang_name}.\n\n"
        "Question:\n" + question + "\n\n"
        "Context:\n" + context + "\n\n"
        f"Answer (in {lang_name}):"
    )
    return run_llm(prompt)
