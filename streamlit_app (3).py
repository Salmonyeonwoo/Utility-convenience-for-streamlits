# ========================================
# Streamlit AI í•™ìŠµ ì½”ì¹˜ (ìµœì¢… Firebase ì˜êµ¬ ì €ì¥ì†Œ í†µí•© ë° ì‹œë®¬ë ˆì´í„° í™•ì¥)
# ========================================
import streamlit as st
import os
import tempfile
import time
import json
import re
import base64
import io
import numpy as np
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from datetime import datetime, timedelta # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
from openai import OpenAI # â­ OpenAI SDK ì„í¬íŠ¸ (ì¶”ê°€)

# â­ Admin SDK ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from firebase_admin import credentials, firestore, initialize_app, get_app
# Admin SDKì˜ firestoreì™€ Google Cloud SDKì˜ firestoreë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ alias ì‚¬ìš©
from google.cloud import firestore as gcp_firestore
from google.cloud.firestore import Query # Firestore ì¿¼ë¦¬ìš© import ì¶”ê°€

# ConversationChain ì‚¬ìš©ì„ ìœ„í•´ import ì¶”ê°€
from langchain.chains import ConversationalRetrievalChain, ConversationChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.schema.document import Document
from langchain.prompts import PromptTemplate # â­ PromptTemplate ì„í¬íŠ¸


# ================================
# 1. Firebase Admin SDK ì´ˆê¸°í™” ë° Secrets ì²˜ë¦¬ í•¨ìˆ˜
# ================================

def _get_admin_credentials():
    """Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # Secrets í‚¤ë¥¼ 'FIREBASE_SERVICE_ACCOUNT_JSON'ìœ¼ë¡œ í‘œì¤€í™”
    if "FIREBASE_SERVICE_ACCOUNT_JSON" not in st.secrets:
        return None, "FIREBASE_SERVICE_ACCOUNT_JSON Secretì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    service_account_data = st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"]
    sa_info = None

    if isinstance(service_account_data, str):
        try:
            sa_info = json.loads(service_account_data.strip())
        except json.JSONDecodeError as e:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ JSON êµ¬ë¬¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ê°’ì„ í™•ì¸í•˜ì„¸ìš”. ìƒì„¸ ì˜¤ë¥˜: {e}"
    elif hasattr(service_account_data, 'get'):
        try:
            sa_info = dict(service_account_data) # AttrDictë¥¼ í‘œì¤€ dictë¡œ ë³€í™˜
        except Exception:
             return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì‹¤íŒ¨. íƒ€ì…: {type(service_account_data)}"
    else:
        return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (Type: {type(service_account_data)})"
    
    if not sa_info.get("project_id") or not sa_info.get("private_key"):
        return None, "JSON ë‚´ 'project_id' ë˜ëŠ” 'private_key' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."

    return sa_info, None

@st.cache_resource(ttl=None)
def initialize_firestore_admin():
    """Secretsì—ì„œ ë¡œë“œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    sa_info, error_message = _get_admin_credentials()

    if error_message:
        st.error(f"âŒ Firebase Secret ì˜¤ë¥˜: {error_message}")
        return None

    try:
        get_app()
    except ValueError:
        pass 
    else:
        try:
            return firestore.client()
        except Exception as e:
            st.error(f"ğŸ”¥ Firebase í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    try:
        cred = credentials.Certificate(sa_info) 
        initialize_app(cred)
        
        db_client = firestore.client()
        st.session_state["db"] = db_client
        st.success("âœ… Firebase Admin SDK ì´ˆê¸°í™” ì™„ë£Œ! (Secrets ê¸°ë°˜)")
        return db_client
    except Exception as e:
        st.error(f"ğŸ”¥ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¬¸ì œ. ì˜¤ë¥˜: {e}")
        return None


def save_index_to_firestore(db, vector_store, index_id="user_portfolio_rag"):
    """FAISS ì¸ë±ìŠ¤ë¥¼ Firestoreì— Base64 í˜•íƒœë¡œ ì§ë ¬í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    if not db: return False
    temp_dir = tempfile.mkdtemp()
    
    try:
        vector_store.save_local(folder_path=temp_dir, index_name="index")
        
        with open(f"{temp_dir}/index.faiss", "rb") as f: faiss_bytes = f.read()
        with open(f"{temp_dir}/index.pkl", "rb") as f: metadata_bytes = f.read()
        
        encoded_data = {
            "faiss_data": base64.b64encode(faiss_bytes).decode('utf-8'),
            "metadata_data": base64.b64encode(metadata_bytes).decode('utf-8'),
            "timestamp": gcp_firestore.SERVER_TIMESTAMP 
        }
        
        db.collection("rag_indices").document(index_id).set(encoded_data)
        return True
    
    except Exception as e:
        st.error(f"DB ì €ì¥ ì‹œë„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"Error saving index to Firestore: {e}")
        return False

def load_index_from_firestore(db, embeddings, index_id="user_portfolio_rag"):
    """Firestoreì—ì„œ Base64 ë¬¸ìì—´ì„ ë¡œë“œí•˜ì—¬ FAISS ì¸ë±ìŠ¤ë¡œ ì—­ì§ë ¬í™”í•©ë‹ˆë‹¤."""
    if not db: return False

    try:
        doc = db.collection("rag_indices").document(index_id).get()
        if not doc.exists:
            return None 

        encoded_data = doc.to_dict()
        
        faiss_bytes = base64.b64decode(encoded_data["faiss_data"])
        metadata_bytes = base64.b64decode(encoded_data["metadata_data"])
        
        temp_dir = tempfile.mkdtemp()
        with open(f"{temp_dir}/index.faiss", "wb") as f: f.write(faiss_bytes)
        with open(f"{temp_dir}/index.pkl", "wb") as f: f.write(metadata_bytes)
        
        vector_store = FAISS.load_local(folder_path=temp_dir, embeddings=embeddings, index_name="index")
        return vector_store
        
    except Exception as e:
        print(f"Error loading index from Firestore: {e}")
        return None

# â­ ìƒë‹´ ì´ë ¥ ì €ì¥ í•¨ìˆ˜ ìˆ˜ì • (ì–¸ì–´ í‚¤ ì¶”ê°€)
def save_simulation_history(db, initial_query, customer_type, messages):
    """Firestoreì— ìƒë‹´ ì´ë ¥ì„ ì €ì¥í•©ë‹ˆë‹¤."""
    if not db: 
        st.sidebar.warning("âŒ DB ì—°ê²° ì‹¤íŒ¨: ìƒë‹´ ì´ë ¥ ì €ì¥ ë¶ˆê°€")
        return False
    
    # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    history_data = [{k: v for k, v in msg.items()} for msg in messages]

    data = {
        "initial_query": initial_query,
        "customer_type": customer_type,
        "messages": history_data,
        "language_key": st.session_state.language, # â­ ì–¸ì–´ í‚¤ ì¶”ê°€
        "timestamp": firestore.SERVER_TIMESTAMP
    }
    
    try:
        db.collection("simulation_histories").add(data)
        st.sidebar.success("âœ… ìƒë‹´ ì´ë ¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        st.sidebar.error(f"âŒ ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# â­ ìƒë‹´ ì´ë ¥ ë¡œë“œ í•¨ìˆ˜ ìˆ˜ì • (ì–¸ì–´ í•„í„°ë§ ì¶”ê°€)
def load_simulation_histories(db):
    """Firestoreì—ì„œ í˜„ì¬ ì–¸ì–´ì— í•´ë‹¹í•˜ëŠ” ìµœê·¼ ìƒë‹´ ì´ë ¥ì„ ë¡œë“œí•©ë‹ˆë‹¤ (ìµœëŒ€ 10ê°œ)."""
    current_lang_key = st.session_state.language # â­ í˜„ì¬ ì–¸ì–´ í‚¤ë¥¼ ì„¸ì…˜ ìƒíƒœì—ì„œ ê°€ì ¸ì˜´
    if not db: return []
    
    try:
        # í˜„ì¬ ì„ íƒëœ ì–¸ì–´ í‚¤ë¡œ í•„í„°ë§
        histories = (
            db.collection("simulation_histories")
            .where("language_key", "==", current_lang_key) # â­ ì–¸ì–´ í•„í„°ë§ ì ìš©
            .order_by("timestamp", direction=Query.DESCENDING)
            .limit(10)
            .stream()
        )
        
        results = []
        for doc in histories:
            data = doc.to_dict()
            data['id'] = doc.id
            
            # ë©”ì‹œì§€ ë°ì´í„°ê°€ ì§ë ¬í™”ëœ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if 'messages' in data and isinstance(data['messages'], list) and data['messages']:
                results.append(data)

        return results
    except Exception as e:
        # st.error(f"âŒ ì´ë ¥ ë¡œë“œ ì‹¤íŒ¨: {e}") # ì‚¬ìš©ìì—ê²Œ ë„ˆë¬´ ë§ì€ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•˜ì§€ ì•Šë„ë¡ ì£¼ì„ ì²˜ë¦¬
        print(f"Error loading histories: {e}")
        return []

# â­ ì´ë ¥ ì‚­ì œ í•¨ìˆ˜ (Firestore ì—°ë™)
def delete_all_history(db):
    """Firestoreì˜ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    L = LANG[st.session_state.language] # í•¨ìˆ˜ ë‚´ì—ì„œ Lì„ ë‹¤ì‹œ ì •ì˜
    
    if not db:
        st.error(L["firestore_no_index"])
        return
    
    try:
        # ì´í„°ë ˆì´ì…˜ì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©
        docs = db.collection("simulation_histories").stream()
        for doc in docs:
            doc.reference.delete()
        
        # ì„¸ì…˜ ìƒíƒœë„ ì´ˆê¸°í™”
        st.session_state.simulator_messages = []
        st.session_state.simulator_memory.clear()
        st.session_state.show_delete_confirm = False
        st.success(L["delete_success"]) # â­ ë‹¤êµ­ì–´ ì ìš©
        st.rerun()
        
    except Exception as e:
        st.error(f"ì´ë ¥ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ================================
# 2. JSON/RAG/LSTM/TTS ë° WHISPER í•¨ìˆ˜ ì •ì˜
# ================================
def clean_and_load_json(text):
    """LLM ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë§Œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ë¡œë“œ"""
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None

# â­ Whisper API ì—°ë™ í•¨ìˆ˜ (OpenAI Client ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¸ìˆ˜ë¡œ ë°›ìŒ)
def transcribe_audio_with_whisper(audio_file, client, lang_key):
    """Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•©ë‹ˆë‹¤."""
    L = LANG[lang_key] # í˜„ì¬ ì–¸ì–´ í‚¤ ë¡œë“œ
    
    if client is None:
        # OpenAI Keyê°€ ì—†ëŠ” ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
        return L.get("whisper_client_error", "âŒ ì˜¤ë¥˜: Whisper API Clientê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    
    # UploadedFile ê°ì²´ì˜ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ê¸°ë¡
    temp_dir = tempfile.mkdtemp()
    temp_audio_path = "" # ì´ˆê¸°í™”
    
    try:
        # st.audio_inputì€ íŒŒì¼ ì´ë¦„ì´ ì—†ì–´ name ì†ì„±ì´ Noneì¼ ìˆ˜ ìˆìŒ (Streamlit ë²„ì „ 1.38.0 ê°€ì •)
        # BytesIO ê°ì²´ì—ì„œ MIME íƒ€ì… ê°€ì ¸ì˜¤ê¸° ì‹œë„
        mime_type = audio_file.type if hasattr(audio_file, 'type') and audio_file.type else 'audio/wav'
        # íŒŒì¼ í™•ì¥ì ì¶”ì •
        file_extension = mime_type.split('/')[-1].lower() if '/' in mime_type else 'wav' 
        
        # Whisperê°€ ì§€ì›í•˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸ (st.audio_inputì€ ë³´í†µ WAV/MP3/M4A ë“±ì„ ë°˜í™˜)
        supported_extensions = ["mp3", "mp4", "mpeg", "mpga", "m4a", "wav", "webm"]
        if file_extension not in supported_extensions and mime_type not in ['audio/wav', 'audio/mpeg']:
             return L.get("whisper_format_error", f"âŒ ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ ({mime_type} ë˜ëŠ” .{file_extension})ì…ë‹ˆë‹¤.")

        temp_audio_path = os.path.join(temp_dir, f"temp_audio_{time.time()}.{file_extension}")
        
        # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê³  ë‚´ìš©ì„ ê¸°ë¡
        audio_file.seek(0)
        with open(temp_audio_path, "wb") as f:
            f.write(audio_file.read())
        
        # 3. Whisper API í˜¸ì¶œ
        with open(temp_audio_path, "rb") as audio_data:
            # Whisper API í˜¸ì¶œ
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_data
                # WhisperëŠ” ì–¸ì–´ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ë¯€ë¡œ language íŒŒë¼ë¯¸í„°ëŠ” ì œê±°í–ˆìŠµë‹ˆë‹¤.
            )
        
        # 4. API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        return transcript.text
    
    except Exception as e:
        # OpenAI ê´€ë ¨ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
        error_msg = str(e)
        if "Authentication" in error_msg or "api_key" in error_msg:
             return L.get("whisper_auth_error", "âŒ Whisper API ì¸ì¦ ì‹¤íŒ¨: API Keyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return f"âŒ Whisper API í˜¸ì¶œ ì‹¤íŒ¨: {error_msg}"
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (try-except-finally êµ¬ë¬¸ ë³´ì¥)
        if os.path.exists(temp_audio_path):
             os.remove(temp_audio_path)
        try:
             os.rmdir(temp_dir)
        except OSError:
             # ì„ì‹œ í´ë” ì‚­ì œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
             pass


def synthesize_and_play_audio(current_lang_key):
    """TTS API ëŒ€ì‹  Web Speech APIë¥¼ ìœ„í•œ JS ìœ í‹¸ë¦¬í‹°ë¥¼ Streamlitì— ì‚½ì…í•©ë‹ˆë‹¤."""
    
    # í…œí”Œë¦¿ ë¦¬í„°ëŸ´ ë‚´ë¶€ì—ì„œ L ë”•ì…”ë„ˆë¦¬ë¥¼ ì§ì ‘ ì°¸ì¡°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš©
    ko_ready = "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨"
    en_ready = "Ready to listen"
    ja_ready = "éŸ³å£°å†ç”Ÿã®æº–å‚™ãŒã§ãã¾ã—ãŸ"

    tts_js_code = f"""
    <script>
    if (!window.speechSynthesis) {{
        document.getElementById('tts_status').innerText = 'âŒ TTS Not Supported';
    }}

    window.speakText = function(text, langKey) {{
        if (!window.speechSynthesis || !text) return;

        const statusElement = document.getElementById('tts_status');
        const utterance = new SpeechSynthesisUtterance(text);
        
        // ë™ì ìœ¼ë¡œ ì–¸ì–´ ì½”ë“œ ì„¤ì •
        const langCode = {{ "ko": "ko-KR", "en": "en-US", "ja": "ja-JP" }}[langKey] || "en-US";
        utterance.lang = langCode; 

        // ë™ì ìœ¼ë¡œ ì¤€ë¹„ ìƒíƒœ ë©”ì‹œì§€ ì„¤ì • (L ë”•ì…”ë„ˆë¦¬ ê°’ì„ ì§ì ‘ ì‚¬ìš©)
        const getReadyText = (key) => {{
            if (key === 'ko') return '{ko_ready}';
            if (key === 'en') return '{en_ready}';
            if (key === 'ja') return '{ja_ready}';
            return '{en_ready}';
        }};

        let voicesLoaded = false;
        const setVoiceAndSpeak = () => {{
            const voices = window.speechSynthesis.getVoices();
            if (voices.length > 0) {{
                // í˜„ì¬ ì–¸ì–´ ì½”ë“œì™€ ì¼ì¹˜í•˜ëŠ” ìŒì„±ì„ ì°¾ê±°ë‚˜, ì²« ë²ˆì§¸ ìŒì„±ì„ ì‚¬ìš©
                utterance.voice = voices.find(v => v.lang.startsWith(langCode.substring(0, 2))) || voices[0];
                voicesLoaded = true;
                window.speechSynthesis.speak(utterance);
            }} else if (!voicesLoaded) {{
                // ìŒì„±ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°, ì ì‹œ í›„ ì¬ì‹œë„ (ë¹„ë™ê¸° ë¡œë“œ ë¬¸ì œ í•´ê²°)
                setTimeout(setVoiceAndSpeak, 100);
            }}
        }};
        
        // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        utterance.onstart = () => {{
            statusElement.innerText = '{LANG[current_lang_key].get("tts_status_generating", "ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªç”Ÿæˆä¸­...")}';
            statusElement.style.backgroundColor = '#fff3e0';
        }};
        
        utterance.onend = () => {{
            statusElement.innerText = '{LANG[current_lang_key].get("tts_status_success", "âœ… ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå†ç”Ÿå®Œäº†!")}';
            statusElement.style.backgroundColor = '#e8f5e9';
             setTimeout(() => {{ 
                 statusElement.innerText = getReadyText(langKey);
                 statusElement.style.backgroundColor = '#f0f0f0';
             }}, 3000);
        }};
        
        utterance.onerror = (event) => {{
            statusElement.innerText = '{LANG[current_lang_key].get("tts_status_error", "âŒ TTSã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ")}';
            statusElement.style.backgroundColor = '#ffebee';
            console.error("SpeechSynthesis Error:", event);
             setTimeout(() => {{ 
                 statusElement.innerText = getReadyText(langKey);
                 statusElement.style.backgroundColor = '#f0f0f0';
             }}, 3999);
        }};

        window.speechSynthesis.cancel(); // Stop any current speech
        setVoiceAndSpeak(); // å†ç”Ÿé–‹å§‹

    }};
    </script>
    """
    # JS ìœ í‹¸ë¦¬í‹°ë¥¼ Streamlit ì•±ì— ì»´í¬ë„ŒíŠ¸ë¡œ ì‚½ì… (ë†’ì´ ì¡°ì •í•˜ì—¬ ìƒíƒœì°½ë§Œ ë³´ì´ë„ë¡)
    st.components.v1.html(tts_js_code, height=5, width=0)

def render_tts_button(text_to_speak, current_lang_key):
    """TTS ë²„íŠ¼ UIë¥¼ ë Œë”ë§í•˜ê³  í´ë¦­ ì‹œ JS í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
    
    # ì¤„ ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ë”°ì˜´í‘œë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
    safe_text = text_to_speak.replace('\n', ' ').replace('"', '\\"').replace("'", "\\'")
    
    # â­ JS í•¨ìˆ˜ì— ì–¸ì–´ í‚¤ë„ í•¨ê»˜ ì „ë‹¬
    js_call = f"window.speakText('{safe_text}', '{current_lang_key}')"

    st.markdown(f"""
        <button onclick="{js_call}"
                style="background-color: #4338CA; color: white; padding: 10px 20px; border-radius: 5px; cursor: pointer; border: none; width: 100%; font-weight: bold; margin-bottom: 10px;">
            {LANG[current_lang_key].get("button_listen_audio", "éŸ³å£°ã§èã")} ğŸ§
        </button>
    """, unsafe_allow_html=True)


def get_mock_response_data(lang_key, customer_type):
    """API Keyê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê°€ìƒ ì‘ëŒ€ ë°ì´í„° (ë‹¤êµ­ì–´ ì§€ì›)"""
    
    L = LANG[lang_key]
    
    if lang_key == 'ko':
        # â­ ìˆ˜ì •ëœ ì¤‘ë¦½ì ì¸ ëª©ì—… ë°ì´í„° í…œí”Œë¦¿
        initial_check = "ê³ ê°ë‹˜ì˜ ì„±í•¨, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼ ë“± ì •í™•í•œ ì—°ë½ì²˜ ì •ë³´ë¥¼ í™•ì¸í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤."
        tone = "ê³µê° ë° í•´ê²° ì¤‘ì‹¬"
        advice = "ì´ ê³ ê°ì€ {customer_type} ì„±í–¥ì´ì§€ë§Œ, ë¬¸ì œ í•´ê²°ì„ ê°„ì ˆíˆ ì›í•©ë‹ˆë‹¤. ê³µê°ê³¼ í•¨ê»˜, ë¬¸ì œ í•´ê²°ì— í•„ìˆ˜ì ì¸ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ìš”ì²­í•´ì•¼ í•©ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ì‚¬ì¡±ì„ í”¼í•˜ê³  ì‹ ë¢°ë¥¼ ì£¼ë„ë¡ í•˜ì„¸ìš”."
        draft = f"""
{initial_check}

> ê³ ê°ë‹˜, ë¶ˆí¸ì„ ê²ªê²Œ í•´ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ê³ ê°ë‹˜ì˜ ìƒí™©ì„ ì¶©ë¶„íˆ ì´í•´í•˜ê³  ìˆìŠµë‹ˆë‹¤.
> ë¬¸ì œ í•´ê²°ì„ ìœ„í•´, ì•„ë˜ ì„¸ ê°€ì§€ í•„ìˆ˜ ì •ë³´ë¥¼ í™•ì¸í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ì´ ì •ë³´ê°€ ìˆì–´ì•¼ ê³ ê°ë‹˜ ìƒí™©ì— ë§ëŠ” ì •í™•í•œ í•´ê²°ì±…ì„ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> 1. ë¬¸ì œ ë°œìƒê³¼ ê´€ë ¨ëœ ìƒí’ˆ/ì„œë¹„ìŠ¤ì˜ **ì •í™•í•œ ëª…ì¹­ ë° ì˜ˆì•½ ë²ˆí˜¸** (ì˜ˆ: íŒŒë¦¬ eSIM, ì˜ˆì•½ë²ˆí˜¸ 1234567)
> 2. í˜„ì¬ **ë¬¸ì œ ìƒí™©**ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì„¤ëª… (ì˜ˆ: íœ´ëŒ€í°ì´ ì•ˆ ë¨, í™˜ë¶ˆ ìš”ì²­, ì •ë³´ ë¬¸ì˜)
> 3. ì´ë¯¸ **ì‹œë„í•˜ì‹  í•´ê²° ë‹¨ê³„** (ì˜ˆ: ê¸°ê¸° ì¬ë¶€íŒ…, ì„¤ì • í™•ì¸ ë“±)

> ê³ ê°ë‹˜ê³¼ì˜ ì›í™œí•œ ì†Œí†µì„ í†µí•´ ì‹ ì†í•˜ê²Œ ë¬¸ì œ í•´ê²°ì„ ë•ê² ìŠµë‹ˆë‹¤. ë‹µë³€ ê¸°ë‹¤ë¦¬ê² ìŠµë‹ˆë‹¤.
"""
    elif lang_key == 'en':
        initial_check = "Could you please confirm your accurate contact details, such as your full name, phone number, and email address?"
        tone = "Empathy and Solution-Focused"
        advice = "This customer is {customer_type} but desperately wants a solution. Show empathy, but clearly request the essential information needed for troubleshooting. Be direct and build trust."
        draft = f"""
{initial_check}

> Dear Customer, I sincerely apologize for the inconvenience you are facing. I completely understand your frustration.
> To proceed with troubleshooting, please confirm the three essential pieces of information below. This data is critical for providing you with the correct, tailored solution:
> 1. The **exact name and booking number** of the product/service concerned (e.g., Paris eSIM, Booking #1234567).
> 2. A specific description of the **current issue** (e.g., phone not connecting, refund request, information inquiry).
> 3. Any **troubleshooting steps already attempted** (e.g., device rebooted, settings checked, etc.).

> We aim to resolve your issue as quickly as possible with your cooperation. We await your response.
"""
    elif lang_key == 'ja':
        initial_check = "ãŠå®¢æ§˜ã®æ°åã€ãŠé›»è©±ç•ªå·ã€Eãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãªã©ã€æ­£ç¢ºãªé€£çµ¡å…ˆæƒ…å ±ã‚’ç¢ºèªã•ã›ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚"
        tone = "å…±æ„Ÿã¨è§£æ±ºä¸­å¿ƒ"
        advice = "ã“ã®ãŠå®¢æ§˜ã¯{customer_type}å‚¾å‘ã§ã™ãŒã€å•é¡Œã®è§£æ±ºã‚’å¼·ãæœ›ã‚“ã§ã„ã¾ã™ã€‚å…±æ„Ÿã‚’ç¤ºã—ã¤ã¤ã‚‚ã€å•é¡Œè§£æ±ºã«ä¸å¯æ¬ ãªæƒ…å ±ã‚’æ˜ç¢ºã«å°‹ã­ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å†—é•·ãªèª¬æ˜ã‚’é¿ã‘ã€ä¿¡é ¼æ„Ÿã‚’ä¸ãˆã‚‹å¯¾å¿œã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
        draft = f"""
{initial_check}

> ãŠå®¢æ§˜ã€ã”ä¸ä¾¿ã‚’ãŠã‹ã‘ã—ã€èª ã«ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã®çŠ¶æ³ã€ååˆ†æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚
> å•é¡Œã‚’è¿…é€Ÿã«è§£æ±ºã™ã‚‹ãŸã‚ã€æã‚Œå…¥ã‚Šã¾ã™ãŒã€ä»¥ä¸‹ã®3ç‚¹ã®å¿…é ˆæƒ…å ±ã«ã¤ã„ã¦ã”ç¢ºèªã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚ã“ã®æƒ…å ±ãŒãªã„ã¨ã€ãŠå®¢æ§˜ã®çŠ¶æ³ã«åˆã‚ã›ãŸçš„ç¢ºãªè§£æ±ºç­–ã‚’ã”æ¡ˆå†…ã§ãã¾ã›ã‚“ã€‚
> 1. å•é¡Œã®å¯¾è±¡ã¨ãªã‚‹**å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ã®æ­£ç¢ºãªåç§°ã¨äºˆç´„ç•ªå·** (ä¾‹: ãƒ‘ãƒªeSIMã€äºˆç´„ç•ªå·1234567)
> 2. ç¾åœ¨ã®**å…·ä½“çš„ãªå•é¡ŒçŠ¶æ³** (ä¾‹: æºå¸¯é›»è©±ãŒä½¿ãˆãªã„ã€è¿”é‡‘ã‚’å¸Œæœ›ã™ã‚‹ã€æƒ…å ±ãŒçŸ¥ã‚ŠãŸã„)
> 3. æ—¢ã«**ãŠè©¦ã—ã„ãŸã ã„ãŸè§£æ±ºæ‰‹é †** (ä¾‹: ç«¯æœ«ã®å†èµ·å‹•ã€è¨­å®šç¢ºèªãªã©)

> ãŠå®¢æ§˜ã¨ã®å††æ»‘ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€è¿…é€Ÿã«å•é¡Œè§£æ±ºã‚’ã‚µãƒãƒ¼ãƒˆã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ã”è¿”ä¿¡ã‚’ãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™ã€‚
"""
    
    # advice ë¬¸ìì—´ ë‚´ë¶€ì˜ {customer_type}ì„ ì‹¤ì œ ì„ íƒ ê°’ìœ¼ë¡œ ëŒ€ì²´
    advice_text = advice.replace("{customer_type}", customer_type)

    return {
        "advice_header": f"{L['simulation_advice_header']}",
        "advice": advice_text,
        "draft_header": f"{L['simulation_draft_header']} ({tone})",
        "draft": draft
    }

def get_closing_messages(lang_key):
    """ê³ ê° ì‘ëŒ€ ì¢…ë£Œ ì‹œ ì‚¬ìš©í•˜ëŠ” ë‹¤êµ­ì–´ ë©”ì‹œì§€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    if lang_key == 'ko':
        return {
            "additional_query": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?",
            "chat_closing": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ ì±„íŒ…ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤. ê³ ê° ë¬¸ì˜ ì„¼í„°ì— ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©°, ì¶”ê°€ë¡œ ì €í¬ ì‘ëŒ€ ì†”ë£¨ì…˜ì— ëŒ€í•œ ì„¤ë¬¸ ì¡°ì‚¬ì— ì‘í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹­ì‹œì˜¤."
        }
    elif lang_key == 'en':
        return {
            "additional_query": "Is there anything else we can assist you with today?",
            "chat_closing": "As there are no further inquiries, we will now end this chat session. Thank you for contacting our Customer Support Center. We would be grateful if you could participate in a short survey about our service solution. Please feel free to contact us anytime if you have any additional questions."
        }
    elif lang_key == 'ja':
        return {
            "additional_query": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
            "chat_closing": "ãŠå®¢æ§˜ã‹ã‚‰ã®è¿½åŠ ã®ãŠå•ã„åˆã‚ã›ãŒãªã„ãŸã‚ã€æœ¬ãƒãƒ£ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆã‚’çµ‚äº†ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚å¼Šç¤¾ã®å¯¾å¿œã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹ç°¡å˜ãªã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«ã”å”åŠ›ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚è¿½åŠ ã®ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã„ã¤ã§ã‚‚ã”é€£çµ¡ãã ã•ã„ã€‚"
        }
    return get_closing_messages('ko') # ê¸°ë³¸ê°’


def get_document_chunks(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
    documents = []
    temp_dir = tempfile.mkdtemp()
    for uploaded_file in files:
        temp_filepath = os.path.join(temp_dir, uploaded_file.name)
        file_extension = uploaded_file.name.split('.')[-1].lower()
        if file_extension == "pdf":
            with open(temp_filepath, "wb") as f: f.write(uploaded_file.getvalue())
            loader = PyPDFLoader(temp_filepath)
            documents.extend(loader.load())
        elif file_extension == "html":
            raw_html = uploaded_file.getvalue().decode('utf-8')
            soup = BeautifulSoup(raw_html, 'html.parser')
            text_content = soup.get_text(separator=' ', strip=True)
            documents.append(Document(page_content=text_content, metadata={"source": uploaded_file.name}))
        elif file_extension == "txt":
            with open(temp_filepath, "wb") as f: f.write(uploaded_file.getvalue())
            loader = TextLoader(temp_filepath, encoding="utf-8")
            documents.extend(loader.load())
        else:
            print(f"File '{uploaded_file.name}' not supported.")
            continue
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    return text_splitter.split_documents(documents)

def get_vector_store(text_chunks):
    """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  Vector Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    cache_key = tuple(doc.page_content for doc in text_chunks)
    if cache_key in st.session_state.embedding_cache: return st.session_state.embedding_cache[cache_key]
    if not st.session_state.is_llm_ready: return None
    try:
        vector_store = FAISS.from_documents(text_chunks, embedding=st.session_state.embeddings)
        st.session_state.embedding_cache[cache_key] = vector_store
        return vector_store
    except Exception as e:
        if "429" in str(e): return None
        else:
            print(f"Vector Store creation failed: {e}") 
            return None

def get_rag_chain(vector_store):
    """ê²€ìƒ‰ ì²´ì¸(ConversationalRetrievalChain)ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if vector_store is None: return None
    # â­ RAG ì²´ì¸ì— memory_keyë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
    return ConversationalRetrievalChain.from_llm(
        llm=st.session_state.llm,
        retriever=vector_store.as_retriever(),
        memory=st.session_state.memory
    )

@st.cache_resource
def load_or_train_lstm():
    """ê°€ìƒì˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ì„ ìœ„í•œ LSTM ëª¨ë¸ì„ ìƒì„±í•˜ê³  í•™ìŠµí•©ë‹ˆë‹¤."""
    np.random.seed(int(time.time())) # â­ LSTM ê²°ê³¼ë¥¼ ëœë¤í™”í•˜ê¸° ìœ„í•´ ì‹œë“œì— í˜„ì¬ ì‹œê°„ì„ ì‚¬ìš©
    data = np.cumsum(np.random.normal(loc=5, scale=5, size=50)) + 60
    data = np.clip(data, 50, 95)
    def create_dataset(dataset, look_back=3):
        X, Y = [], []
        for i in range(len(dataset) - look_back):
            X.append(dataset[i:(i + look_back)])
            Y.append(dataset[i + look_back])
        return np.array(X), np.array(Y)
    look_back = 5
    X, Y = create_dataset(data, look_back)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    model = Sequential([
        LSTM(50, activation='relu', input_shape=(look_back, 1)),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    model.fit(X, Y, epochs=10, batch_size=1, verbose=0)
    return model, data

def force_rerun_lstm():
    """ìºì‹œëœ LSTM ëª¨ë¸ì„ ë¬´íš¨í™”í•˜ê³  ìƒˆë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # st.cache_resource í•¨ìˆ˜ì˜ ìºì‹œë¥¼ ì§ì ‘ ì§€ìš¸ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, 
    # Streamlitì˜ ì¬ì‹¤í–‰ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ load_or_train_lstmì´
    # time.time() ì‹œë“œë¡œ ìƒˆ ê²°ê³¼ë¥¼ ìƒì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
    st.session_state.lstm_rerun_trigger = time.time()
    st.rerun()


def clean_and_load_json(text):
    """LLM ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ê°ì²´ë§Œ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ë¡œë“œ"""
    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        json_str = match.group(0)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            return None
    return None

def render_interactive_quiz(quiz_data, current_lang):
    """ìƒì„±ëœ í€´ì¦ˆ ë°ì´í„°ë¥¼ Streamlit UIë¡œ ë Œë”ë§í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤."""
    L = LANG[current_lang]
    if not quiz_data or 'quiz_questions' not in quiz_data: return

    questions = quiz_data['quiz_questions']
    num_questions = len(questions)

    if "current_question" not in st.session_state or st.session_state.current_question >= num_questions:
        st.session_state.current_question = 0
        st.session_state.quiz_results = [None] * num_questions
        st.session_state.quiz_submitted = False
        
    q_index = st.session_state.current_question
    q_data = questions[q_index]
    
    st.subheader(f"{q_index + 1}. {q_data['question']}")
    
    options_dict = {}
    try:
        options_dict = {f"{opt['option']}": f"{opt['option']}) {opt['text']}" for opt in q_data['options']}
    except KeyError:
        st.error(L["quiz_fail_structure"])
        if 'quiz_data_raw' in st.session_state: st.code(st.session_state.quiz_data_raw, language="json")
        return

    options_list = list(options_dict.values())
    
    selected_answer = st.radio(
        L.get("select_answer", "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”"),
        options=options_list,
        key=f"q_radio_{q_index}"
    )

    col1, col2 = st.columns(2)

    if col1.button(L.get("check_answer", "ì •ë‹µ í™•ì¸"), key=f"check_btn_{q_index}", disabled=st.session_state.quiz_submitted):
        user_choice_letter = selected_answer.split(')')[0] if selected_answer else None
        correct_answer_letter = q_data['correct_answer']

        is_correct = (user_choice_letter == correct_answer_letter)
        
        st.session_state.quiz_results[q_index] = is_correct
        st.session_state.quiz_submitted = True
        
        if is_correct:
            st.success(L.get("correct_answer", "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰"))
        else:
            st.error(L.get("incorrect_answer", "ì˜¤ë‹µì…ë‹ˆë‹¤.ğŸ˜"))
        
        st.markdown(f"**{L.get('correct_is', 'ì •ë‹µ')}: {correct_answer_letter}**")
        st.info(f"**{L.get('explanation', 'í•´ì„¤')}:** {q_data['explanation']}")

    if st.session_state.quiz_submitted:
        if q_index < num_questions - 1:
            if col2.button(L.get("next_question", "ë‹¤ìŒ ë¬¸í•­"), key=f"next_btn_{q_index}"):
                st.session_state.current_question += 1
                st.session_state.quiz_submitted = False
                st.rerun()
        else:
            total_correct = st.session_state.quiz_results.count(True)
            total_questions = len(st.session_state.quiz_results)
            st.success(f"**{L.get('quiz_complete', 'í€´ì¦ˆ ì™„ë£Œ!')}** {L.get('score', 'ì ìˆ˜')}: {total_correct}/{total_questions}")
            if st.button(L.get("retake_quiz", "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°"), key="retake"):
                st.session_state.current_question = 0
                st.session_state.quiz_results = [None] * num_questions
                st.session_state.quiz_submitted = False
                st.rerun()

# ================================
# 3. ë‹¤êµ­ì–´ ì§€ì› ë”•ì…”ë„ˆë¦¬ (Language Dictionary)
# ================================
LANG = {
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        "file_uploader": "í•™ìŠµ ìë£Œ ì—…ë¡œë“œ (PDF, TXT, HTML)",
        "button_start_analysis": "ìë£Œ ë¶„ì„ ì‹œì‘ (RAG Indexing)",
        "rag_tab": "RAG ì§€ì‹ ì±—ë´‡",
        "content_tab": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "lstm_tab": "LSTM ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "simulator_tab": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°", 
        "rag_header": "RAG ì§€ì‹ ì±—ë´‡ (ë¬¸ì„œ ê¸°ë°˜ Q&A)",
        "rag_desc": "ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤ã€‚",
        "rag_input_placeholder": "í•™ìŠµ ìë£Œì— ëŒ€í•´ ì§ˆë¬¸í•´ ë³´ì„¸ìš”",
        "llm_error_key": "âš ï¸ ê²½ê³ : GEMINI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”ã€‚",
        "llm_error_init": "LLM ì´ˆê¸°í™” ì˜¤ë¥˜: API í‚¤ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”ã€‚",
        "content_header": "ë§ì¶¤í˜• í•™ìŠµ ì½˜í…ì¸  ìƒì„±",
        "content_desc": "í•™ìŠµ ì£¼ì œì™€ ë‚œì´ë„ì— ë§ì¶° ì½˜í…ì¸  ìƒì„±",
        "topic_label": "í•™ìŠµ ì£¼ì œ",
        "level_label": "ë‚œì´ë„",
        "content_type_label": "ì½˜í…ì¸  í˜•ì‹",
        "level_options": ["ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"],
        "content_options": ["í•µì‹¬ ìš”ì•½ ë…¸íŠ¸", "ê°ê´€ì‹ í€´ì¦ˆ 10ë¬¸í•­", "ì‹¤ìŠµ ì˜ˆì œ ì•„ì´ë””ì–´"],
        "button_generate": "ì½˜í…ì¸  ìƒì„±",
        "warning_topic": "í•™ìŠµ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”ã€‚",
        "lstm_header": "LSTM ê¸°ë°˜ í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ",
        "lstm_desc": "ê°€ìƒì˜ ê³¼ê±° í€´ì¦ˆ ì ìˆ˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¯¸ë˜ ì„±ì·¨ë„ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤ã€‚",
        "lstm_disabled_error": "The LSTM feature is temporarily disabled due to build environment issues. Please use the 'Custom Content Generation' feature first.",
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "embed_success": "ì´ {count}ê°œ ì²­í¬ë¡œ í•™ìŠµ DB êµ¬ì¶• ì™„ë£Œ!",
        "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "warning_no_files": "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚",
        "warning_rag_not_ready": "RAGê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•˜ì„¸ìš”ã€‚",
        "quiz_fail_structure": "í€´ì¦ˆ ë°ì´í„° êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ã€‚",
        "select_answer": "ì •ë‹µì„ ì„ íƒí•˜ì„¸ìš”",
        "check_answer": "ì •ë‹µ í™•ì¸",
        "next_question": "ë‹¤ìŒ ë¬¸í•­",
        "correct_answer": "ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰",
        "incorrect_answer": "ì˜¤ë‹µì…ë‹ˆë‹¤. ğŸ˜",
        "correct_is": "ì •ë‹µ",
        "explanation": "í•´ì„¤",
        "quiz_complete": "í€´ì¦ˆ ì™„ë£Œ!",
        "score": "ì ìˆ˜",
        "retake_quiz": "í€´ì¦ˆ ë‹¤ì‹œ í’€ê¸°",
        "quiz_error_llm": "í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: LLMì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ì‘ë‹µ ì›ë³¸ì„ í™•ì¸í•˜ì„¸ìš”ã€‚",
        "quiz_original_response": "LLM ì›ë³¸ ì‘ë‹µ",
        "firestore_loading": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ RAG ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...",
        "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚", 
        "db_save_complete": "(DB ì €ì¥ ì™„ë£Œ)", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "data_analysis_progress": "ìë£Œ ë¶„ì„ ë° í•™ìŠµ DB êµ¬ì¶• ì¤‘...", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "response_generating": "ë‹µë³€ ìƒì„± ì¤‘...", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "lstm_result_header": "í•™ìŠµ ì„±ì·¨ë„ ì˜ˆì¸¡ ê²°ê³¼",
        "lstm_score_metric": "í˜„ì¬ ì˜ˆì¸¡ ì„±ì·¨ë„",
        "lstm_score_info": "ë‹¤ìŒ í€´ì¦ˆ ì˜ˆìƒ ì ìˆ˜ëŠ” ì•½ **{predicted_score:.1f}ì **ì…ë‹ˆë‹¤. í•™ìŠµ ì„±ê³¼ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ê°œì„ í•˜ì„¸ìš”!",
        "lstm_rerun_button": "ìƒˆë¡œìš´ ê°€ìƒ ë°ì´í„°ë¡œ ì˜ˆì¸¡",
        
        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AI ê³ ê° ì‘ëŒ€ ì‹œë®¬ë ˆì´í„°",
        "simulator_desc": "ê¹Œë‹¤ë¡œìš´ ê³ ê° ë¬¸ì˜ì— ëŒ€í•´ AIì˜ ì‘ëŒ€ ì´ˆì•ˆ ë° ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "customer_query_label": "ê³ ê° ë¬¸ì˜ ë‚´ìš© (ë§í¬ í¬í•¨ ê°€ëŠ¥)",
        "customer_type_label": "ê³ ê° ì„±í–¥",
        "customer_type_options": ["ì¼ë°˜ì ì¸ ë¬¸ì˜", "ê¹Œë‹¤ë¡œìš´ ê³ ê°", "ë§¤ìš° ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ê³ ê°"],
        "button_simulate": "ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­",
        "simulation_warning_query": "ê³ ê° ë¬¸ì˜ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”ã€‚",
        "simulation_no_key_warning": "âš ï¸ API Keyê°€ ì—†ëŠ” ê²½ìš°, ì‘ë‹µ ìƒì„±ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (UI êµ¬ì„±ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.)",
        "simulation_advice_ready": "AIì˜ ì‘ëŒ€ ì¡°ì–¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!",
        "simulation_advice_header": "AIì˜ ì‘ëŒ€ ê°€ì´ë“œë¼ì¸",
        "simulation_draft_header": "ì¶”ì²œ ì‘ëŒ€ ì´ˆì•ˆ",
        "button_listen_audio": "ìŒì„±ìœ¼ë¡œ ë“£ê¸°",
        "tts_status_ready": "ìŒì„±ìœ¼ë¡œ ë“£ê¸° ì¤€ë¹„ë¨",
        "tts_status_generating": "ì˜¤ë””ì˜¤ ìƒì„± ì¤‘...",
        "tts_status_success": "âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ!",
        "tts_status_fail": "âŒ TTS ìƒì„± ì‹¤íŒ¨ (ë°ì´í„° ì—†ìŒ)",
        "tts_status_error": "âŒ TTS ì˜¤ë¥˜ ë°œìƒ",
        "history_expander_title": "ğŸ“ ì´ì „ ìƒë‹´ ì´ë ¥ ë¡œë“œ (ìµœê·¼ 10ê°œ)", 
        "initial_query_sample": "í”„ë‘ìŠ¤ íŒŒë¦¬ì— ë„ì°©í–ˆëŠ”ë°, í´ë£©ì—ì„œ êµ¬ë§¤í•œ eSIMì´ í™œì„±í™”ê°€ ì•ˆ ë©ë‹ˆë‹¤. ì—°ê²°ì´ ì•ˆ ë¼ì„œ ë„ˆë¬´ ê³¤ë€í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?", 
        
        # â­ ëŒ€í™”í˜•/ì¢…ë£Œ ë©”ì‹œì§€
        "button_mic_input": "ìŒì„± ì…ë ¥",
        "prompt_customer_end": "ê³ ê°ë‹˜ì˜ ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ì—†ì–´, ì´ ìƒë‹´ ì±„íŒ…ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤ã€‚",
        "prompt_survey": "ê³ ê° ë¬¸ì˜ ì„¼í„°ì— ì—°ë½ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©°, ì¶”ê°€ë¡œ ì €í¬ ì‘ëŒ€ ì†”ë£¨ì…˜ì— ëŒ€í•œ ì„¤ë¬¸ ì¡°ì‚¬ì— ì‘í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ ë¬¸ì˜ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹­ì‹œì˜¤ã€‚",
        "customer_closing_confirm": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?",
        "customer_positive_response": "ì¢‹ì€ ë§ì”€/ì¹œì ˆí•œ ìƒë‹´ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤ã€‚",
        "button_end_chat": "ì‘ëŒ€ ì¢…ë£Œ (ì„¤ë¬¸ ì¡°ì‚¬ ìš”ì²­)",
        "agent_response_header": "âœï¸ ì—ì´ì „íŠ¸ ì‘ë‹µ",
        "agent_response_placeholder": "ê³ ê°ì—ê²Œ ì‘ë‹µí•˜ì„¸ìš” (ê³ ê°ì˜ í•„ìˆ˜ ì •ë³´ë¥¼ ìš”ì²­/í™•ì¸í•˜ê±°ë‚˜, ë¬¸ì œ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”)",
        "send_response_button": "ì‘ë‹µ ì „ì†¡",
        "request_rebuttal_button": "ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­",
        "new_simulation_button": "ìƒˆ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘",
        "history_selectbox_label": "ë¡œë“œí•  ì´ë ¥ì„ ì„ íƒí•˜ì„¸ìš”:",
        "history_load_button": "ì„ íƒëœ ì´ë ¥ ë¡œë“œ",
        "delete_history_button": "âŒ ëª¨ë“  ì´ë ¥ ì‚­ì œ", 
        "delete_confirm_message": "ì •ë§ë¡œ ëª¨ë“  ìƒë‹´ ì´ë ¥ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ã€‚", 
        "delete_confirm_yes": "ì˜ˆ, ì‚­ì œí•©ë‹ˆë‹¤", 
        "delete_confirm_no": "ì•„ë‹ˆì˜¤, ìœ ì§€í•©ë‹ˆë‹¤", 
        "delete_success": "âœ… ëª¨ë“  ìƒë‹´ ì´ë ¥ ì‚­ì œ ì™„ë£Œ!",
        "deleting_history_progress": "ì´ë ¥ ì‚­ì œ ì¤‘...", 
        "search_history_label": "ì´ë ¥ í‚¤ì›Œë“œ ê²€ìƒ‰", 
        "date_range_label": "ë‚ ì§œ ë²”ìœ„ í•„í„°", 
        "no_history_found": "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤ã€‚" 
    },
    "en": {
        "title": "Personalized AI Study Coach",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "file_uploader": "Upload Study Materials (PDF, TXT, HTML)",
        "button_start_analysis": "Start Analysis (RAG Indexing)",
        "rag_tab": "RAG Knowledge Chatbot",
        "content_tab": "Custom Content Generation",
        "lstm_tab": "LSTM Achievement Prediction",
        "simulator_tab": "AI Customer Response Simulator", 
        "rag_header": "RAG Knowledge Chatbot (Document Q&A)",
        "rag_desc": "Answers questions based on the uploaded documents.",
        "rag_input_placeholder": "Ask a question about your study materials",
        "llm_error_key": "âš ï¸ Warning: GEMINI API Key is not set. Please set 'GEMINI_API_KEY' in Streamlit Secrets.",
        "llm_error_init": "LLM initialization error: Please check your API key.",
        "content_header": "Custom Learning Content Generation",
        "content_desc": "Generate content tailored to your topic and difficulty.",
        "topic_label": "Learning Topic",
        "level_label": "Difficulty",
        "content_type_label": "Content Type",
        "level_options": ["Beginner", "Intermediate", "Advanced"],
        "content_options": ["Key Summary Note", "10 Multiple-Choice Questions", "Practical Example Idea"],
        "button_generate": "Generate Content",
        "warning_topic": "Please enter a learning topic.",
        "lstm_header": "LSTM Based Achievement Prediction",
        "lstm_desc": "Trains an LSTM model on hypothetical past quiz scores to predict future achievement.",
        "lstm_disabled_error": "The LSTM feature is temporarily disabled due to build environment issues. Please use the 'Custom Content Generation' feature first.",
        "lang_select": "Select Language",
        "embed_success": "Learning DB built with {count} chunks!",
        "embed_fail": "Embedding failed: Free tier quota exceeded or network issue.",
        "warning_no_files": "Please upload study materials first.",
        "warning_rag_not_ready": "RAG is not ready. Upload materials and click Start Analysis.",
        "quiz_fail_structure": "Quiz data structure is incorrect.",
        "select_answer": "Select answer",
        "check_answer": "Confirm answer",
        "next_question": "Next Question",
        "correct_answer": "Correct! ğŸ‰",
        "incorrect_answer": "Incorrect. ğŸ˜",
        "correct_is": "Correct answer",
        "explanation": "Explanation",
        "quiz_complete": "Quiz completed!",
        "score": "Score",
        "retake_quiz": "Retake Quiz",
        "quiz_error_llm": "Quiz generation failed: LLM did not return a valid JSON format. Check the original LLM response.",
        "quiz_original_response": "Original LLM Response",
        "firestore_loading": "Loading RAG index from database...",
        "firestore_no_index": "Could not find existing RAG index in database. Please upload files and create a new one.", 
        "db_save_complete": "(DB Save Complete)", 
        "data_analysis_progress": "Analyzing materials and building learning DB...", 
        "response_generating": "Generating response...", 
        "lstm_result_header": "Prediction Results",
        "lstm_score_metric": "Current Predicted Achievement",
        "lstm_score_info": "Your next estimated quiz score is **{predicted_score:.1f}**. Maintain or improve your learning progress!",
        "lstm_rerun_button": "Predict with New Hypothetical Data",

        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AI Customer Response Simulator",
        "simulator_desc": "Provides AI-generated response drafts and guidelines for handling challenging customer inquiries.",
        "customer_query_label": "Customer Query (Link optional)",
        "customer_type_label": "Customer Sentiment",
        "customer_type_options": ["General Inquiry", "Challenging Customer", "Highly Dissatisfied Customer"],
        "button_simulate": "Request Response Advice",
        "simulation_warning_query": "Please enter the customer's query.",
        "simulation_no_key_warning": "âš ï¸ API Key is missing. Response generation cannot proceed. (UI configuration is complete.)",
        "simulation_advice_ready": "AI's response advice is ready!",
        "simulation_advice_header": "AI Response Guidelines",
        "simulation_draft_header": "Recommended Response Draft",
        "button_listen_audio": "Listen to Audio",
        "tts_status_ready": "Ready to listen",
        "tts_status_generating": "Generating audio...",
        "tts_status_success": "âœ… Audio playback complete!",
        "tts_status_fail": "âŒ TTS generation failed (No data)",
        "tts_status_error": "âŒ TTS API error occurred",
        "history_expander_title": "ğŸ“ Load Previous Simulation History (Last 10)", 
        "initial_query_sample": "I arrived in Paris, France, but the eSIM I bought from Klook won't activate. I'm really struggling to get connected. What should I do?", 

        # â­ ëŒ€í™”í˜•/ì¢…ë£Œ ë©”ì‹œì§€
        "button_mic_input": "Voice Input",
        "prompt_customer_end": "As there are no further inquiries, we will now end this chat session.",
        "prompt_survey": "Thank you for contacting our Customer Support Center. We would be grateful if you could participate in a short survey about our service solution. Please feel free to contact us anytime if you have any additional questions.",
        "customer_closing_confirm": "Is there anything else we can assist you with today?",
        "customer_positive_response": "Thank you for your kind understanding/friendly advice.",
        "button_end_chat": "End Chat (Request Survey)",
        "agent_response_header": "âœï¸ Agent Response",
        "agent_response_placeholder": "Respond to the customer (Request/confirm essential information or provide solution steps)",
        "send_response_button": "Send Response",
        "request_rebuttal_button": "Request Customer's Next Reaction",
        "new_simulation_button": "Start New Simulation",
        "history_selectbox_label": "Select history to load:",
        "history_load_button": "Load Selected History",
        "delete_history_button": "âŒ Delete All History", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_confirm_message": "Are you sure you want to delete ALL simulation history? This action cannot be undone.", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_confirm_yes": "Yes, Delete", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_confirm_no": "No, Keep", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_success": "âœ… Successfully deleted!", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "deleting_history_progress": "Deleting history...", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "search_history_label": "Search History by Keyword", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "date_range_label": "Date Range Filter", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "no_history_found": "No history found matching the criteria." # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
    },
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "file_uploader": "å­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT, HTML)",
        "button_start_analysis": "è³‡æ–™åˆ†æé–‹å§‹ (RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ)",
        "rag_tab": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        "content_tab": "ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "lstm_tab": "LSTMé”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "simulator_tab": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼", 
        "rag_header": "RAGçŸ¥è­˜ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆQ&A)",
        "rag_desc": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚",
        "rag_input_placeholder": "å­¦ç¿’è³‡æ–™ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
        "llm_error_key": "âš ï¸ è­¦å‘Š: GEMINI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secretsã«'GEMINI_API_KEY'ã‚’è¨­ç½®ã—ã¦ãã ã•ã„ã€‚",
        "llm_error_init": "LLMåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼šAPIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "content_header": "ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "content_desc": "å­¦ç¿’ãƒ†ãƒ¼ãƒã¨é›£æ˜“åº¦ã«åˆã‚ã›ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚",
        "topic_label": "å­¦ç¿’ãƒ†ãƒ¼ãƒ",
        "level_label": "é›£æ˜“åº¦",
        "content_type_label": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼",
        "level_options": ["åˆç´š", "ä¸­ç´š", "ä¸Šç´š"],
        "content_options": ["æ ¸å¿ƒè¦ç´„ãƒãƒ¼ãƒˆ", "é¸æŠå¼ã‚¯ã‚¤ã‚º10å•", "å®Ÿè·µä¾‹ã®ã‚¢ã‚¤ãƒ‡ã‚¢"],
        "button_generate": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ",
        "warning_topic": "å­¦ç¿’ãƒ†ãƒ¼ãƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "lstm_header": "LSTMãƒ™ãƒ¼ã‚¹é”æˆåº¦äºˆæ¸¬ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
        "lstm_desc": "ä»®æƒ³ã®éå»ã‚¯ã‚¤ã‚ºã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€LSTMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦å°†æ¥ã®é”æˆåº¦ã‚’äºˆæ¸¬ã—è¡¨ç¤ºã—ã¾ã™ã€‚",
        "lstm_disabled_error": "ç¾åœ¨ã€ãƒ“ãƒ«ãƒ‰ç’°å¢ƒã®å•é¡Œã«ã‚ˆã‚ŠLSTMæ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã€Œã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã€æ©Ÿèƒ½ã‚’å…ˆã«ã”åˆ©ç”¨ãã ã•ã„ã€‚ã€",
        "lang_select": "è¨€èªé¸æŠ",
        "embed_success": "å…¨{count}ãƒãƒ£ãƒ³ã‚¯ã§å­¦ç¿’DBæ§‹ç¯‰å®Œäº†!",
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—: ãƒ•ãƒªãƒ¼ãƒ†ã‚£ã‚¢ã®ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã€‚",
        "warning_no_files": "ã¾ãšå­¦ç¿’è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
        "warning_rag_not_ready": "RAGãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€åˆ†æé–‹å§‹ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚",
        "quiz_fail_structure": "ã‚¯ã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚",
        "select_answer": "æ­£è§£ã‚’é¸æŠã—ã¦ãã ã•ã„",
        "check_answer": "æ­£è§£ã‚’ç¢ºèª",
        "next_question": "æ¬¡ã®è³ªå•",
        "correct_answer": "æ­£è§£ã§ã™! ğŸ‰",
        "incorrect_answer": "ä¸æ­£è§£ã§ã™ã€‚ğŸ˜",
        "correct_is": "æ­£è§£",
        "explanation": "è§£èª¬",
        "quiz_complete": "ã‚¯ã‚¤ã‚ºå®Œäº†!",
        "score": "ã‚¹ã‚³ã‚¢",
        "retake_quiz": "ã‚¯ã‚¤ã‚ºã‚’å†æŒ‘æˆ¦",
        "quiz_error_llm": "LLMãŒæ­£ã—ã„JSONã®å½¢å¼ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸã®ã§ã€ã‚¯ã‚¤ã‚ºã®ç”ŸæˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚",
        "quiz_original_response": "LLM åŸæœ¬å¿œç­”",
        "firestore_loading": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...",
        "firestore_no_index": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ—¢å­˜ã®RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°ã—ãä½œæˆã—ã¦ãã ã•ã„ã€‚", 
        "db_save_complete": "(DBä¿å­˜å®Œäº†)", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "data_analysis_progress": "è³‡æ–™åˆ†æãŠã‚ˆã³å­¦ç¿’DBæ§‹ç¯‰ä¸­...", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "response_generating": "å¿œç­”ç”Ÿæˆä¸­...", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "lstm_result_header": "é”æˆåº¦äºˆæ¸¬çµæœ",
        "lstm_score_metric": "ç¾åœ¨ã®äºˆæ¸¬é”æˆåº¦",
        "lstm_score_info": "æ¬¡ã®ã‚¯ã‚¤ã‚ºã®æ¨å®šã‚¹ã‚³ã‚¢ã¯ç´„ **{predicted_score:.1f}ç‚¹**ã§ã™ã€‚å­¦ç¿’ã®æˆæœã‚’ç¶­æŒã¾ãŸã¯å‘ä¸Šã•ã›ã¦ãã ã•ã„ï¼",
        "lstm_rerun_button": "æ–°ã—ã„ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬",

        # â­ ì‹œë®¬ë ˆì´í„° ê´€ë ¨ í…ìŠ¤íŠ¸
        "simulator_header": "AIé¡§å®¢å¯¾å¿œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼",
        "simulator_desc": "é›£ã—ã„é¡§å®¢ã®å•ã„åˆã‚ã›ã«å¯¾ã—ã¦ã€AIã«ã‚ˆã‚‹å¯¾å¿œæ¡ˆã¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã—ã¾ã™ã€‚",
        "customer_query_label": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ï¼ˆãƒªãƒ³ã‚¯ä»»æ„ï¼‰",
        "customer_type_label": "é¡§å®¢ã®å‚¾å‘",
        "customer_type_options": ["ä¸€èˆ¬çš„ãªå•ã„åˆã‚ã›", "æ‰‹ã”ã‚ã„é¡§å®¢", "éå¸¸ã«ä¸æº€ãªé¡§å®¢"],
        "button_simulate": "å¯¾å¿œã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¦æ±‚",
        "simulation_warning_query": "é¡§å®¢ã®å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
        "simulation_no_key_warning": "âš ï¸ APIã‚­ãƒ¼ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚å¿œç­”ã®ç”Ÿæˆã¯ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚ï¼ˆUIè¨­å®šã¯å®Œäº†ã—ã¦ã„ã¾ã™ã€‚ï¼‰",
        "simulation_advice_ready": "AIã®å¯¾å¿œã‚¢ãƒ‰ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼",
        "simulation_advice_header": "AIå¯¾å¿œã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³",
        "simulation_draft_header": "æ¨å¥¨ã•ã‚Œã‚‹å¯¾å¿œè‰æ¡ˆ",
        "button_listen_audio": "éŸ³å£°ã§èã",
        "tts_status_ready": "éŸ³å£°å†ç”Ÿã®æº–å‚™ãŒã§ãã¾ã—ãŸ",
        "tts_status_generating": "éŸ³å£°ç”Ÿæˆä¸­...",
        "tts_status_success": "âœ… éŸ³å£°å†ç”Ÿå®Œäº†!",
        "tts_status_fail": "âŒ TTSç”Ÿæˆå¤±æ•—ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰",
        "tts_status_error": "âŒ TTS APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
        "history_expander_title": "ğŸ“ ä»¥å‰ã®å¯¾å¿œå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰ (æœ€æ–° 10ä»¶)", 
        "initial_query_sample": "ãƒ•ãƒ©ãƒ³ã‚¹ã®ãƒ‘ãƒªã«åˆ°ç€ã—ã¾ã—ãŸãŒã€Klookã§è³¼å…¥ã—ãŸeSIMãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚æ¥ç¶šã§ããªãã¦å›°ã£ã¦ã„ã¾ã™ã€‚ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ", 

        # â­ ëŒ€í™”í˜•/ì¢…ë£Œ ë©”ì‹œì§€
        "button_mic_input": "éŸ³å£°å…¥åŠ›",
        "prompt_customer_end": "ãŠå®¢æ§˜ã‹ã‚‰ã®è¿½åŠ ã®ãŠå•ã„åˆã‚ã›ãŒãªã„ãŸã‚ã€æœ¬ãƒãƒ£ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆã‚’çµ‚äº†ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
        "prompt_survey": "ãŠå•ã„åˆã‚ã›ã„ãŸã ãã€èª ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚å¼Šç¤¾ã®å¯¾å¿œã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹ç°¡å˜ãªã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«ã”å”åŠ›ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚è¿½åŠ ã®ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ã„ã¤ã§ã‚‚ã”é€£çµ¡ãã ã•ã„ã€‚",
        "customer_closing_confirm": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ",
        "customer_positive_response": "è¦ªåˆ‡ãªã”å¯¾å¿œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚",
        "button_end_chat": "å¯¾å¿œçµ‚äº† (ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã‚’ä¾é ¼)",
        "agent_response_header": "âœï¸ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”",
        "agent_response_placeholder": "é¡§å®¢ã«è¿”ä¿¡ (å¿…é ˆæƒ…å ±ã®è¦æ±‚/ç¢ºèªã€ã¾ãŸã¯è§£æ±ºç­–ã®æç¤º)",
        "send_response_button": "å¿œç­”é€ä¿¡",
        "request_rebuttal_button": "é¡§å®¢ã®æ¬¡ã®åå¿œã‚’è¦æ±‚", 
        "new_simulation_button": "æ–°ã—ã„ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹",
        "history_selectbox_label": "å±¥æ­´ã‚’é¸æŠã—ã¦ãƒ­ãƒ¼ãƒ‰:",
        "history_load_button": "é¸æŠã•ã‚ŒãŸå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰",
        "delete_history_button": "âŒ å…¨å±¥æ­´ã‚’å‰Šé™¤", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_confirm_message": "æœ¬å½“ã«ã™ã¹ã¦ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å±¥æ­´ã‚’å‰Šé™¤ã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿã“ã®æ“ä½œã¯å…ƒã«æˆ»ã›ã¾ã›ã‚“ã€‚", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_confirm_yes": "ã¯ã„ã€å‰Šé™¤ã—ã¾ã™", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_confirm_no": "ã„ã„ãˆã€ç¶­æŒã—ã¾ã™", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "delete_success": "âœ… å‰Šé™¤ãŒå®Œäº†ã•ã‚Œã¾ã—ãŸ!", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "deleting_history_progress": "å±¥æ­´å‰Šé™¤ä¸­...", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "search_history_label": "å±¥æ­´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "date_range_label": "æ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
        "no_history_found": "æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚" # â­ ë‹¤êµ­ì–´ í‚¤ ì¶”ê°€
    }
}


# ================================
# 4. Streamlit í•µì‹¬ Config ì„¤ì • ë° Session State ì´ˆê¸°í™” (CRITICAL ZONE)
# ================================

if 'language' not in st.session_state: st.session_state.language = 'ko'
if 'uploaded_files_state' not in st.session_state: st.session_state.uploaded_files_state = None
if 'is_llm_ready' not in st.session_state: st.session_state.is_llm_ready = False
if 'is_rag_ready' not in st.session_state: st.session_state.is_rag_ready = False
if 'firestore_db' not in st.session_state: st.session_state.firestore_db = None
if 'llm_init_error_msg' not in st.session_state: st.session_state.llm_init_error_msg = None
if 'firestore_load_success' not in st.session_state: st.session_state.firestore_load_success = False

# â­ ì‹œë®¬ë ˆì´í„° ì „ìš© ìƒíƒœ ì´ˆê¸°í™” ì¶”ê°€
if "simulator_memory" not in st.session_state:
    # ConversationChainì—ì„œ ì‚¬ìš©í•  ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
if "simulator_messages" not in st.session_state:
    st.session_state.simulator_messages = []
if "initial_advice_provided" not in st.session_state:
    st.session_state.initial_advice_provided = False
if "simulator_chain" not in st.session_state:
    st.session_state.simulator_chain = None
# â­ ì‹œë®¬ë ˆì´í„° ì§„í–‰ ìƒíƒœ ì¶”ê°€
if "is_chat_ended" not in st.session_state:
    st.session_state.is_chat_ended = False

# â­ ì´ë ¥ ì‚­ì œ í™•ì¸ ëª¨ë‹¬ ìƒíƒœ
if "show_delete_confirm" not in st.session_state:
    st.session_state.show_delete_confirm = False

# ì–¸ì–´ ì„¤ì • ë¡œë“œ (UI ì¶œë ¥ ì „ í•„ìˆ˜)
L = LANG[st.session_state.language] 
API_KEY = os.environ.get("GEMINI_API_KEY")

# =======================================================
# 5. Streamlit UI í˜ì´ì§€ ì„¤ì • (ìŠ¤í¬ë¦½íŠ¸ ë‚´ ì²« ë²ˆì§¸ ST ëª…ë ¹)
# =======================================================
st.set_page_config(page_title=L["title"], layout="wide")

# =======================================================
# 6. ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë° LLM/DB ë¡œì§ (í˜ì´ì§€ ì„¤ì • í›„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰)
# =======================================================

if 'llm' not in st.session_state: 
    llm_init_error = None # â­ safety initialization
    if not API_KEY:
        llm_init_error = L["llm_error_key"]
    else:
        try:
            # LLM ë° Embeddings ì´ˆê¸°í™”
            st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
            st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
            st.session_state.is_llm_ready = True
            
            # Admin SDK í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” 
            sa_info, error_message = _get_admin_credentials()
            
            if error_message:
                llm_init_error = f"{L['llm_error_init']} (DB Auth Error: {error_message})" 
            elif sa_info:
                db = initialize_firestore_admin() 
                st.session_state.firestore_db = db
                
                if not db:
                    llm_init_error = f"{L['llm_error_init']} (DB Client Error: Firebase Admin Init Failed)" 
                else:
                    # DB ë¡œë”© ë¡œì§ (RAG ì±—ë´‡ìš©)
                    if 'conversation_chain' not in st.session_state:
                        # DB ë¡œë”© ì‹œë„
                        loaded_index = load_index_from_firestore(st.session_state.firestore_db, st.session_state.embeddings)
                        
                        if loaded_index:
                            st.session_state.conversation_chain = get_rag_chain(loaded_index)
                            st.session_state.is_rag_ready = True
                            st.session_state.firestore_load_success = True
                        else:
                            st.session_state.firestore_load_success = False
            
            # â­ ì‹œë®¬ë ˆì´í„° ì²´ì¸ ì´ˆê¸°í™”
            SIMULATOR_PROMPT = PromptTemplate(
                template="The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n\n{chat_history}\nHuman: {input}\nAI:",
                input_variables=["input", "chat_history"]
            )
            
            st.session_state.simulator_chain = ConversationChain(
                llm=st.session_state.llm,
                memory=st.session_state.simulator_memory,
                prompt=SIMULATOR_PROMPT,
                input_key="input", 
            )


        except Exception as e:
            # LLM ì´ˆê¸°í™” ì˜¤ë¥˜ ì²˜ë¦¬ 
            llm_init_error = f"{L['llm_error_init']} {e}" 
            st.session_state.is_llm_ready = False
    
    if llm_init_error:
        st.session_state.is_llm_ready = False
        st.session_state.llm_init_error_msg = llm_init_error 

# ë‚˜ë¨¸ì§€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "memory" not in st.session_state:
    # RAG ì²´ì¸ìš© ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

if "embedding_cache" not in st.session_state:
    st.session_state.embedding_cache = {}

# â­ LSTM ë¦¬ëŸ° íŠ¸ë¦¬ê±° ì´ˆê¸°í™” (ì¶”ê°€)
if "lstm_rerun_trigger" not in st.session_state:
    st.session_state.lstm_rerun_trigger = time.time()

# ================================
# 7. ì´ˆê¸°í™” ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ ë° DB ìƒíƒœ ì•Œë¦¼
# ================================

if st.session_state.llm_init_error_msg:
    st.error(st.session_state.llm_init_error_msg)
    
if st.session_state.get('firestore_db'):
    if st.session_state.get('firestore_load_success', False):
        st.success("âœ… RAG ì¸ë±ìŠ¤ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    elif not st.session_state.get('is_rag_ready', False):
        st.info(L["firestore_no_index"]) # â­ ë‹¤êµ­ì–´ ì ìš©


# ================================
# 8. Streamlit UI ì‹œì‘
# ================================

with st.sidebar:
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=['ko', 'en', 'ja'],
        index=['ko', 'en', 'ja'].index(st.session_state.language),
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
    )
    
    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        st.rerun() 
    
    L = LANG[st.session_state.language] 
    
    st.title(L["sidebar_title"])
    
    st.markdown("---")
    
    uploaded_files_widget = st.file_uploader(
        L["file_uploader"],
        type=["pdf","txt","html"],
        accept_multiple_files=True
    )
    
    if uploaded_files_widget:
        st.session_state.uploaded_files_state = uploaded_files_widget
    elif 'uploaded_files_state' not in st.session_state:
        st.session_state.uploaded_files_state = None
    
    files_to_process = st.session_state.uploaded_files_state if st.session_state.uploaded_files_state else []
    
    if files_to_process and st.session_state.is_llm_ready:
        if st.button(L["button_start_analysis"], key="start_analysis"):
            with st.spinner(L["data_analysis_progress"]): # â­ ë‹¤êµ­ì–´ ì ìš©
                text_chunks = get_document_chunks(files_to_process)
                vector_store = get_vector_store(text_chunks)
                
                if vector_store:
                    # RAG ì¸ë±ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ë©´ Firestoreì— ì €ì¥ ì‹œë„
                    db = st.session_state.firestore_db
                    save_success = False
                    if db:
                        save_success = save_index_to_firestore(db, vector_store)
                    
                    if save_success:
                        st.success(L["embed_success"].format(count=len(text_chunks)) + " " + L["db_save_complete"]) # â­ ë‹¤êµ­ì–´ ì ìš©
                    else:
                        st.success(L["embed_success"].format(count=len(text_chunks)) + " (DB ì €ì¥ ì‹¤íŒ¨)")

                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                else:
                    st.session_state.is_rag_ready = False
                    st.error(L["embed_fail"])

    else:
        st.session_state.is_rag_ready = False
        st.warning(L.get("warning_no_files", "ë¨¼ì € í•™ìŠµ ìë£Œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")) 

    st.markdown("---")
    # â­ ìƒˆë¡œìš´ íƒ­(ì‹œë®¬ë ˆì´í„°)ì„ í¬í•¨í•˜ì—¬ ë¼ë””ì˜¤ ë²„íŠ¼ ì—…ë°ì´íŠ¸
    feature_selection = st.radio(
        L["content_tab"], 
        [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["simulator_tab"]]
    )

st.title(L["title"])

# ================================
# 9. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================

if feature_selection == L["simulator_tab"]: 
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])
    
    # â­ OpenAI Client ì´ˆê¸°í™” ì‹œë„
    openai_key = st.secrets.get("OPENAI_API_KEY")
    openai_client = None
    if openai_key:
        try:
            openai_client = OpenAI(api_key=openai_key)
        except Exception as e:
            # ì¸ì¦ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê²½ê³ ë§Œ í‘œì‹œí•˜ê³  ì•±ì€ ê³„ì† ì‹¤í–‰
            st.warning(L.get("whisper_auth_error", f"OpenAI Client ì´ˆê¸°í™” ì˜¤ë¥˜: {e}"))
            openai_client = None
    
    # 1. TTS ìœ í‹¸ë¦¬í‹° (ìƒíƒœ í‘œì‹œê¸° ë° JS í•¨ìˆ˜)ë¥¼ í˜ì´ì§€ ìƒë‹¨ì— ì‚½ì…
    st.markdown(f'<div id="tts_status" style="padding: 5px; text-align: center; border-radius: 5px; background-color: #f0f0f0; margin-bottom: 10px;">{L["tts_status_ready"]}</div>', unsafe_allow_html=True)
    
    # TTS JS ìœ í‹¸ë¦¬í‹°ë¥¼ í˜ì´ì§€ ë¡œë“œ ì‹œ ë‹¨ í•œ ë²ˆë§Œ ì‚½ì… (TTS í•¨ìˆ˜ê°€ ê¸€ë¡œë²Œë¡œ ì •ì˜ë˜ë„ë¡)
    if "tts_js_loaded" not in st.session_state:
         synthesize_and_play_audio(st.session_state.language) 
         st.session_state.tts_js_loaded = True

    # 1.5 ì´ë ¥ ì‚­ì œ ë²„íŠ¼ ë° ëª¨ë‹¬
    db = st.session_state.get('firestore_db')
    col_delete, _ = st.columns([1, 4])
    with col_delete:
        if st.button(L["delete_history_button"], key="trigger_delete_history"):
            st.session_state.show_delete_confirm = True

    if st.session_state.show_delete_confirm:
        with st.container(border=True):
            st.warning(L["delete_confirm_message"])
            col_yes, col_no = st.columns(2)
            if col_yes.button(L["delete_confirm_yes"], key="confirm_delete_yes", type="primary"):
                with st.spinner(L["deleting_history_progress"]): # â­ ì‚­ì œ ë¡œë”© ìŠ¤í”¼ë„ˆ ì¶”ê°€
                    delete_all_history(db)
            if col_no.button(L["delete_confirm_no"], key="confirm_delete_no"):
                st.session_state.show_delete_confirm = False
                st.rerun()

    # â­ Firebase ìƒë‹´ ì´ë ¥ ë¡œë“œ ë° ì„ íƒ ì„¹ì…˜
    if db:
        with st.expander(L["history_expander_title"]): # â­ ë‹¤êµ­ì–´ ì ìš©
            
            # 2. ì´ë ¥ ê²€ìƒ‰ ë° í•„í„°ë§ ê¸°ëŠ¥ ì¶”ê°€
            histories = load_simulation_histories(db)
            
            # 2-1. ê²€ìƒ‰ í•„í„°
            search_query = st.text_input(L["search_history_label"], key="history_search", value="")
            
            # 2-2. ë‚ ì§œ í•„í„° (st.date_inputì€ ë¸Œë¼ìš°ì € ë¡œì¼€ì¼ì„ ë”°ë¦„)
            today = datetime.now().date()
            default_start_date = today - timedelta(days=7)
            
            date_range_input = st.date_input(
                L["date_range_label"], 
                value=[default_start_date, today],
                key="history_date_range"
            )

            # í•„í„°ë§ ë¡œì§
            filtered_histories = []
            if histories:
                if isinstance(date_range_input, list) and len(date_range_input) == 2:
                    start_date = min(date_range_input)
                    end_date = max(date_range_input) + timedelta(days=1)
                else:
                    start_date = datetime.min.date()
                    end_date = datetime.max.date()
                    
                for h in histories:
                    # í…ìŠ¤íŠ¸ ê²€ìƒ‰ (initial_query, customer_type)
                    search_match = True
                    if search_query:
                        query_lower = search_query.lower()
                        # initial_queryì™€ customer_typeì„ ëª¨ë‘ ê²€ìƒ‰ ëŒ€ìƒìœ¼ë¡œ í¬í•¨
                        searchable_text = h['initial_query'].lower() + " " + h['customer_type'].lower()
                        if query_lower not in searchable_text:
                            search_match = False
                    
                    # ë‚ ì§œ í•„í„°
                    date_match = True
                    if h.get('timestamp'):
                        h_date = h['timestamp'].date()
                        if not (start_date <= h_date < end_date):
                            date_match = False
                            
                    if search_match and date_match:
                        filtered_histories.append(h)
            
            
            if filtered_histories:
                history_options = {
                    f"[{h['timestamp'].strftime('%m-%d %H:%M')}] {h['customer_type']} - {h['initial_query'][:30]}...": h
                    for h in filtered_histories
                }
                
                selected_key = st.selectbox(
                    L["history_selectbox_label"], 
                    options=list(history_options.keys())
                )
                
                if st.button(L["history_load_button"]): 
                    selected_history = history_options[selected_key]
                    
                    # ìƒíƒœ ë³µì›
                    st.session_state.customer_query_text_area = selected_history['initial_query']
                    st.session_state.initial_advice_provided = True
                    st.session_state.simulator_messages = selected_history['messages']
                    st.session_state.is_chat_ended = selected_history.get('is_chat_ended', False)
                    
                    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë° ë©”ì‹œì§€ ì¬êµ¬ì„± (LangChain í˜¸í™˜ì„±ì„ ìœ„í•´)
                    st.session_state.simulator_memory.clear()
                    
                    # LLM ë©”ëª¨ë¦¬ì— ëŒ€í™” ì´ë ¥ ì¬ì£¼ì… (ì‹¤ì œ LLMì´ ì‘ëŒ€í•  ìˆ˜ ìˆë„ë¡)
                    for i, msg in enumerate(selected_history['messages']):
                         if msg['role'] == 'customer':
                             st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
                         elif msg['role'] in ['supervisor', 'customer_rebuttal', 'customer_end', 'system_end']:
                             st.session_state.simulator_memory.chat_memory.add_ai_message(msg['content'])
                         elif msg['role'] == 'agent_response':
                             st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
                    
                    st.rerun()
            else:
                 st.info(L.get("no_history_found", "ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."))


    # â­ LLM ì´ˆê¸°í™”ê°€ ë˜ì–´ìˆì§€ ì•Šì•„ë„ (API Keyê°€ ì—†ì–´ë„) UIê°€ ì‘ë™í•´ì•¼ í•¨
    if st.session_state.is_llm_ready or not API_KEY:
        if st.session_state.is_chat_ended:
            st.success(L["prompt_customer_end"] + " " + L["prompt_survey"])
            
            if st.button(L["new_simulation_button"], key="new_simulation"): 
                 st.session_state.is_chat_ended = False
                 st.session_state.initial_advice_provided = False
                 st.session_state.simulator_messages = []
                 st.session_state.simulator_memory.clear()
                 st.rerun()
            st.stop()
        
        # 1. ê³ ê° ë¬¸ì˜ ì…ë ¥ í•„ë“œ
        if 'customer_query_text_area' not in st.session_state:
            st.session_state.customer_query_text_area = ""

        # â­ ì´ˆê¸°ê°’ ì„¤ì •: Klook eSIM ì´ìŠˆ ë° í•„ìˆ˜ ì •ë³´ ìš”ì²­ ìœ ë„ (ë‹¤êµ­ì–´ ì ìš©)
        initial_query_placeholder = L["initial_query_sample"]
        
        customer_query = st.text_area(
            L["customer_query_label"],
            key="customer_query_text_area",
            height=150,
            placeholder=initial_query_placeholder, # â­ ë‹¤êµ­ì–´ ì ìš©
            disabled=st.session_state.initial_advice_provided
        )

        # 2. ê³ ê° ì„±í–¥ ì„ íƒ
        # â­ ê¸°ë³¸ê°’ì„ 'ê¹Œë‹¤ë¡œìš´ ê³ ê°'ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë‚œì´ë„ ë¶€ì—¬
        customer_type_options_list = L["customer_type_options"]
        default_index = 1 if len(customer_type_options_list) > 1 else 0 # 'ê¹Œë‹¤ë¡œìš´ ê³ ê°' ë˜ëŠ” 'Challenging Customer'
        
        customer_type_display = st.selectbox(
            L["customer_type_label"],
            customer_type_options_list,
            index=default_index,
            disabled=st.session_state.initial_advice_provided
        )
        
        # ì„ íƒëœ ì–¸ì–´ í‚¤
        current_lang_key = st.session_state.language 

        # 4. 'ì‘ëŒ€ ì¡°ì–¸ ìš”ì²­' ë²„íŠ¼: ì´ˆê¸° ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ë° ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        if st.button(L["button_simulate"], key="start_simulation", disabled=st.session_state.initial_advice_provided):
            if not customer_query:
                st.warning(L["simulation_warning_query"])
                st.stop()
            
            # ì´ˆê¸°í™”
            st.session_state.simulator_memory.clear()
            st.session_state.simulator_messages = []
            st.session_state.is_chat_ended = False
            
            st.session_state.simulator_messages.append({"role": "customer", "content": customer_query})
            st.session_state.simulator_memory.chat_memory.add_user_message(customer_query)
            
            # â­ LLM í”„ë¡¬í”„íŠ¸ì— ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬ ë° í˜‘ì¡°ì ì¸ ê³ ê° ì—­í• ì„ ë¶€ì—¬
            initial_prompt = f"""
            You are an AI Customer Support Supervisor. Your task is to provide expert guidance to a customer support agent.
            The customer sentiment is: {customer_type_display}.
            The customer's initial inquiry is: "{customer_query}"
            
            Based on this, provide:
            1. Crucial advice on the tone and strategy for dealing with this specific sentiment. 
            2. A concise and compassionate recommended response draft.
            
            The recommended draft MUST be strictly in {LANG[current_lang_key]['lang_select']}.
            
            **CRITICAL RULE FOR DRAFT CONTENT:**
            - **Core Topic Filtering:** Analyze the customer's inquiry to determine its main subject. 
            - **Draft Content:** The draft MUST address the core topic directly. The draft MUST ONLY request *general* information needed for ALL inquiries (like booking ID, contact info). 
            - **Technical Info:** The draft MUST NOT include specific technical troubleshooting requests (Smartphone model, Location, Last Step of troubleshooting) **UNLESS** the core inquiry is explicitly about connection/activation failures (like "won't activate" or "no connection"). If the inquiry is about eSIM activation failure, use a standard troubleshooting request template.
            
            When the Agent subsequently asks for information, **Roleplay as the Customer** who is frustrated but **MUST BE HIGHLY COOPERATIVE** and provide the requested details piece by piece (not all at once). The customer MUST NOT argue or ask why the information is needed.
            """
            
            if not API_KEY:
                # API Keyê°€ ì—†ì„ ê²½ìš° ëª¨ì˜(Mock) ë°ì´í„° ì‚¬ìš©
                mock_data = get_mock_response_data(current_lang_key, customer_type_display)
                ai_advice_text = f"### {mock_data['advice_header']}\n\n{mock_data['advice']}\n\n### {mock_data['draft_header']}\n\n{mock_data['draft']}"
                
                # ë©”ëª¨ë¦¬ì— ì¶”ê°€
                st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
                st.session_state.simulator_memory.chat_memory.add_ai_message(ai_advice_text)

                st.session_state.initial_advice_provided = True
                save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
                
                st.rerun() 
            
            if API_KEY:
                # API Keyê°€ ìˆì„ ê²½ìš° LLM í˜¸ì¶œ
                with st.spinner(L["response_generating"]): # â­ ë‹¤êµ­ì–´ ì ìš©
                    try:
                        if st.session_state.simulator_chain is None:
                            st.error(L['llm_error_init'] + " (ì‹œë®¬ë ˆì´í„° ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨)")
                            st.stop()

                        response_text = st.session_state.simulator_chain.predict(input=initial_prompt)
                        ai_advice_text = response_text
                        
                        st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
                        st.session_state.initial_advice_provided = True
                        
                        save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
                        
                        st.rerun() 
                    except Exception as e:
                        st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # 5. ì‹œë®¬ë ˆì´ì…˜ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
        st.markdown("---")
        
        # ì±„íŒ… ê¸°ë¡ ë Œë”ë§
        for message in st.session_state.simulator_messages:
            if message["role"] == "customer":
                with st.chat_message("user", avatar="ğŸ™‹"):
                    st.markdown(message["content"])
            elif message["role"] == "supervisor":
                with st.chat_message("assistant", avatar="ğŸ¤–"):
                    st.markdown(message["content"])
                    render_tts_button(message["content"], st.session_state.language) 
            elif message["role"] == "agent_response":
                 with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
                    st.markdown(message["content"])
            elif message["role"] == "customer_rebuttal":
                 with st.chat_message("assistant", avatar="ğŸ˜ "):
                    st.markdown(message["content"])
            elif message["role"] == "customer_end":
                 with st.chat_message("assistant", avatar="ğŸ˜Š"):
                    st.markdown(message["content"])
            elif message["role"] == "system_end":
                 with st.chat_message("assistant", avatar="âœ¨"):
                    st.markdown(message["content"])

        # 6. ëŒ€í™”í˜• ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰ (ì¶”ê°€ ì±„íŒ…)
        if st.session_state.initial_advice_provided and not st.session_state.is_chat_ended:
            
            last_role = st.session_state.simulator_messages[-1]['role'] if st.session_state.simulator_messages else None
            
            # 1. ì—ì´ì „íŠ¸(ì‚¬ìš©ì)ê°€ ì‘ë‹µí•  ì°¨ë¡€ (ì´ˆê¸° ë¬¸ì˜ í›„, ì¬ë°˜ë°• í›„, ë§¤ë„ˆ ì§ˆë¬¸ í›„)
            if last_role in ["customer_rebuttal", "customer_end", "supervisor", "customer"]:
                
                st.markdown(f"### {L['agent_response_header']}") 
                
                # --- â­ Whisper ì˜¤ë””ì˜¤ ì „ì‚¬ ê¸°ëŠ¥ ì¶”ê°€ ---
                col_audio, col_text_area = st.columns([1, 2])
                
                # OpenAI Client ì´ˆê¸°í™” (Secretsì—ì„œ í‚¤ë¥¼ ë¡œë“œ)
                openai_key = st.secrets.get("OPENAI_API_KEY")
                openai_client = None
                if openai_key:
                    try:
                        openai_client = OpenAI(api_key=openai_key)
                    except Exception:
                        openai_client = None

                # ì „ì‚¬ ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™”
                if 'transcribed_text' not in st.session_state:
                    st.session_state.transcribed_text = ""
                
                # ì˜¤ë””ì˜¤ íŒŒì¼ ë…¹ìŒ/ì—…ë¡œë“œ (st.audio_input)
                with col_audio:
                    # â­ st.audio_input ìœ„ì ¯ ì‚¬ìš©
                    audio_file = st.audio_input(L["button_mic_input"], key="simulator_audio_input_file")
                
                if audio_file:
                    if openai_client is None:
                        st.error(L.get("whisper_client_error", "OpenAI Keyê°€ ì—†ì–´ ìŒì„± ì¸ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                    else:
                        with st.spinner(L.get("whisper_processing", "ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")):
                            try:
                                # ì „ì‚¬ í•¨ìˆ˜ í˜¸ì¶œ
                                transcribed_text = transcribe_audio_with_whisper(audio_file, openai_client, current_lang_key)
                                
                                if transcribed_text.startswith("âŒ"):
                                    st.error(transcribed_text)
                                    st.session_state.transcribed_text = ""
                                else:
                                    st.session_state.transcribed_text = transcribed_text
                                    st.success(L.get("whisper_success", "âœ… ìŒì„± ì „ì‚¬ ì™„ë£Œ! í…ìŠ¤íŠ¸ ì°½ì„ í™•ì¸í•˜ì„¸ìš”."))
                                
                                # st.audio_inputì€ íŒŒì¼ ê°ì²´ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, rerunì„ í†µí•´ í…ìŠ¤íŠ¸ ì˜ì—­ì— ê°’ì„ ë°˜ì˜í•©ë‹ˆë‹¤.
                                st.rerun() 
                                
                            except Exception as e:
                                st.error(f"ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                st.session_state.transcribed_text = ""


                # st.text_areaëŠ” ì „ì‚¬ ê²°ê³¼ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                agent_response = col_text_area.text_area(
                    L["agent_response_placeholder"], 
                    value=st.session_state.transcribed_text,
                    key="agent_response_area_text",
                    height=150
                )
                
                # --- Enter í‚¤ ì „ì†¡ ë¡œì§ ---
                js_code_for_enter = f"""
                <script>
                // st.text_areaì˜ í‚¤ê°€ 'agent_response_area_text'ì¸ ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                const textarea = document.querySelector('textarea[key="agent_response_area_text"]');
                const button = document.querySelector('button[key="send_agent_response"]');
                
                if (textarea && button) {{
                    textarea.addEventListener('keydown', function(event) {{
                        // Shift + Enter ë˜ëŠ” Ctrl + EnterëŠ” ì¤„ë°”ê¿ˆ
                        if (event.key === 'Enter' && (event.shiftKey || event.ctrlKey)) {{
                            // ê¸°ë³¸ ë™ì‘(ì¤„ë°”ê¿ˆ) í—ˆìš©
                        }} 
                        // Enterë§Œ ëˆŒë €ì„ ë•Œ ì „ì†¡
                        else if (event.key === 'Enter') {{
                            event.preventDefault(); // ê¸°ë³¸ Enter ë™ì‘(ì¤„ë°”ê¿ˆ) ë°©ì§€
                            button.click();
                        }}
                    }});
                }}
                </script>
                """
                
                # Streamlitì— JavaScript ì‚½ì…
                st.components.v1.html(js_code_for_enter, height=0, width=0)
                
                if st.button(L["send_response_button"], key="send_agent_response"): 
                    if agent_response.strip():
                        # ì „ì†¡ í›„ ì „ì‚¬ ê²°ê³¼ ìƒíƒœ ì´ˆê¸°í™”
                        st.session_state.transcribed_text = ""
                        
                        st.session_state.simulator_messages.append(
                            {"role": "agent_response", "content": agent_response}
                        )
                        st.session_state.simulator_memory.chat_memory.add_user_message(agent_response)
                        # DB ì €ì¥ ë° ë¦¬ëŸ°
                        save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                        st.rerun()
                    else:
                        st.warning(L.get("empty_response_warning", "ì‘ë‹µ ë‚´ìš©ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."))
            
            # 2. ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­ (LLM í˜¸ì¶œ) ë˜ëŠ” ì¢…ë£Œ ë²„íŠ¼ í‘œì‹œ
            # ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ í›„, ê³ ê° ë°˜ì‘ ìš”ì²­ ë²„íŠ¼ ë˜ëŠ” ì¢…ë£Œ ë²„íŠ¼ í‘œì‹œ
            if last_role == "agent_response":
                
                col_end, col_next = st.columns([1, 2])
                
                # A) ì‘ëŒ€ ì¢…ë£Œ ë²„íŠ¼ (ë§¤ë„ˆ ì¢…ë£Œ)
                if col_end.button(L["button_end_chat"], key="end_chat"): 
                    closing_messages = get_closing_messages(current_lang_key)
                    
                    # ë§¤ë„ˆ ì§ˆë¬¸ê³¼ ìµœì¢… ì¢…ë£Œ ì¸ì‚¬ëŠ” AIì˜ ì‘ë‹µìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                    st.session_state.simulator_messages.append({"role": "supervisor", "content": closing_messages["additional_query"]}) # ë§¤ë„ˆ ì§ˆë¬¸
                    st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["additional_query"])

                    st.session_state.simulator_messages.append({"role": "system_end", "content": closing_messages["chat_closing"]}) # ìµœì¢… ì¢…ë£Œ ì¸ì‚¬
                    st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["chat_closing"])
                    
                    st.session_state.is_chat_ended = True
                    
                    # â­ Firebase ì´ë ¥ ì—…ë°ì´íŠ¸: ìµœì¢… ì¢…ë£Œ ìƒíƒœ ì €ì¥
                    save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                    
                    st.rerun()

                # B) ê³ ê°ì˜ ë‹¤ìŒ ë°˜ì‘ ìš”ì²­ (LLM í˜¸ì¶œ)
                if col_next.button(L["request_rebuttal_button"], key="request_rebuttal"): # â­ LLM í˜¸ì¶œ í…ìŠ¤íŠ¸ ì œê±°
                    if not API_KEY:
                        st.warning("API Keyê°€ ì—†ê¸° ë•Œë¬¸ì— LLMì„ í†µí•œ ëŒ€í™”í˜• ì‹œë®¬ë ˆì´ì…˜ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        st.stop()
                    
                    if st.session_state.simulator_chain is None:
                        st.error(L['llm_error_init'] + " (ì‹œë®¬ë ˆì´í„° ì²´ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨)")
                        st.stop()
                        
                    # â­ í•µì‹¬ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ (ê°•ë ¥í•˜ê²Œ í˜‘ì¡°ì ì¸ ê³ ê°ì„ ìœ ë„)
                    next_reaction_prompt = f"""
                    Analyze the entire chat history. Roleplay as the customer ({customer_type_display}). 
                    Based on the agent's last message, generate ONE of the following responses in the customer's voice:
                    1. Provide **ONE** of the crucial, previously requested details (Model, Location, or Last Step) in a cooperative tone.
                    2. A short, positive closing remark (e.g., "{L['customer_positive_response']}").
                    
                    Crucially, the customer MUST be highly cooperative. If the agent asks for information, the customer MUST provide the detail requested (Model, Location, or Last Step) without arguing or asking why. The purpose of this simulation is for the agent (human user) to practice systematically collecting information and troubleshooting.
                    
                    The response MUST be strictly in {LANG[current_lang_key]['lang_select']}.
                    """
                    
                    with st.spinner(L["response_generating"]): # â­ ë‹¤êµ­ì–´ ì ìš©
                        try:
                            customer_reaction = st.session_state.simulator_chain.predict(input=next_reaction_prompt)
                        except Exception as e:
                            st.error(f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            st.stop()
                        
                        # ê¸ì •ì  ì¢…ë£Œ í‚¤ì›Œë“œ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                        positive_keywords = ["ê°ì‚¬", "thank you", "ã‚ã‚ŠãŒã¨ã†", L['customer_positive_response'].lower().split('/')[-1].strip()]
                        is_positive_close = any(keyword in customer_reaction.lower() for keyword in positive_keywords)
                        
                        if is_positive_close:
                            role = "customer_end" # ê¸ì •ì  ì¢…ë£Œ
                            st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                            st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)

                            # ê¸ì • ì¢…ë£Œ í›„ ì—ì´ì „íŠ¸ì—ê²Œ ë§¤ë„ˆ ì§ˆë¬¸ ìš”ì²­
                            st.session_state.simulator_messages.append({"role": "supervisor", "content": L["customer_closing_confirm"]})
                            st.session_state.simulator_memory.chat_memory.add_ai_message(L["customer_closing_confirm"])
                        else:
                            role = "customer_rebuttal" # ì¬ë°˜ë°•, ì¶”ê°€ ì§ˆë¬¸, ë˜ëŠ” ì •ë³´ ì œê³µ
                            st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                            st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)
                             
                        # DB ì €ì¥ ë° ë¦¬ëŸ°
                        save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                        st.rerun()

    else:
        # LLM ì´ˆê¸°í™” ìì²´ì— ë¬¸ì œê°€ ìˆì„ ê²½ìš°ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ (ë‹¤êµ­ì–´)
        st.error(L["llm_error_init"])

elif feature_selection == L["rag_tab"]:
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    if st.session_state.get('is_rag_ready', False) and st.session_state.get('conversation_chain'):
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner(L["response_generating"]): # â­ ë‹¤êµ­ì–´ ì ìš©
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer','ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' if st.session_state.language == 'ko' else 'Could not generate response.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e:
                        st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}")
                        st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ" if st.session_state.language == 'ko' else "An error occurred"})
    else:
        st.warning(L["warning_rag_not_ready"])

elif feature_selection == L["content_tab"]:
    st.header(L["content_header"])
    st.markdown(L["content_desc"])

    if st.session_state.is_llm_ready:
        topic = st.text_input(L["topic_label"])
        
        level_map = dict(zip(L["level_options"], ["Beginner", "Intermediate", "Advanced"]))
        content_map = dict(zip(L["content_options"], ["summary", "quiz", "example"]))
        
        level_display = st.selectbox(L["level_label"], L["level_options"])
        content_type_display = st.selectbox(L["content_type_label"], L["content_options"])

        level = level_map[level_display]
        content_type = content_map[content_type_display]

        if st.button(L["button_generate"]):
            if topic:
                target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]
                
                if content_type == 'quiz':
                    # 10ë¬¸í•­ìœ¼ë¡œ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸
                    full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate exactly 10 multiple-choice questions about the topic in {target_lang}.
Your entire response MUST be a valid JSON object wrapped in ```json tags.
The JSON must have a single key named 'quiz_questions', which is an array of objects.
Each question object must contain: 'question' (string), 'options' (array of objects with 'option' (A,B,C,D) and 'text' (string)), 'correct_answer' (A,B,C, or D), and 'explanation' (string).

Topic: {topic}"""
                else:
                    display_type_text = L["content_options"][L["content_options"].index(content_type_display)]
                    full_prompt = f"""You are a professional AI coach at the {level} level.
Please generate clear and educational content in the requested {display_type_text} format based on the topic.
The response MUST be strictly in {target_lang}.

Topic: {topic}
Requested Format: {display_type_text}"""
                
                
                with st.spinner(f"Generating {content_type_display} for {topic}..."):
                    
                    quiz_data_raw = None
                    try:
                        response = st.session_state.llm.invoke(full_prompt)
                        quiz_data_raw = response.content
                        st.session_state.quiz_data_raw = quiz_data_raw # ë””ë²„ê¹…ì„ ìœ„í•´ raw data ì €ì¥
                        
                        if content_type == 'quiz':
                            quiz_data = clean_and_load_json(quiz_data_raw)
                            
                            if quiz_data and 'quiz_questions' in quiz_data:
                                st.session_state.quiz_data = quiz_data
                                st.session_state.current_question = 0
                                st.session_state.quiz_submitted = False
                                st.session_state.quiz_results = [None] * len(quiz_data.get('quiz_questions',[]))
                                
                                st.success(f"**{topic}** - **{content_type_display}** Result:")
                            else:
                                st.error(L["quiz_error_llm"])
                                st.markdown(f"**{L['quiz_original_response']}**:")
                                st.code(quiz_data_raw, language="json")

                        else: # ì¼ë°˜ ì½˜í…ì¸  (ìš”ì•½, ì˜ˆì œ)
                            st.success(f"**{topic}** - **{content_type_display}** Result:")
                            st.markdown(response.content)

                    except Exception as e:
                        st.error(f"Content Generation Error: {e}")
                        if quiz_data_raw:
                            st.markdown(f"**{L['quiz_original_response']}**: {quiz_data_raw}")

            else:
                st.warning(L["warning_topic"])
    else:
        st.error(L["llm_error_init"])
        
    # í€´ì¦ˆ í’€ì´ ë Œë”ë§ì„ ë©”ì¸ ë£¨í”„ì—ì„œ ì¡°ê±´ë¶€ë¡œ ë‹¨ í•œ ë²ˆ í˜¸ì¶œ
    is_quiz_ready = content_type == 'quiz' and 'quiz_data' in st.session_state and st.session_state.quiz_data
    if is_quiz_ready and st.session_state.get('current_question', 0) < len(st.session_state.quiz_data.get('quiz_questions', [])):
        render_interactive_quiz(st.session_state.quiz_data, st.session_state.language)

elif feature_selection == L["lstm_tab"]:
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])
    
    if st.button(L["lstm_rerun_button"], key="rerun_lstm", on_click=force_rerun_lstm):
        pass
    
    try:
        # st.session_state.lstm_rerun_triggerê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ìºì‹œê°€ ë¬´íš¨í™”ë˜ê³  í•¨ìˆ˜ê°€ ì‹¤í–‰ë¨
        model, data = load_or_train_lstm()
        look_back = 5
        # ì˜ˆì¸¡
        X_input = data[-look_back:]
        X_input = np.reshape(X_input, (1, look_back, 1))
        predicted_score = model.predict(X_input, verbose=0)[0][0]

        st.markdown("---")
        st.subheader(L["lstm_result_header"]) # â­ ë‹¤êµ­ì–´ ì ìš©
        col_score, col_chart = st.columns([1, 2])
        
        with col_score:
            st.metric(L["lstm_score_metric"], f"{predicted_score:.1f}{'ì ' if st.session_state.language == 'ko' else ''}") # â­ ë‹¤êµ­ì–´ ì ìš©
            st.info(L["lstm_score_info"].format(predicted_score=predicted_score)) # â­ ë‹¤êµ­ì–´ ì ìš©

        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label='Past Scores', marker='o')
            ax.plot(len(data), predicted_score, label='Predicted Next Score', marker='*', color='red', markersize=10)
            ax.set_title(L["lstm_header"]) # â­ ë‹¤êµ­ì–´ ì ìš©
            ax.set_xlabel(f"Time ({L.get('score', 'Score')} attempts)")
            ax.set_ylabel(f"{L.get('score', 'Score')} (0-100)")
            ax.legend()
            st.pyplot(fig)

    except Exception as e:
        # Streamlit í™˜ê²½ì—ì„œ tensorflow/matplotlib/LSTM ê´€ë ¨ ë¬¸ì œê°€ ë°œìƒí•  ê²½ìš°ì˜ fallback
        st.error("LSTM ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í™˜ê²½ ì¢…ì†ì„± ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜ ë©”ì‹œì§€: %s)" % e)
        st.info("ì´ ê¸°ëŠ¥ì€ LLM ë° RAG ê¸°ëŠ¥ê³¼ëŠ” ë³„ê°œë¡œ, í•™ìŠµ ì„±ê³¼ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ ì œê³µë©ë‹ˆë‹¤.")
